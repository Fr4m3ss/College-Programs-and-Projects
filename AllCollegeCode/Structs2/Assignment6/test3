<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0063)http://www.cs.fsu.edu/~asriniva/courses/cop4530/projs/words.dat -->
<HTML><HEAD>
<META http-equiv=Content-Type content="text/html; charset=windows-1252">
<META content="MSHTML 6.00.2800.1276" name=GENERATOR></HEAD>
<BODY><PRE>* File "words.dat" from the Stanford GraphBase (C) 1993 Stanford University
* A database of English five-letter words
* This file may be freely copied but please do not change it in any way!
* (Checksum parameters 5757,526296596)
aargh
abaca 2
abaci+1
aback*2,2,3
abaft
abase+,,,,3
abash+
abate*,,2,,2
abbey*3,1,3
abbot*3,1
abeam
abend
abets+1
abhor*,,,,19
abide*10,6,3,,34,1
abled
abler*,2
abode+5,4,3,,11
abort*,,,,,7
about*12496,1813,1898,186,846,325,181
above*2298,295,296,24,174,181,31
absit ,,2
abuse*9,16,10,12,13,1,3
abuts+,,1,,,1
abuzz
abyss*5,4,4,,7
ached*27,3,7
aches*10,1
achoo+1
acids*59,7,1
acing*
acked
acmes+
acned
acnes*
acorn*10,,1
acres*159,42,31,,1
acrid*1,1,3,1
acted*83,18,21,2,38,3
actin
actor*44,24,18,2
acute*38,13,21,2,,5
adage*1,3,4
adapt*25,5,5,,,7
added*972,172,221,5,29,67,18
adder*2,,,,4
addle+
adept*4,4,1,2
adieu+14,1,1
adios*,1,2
adlib+
adman+
admen 2
admit*74,37,46,2,4,,2
admix+
adobe*40,2
adopt*16,13,23,9,2,4,3
adore*4,2,4
adorn*6,1,4,1,8
adult*152,25,36,6,,,1
adzes*,,1
aegis+,1
aerie+1
affix*5,1,22
afire*13,1
afoot*14,1,2
afore*7,,1
afoul+2
after*5915,1067,1120,86,704,571,109
again*3892,576,663,36,486,113,78
agape 1,,2,,1
agars
agate*2,,,,5
agave+2
agent*79,44,29,,1
agile*11,2,2,,1
aging*13,4,,,1
agley
aglow+3,,2,,1
agone
agony*23,9,23,,5
agora+
agree*262,51,107,23,15,14,1
agues+
ahead*639,109,92,17,23,35,2
ahhhh+,,1
ahold+
ahoys
aided*25,11,8,,3
aider
aides*6,4,1
ailed*1
aimed*45,24,21,8,1
aimer+
aioli
aired*,2,3
airer
aisle*15,6,5
aitch
ajuga
alack+6
alarm*105,16,28,1,17,1
album*11,6,1
alder*8
aleck+2,2
aleph+4
alert*63,32,16,3,3,2,1
algae*64,7,4
algal 1
algin
alias*,1,1
alibi*3,8,3
alien*17,16,26,2,21
align+4,2,,,,1
alike*463,20,24,8,29
alive*376,57,52,3,132,4
alkyd
alkyl
allay*2,1,2,1
alley*43,8,4,1
allot*3,1,,,5
allow*229,72,75,24,15,33,16
alloy*34,3,14,,1
aloes+1,2,,,5
aloft*25,3,4,,3
aloha+8
alone*825,194,218,10,167,2,5
along*2835,355,250,15,114,10,11
aloof*8,5,6,1,6
aloud*230,13,13,1,51,,1
alpha+9,,1,,,6
altar*24,4,13,,432
alter*32,15,14,2,3,1
altho ,4,2
altos*3,,1,,,1
alums*
alway 2
amahs ,,2
amass*2,2
amaze*2,3
amber*14,3,6
ambit ,,2
amble*1,,2
ameba+23
amend*2,2,4,1,5
amens+
amide ,1
amigo*5,2
amine
amino+38,1
amiss*2,2,2,2,4,2
amity+1,1,1
ammos
among*1308,369,313,40,1021,9,22
amour+
amped
ample*18,16,15,1,4
amply+2,4,4,1,1,,1
amuck+2
amuse*23,3,3
amyls
anded+
anent
angel*45,9,13,,242
anger*108,48,39,,316
angle*462,51,40,,,67,2
angry*402,44,42,4,117
angst+
anile
anima ,,1
anion+,1,4
anise+1,2
ankhs+
ankle*40,8,8
annas 4
annex*14,1,2
annoy*13,2,,,3
annul+1,,1,,4
annum+5,3,17
anode*4,77,5
anole 3
anted+
antes+
antic*2,1
antis
antsy+
anvil*33,1,,,2
aorta*5,3
apace+2,,1,,2
apart*414,57,134,10,57,23,8
apers
aphid*4
aphis 1,,1
apian
aping*,,1
apish
apnea
aport
apple*294,9,7,,8,,3
apply*192,56,67,9,5,42,56
apron*96,7,12
apses+,1
apsos
aptly*5,4,2,2
aquae
aquas+
arbor*7,,1
arced*
ardor*3,3
areal 1
areas*689,236,117,27,,3,4
arena*26,7,5,,2
argon*15,6,1
argot+,1
argue*49,29,27,10,6,,3
arias+,,2
arise*52,28,33,3,76,18,23
arity
armed*103,35,22,8,49,1,1
armor*60,4,,,32
aroma*9,3,5,,2
arose*77,18,20,2,153,2
arras
array*57,11,3,,20,12,16
arrow*193,13,1,,25,13
arses
arson*2,2,1
artsy+
arums
asana
ascot+
ashen*2,2,1
ashes*87,6,5,1,64
aside*182,66,38,11,118,,1
asked*2924,398,448,30,178,12,6
asker
askew*2,1,2
aspen*9,2
aspic*1
assai
assay*,1,3,,1
assed
asses*6,2,,,64
asset*13,5,11,1
aster*3
astir*7,,1
astro 1
atilt
atlas*39,,4,,,1
atoll*6
atoms*332,41,9,,,24
atone*,1,1,,3
atria
attar
attic*45,14,15
audio+9,2
audit+,4
auger*13,,4
aught 4
augur+,,1,,1
aunts*26,4,1
aurae
aural 1,1,3
auras+
auric
autos*8,4
avail+9,4,4,,11
avant+4,1,1
avast 3
avers*
avert*6,1,3,,7
avian+
avoid*246,58,69,14,13,42,6
avows+
await*19,9,8,1,5
awake*134,19,13,1,29
award*30,38,25,,1
aware*172,84,83,7,14,6,2
awash+3,1,,1
aways+2
awful*2,17,19,1,1,4
awing
awoke*54,9,6,,19
axels
axial+2,2
axing+
axiom+10,1,5
axled
axles*7,1,1,,4
axman+
axmen 1
axons+2
ayins+
azine
azoic
azure+8
babel+,,1
babes*1,3,1,,15
backs*99,15,3,,12,3
bacon*99,8,21,,,2
baddy
badge*12,5,1
badly*159,34,44,1,3,3
bagel*
baggy*11,4,3
bahts
bails*,,1
bairn
baits*1
baize+,,1
baked*93,8,5,,16,1
baker*19,10,2,,9
bakes*10,1,,,1
balds
baldy
baled*3
baler+1,,1
bales*21,3,8
balks+,1,,,,1
balky+1
balls*170,17,3,2,,,2
bally 1
balms+
balmy*3,2
balsa*6
banal+1,2
bands*142,11,28,2,10,1
bandy+3
banes*
bangs*10,4
banjo*32,1
banks*219,23,24,5,1
banns+,,2
barbs*16,2
bards+2,2
bared*14,,1,,2
barer*
bares*1,,1
barfs
barfy
barge*35,7,11
baric
barks*19,,1
barky
barms
barmy+
barns*44,4,,,4
baron*13,2,2,,,1
basal+3,,3
based*361,119,140,25,5,73,31
baser*1,1,2
bases*131,23,26,2,54,5
basic*517,170,81,14,,45,42
basil+5
basin*62,5,11,,17
basis*189,184,144,10,1,6,15
basks*
bassi
basso+1,1
baste*5,,1
batch*9,5,11,1
bated+2
bates
bathe*22,4,6,1,29
baths*18,5,14,,9
batik+
baton*11,5,5
batty+
bauds
baulk ,,1
bawdy+1,3
bawls*
bayed*,3
bayou*3
bazar ,,2
beach*308,33,35,7,5
beads*143,3,7,,1,,2
beady+7,1,2,1
beaks*29,,2
beaky ,,1
beams*66,13,2,,14
beamy
beano
beans*199,9,7,,2
beard*89,25,7,,18
bears*195,17,20,1,37,,1
beast*103,7,10,,147
beats*210,4,7
beaus*
beaut+1
beaux ,,1
bebop+,1
bebug
becks
bedew
bedim
beech*19,2,5
beefs*
beefy+,1,1
beeps*5,3
beers*1,1,1
beery
beets*44,2
befit*1,,,,1
befog+
began*2491,312,270,6,226,17,8
begat 1
beget+1,1,2,,3
begin*976,84,86,15,27,69,18
begot 2,,,,5
begun*205,51,53,9,15,14,1
beige*15,1,3
being*2092,709,962,86,303,187,28
belay 2
belch*1,2,,,1
belie+,,1
belle+3,1,2
belli ,,1
bells*303,6,10,,5,1
belly*46,23,6,,22
below*3276,145,150,16,19,123,14
belts*86,8,2,,3,,1
bench*140,28,28,6
bends*73,2,5,,5,11
bents
beret*4,,2,1
bergs ,1
berms
berry*16,4,3,,,1
berth*14,3
beryl+6,,,,6
beset*6,7,4,,8
besot
bests*
betas+
betel+1
beths
bevel*27,2,1
bezel
bhang
bhoys ,,1
bibbs+
bible*1,1,6
biddy+
bided*
bider
bides*
bidet+,,3
biers*
biffs
biffy
biggy
bight
bigly
bigot*
biked*
biker+
bikes*19,,,1
biles
bilge+1,2
bilgy
bilks+
bills*96,45,17,1,,2
billy+15,,,1
bimbo+
binds*4,2,1,2,14
binge+1,1
bingo*1,,1
biome
biped+
bipod 1
birch*33,1,1
birds*1203,47,70,1,109
birth*147,63,63,6,79
bison*18,1
bitch*4,6,1
biter+,1
bites*41,8,3,,5
bitsy 2
bitty 3
blabs+
black*1556,162,164,4,16,52
blade*137,13,17,,4,1
blahs+
blame*79,34,27,12,8
bland*6,3,6,4
blank*255,14,10,,,113,1
blare*5
blash ,,,,,2
blast*74,15,15,,11
blats+
blaze*34,7,7,,2
bleak*19,9,22,3
blear
bleat*8,1,,,1
blebs ,1
bleed*13,2,1
blend*175,9,10,,,3
bless*36,9,11,,13
blest+6,3
blimp*
blind*181,47,25,2,82
blini
blink*16,4,2
blips+,1
bliss*60,4,5
blitz*,2,1,1
bloat*2,8
blobs*2,,2,,,1
block*328,66,45,8,16,8,1
blocs+2,,1
bloke+,1,4
blond*22,11,11
blood*705,119,168,2,466
bloom*63,11,4,,4
blots*,4,1,,2,1
blown*88,9,12,,9
blows*124,8,12,,16
blowy 1
blued+1
bluer*1
blues*35,13,5
bluff*27,8,6,2
blunt*25,9,9,,1,4
blurb*,,1
blurs*1
blurt*
blush*13,1,5,,4
board*536,174,179,26,6,1,5
boars*2,,,,1
boast*18,8,5,,53
boats*391,50,19,5,1
bobby+7,13
bocce
bocci
bocks
boded+,,1
bodes+1,1
bodge
boffo+
boffs
bogey*,5
boggy 2
bogie*
bogus+1,3,4
boils*23,2,1,1,6,1,2
bolas 2
bolls+9
bolos
bolts*37,1,4,2,7
bombe+
bombs*38,35,16
bonds*50,47,7,,23
boned*1
boner*
bones*491,20,17,1,101
bongo+4,1
bongs*1
bonks
bonne ,,2
bonny+8,,2
bonus*21,2,18,,,3,11
boobs+
booby*7,4
booed*4
books*902,95,156,3,17,24,1
booky
booms*9,,,1
boomy
boons+
boors+,1
boost*12,15,5,3
booth*32,5,2,,5
boots*176,19,26,,,,5
booty*5,2,3,,23
booze*,4,1
boozy+
borax+14,1,4
bored*48,14,15,,1,1
borer+2,1
bores*2,2,4
boric+4
borne+25,8,13,,44
boron+6
bosky
bosom*16,8,6,,42
boson
bossa+
bossy*8,,1
bosun+
botch+
bough*20,2,1,,3
boule
bound*170,42,74,5,103,8,35
bouts*6,3,7
bowed*66,7,10,,72
bowel*1,,1
bower+4,1
bowie+1
bowls*53,3,5,,32,1
boxed*11,2,1,,,1
boxer*23,1,5,1
boxes*331,14,24,2,1,245,3
bozos+
brace*20,11,4,,,47
brack ,,1
bract
brads+2
braes
brags*4
braid*20,,4
brain*330,44,4
brake*73,2,7
brand*52,17,22,1,2,3,1
brans*
brant
brash+2,1,3
brass*191,18,33,1,11
brats*,,4
brava
brave*282,20,20,1,13,2
bravo*7
brawl*3,1,1
brawn*2
brays*1
braze+1
bread*515,41,41,2,347
break*516,88,75,6,141,183,1
bream ,,23
breed*47,16,21,1,4
brent ,,1
breve+2,,,,,3
brews*3,,1
briar*2,,3
bribe*6,1,1,,22,,1
brick*132,18,12,,4,,3
bride*53,32,17,,24
brief*160,73,60,2,8,5
brier+6,,1,,3
bries+
brigs+
brims*1
brine*14
bring*1016,158,191,16,702,5,4
brink*13,3,4,4,5
briny+3
brisk*26,7,3
broad*292,83,37,10,38,1
broil*9,2
broke*396,71,70,3,101,1,2
bromo
bronc+1,3
bronx 1
brood*20,9,4,,11
brook*76,1,1,,37
broom*78,2,1,1,5
broth*16,3,8,,4
brown*557,68,68,3,,1
brows*12,5,6
bruin+
bruit
brung 4
brunt*2,1,1
brush*251,42,14,2,1,1
brusk
brute*11,6,4,,,1,1
bubba
bucks*10,5
buddy*9,12
budge*21,3,2
buena
bueno 1,1
buffa 1,,1
buffo
buffs*2,1
buggy*51,6,,,,1
bugle*48,1,,,1
build*857,86,43,12,164,9,2
built*1249,102,150,14,249,26,2
bulbs*40,3,2,,,1
bulge*14,4,9
bulgy+1,,1
bulks*2,1
bulky*25,9,6
bulls*20,2,6,,63
bully*16,4,6
bumph
bumps*24,1,1,,,2
bumpy+17,,1
bunch*105,17,12,,1,8,5
bunco+
bunds
bungs
bunko
bunks*11,17,2
bunny*5
bunts*
buoys*7,1
buret
burgs+
burls+
burly*10,3,5
burns*91,16,2,1,20,1
burnt*43,5,11,,334
burps+
burro*55
burrs*8,1
burry
burst*194,33,28,,2
busby
bused*
buses*61,6,12,9
bushy*25,,1
busks
busts*,3,3
busty+
butch+1
butte*4
butts*3,5,2
butyl+
buxom*,1,1
buyer*30,2,6,1,5
buzzy
bwana
bylaw*
byres ,,1
bytes+,,,,,12
byway+
cabal+
cabby+
cabin*404,21,26,1,,,1
cable*66,5,23
cacao*26,1
cache*5,1,1
cacti+16
caddy+,,2
cadet*6,2,3
cadge+
cadre+,3,2
cafes*9,5,4
caged*7,1,2
cager+
cages*42,2,5
cagey+,2
cairn+
caked*11,2
cakes*133,3,12,,28
calix
calks
calla
calls*216,70,55,3,41,19,5
calms*7,,1
calve+,,1
calyx+5
camel*72,1,5,,1
cameo+3
campo
camps*49,18,8,1,1
campy+
canal*153,,20,1,2
candy*256,16,2
caned*
caner
canes*7,,2
canna
canny*2,2
canoe*164,6,,1
canon*21,3,3
canst+6,,,,4
canto+,3,,,,1
cants
caped+2
caper*3,2,1
capes*6,4
capon+
capos+
carat*5
cards*186,33,19,5,,,26
cared*68,15,9,,4
carer
cares*35,8,3,2,14
caret+1,,,,,4
cargo*106,6,7,,5
carne 3,3
carny+
carob+,1
carol*22,1,3
carom+
caron+,,,,,1
carps*
carpy
carry*940,88,86,12,121,9,16
carte ,1,2
carts*40,5,,,2
carve*30,3,1,1,1
casas
cased*,1
cases*307,148,209,17,11,135,124
casks*4,1,1
caste*16,3,5
casts*23,6,,,16
casus ,,1
catch*679,43,43,1,11,5,1
cater*1,3,5,1
catty+
caulk+
cauls
cause*502,130,119,13,151,47,4
caved+4,1
caves*75,5,3,,9
cavil+
cawed+3
cease*27,15,22,1,51
cedar*31,,2,,55
ceded*1,,,1
ceder
cedes*
ceils
celeb
cello*28
cells*747,81,56,,1,7,6
cento
cents*479,25,4,3,,1,5
chafe*,1,1
chaff*2,,,,16
chain*204,48,38,1,11,,4
chair*421,66,87,1,1,1
chalk*109,3,7
champ*11,,2,,,,1
chant*61,2,1,,2
chaos*16,17,13,5,6,4,1
chaps*12,1,5
chard+
charm*54,26,19,,5
chars ,,,,,1
chart*393,17,4,,,5
chary+,,1
chase*88,10,18,,5,1,1
chasm*2,2,1,,1
chats*1
chaws+
cheap*60,24,38,8,1,1,1
cheat*12,2,8,1,2,,1
check*1024,88,58,3,,32,37
cheek*87,20,22,,9
cheep*6,,1
cheer*88,7,11,,10,1
chefs*1
chert ,,1
chess*21,3,2
chest*231,53,42,,6
chews*8,,,,5
chewy*2
chick*21,2,1
chide*2,2,1,,1
chief*428,89,95,21,187,5
chiff+
child*730,210,245,7,212,3
chile 1
chili*12,6,1
chill*57,14,9
chime*3,,3
chimp*5
china*34,6,5
chine ,,7
chink+1
chino
chins*5,2
chips*42,2,4
chirp*12,,,,1
chits+
chive+1,1
chock+
choir*30,4,8
choke*12,9,5,,2
chomp+,1
choos 1
chops*22,3,3
chord*303,7,6
chore*13,7,1,,,1
chose*171,37,17,3,65,3,3
chows*
chuck*21,11,3
chuff
chugs*4
chump+,1
chums*3,,1
chunk*21,2
churl
churn*21,,,1
chute*17,2
cider*19,2,2,1
cigar*30,10,5
cilia+22,1
cills ,,4
cinch*6,3
circa+,1,3
cirri
cited*7,24,12,2,,1,1
cites*4,10,1
civet*2
civic*12,19,12,5
civil*81,45,95,13
civvy
clack+,,3
clads+
claim*125,97,106,23,8,2,5
clamp*41,,6
clams*50,2
clang*9,1,1
clank*6
clans*,,2,,9
claps*9,2,,,2
clash*11,5,17,,,2
clasp*6,,12,,1
class*1211,166,138,4,1,24,15
clave 3
claws*146,3,1,,3
clays*5,1,3
clean*521,69,70,4,127,3,1
clear*811,219,203,31,43,27,17
cleat*1,1
clefs*3
cleft*12,2,1,,8
clerk*104,34,17,,1
clews
click*31,2,7
cliff*94,9,12,,2
climb*288,12,18,,4,,3
clime+,,1
cling*36,6,2,,2
clink*7
clips*30,2,1
cloak*42,3,10,1,9
clock*330,20,35,2,,2,1
clods*3,4,3,,3
clogs*7,,1
clomp+3
clone+
clops+
close*1288,233,204,18,45,33,17
cloth*427,42,18,,17
clots*4
cloud*314,26,25,5,111
clout*3,1,1,4
clove*3,1
clown*62,3
cloys+
clubs*73,19,31,2,5
cluck*7,2
clued*
clues*125,10,6,1,,6
clump*36,4,1
clung*54,14,9,1,6
clunk+3
coach*147,19,4
coals*42,8,6,,23
coast*594,33,41,5,6
coati+1
coats*120,10,9,,11
cobra*15,3
cocas
cocci*2,,1
cocks*11,,1
cocky*2,3,1
cocoa*57,3,7
cocos*1
codas+
coded*5,1,2,,,4
coder+
codes*10,17,4,,,121
codex+,,12
codon+
coeds*,1
cohos+3
coifs+
coils*32,2,1
coins*161,9,7,,4,,16
coked+
cokes*,,1
colas+
colds*15,2,1
colic*,,,,1
colon*19,2,2,,,22
color*1109,139,,3,9,4,9
colts*30,6,2,,1
comas*,1
combo+9,4
combs*14
comer+2,1
comes*1289,134,133,10,319,103,45
comet*21,1,7
comfy+1
comic*39,8,16
comma*90,2,2,,,27
comps+
conch*3
condo+
coned+
cones*62,2,,,,,1
coney ,1
conga+2,1
conic*3,2,,,,,1
conks+
cooch ,2
cooed*4
cooks*36,8,6,,1
cooky*13,2
cools*77,2
coons+5
coops*1,1
coots+1
coped*2,,1
coper
copes*,1,,,,1
copra*5,2
copse*1
coqui 1
coral*85,1,4,,3
cords*51,2,15,,34
cordy
cored*
corer+
cores*14,3,1
corgi
corks*2,1
corky 1
corms ,,1
corns*1,2
cornu 1
corny*4,1,2
corps*11,25,9,1
coset+
costa ,,1
costs*159,176,82,25,,4,2
cotes
cotta+1
couch*37,12,9,,13
cough*30,7,5,,,2
could*8585,1597,1741,164,258,158,76
count*435,44,36,2,33,16,1
coupe+25,2
coups+,2,2
court*236,122,166,35,137,1
couth
coven
cover*604,87,92,9,77,12,11
coves*3,1
covet*4,1,2,,11
covey*1,,1
cowed*,,1
cower*,,,,4
cowls*
cowry+
coxed+
coxes+
coyer
coyly*3,1,1
coypu ,,8
cozen
crabs*40,2
crack*144,21,10,,1,,1
craft*87,22,46,,5,2
crags*7,2,,,2
cramp*4,2,13,,,1
crams*
crane*36,1,10,,2
crank*36,1,3,,,1
craps+
crash*106,20,10,,7
crass+,2
crate*21,2,3
crave*1,2,1,,2
crawl*67,11,3,,,,3
craws+
craze*2,2,2
crazy*86,32,22,,,1,1
creak*12,1,3
cream*300,20,23
credo+1,7
creed*8,6,4
creek*102,8,5,1
creel+
creep*36,10,7,,2
creme
crepe+7,1,1
crept*89,11,5,,,2
cress*
crest*41,12,4
crews*39,2,7
cribs*2,3
crick+1
cried*1043,30,40,,165,8,1
crier+6
cries*89,6,20,2,23,,2
crime*52,33,44,8,14
crimp+1
crink
crisp*62,8,3,,,1
crits
croak*9,1,,,1
crock*3
crocs 2
croft+,,2
crone*,2,1
crony*
crook*17,3,3
croon*3
crops*536,18,17,,11
cross*498,44,71,1,45,15,6
croup*1
crowd*396,52,62,,84
crown*87,17,51,1,88
crows*43,1,,,6
crude*67,15,13,5,,,1
cruds
cruel*95,15,16,,24
cruet*
cruft
crumb*7,,1
crump
cruse+,,,,3
crush*23,3,1,,3
crust*160,1,4
crypt+,1
cubby+,,1
cubed*2,1,1,,,,1
cuber
cubes*73,4,1,,,1,6
cubic*94,15,9,,,,1
cubit+6,,,,41
cuffs*28,2,5
cuing
cukes
culls*,,1,,,1
culpa 2
cults+3,4,2
cumin+2,1
cunts
cupid*
cuppa
cuppy
curbs*5,3
curds*18,1,,,8
curdy
cured*24,7,9,,9,3
curer
cures*10,3,2,,1
curia ,,1
curie+
curio+,2,1
curls*42,1,5,,,2
curly*44,5,3,,,13,1
curry*13,1,6
curse*13,11,4,,104
curve*174,45,47,,,79,9
curvy+2
cushy+
cusps+1,,,,,2
cuspy+
cuter*
cutie+1
cutup+
cycad ,,1
cycle*125,24,26,,1,66,4
cynic*1,,1
cysts*,3
czars*1
dacha+1
daddy*14,3,2
dados+
daffy+
daily*229,99,82,3,34
dairy*131,19,23
daisy*9,,1
dales*1,1
dally*
dames*,,1
damns*,,2
damps*3
dance*608,84,61,1,15
dandy*17,4
dared*68,14,12,,7
darer
dares*8,3,1,,3
darks*3
darky
darns*1
darts*36,,,,2
dashy
dated*14,19,7,2
dater
dates*109,30,29,,,1
datum*1,1,1
daubs*,,1
daunt*,1,1
davit
dawns*2,1,,,4
dazed*,4,10,,1
dazes*
deads*
deals*37,15,25,3,10,16,3
dealt*21,22,31,3,61,2
deans*,1
dears*3
deary 5
death*518,268,229,12,542,1,3
debar
debit+
debts*36,11,10,1,4
debug+,,,,,1
debut*5,14,12
decaf+
decal*
decay*67,13,24,,3
decks*24,6,3,,3
decor+2,4,3
decoy*5,,,,1
decry+,2,1
deeds*39,8,9,,197
deems+2,,1,,1
deeps+4,1,1,,7
defer*,1,3,3,3,3,2
defog
defun
degas
degum
deice+
deify+
deign*,,1,,1
deism+
deist+
deity*4,2,1,,2
delay*42,21,28,8,31,3
delft
delis+
dells*1
delta*25,,1,,,1,1
delve*,,2,,,1
demit
demon*17,5,9,,28
demos+
demur+1,,1
denim*11
dense*86,9,6,,1
dents*4
depot*19,3,5
depth*167,53,55,1,8,164,3
deque
derby*3,4
desex
desks*37,4,3
deter*5,1,4
deuce+
devil*64,20,27,,32
dewed
dewey+,2
dhows
dials*10,1,2
diary*57,3,15
diazo
diced*1
dicer
dices*
dicey+,,,,,,1
dicks+,1
dicky
dicot 1
dicta 1,,5
dictu ,,1
dicut
diddy
didos 1
didot ,,,,,4
didst 5,,,,283
diems
diest
dieth ,,2
diets*17,3,2
digit*191,1,1,,,25,9
diked*
dikes*37
dildo
dills*
dilly+6
dimer+
dimes*129,2,,,,,3
dimly*32,12,2,,2
dinar+
dined*11,3,10,,2
diner*8,,1
dines*2,1
dingo+,1
dings+,,1
dingy*11,5,1
dinks
dinky+2,,1
dints*
diode+3
dippy+
dipso
direr
dirge*,2,,,2
dirks*
dirts*
dirty*143,35,26,1,,4
disco+
discs*14,7,8
dishy
disks*22,4,,,,,29
ditch*63,9,3,1
ditto+,,,,,2
ditty*2,1,1
divan*,6,1
divas+
dived*62,5,6
diver*87,1,4
dives*15,4,1
divot+1
divvy+
dixit
dizzy*11,4,5
djinn
docks*45,1,3
dodge*11,7,4,1
dodgy
dodos*,,1
doers+7,1,1,,4
doest ,,,,5
doeth
doffs*
doges
doggo
doggy+
dogie*4
dogma*3,4,2
doily*1
doing*927,163,202,13,140,51,2
dolce ,,1
doled*2,1
doles*,,2
dolls*73,12,5
dolly+6,1
dolor+
dolts*1
domed*5,2,1
domes*6,8
donee
donna 1,2
donor*6,5,3
donut+
dooms*,1
doors*225,36,38,3,84
doozy+
doped*1,1,1
doper+
dopes*
dopey+
dorks+
dorky+
dorms+
dosed*1,2
doser
doses*6,13,8
doted*1,,1,,6
doter
dotes*
dotty+,,3
doubt*222,114,211,34,10,,4
dough*55,13,,,6
douse*1
doves*23,1,2,,9
dovey
dowdy*,,2
dowel*6,2,1
dower ,1
downs+7,3,1
downy*13
dowry*3,2,1,,3
dowse+
doxie
doyen+
dozed*16,5,2
dozen*253,52,49,4,,7,2
dozer 1
dozes*2
drabs
draft*57,16,22,5,,3
drags*19,,,1,2
drain*43,18,15,2,4
drake*
drama*84,43,18,4,,1
drams+
drank*106,19,20,,52
drape*2
drawl*7,2,1
drawn*344,70,91,6,31,58,2
draws*43,14,17,3,13,12
drays+
dread*29,9,6,,40,2
dream*232,60,56,1,85
drear+1
dreck
dregs*1,4,,,2
dress*396,67,76,,3,1,1
dribs
dried*218,28,32,1,28
drier*23,3,4
dries*26,1,,,4
drift*71,18,28,3,1,3
drill*136,33,53
drily+
drink*347,82,112,3,362
drips*2,1
drive*543,94,88,6,66,3
droid+
droll*
drone*11,3,3,,,,3
drool*1
droop*12,1
drops*234,18,3,,8,2,3
dross+1,4,,,7
drove*361,62,62,,49
drown*29,3,6,,1
drubs+
drugs*96,28,47,12
druid+
drums*128,15,8,,1
drunk*29,36,18,1,52
dryad
dryer*8,4,1
dryly*8,4,2
duals+
ducal+1,,1
ducat+1
duces
duchy+
ducks*149,4,2
ducky+,,1
ducts*14,6,4
duddy
dudes*2
duels*1,1
duets*3,1
duffs+
dukes*3
dulls*1,1
dully*8,3,4
dulse
dummy*7,3,22,,,2
dumps*8,1,1
dumpy*6
dunce*
dunes*51,8,1
dungs
dungy
dunks*
dunno+5,,,,,,1
duomo
duped*1,1
duper+
dupes*
duple+6
durst+
dusks*
dusky*10,2
dusts*2,1
dusty*79,16,17,1
dutch+
duvet+
dwarf*19,3,3,,1
dweeb
dwell*27,8,6,1,308,,2
dwelt*22,1,5,1,184,1
dyads 2
dyers+,,2
dying*108,33,21,,15,1
dykes+2,,1
dynes+
eager*159,27,21,2,14
eagle*95,4,8,,31
eared 2,1
earls*2
early*1439,352,283,26,82,4,9
earns*20,2,7,,4
earth*2690,116,112,8,1064,1,1
eased*25,8,15,,1
easel*10,5,3
eases*3,,1,,1
easts
eaten*251,12,24,,91
eater*24,,,,3
eaves*11,,1
ebbed+1,,3,1
ebony*4,3,3,,1
echos+2
eclat 1,1
edema ,2
edged*22,7,6,1,,1
edger+3
edges*269,37,20,1,9,66,14
edict*3,,,1,9
edify+,,,,1
edits*1,,1
educe+
eerie*21,2,7
egads+
egged+1,1,2
egger
egret*1
eider*3
eight*651,101,92,9,68,23,7
eject*1,1,,,,3
eking*
eland
elans
elate+
elbow*52,7,11
elder*30,9,20,,18
elect*43,8,11,5,2
elegy*2,1,1
elfin*1,1
elide+
elite+8,12,2,2
elope*
elude*1,,3
elves*14,,1
email
embed*,,1,,,3
ember*5,,1
emcee+,1
emend+,,1
emery*15
emirs+1
emits*3,,,,,1
emote+
empty*384,64,70,4,48,121,3
enact*4,7,2
ended*240,59,51,4,33,19,3
ender
endow*4,2
endue
enema*3,,1
enemy*301,88,36,,171
enjoy*422,44,58,7,35,11
ennui+,,1,1
enrol*,,2,,2
ensue*,2,,,,3
enter*249,78,49,1,190,17,2
entry*162,25,64,1,3,91,8
envoi+
envoy*3,,,,2
epact+
epees+
ephah ,,,,5
ephod ,,,,51
epics*3,2,1
epoch*10,6,14
epoxy+,2
epsom ,1
equal*565,90,80,7,28,126,85
equip*2,1,4,1,3
erase*14,1,1,1,,24,1
erect*36,13,12,,1
erode*4,,,1
erred*2,3,1,,3
error*86,36,30,1,24,203,75
eruct
erupt*11,2
essay*44,19,22
esses+
ester+27
estop+
etext
ether*,1,1
ethic+3,4,1
ethos+,4,3,2
ethyl+17,4
etude*1
evade*4,1,3,1
evens*
event*179,81,80,7,,1,9
every*3398,488,499,49,1089,145,97
evict*1
evils*8,9,10,2,29
evoke*4,6,2
exact*236,27,39,1,8,16,21
exalt*,1,1,,69
exams*6,,4,,,1,1
excel*2,1,1,,3
excon
exeat ,,3
execs+
exert*18,11,6,2,2
exile*17,4,9,1,69
exist*135,59,30,1,20,13,24
exits*14,1,,,4
expat
expel*3,2,1
expos
extol*4,,1,,12
extra*284,50,71,12,,118,13
exude+
exult*1,,,,24
exurb
eyers
eying*5,2
eyrie+2
fable*37,2
faced*130,54,55,10,16,5,6
facer
faces*351,71,43,12,86,4,9
facet*3,2,3
facie ,,5,1
facto 2,7,2,2
facts*519,87,86,5,3,11,18
faddy+
faded*63,18,20,,2
fader
fades*9,,,1,5
faery
fagot+2
fails*24,14,19,3,13,13,12
faint*117,25,28,,44
faire 3
fairs*14,1,1
fairy*71,2,17,,,1
faith*124,106,53,12,281,,1
faked*3,2
faker+,1
fakes*2,,1,,,,1
fakir+1
falls*329,23,30,4,50,3,3
false*225,29,42,5,107,72,12
famed+23,5,2,,1
fames ,1
fancy*103,16,23,,1,3,1
fangs*23,2,1,,3
fanin
fanny+,1
farad+4,,1
farce*4,3,8
fared*3,,5,2,7
fares*3,3,21,5,2
farms*481,16,36
farts
fasts*1,,3,,2
fatal*37,19,24,1,1,6
fated+2,,2
fates*,3,3
fatly
fatso+
fatty+13,7,1
fatwa
fault*120,22,39,3,16,1
fauna*19,1,1
fauns
favor*107,78,1,13,156,1,1
fawns*6,,,,3
fawny
faxed+
faxer
faxes+
fazed+
fazes+
fears*37,47,30,15,32,,1
feast*103,3,16,,166
feats*16,3,2
fecal
feces
feeds*46,12,3,,5,1
feels*182,45,46,6,4,2
feign*1,,,,1
feint+1,2
feist 2
fella+5,6,1
fells*
felon+,1,1
felts*3,,1
femme ,1,5
femur+1,,1
fence*346,30,21,2,3
fends+
fenny
feral
fermi
ferns*38,1
ferny+6
ferry*41,2,2,1
fetal 2
fetch*65,6,12,,12
feted*,2
fetes*,1
fetid+,2
fetor
fetus*4
feuar ,,1
feuds*3,2,1
feued ,,1
fever*150,19,6,,1
fewer*155,33,21,10,3,9,7
fiats+
fiber*57,27
fibre 3,,3
fiche+,1
fichu+,,1
fiefs*
field*919,249,256,3,275,44,2
fiend*10,3,1
fiery*49,7,1,,26
fifes*4
fifth*185,24,32,1,65,9,4
fifty*306,68,98,,129,1
fight*529,98,87,18,133
filar
filch*
filed*23,33,3,2,,2
filer
files*17,13,9,1,,101
filet*1,,1
fills*49,5,7,,15,9
filly*9,9,3
films*31,31,74,,5
filmy*3,1,1
filth*8,2,1,,6
final*460,153,152,18,6,110,64
finch*6
finds*137,59,50,5,42,25,3
fined*3,4,12,,3
finer*35,2,6,,,2
fines*3,2,11
finif
finis
finks+
finny
fiord*7
fired*131,44,23,,2
firer
fires*154,16,9,1,9
firma ,1
firms*28,55,54,6
first*7655,1312,1286,92,484,583,353
firth*4,,4
fishy*4,,1
fists*35,14,5,,1
fisty
fitly*1,,,,2
fiver+
fives*31,2
fixed*196,86,68,6,20,33,35
fixer+
fixes*9,,1
fixit+
fizzy+
fjord*2
flabs
flack+
flags*66,5,5
flail*5,1,3,,1
flair*3,8,2
flake*8,1
flaks
flaky*7,2,1,,,2,2
flame*151,17,19,1,43,1
flams 1
flank*16,2,5,,1
flaps*31,,2
flare*14,3,2
flash*114,21,10,,11,3,2
flask*59,5,4,,9
flats*37,3,26,3,,1
flaws*6,1,4,1
flays*
fleas*13,2,1
fleck*2,1
flees*,1,1,,9
fleet*95,13,6,6,1
flesh*95,52,34,,359
flick*14,2,5
flics
flied*1
flier*8,1,1
flies*220,12,11,,16
fling*17,2,3,,1
flint*32,1,2,,8
flips*9,1,,,,,13
flirt*3,1,1
flits*
float*125,3,2,1,1,1
flock*69,7,15,1,13
floes*5,1
flogs*
flood*127,17,21,2,45
floor*935,157,136,6,47,15,4
flops*3,1
flora*6,1,1
floss*
flour*224,8,12,,59
flout*
flown*55,4,8
flows*218,5,11,2,8,1
flubs+
flues*
fluff*11,1
fluid*55,21,16
fluke+8,1,1,1
fluky
flume+
flung*70,14,19,,3
flunk*1
flush*13,11,4,,,46
flute*80,1,3,,1
flyby+1
flyer*5,3
foals*4,1
foams*,22,,,2
foamy+5,3
focal+11,8,1,,,2
focus*56,40,20,3,,4,2
fogey*
foggy*37,3
foils*1,,3
foist+
folds*51,3,5,,7
folia
folic
folio+,,1,1,,2
folks*153,19,6,,,1
folky ,,2
folly*7,7,14,2,57
fondu ,,1
fonts+,,,,,243,1
foods*518,44,27,,14
fools*17,5,3,1,44
foots ,,1
foray*4,1,1
force*651,200,168,30,54,34,3
fords*,2,,,9
fores ,,1
forge*21,4,1
forgo*1,1,1,1
forks*20,2,4,,5
forky
forma ,2,,,,2
forms*992,128,124,7,8,30,33
forte+2,4,1
forth*412,71,22,2,578,7
forts*32,4,3,,2
forty*153,35,45,,114
forum*4,7,1,1
fossa ,,1
fosse ,,1
fouls*3
found*3362,536,625,27,456,78,59
fount+2
fours*47,2,1,,4
fovea
fowls*10,,1,,1
foxed*,,2
foxes*57,,3,,5
foyer*3,3,6,1
frail*30,8,5,,1
frame*249,74,45,1,29
franc*5,1
frank*10,24,11,2,,1
frats+
fraud*7,7,3,1,4
frays*2
freak*3,4,2,,,1
freed*57,12,6,2,14
freer*4,5,,1,1
frees*3,2,,,,1
fresh*573,79,84,7,28,4,1
frets*3
friar*3
fried*61,6,3
frier
fries*2
frigs
frill*2,,1
frisk*1,,1
frizz+1
frock*2,2,4
frogs*140,1,5,,15
frond*1
front*1438,219,167,13,77,10,6
frosh+
frost*83,3,7,,8
froth*4,1,1
frown*41,1,4,,,1
froze*23,5,6
fruit*456,33,45,1,233
frump+
fryer+1
ftped
fucks ,1
fudge*22,,,1,,,3
fudgy+1
fuels*41,1,1,1
fugal
fugit 1
fugue+18,,1
fulls
fully*141,80,111,6,34,10,8
fumed*4,1,1
fumer
fumes*26,5,6,1
funds*52,95,29,23,6
fungi*91,,1
fungo+
funks
funky+1
funny*312,41,21,1,,13,1
furls*
furor+1,3
furry*31
furze+
fused*9,3,2
fusee
fuses*10,3,2
fussy*10,3,4,,,4,1
fusty 1,1
futon+
fuzed
fuzes
fuzzy*17,7,2,1
gabby+
gable*5,2,2
gaffe+,,1
gaffs
gages 2,2
gaily*33,5,6,1,1
gains*41,19,16,12,8,,1
gaits*
galas*,,1
gales*7,,2
galls*10,1,1
gamba+3
gamed+
gamer
games*349,53,20,4,1,1,4
gamey+
gamic
gamin+
gamma+13,4,2
gamut+3,4,1
ganef
gangs*6,6,3,1
gaols
gaped*8,3,4,,1
gaper
gapes*
gappy
garbs*
garde+3,1,1
gases*276,7,9,1
gasps*8,5,4
gassy+,1
gated+
gates*67,13,24,,148
gator
gaudy*8,7,3
gauge*53,12,15,1
gaunt*20,6,6,1,7
gauss*,2,,,,,2
gauze*12,1,4,,1
gauzy*1,,1
gavel*5
gawks*
gawky*2,1,3
gayer*4
gayly*5
gazed*68,7,10,,3
gazer+,1
gazes*7,1
gears*20,2,3
gecko+3,,,,1
geeks
geese*74,3,36
gelds+
genes*37,1,1
genet
genie*16,1
genii 3,1
genre+7,2,7
gents+3
genus*31,2
geode*
geoid
germs*152,1,1
gesso+
getup+1
ghost*138,10,19,1,3,2
ghoti
ghoul+2,1
giant*316,23,21,3,2,,1
gibed+
giber
gibes+,1
giddy*7,2
gifts*115,11,16,,93
gigas 3,,1
gigue 1
gilds*
gills*58,,1
gilts*
gimel+
gimme+1,1,,,,5
gimps
gimpy
ginny
gipsy ,,2
girds*,,,,3
girls*1107,141,122,3,2
girly+
giros
girth*8,1
girts+
gismo+
gists*
given*1661,377,539,55,589,228,133
giver+5,1,1,,2
gives*650,112,127,15,177,81,196
gizmo+
glade*6,,1
glads
gland*29,9,1
glans
glare*36,7,7
glary
glass*913,96,99,,6,,1
glaze*3,11,4,,1
gleam*28,4,8,,2
glean+1,1,1,,9
glebe
glees ,1
glens*3,,2
glide*28,2,1,,,1
glint*4,2,9
glitz+
gloat*2,,3,,2
globe*278,12,4
globs+,,,,,4
gloms
gloom*28,14,9,,24
glory*93,18,23,,462,,1
gloss*8,1,4,2
glove*42,9,2,1
glows*12,1
glued*17,19,12
gluer
glues*4,,,,1,1
gluey 2
gluon
gluts*
glyph+8
gnarl+
gnash*,,,,9
gnats*9,,,,8
gnaws*3,,,,1
gnome*1,1,1
goads*,,,,3
goals*54,39,21,3,,1,1
goats*99,,1,,55
godly*8,,1,,33
goers
goest 2,,,,1
goeth 4
gofer+
going*2832,398,517,16,197,35,32
golds*1
golem
golfs*
golly+14,2,1
gonad
goner*
gongs+4
gonna+57,16,2
gonzo+
goods*420,57,66,14,56
goody*7,1
gooey*3,1
goofs+
goofy*8,,1
gooks
gooky
goons*
goony
goopy 1
goose*184,2,8,3,,2
goosy
gored+3
gores+,,,,3
gorge+10,1,,,2
gorse 2,,1
goths
gotta+15,5,2
gouda+
gouge*13,1,1,,2
gourd+17,2
gouts
gouty+
gowns*9,2,5
goyim+
grabs*13,3
grace*55,32,43,1,108
grade*175,35,19
grads+,2
graft*5,1,,1,1
grail+
grain*340,27,10,1,127
grams*29,18,1
grand*109,31,41,4,,1,1
grant*31,37,37,4,63,1
grape*18,3,,,1
graph*379,17,5,,,14,24
grapy
grasp*69,17,25,1,4,1,1
grass*761,52,64,1,69
grata ,1,1
grate*5,3,5
grave*78,33,28,3,38,3
gravy*19,4,3
grays*4
graze*46,1,,,2
great*3855,617,654,29,1120,16,15
grebe 1
greed*12,3,1,4,7,,2
greek 3,4,,2,,2
green*1216,97,109,11,35,1
greet*42,7,8,1,2
greps
greys*3,,3
grids*5,,7
grief*48,10,15,,44,,1
grift+
grill*8,11,19,,,5
grime*5
grimy*3,,1
grind*38,2,,1,4,1,1
grins*24,2
gripe*1
grips*5,8,5,1
grist*1,2
grits*5,1,1
groan*26,1,4,,21
groat ,,2
grody
grogs
groin*1,4,5
groks
gronk
grook
groom*20,4,2
grope*5,1,,,6
gross*32,35,28,6,1,2,1
group*1570,382,313,25,3,122,2
grout+
grove*37,7
growl*25,4,4,,7
grown*501,43,60,6,43,1
grows*331,22,16,3,20,1,9
grubs*15,1
gruel*5
gruff*8,4,1
grump*
grunt*9,2
guano
guard*158,35,43,4,122,2
guava+,,1
guess*637,55,35,,1,29,2
guest*94,35,29,1,7,,1
guide*347,36,40,3,23,7
guild*6,3,1
guile*5,1,,,12
guilt*12,33,16,,11
guise*2,6,1,1,,,1
gulag+
gulch*
gules
gulfs*5,,3
gulls*66,,1
gully*12,5,1
gulps*6,1,2
gumbo*1
gummy*3,2
gunks
gunky
gunny+2,2
guppy*11
gurus+,,,,,1
gushy*
gusto+2,2,2,1
gusts*16,3,1
gusty*4,2,1
gutsy+
gutta
gutty
guyed
gwine 1
gyppy
gypsy*46,4,4
gyros+,5
gyved
gyves ,,1
habit*130,23,39,4,2,4,1
hacks*,,3,1,,4
hadda
hades
hadst 4,,,,8
hafta 1,3
hafts
haiku*37
hails*2,1
hairs*70,12,4,,15
hairy*29,5,4,,5,1,2
haled+
haler
hales+
hallo 3,,3
halls*47,3,11,1,,2,1
halma
halos*3,1
halts*9,1,,,,1
halve*3,,1
hames
hammy+
hamza+,,,,,1
hands*1357,286,236,15,566,2,2
handy*47,13,8,,,19,13
hangs*60,4,3,1,3
hanks
hanky+1
hapax+
haply
happy*774,92,121,2,26,8,1
hardy*24,9,4
harem+1,2,3,,5
hares*11,,4
harks
harms*,,,,1
harps*3,,,,2
harpy 2,1
harry*5,5,1
harsh*61,12,12,2,9
harts+,,,,2
harum 1
hasps+1,1
haste*36,9,11,,77
hasty*10,5,4,2,1
hatch*114,5,9,,4
hated*96,28,21,1,56
hater+,,,,1
hates*16,4,5,,39
hauls*5,2,4
haunt*5,4,4,,7
haute ,2,1
haven*7,5,3,,5
haves*,,,2
havoc*6,3,4,,1
hawed+
hawks*54,1,,2
hayed+
hayer
hayey
hazed+
hazel*12,2,1
hazer
hazes*,1
heads*358,43,69,5,207,4,45
heady+3,2,1,1
heals*3,,1,,7
heaps*20,1,6,,22
heard*1988,240,240,6,721,2
hears*67,7,8,,61
heart*1032,171,196,8,830,8
heath*3,,7
heats*35,,1
heave*20,2,3
heavy*984,109,144,7,63,9,2
hedge*27,2,17,,6
heeds*,,,,6
heels*109,22,22,,5
heerd 3
hefts*
hefty*5,1,2
heigh
heirs*13,2,7,,11
heist+
helix+1
hello*162,9,14
hells*,1
helms*
helps*554,30,19,,10,11,13
hemps*
hempy
hence*61,58,54,8,10,100,344
henge
henna+,,,,2
henry ,10,2,1
herbs*27,3,12,,4
herby
herds*126,6,11,,4
herem ,,1
heres*1
heron*9,,,,2
heros*
hertz+3
hewed*4,1,,,1
hewer+,,,,1
hexad
hexed*
hexer
hexes*,,,,,1
hicks+
hider+
hides*91,5,4,1,13,1
highs*7,2
hiked*11,1
hiker+1,,1
hikes*4,4
hilar ,4
hills*410,37,21,5,77
hilly*55,,3
hilts*
hilum ,5
himbo
hinds*,,,,6
hinge*19,1,6
hints*27,9,2,,,4,1
hippo+2
hippy*1
hired*69,25,3,,37,,1
hirer+
hires*3,1,1,,2
hitch*14,4,4,1
hived
hiver
hives*1
hoagy+1
hoard*4,,,,1
hoars
hoary*5,,,,3
hobby*74,4,13,,,3
hobos*
hocks+3
hocus+
hodad
hoers
hogan*8,3
hoist*9,1,2
hokey+
hokum+
holds*228,41,29,6,24,13,91
holed+1,1
holer
holes*278,38,15,,9,1,2
holey
holly*24,,1
holon
homed+
homer*3,9,,,9
homes*581,62,55,4,2
homey*2
homme ,,1
homos 3
honed+1,,1
honer
hones+
honey*113,23,11,,63
honks*1
honky
honor*204,64,2,3,192,,2
hooch+,1
hoods*11,2,1
hooey+,,1
hoofs*66,7,1,,9
hooks*49,2,9,,29,2
hooky*4
hoops*8,3,6
hoots*2,1,1
hoped*201,48,76,7,9,1,2
hoper
hopes*112,48,59,7,10,4,2
hoppy
horde*11,2,2,,2
horns*158,9,6,,69
horny*2
horse*1263,112,60,,39,,2
horsy+
hosed*
hoses*10,2
hosts*10,5,2,,304
hotel*106,85,112,5
hotly*10,2,6,,4
hound*44,4,5
houri+
hours*990,175,194,18,5,8
house*2705,403,415,24,1891,1,4
hovel*2,2
hover*9,4
howdy+15
howls*10,3,1
hubba+,1
hubby+,1
huffs*1
huffy+1,,1
huger*
hulas+1
hulks*,2
hulky+
hullo 4,,4
hulls*7,,1
human*710,297,210,28,57,1
humid*38,1,2
humor*72,47
humpf 1
humph 11
humps*13,,1,,1
humpy+1
humus*20,,13
hunch*6,7,2
hunks*1
hunky
hunts*25,2,2,,2
hurls*,,,,1
hurly
hurry*314,36,25,1,1
hurts*23,4,4,,3
husks*13,,1
husky*26,3
hussy+
hutch*4,,1
huzza
hydra*4
hydro ,,,3
hyena*8,1,1,,1
hying
hymen ,13
hymns*24,6,6,,15
hyped+
hyper+
hypes+
hypos
iambs
icers
ichor
icier*
icily*1,,2
icing*10,1,18
icons*,,1
ideal*59,60,36,6,,5,1
ideas*978,142,128,6,,26,11
idiom*11,7,2
idiot*10,2,7,,,1
idled*,1,1
idler*1,1,,,1
idles*1
idols*7,2,2,,137
idyll+,2,1
idyls
igloo*7,,1
ikats+
ikons
ileum ,1
ileus
iliac ,1,3
ilium
image*148,119,43,6,90,21,2
imago ,,1
imams+2
imbed
imbue+,,1
immix
impel*2,,3
imply*11,12,16,2,2,12,8
impro
inane*,1,1
inapt ,1
incur+1,5,,,7
index*91,77,27,4,,64,95
indie
inept*4,2,1
inert*16,5,8
infer*12,1,3
infix+
infra ,1
ingot*1
injun 1
inked*9,,,,,1
inker
inlay+1
inlet*12,4,7
inner*128,53,48,10,75,29,11
inode
input*35,20,1,2,,241,1
inset+4,1
inter+3,2,4
intra ,,1
intro+
inure+,1
ioctl
iodic
ionic+,4,6
iotas+
irate*1,1,2
irked*1
irons*15,7,1,,2
irony*9,12,7,4
isles*6,,,,5
islet+1
issue*97,151,95,29,10,5,3
itchy+4
items*196,71,40,,,69,7
ivied+
ivies*
ivory*29,16,8,,13
ixnay
jacks*4
jaded*1,2,1
jades+
jaggy+
jails*4,3,,4
jakes ,1
jambs*,,,,18
jammy
janes
japan
jaunt*1,,1
jawed*
jazzy+,1
jeans*24,1,1
jeeps*8,,3
jeers*3,1,2
jello+2
jells*
jelly*87,3,15
jenny
jerks*8,1,2
jerky*13,4,4
jerry
jests*2
jetty*4,,3
jewel*20,1,8,,5
jibed*1
jiber
jibes*2,1
jiffs
jiffy*6,2,1
jihad+1,,1
jilts+
jimmy+
jingo+3,,3
jings 2
jinks+
jinns
jived+
jives+
jocks+
joeys
johns
joins*50,2,7,,1,5,1
joint*142,32,53,2,5,1,3
joist*
joked*8,1
joker*2,,1
jokes*77,9,13,1,,8
jolly*41,3,6
jolts*2
joule+,,1
joust+,1,1
jowls+2,4
jowly+
joyed
judge*173,43,65,17,161,1,1
judos*
juice*170,11,32,,3
juicy*40,6
jujus
jukes
julep+,2
jumbo*3,,,,,1
jumps*82,2,7,,,5,1
jumpy*6,2,1
junco
junks*5,1,1
junky*1
junta+8,3,,7
juror*2,4
juste ,,1
jutes*
kabob+
kaiak
kales*
kapok
kappa+
kaput+1
karat*2
karma+
kayak*2
kayos+
kazoo+,1
kebab+
kebob ,1
keels*1
keens
keeps*288,21,16,3,55,24,3
kefir+
kelly+
kelps*2
kelpy
kenaf
kepis
kerbs
kerfs 1
kerns+,,,,,32
ketch*7
keyed*4,3,1
keyer
khaki*6,1,2,1
khans+,,1
kicks*23,3,2
kicky
kiddo+1
kikes
kills*37,8,3,,26
kilns*,,3
kilos+
kilts*1,1
kilty
kinda+8,5
kinds*1545,36,44,3,54,49,1
kings*131,3,7,,343
kinks*5,,1
kinky*2
kiosk+1,1,3
kirks
kited 1
kites*19,,,,1
kiths
kitty*22,2,2
kivas 2
kiwis+1
klieg+
kluge
klugy
klunk 1
klutz+
knack*8,4,4
knave*2,,1,,2
knead*3,1,1,,2
kneed+1
kneel*9,5,4,,2
knees*213,38,28,,29
knell*1,,1
knelt*59,8,9,,13
knife*318,74,38,1,4,2,1
knish+
knits*6
knobs*24,1
knock*85,15,18,1,4
knoll+11,1
knops ,,1
knots*60,1,8,,1
knout
known*1401,245,358,18,255,90,58
knows*505,99,102,9,118,21,8
knurl+1
koala*3,,1
koine
kooks+1
kooky+2
kopek+
kraal ,,,1
kraut+,1
krill+7
krona
krone
kudos*,,1
kudzu
kulak
kyrie ,,1
label*184,19,17,1,,26,1
labia
labor*164,125,,185,81
laced*15,2,2
lacer
laces*11,1,5
lacey+,2
lacks*24,6,5,5,18
laded
laden*20,6,3,1,5
lades
ladle*9,1
lager+,,2
laird ,,1
lairs*1,1,,,1
laity+,3
laker
lakes*261,6,5
lamas+
lambs*46,7,11,,95
lamed*3
lamer*
lames*
lamps*60,6,14,1,39
lanai+
lance*18,3,1,1
lands*602,24,32,,152
lanes*27,4,3,1,2
lanky*21,2,2
lapel*,1,2
lapin
lapis+1
lapse*4,6,6,,1
larch*1
lards*
lardy ,,1
large*2777,361,435,17,79,140,88
largo 1
larks*5,2,1
larva*26
lased
laser*13
lases
lasso*17,2
lasts*40,1,1,1,2,1,2
latch*12,5,1,,1
later*1599,396,464,12,24,135,29
latex*10,2,1
lathe*37,1,3
laths*1
latin+,1,,1
latus
laude 1
lauds*
laugh*287,28,30,,18
lavas*3
laved
laver ,,,,15
laves 1
lawns*37,5,6
lawny
lawzy
laxer+,,1
laxly*
layer*259,12,26,1
layup+
lazed ,,1
lazes
leach*
leads*120,33,31,4,33,10,33
leafs*
leafy*30,1,1,,5
leaks*14,3,1,2,1
leaky*10,2
leans*18,1,2,,4
leant 5,,8
leaps*36,4,3,1,3
leapt*10,2,5
learn*1674,83,115,2,66,64,15
lease*11,10,37,1
leash*16,3,1,,1
least*878,343,314,49,40,71,71
leave*964,205,216,20,147,31,16
ledge*61,4,5,,8
leech*4,,,,1
leeks*,,,,1
leers*1
leery+
lefts*1
lefty+,,,,,1
legal*66,72,50,30,3,28,9
leggo+
leggy+,1,1
legit+,,,,,,1
legos
lemma+,4,,,,,12
lemme 9,1
lemon*44,15,36
lemur*2
lends*14,4,4,,4
lento 1
leper*1,,1,,16
lepta
letup+2
levee*2,1
level*531,212,205,22,13,81,7
lever*128,13,6
levis+,2
liars*1,1,4,,8
libel+1,1,4
libra 4
licit
licks*9,,,,1
liege 3
liens+,1
liers
liest
lieth
lifer+,1
lifts*42,2,3,,11
light*2376,325,284,5,297,6,5
ligne
liked*610,58,54,,,,1
liken*,,1,,6
liker
likes*229,20,27,4,1,4,1
lilac*10,1,5
lilts*
lilty
limbo+,1
limbs*69,5,8,,8
limby
limed ,,2
limen ,,1
limes*8
limey+
limit*92,48,39,11,9,8,42
limns
limos+
limps*,1
lined*,16,6,,4,4,2
linen*58,6,13,,105
liner*19,3,9
lines*1715,197,152,33,3,486,59
lingo+4,2,,,,,1
lings
links*47,7,19,2,,2
lints*
linty+
lions*128,5,4,,58
lipid+
lippy
liras
lisle
lisps*
lists*98,34,15,,,91,19
liter*6,2,3
lites 1
lithe*6,4
litho
litre ,,1,1
lived*1372,115,118,7,150,3,1
liven+2
liver*51,16,7,,17
lives*742,81,97,8,171,1
livid*8,1,2
livre
llama*18
loads*80,10,7,,4,4
loafs*
loams*
loamy+1,,1
loans*26,31,23,3
loath*5,3,1,1,1,1
lobar
lobby*13,19,5
lobed 3
lobes*2,5,5,,,1
local*286,282,402,23,4,50,1
lochs
locks*37,7,4,,10,1
locos 2
locus+6,2,1
lodes*1
lodge*40,11,9,,2
loess 8
lofts*
lofty*32,5,1,,21
loges
loggy
logic*21,16,19,3,1,3,3
login
logos+1,,,,,2
loins*4,2,,,62
lolls*
lolly 8,,2
loner+2
longs*1,1,1,,7
looks*756,78,91,6,31,86,36
looky+1,1
looms*17,2,2,,1
loons*2
loony+
loops*37,1,1,2,14,24
loopy+
loose*227,53,32,,35,16
loots*
loped*4,1
loper
lopes*1
loppy
lords*44,1,1,,43
lordy
lores*,,1
lorry+,,1
loser*9,1,,2,,,1
loses*70,15,14,4,10,3,4
lossy
lotsa
lotta 2
lotto+
lotus*3,1,1,,2
louis 1,8,,2
louse*2,3,2,,,1
lousy*8,11,2
louts+1
loved*347,56,68,2,112
lover*41,16,23,1,6
loves*75,19,14,,83
lowed*
lower*659,119,118,29,19,35,58
lowly*18,,2,,17
loxes
loyal*38,18,15,2,12
luaus+
lubes+
lubra ,2
lucid*4,4,3
lucks*,1
lucky*166,21,39,1,,4,1
lucre 1
lulab ,,2
lulls*2,1
lulus+
lumen+,1
lumps*43,3,5,,1
lumpy+6,2,1
lunar*55,10,2
lunch*357,33,63
lunes
lunge*7,1,1
lungs*207,20,14
lupus
lurch*4,3,1
lured*6,3,1,,1
lurer
lures*6,,,1
lurid*2,3,2
lurks*1,1,,,2,,1
lusts+,1,1,,2
lusty*7,3,1,,1
luted
lutes*2,,,,1
luvya
luxes
lycra
lying*275,36,94,1,82,3
lymph*7,2,1
lynch*1,,,3
lyres*,,,,18
lyric*21,12,5
macaw+
maced
macer
maces
macho+
macro+,,,,,441
madam*34,1,2
madly*19,4,2
mafia+,,,1
magic*279,37,15,,8,9,5
magma+44
magna ,,1,,,,1
magus
mahua ,1
maids*21,12,2,,23
mails*8,7,2
maims*
mains*8,1,5,1
maize*8,,7
major*597,229,110,18,,6
maker*36,12,7,,7
makes*1311,172,147,25,168,207,43
males*44,19,8,1,18
malls*
malts*1
malty
mamas*1
mambo+
mamma*5
mammy 5
maned 4
manes*6,2
mange+
mango*7
mangy+2
mania*2,5,3
manic+1,2
manly*9,2,3,,1
manna+,,3,,18
manor*15,3,8
manse ,1
manta 2
maple*94,6
march*130,23,15,3,24
mares*12,1,2
marge 2,,1
maria
marks*469,28,30,4,7,63,3
marls
marry*141,18,83,,3
marsh*52,,10,,3
marts*,1
maser 3
mashy
masks*33,3,1,,1
mason*6,2,,1
masse ,,2
masts*46,2
match*353,41,57,4,3,25,9
mated*1,4
mater 4,1
mates*20,10,5
matey+3
maths*,,1
matte+
matzo+,,2
mauls*
mauve*1,1,4
maven+
mavis ,1
maxim*6,1,2
maxis
maybe*779,132,85,1,,5,15
mayor*95,12,11,4
mayst 1,1
mazed
mazer
mazes*1
meads
meals*147,26,16,,3,1
mealy*1
means*1962,298,317,44,83,176,54
meant*539,100,113,7,19,11,1
meany*
meats*52,12,3,,1
meaty*3,1
mebbe 5,,2
mecca+1,,,1
mecum ,2
medal*28,3,37,2
media*15,13,9,7
medic*
meets*,35,25,2,10,3
melba+1
melds+
melee+,3
melon*16,1,3
melts*69,,,,9
memes
memos*1,1
mends*3
menus*11,2
meows*
mercy*49,19,24,2,235
merge*9,10,4,,,,2
merit*15,27,11,1,1
merry*97,6,7,,29
merse ,,1
mesas*8
mesne
meson+
messy*12,3,2,1,,1,5
metal*782,60,57,8,1,6
meted*2,2,1,,1
meter*206,6,10,,,,2
metes*
metre 1,1,5
metro+,,2
mewed*3,1
mezzo ,1
miaow
micas*1
micks
micro+
middy*6
midis
midst*67,19,14,,295,16
miens
miffs+
might*2824,671,786,85,448,323,105
miked 1
mikes*3
milch ,,,,3
miler+2
miles*2146,169,124,,7,1,1
milks*4,2
milky*13
mills*175,14,12,,1,,1
mimed+
mimeo+
mimer
mimes+
mimic*5,,1
mimsy 2
minas ,,,,9
mince*8,1,1
minds*110,56,52,10,38,1
mined*58,3
miner*22,1,3
mines*170,28,19,,1
minim+1
minis+
minks*7
minor*223,53,49,11,,4
mints*3,,,1,,,1
minus*53,8,1,2,,93,15
mired*5
mires*
mirth*10,2,2,,13
miser*3,,2
missy ,1
mists*25,2,2,,2
misty*19,4,7
miter*6,1
mites*5
mitre 1,1,2
mitts*1,,1
mixed*324,37,60,4,58,4
mixer+9,2,4
mixes*16,,2,,1,2
mixup+
moans*3,1,1,,3
moats*1,,1
mocha*
mocks*4,,1,,5
modal+8,3
model*332,63,116,3,2,1
modem+
modes*19,8,14,,,70,1
modus 1,1
mogul+,,,1
mohel
moire+,1
moist*135,11,7,1,3,1
molal ,2,1
molar*3,1
molas
molds*63,7,,,2
moldy*5,,,,2
moles*10,,,,1
molls+
molly 1,1
molto 1
molts*9
momma
mommy+2
monad
mondo+
money*1694,263,318,56,189,1,4
monic+,3
monks*2,10,3
monte
month*403,130,95,16,289,7,6
mooch+
moods*32,7,8,,,1
moody*6,2,2
mooed*2,1
moola+
moons*59,3,1,2,13
moony 1
moors*4,1,4
moose*52
moots
moped*2
moper
mopes*1
moral*53,140,85,12,,3,1
moray+1
morel
mores*5,7
morns*3
moron*2
morph
morts 1
mosey+
mossy*12,,2
mosts
motel*34,20,,1
motes+2
motet+,1,1
moths*58,2,2
mothy
motif+3,8,3
motor*198,53,58,2
motto*17,3,6,1
mould+2,1,11,1
moult+,,2
mound*51,8,,,9
mount*42,10,9,7,24,1
mourn*14,2,1,,55
mouse*207,9,6,,1
mousy+,1,2
mouth*725,103,98,1,395,17
moved*994,181,140,5,56,20,1
mover+5,,1
moves*485,36,22,5,7,20,31
movie*151,29,4
mowed*15,1,,,2
mower*13,,3
moxie+
mrads ,2
mucho 1
mucks*
mucky+
mucus*19,2,3
muddy*89,10,4
muffs+
mufti+
muggy*8,1
mujik
mulch*,6
mulct
mules*51,3,3,,12
muley
mulls*
mumbo ,,,,,1
mummy*14,,2
mumps*15
munch*3,1
munge
mungs
mungy
muons
mural+5,1,3
murks*
murky*11,5,1
mused*6,4,6,,1
muser
muses*
mushy*4
music*2100,200,211,15,27,8
musks*
musky*2
musos
mussy
musta ,1
musts*,1
musty*10,,1
muted*10,3,2,1
muter
mutes*3
mutts*
muxes+
mylar+
mynah+
mynas
myrrh*5,2,,,21
myths*36,6,10,1,5
nabla+
nabob+
nacho+
nadir+,2,1
naiad+
nails*168,14,11,1,9
naive*6,7,6,4
naked*54,32,15,1,41,1
named*946,84,46,3,95,15,8
namer+
names*1016,88,100,8,94,82,11
nanny+,,1
napes*
nappy+1
narco+
narcs+
nards+
nares
nasal*22,2,1
nasty*19,5,8,1,,1,1
natal+2,2
natch+,1
nates
natty+1,1
naval*50,21,29
navel*3,2,,,2,,2
naves
nears*8,,,1
neath ,,1
neato+
necks*37,2,5,,14
needs*616,152,107,42,11,52,15
needy*10,5,3,2,54
negro 3,5,1
neigh*4,,,,1
neons*
nerds+
nerdy
nerfs
nerts
nerve*121,12,39,1
nervy+,,1
nests*110,3,3,,8
never*3115,695,689,25,256,97,3
newel+1,1,1
newer*30,20,8,1
newly*90,28,28,1,3,11,3
newsy+2
newts*
nexus+
nicad
nicer*11,2,2,,,5,9
niche*10,3,1,1,1
nicks*3
niece*17,8,14
nifty*
night*2307,400,373,15,365,1,1
nihil
nimbi
nines*6,,1
ninja
ninny+,,1
ninth*41,15,7,,35,2
nippy 1
nisei+
niter
nitro+
nitty
nixed+
nixes+
nixie
nobby+
noble*57,23,28,2,44
nobly+6,,2,,13
nodal 3,,1
noddy
nodes+,2,1,,,,3
noels+
nohow
noire ,1
noise*411,37,47,,38,3
noisy*94,6,9,1,3
nomad*6,,1
nonce+,1
nones
nonny
nooks*2,1
nooky
noons*
noose*7,3
norms+,24,4
north*793,64,98,14,154
nosed*5,,2
noses*56,6,4,,1
nosey+1
notch*27,6,2,,,,5
noted*145,90,65,6,1,3,3
noter
notes*534,55,63,,2,16,3
nouns*530,3,9,,,1
novae*
novas*1,,,,,1
novel*71,59,67,3
noway+
nuder
nudes+,2
nudge*17,2,1
nudie
nuked+
nukes+1
nulls+,,,,,1
numbs+
nurbs
nurse*16,16,14,,16
nutsy+
nutty+3
nylon*58,1,5
nymph*22,1,2
oaken*6,1
oakum
oared
oases*22,2
oasis*27
oaten
oaths*12,3,2,,13
obeah
obese+2,,3
obeys*6,1,3,,6,4,1
obits+
oboes*9,,4
occur*215,43,65,2,3,121,37
ocean*843,32,9,4
ocher ,1
ochre+2,2
octal+,,,,,26
octet+
odder*2,,1
oddly*28,9,19,1
odium+,,1,1
odors*37,7,,,3
odour ,,5
offal 1,1,1
offed
offen 2
offer*185,80,103,13,218,1,1
often*2611,368,416,20,49,122,91
ogled+,2
ogler+
ogles+
ogres*
ohhhh 2
ohmic ,1
oiled+18,3,4
oiler
oinks+
oinky
okapi*
okays*
okras
olden*10,1,2,,,2
older*404,93,102,8,13,,1
oldie+
oleos
olios
olive*65,4,5,,45
ombre
omega+,,,2,,3
omens*7,,1,,3
omits*5,2,2,,,4
oncet 1
onion*42,15,3
onset*8,15,3
oodle+
oomph+1
oozed*6,2
oozes*3
opals*4
opens*82,16,20,1,25,3
opera*177,29,54,2
opine+
opium*5,16,1
opted+,2,2
optic*3
orals+
orate ,1
orbed
orbit*233,16,9,3,,,1
orcas+
order*1507,363,336,32,187,181,82
organ*90,12,17,1,2
oring
orlon+1
ortho 1
osier
other*10729,1700,1535,181,510,520,272
otter*29,5,1
ought*220,68,106,15,56,27,11
ouija+,,,1
ounce*48,3,2
ousel
ousts*
outdo*5,3,2,,1
outen 3
outer*252,27,29,3,33,34,7
outgo+1
outta 1,2
ouzel 1
ovals*4,2,,,,,1
ovary*29
ovate
ovens*11,1,6,,1
overs
overt+2,11,6,1
ovoid ,,3
ovule*2
owest
oweth
owing*20,4,29,2,1
owlet+
owned*198,34,21,5,1
owner*172,33,42,1,24,,2
oxbow+
oxeye
oxide*51,3,2
oxlip
ozone*2,3,2
paced*15,11,3
pacer+6,1
paces*22,7,1,,1,1
packs*25,2,5
pacts*2,,1
paddy*16,,2
padre+6,,1
paean+,2
pagan*13,1,10,,,1
paged*
pager+
pages*723,31,42,2,,112,11
pails*22,4
pains*33,15,17,1,14,2
paint*437,37,45,4,4
pairs*455,14,21,,5,43,29
paled*6,1,1
paler*4,,2
pales*1,1,,,,,1
palls*1
pally
palms*55,8,5,,7
palmy
palsy*2,1,1
pampa+
panda*9
paned+
panel*49,31,33
panes*18,3,1,,,,1
panga ,,1
pangs*10,2,2,,2
panic*44,22,15,,18,1,4
pansy*10,6,1
pants*77,9,2
panty+
papal+11,6,3
papas*
papaw*
paper*2372,155,187,23,3,34,5
pappy 1
paras ,,3
parch*
pards
pared*,,1
paren
parer 1,,,1
pares*
parka*4
parks*92,15,46,9,1
parry*
parse+
parts*2331,110,122,9,65,49,47
party*625,195,400,209,12
pasha
passe
pasta*1
paste*96,10,6,,,3
pasts
pasty+,2
patch*97,13,20,2,2,,2
paten
pater 2,1
pates+
paths*73,14,6,,56,96,1
patio*8,2
patsy+,1
patty*2
pause*109,21,29,,,5,1
pavan
paved*45,5,3,,2
paver
paves*,,1
pawed*7
pawer
pawky ,,1
pawls
pawns*1
payed 1
payee+,,1
payer+,,1
peace*343,138,159,16,399,1
peach*76,2,6
peaks*102,8,8,,,,1
peaky+,1
peals*,1,,,4
pearl*85,3,7,,2
pears*31,2
pease
peats
peaty+,,2
pecan*6,1
pecks*8
pedal*39,4,8
peeks*2,,,,,,1
peels*1,1
peens
peeps*
peers*8,8,6,1,2
peeve+
pekoe+
pelts*15,9
penal+1,1,3,2
pence*4,,5
pends
penes
pengo
penis*1
penny*134,10,8,1,3,,2
peons*2
peony*1
peppy*
perch*36,1,4
perdu ,,3
peril*16,8,5,2,9
perks*,,,2
perky*1,2,1
perms+
pesky+2
pesos*9,,1
pesto+
pests*21,2,5
petal*9
peter+,4,,4
petit 3,1,2
petri
petty*7,6,12,1,1
pewee*
pewit
pffft
phage+
phase*37,66,39
phial
phlox*1
phone*73,53,21,,,2
phony*1,12,3,1
photo*50,5,1
phyla+,1
piano*418,30,17,,,4
picas+,,,,,5
picks*70,4,4,,3,2
picky+2,,,,,,1
picot
piece*1198,129,107,10,42,16,3
piers*10,5,2
pieta
piety*5,4,4,,13
piggy*5,,1
pigmy*2
piing
piker+
pikes*4,,1
pilaf+
pilau
piled*90,16,4,,3
piles*67,2,3,1
pills*18,8,2
pilot*167,44,19,2,2
pimps+,2,1
pinch*30,6,5,,,1
pined*1,,2,,2
pines*37,2,2,,1
piney+5
pings+
pinko+
pinks*3,2
pinky*
pinto*17,1
pints*49,,6
pinup+
pions ,,3
pious*7,10,6,,3
piped*22,2,6,,2
piper+4,,2
pipes*117,7,13,1,2
pipet
pique+2,2,1
pismo
pitas+
pitch*273,22,22,1,15
piths*
pithy*2,1
piton+
pivot*23,2,2
pixel+,,,,,151
pixie*
pizza*16,3,1,,,,3
place*4240,551,471,26,905,104,37
plaid*15,1,1
plain*338,48,78,7,63,471
plait*1
plane*990,114,49,,5,,9
plank*35,7,2
plans*308,112,83,27,33,1
plant*1051,123,85,7,62,3
plash
plasm 1,1
plate*346,20,37,1,21
plats 1
playa
plays*304,66,54,2,2,3,9
plaza*12
plead*7,5,5,1,12
pleas*3,1,3,,1
pleat*12
plebe+
plebs
plein ,,2
plena
plied*8,2
plies*3
plink
plods*2
plonk
plops*
plots*19,8,7,,11,2
plows*31,3,,,2
ploys+
pluck*50,2,,2,19
plugs*14,2,1
plumb*15,5,,,5
plume*10,2,1
plump*37,4,14,,2
plums*25,,1
plumy 1
plunk 1
plush*5,3,1
plyer
poach*,1,,,,2
pocks
pocky
podgy ,,1
podia
poems*156,78,28
poesy ,1
poets*79,32,20,,1,1
point*1904,376,423,35,31,335,42
poise*14,6,6
poked*42,4,7
poker*9,5,1
pokes*8,3
pokey*
polar*85,7,5,1,,,1
poled*4
poler
poles*264,8,9,1,39
polio*57,1,1
polis ,1
polka*12,1,1
polls*8,10,6,31
polly+,,,1
polos
polyp*7,,4
pomps
ponds*70,7,1,,1
pones
pooch*
pooey
poohs
pools*57,15,12,1,11
poops
popes*6
poppy*15,2,2
porch*168,42,9,,3,,1
pored*3,1,2
pores*17,3,3
porgy
porks
porky+
porno+
ports*79,4,9,,1
posed*14,7,6,1,,1,1
poser+,,1
poses*9,1,4,1
poset+
posit+
posse*5,11,3
poste
posts*75,22,9,2,8
potty+
pouch*35,2
poufs+
pound*227,28,20,,5,7
pours*40,2,4,,15
pouts*
power*1065,329,318,108,348,5,109
poxed
poxes
prams+,1
prank*,1,1
prate+
prats
prawn+
prays*3,,3,,14
preen*3
preps+
press*247,109,94,25,29,1,1
prest
prexy+
preys*2,,,,1
price*289,103,124,17,34,2
prick+9,2,,,1
pride*145,40,40,1,73,1
pried*1
prier
pries*2
prigs+,,1
prima 1,,5,1
prime*335,37,14,68,2,13,167
primo
primp+
prims
prink+
print*134,18,16,6,1,27
prior*19,47,28,3,1,3
prise
prism*80,,1
privy+2,1
prize*173,18,28,3,1
probe*31,6,11,1,,,5
prods*1,,1
proem
profs+
promo+
proms+
prone*7,14,7,1
prong*2,,1
proof*80,40,49,,9,32,11
props*6,6,1
prose*30,13,7,1
prosy+
proud*361,50,44,5,58
prove*257,53,81,10,28,10,337
prowl*10,2,,,2
prows*1
proxy*,7
prude*
prune*8,1,,2,2
pruta ,1
pryer
psalm*6,2,10,,4
pseud
pshaw 1
psoas ,,1
pssst 1
psych+
pubes
pubic
pubis
pucks*
pudgy+2
puffs*27,1,3,,2
puffy*4,2
puked+
pukes+
pukka 1,,1
pulls*134,9,4
pulps*
pulpy*2,,2
pulse*53,9,7
pumas*3
pumps*51,5,1
punch*54,5,8,1,,,4
punks*1,1
punky
punny
punts*,,1
pupae*19
pupal
pupas*1
pupil*95,20,23,,1
puppy*87,2
puree+2
purer*4,,1,,2
purge*4,2,6,1,14
purls 1
purrs*
purse*59,12,16,2,6
purty 3
pushy+
pussy*8,5
putts*
putty*10,1,1
pygmy*5
pylon+2
pyres*
pyxie
qophs
quack*23,9,3
quads+1,,,,,2
quaff*,,1
quail*22,,1
quais+
quake*9,2,,,8
qualm*
quals+
quark+
quart*85,3,2,,1
quash+
quasi+,,1
quays*3,,1
queen*206,21,13,3,36,,2
queer*102,6,14
quell*1,2,,,,,1
query*3,1,3
quest*18,16,12,4,,,2
queue*4,,7,1
quick*394,67,70,4,8,7,6
quids
quiet*533,75,79,2,48,2
quiff ,,1
quill*17,9,1
quilt*34,,15
quint+,4,2
quips*
quipu+6
quire+2,,1
quirk*4,1
quirt 2,8
quite*967,281,484,28,6,80,38
quits*2,1,1
quoin
quoit
quota*12,3,7,3
quote*13,17,17,1,1,34
quoth+7
rabbi*5,7,,,2
rabid*,2
raced*181,12,8,,1
racer*1
races*109,21,29,,1
racks*9,2,1,,3
radar*70,23,1
radii*7,4,1
radio*587,113,49,18,,1,1
radix+16,,,,,3,3
radon+1
rafts*16,1,1,,3
raged*25,8,2,,4
rages*5,1,,1,2
raids*19,5,4,,5
rails*59,9,5
rains*121,5,3,,5
rainy*122,4,3,,1
raise*344,51,38,12,77,3,4
rajah*,1
rajas
raked*18,4,1
raker+
rakes*,,1
rally*63,10,20,1,3
ramps*1,1,1
ranch*243,26,5,1,,,1
rands
randy+
range*351,159,141,16,2,24,37
rangy+1,2
ranks*51,20,22,3,12,2
rants+
raped+2,2
raper+
rapes+,1
rapid*107,43,39,3,,1
rarer*5,1,2,,,1
rasae
rasps*1,1
raspy+1
rated*17,9,6,,1,3
rater
rates*86,102,77,14,,3,2
raths 2
ratio*111,36,61,,,26,44
ratty+1
raved*,,1,,2
ravel*2
raven*4,,,,7
raver
raves*2
rawer+
rawly
rayed 1
rayon*47,,1
razed*1,,2,,4
razer
razes*
razor*20,15,5,,8,1
reach*648,106,83,11,27,17,1
react*45,15,11,3
reads*120,16,14,2,3,61,4
ready*1207,143,138,9,125,27,9
realm*26,19,19,1,8,1,1
reals+,,,,,,5
reams*3,2
reaps*,,,,2
rearm+,,1
rears*,,,,,1
rebar
rebel*23,11,17,1,14
rebid+
rebox ,,,,,3
rebus+1
rebut+,6,1
recap+,,,,,,1
recta
recto+
recur*4,2,,1,,1
recut+
redid+
redip
redly 1
redox
redux
reeds*70,,2,,8
reedy+3,2,2
reefs*17,1,1
reeks+
reeky
reels*5,,1,,1
reeve
refer*202,27,50,2,,43,1
refit*1,,1
refix+
refly
refry
regal*5,2,1,,1
rehab+
reify+
reign*30,7,27,1,186,1
reins*53,9,8,1
relax*40,18,12,3,2,9,2
relay*44,2,6
relet
relic*4,6,3,,1
reman
remap
remit*,,1,,,1
remix+
renal ,1,3
rends*
renew*12,4,9,,15
rente
rents*7,4,33,2
repay*15,7,6,,27
repel*22,8,3,,1
reply*96,42,83,1,12,6
repro+
reran+
rerun+,,,,,,1
resaw
resay
reset+3,,,,,11
resew+
resin*17,9,16
rests*49,18,13,1,17
retch+,1,1
retro+
retry+
reuse+
revel*3,3,1,,3
revet
revue+3,,6
rewed+
rheas+
rheum 1,1
rhino+14,,1
rhumb
rhyme*226,3,3,,,2
rials
ribby
riced
ricer
rices
rider*112,12,4,,13
rides*86,10,8,,5
ridge*82,12,15,,1
ridgy
rifer
rifle*140,61,2
rifts*1
right*4815,607,597,79,480,387,117
rigid*49,24,23,3,1,5
rigor*2,,2,,4,,2
riled+2
riles+,,1
rille
rills*2
rimed 1
rimer
rimes
rinds*2
rings*158,6,21,3,47,,1
rinks*1
rinse*22,6,2,,1
riots*15,1,7,1
ripen*15,,1,,2
riper*
risen*44,10,27,1,33
riser+7,,1
rises*174,19,19,17,36,1
risks*21,5,20,4,,1,1
risky*16,3,4,1
rites*10,4,4,,6
ritzy+
rival*34,11,22,5,5
rived
riven+,1,,1
river*1170,78,83,12,106
rives
rivet*22,,2
roach*3,1,16
roads*359,55,33,11,8
roams*1,,1
roans+
roars*17,1,2,,3
roast*55,10,6,,1
robed*3,1,,,3
robes*33,4,8,1,29
robin*57,1,1,2
roble
robot*71,1,3
rocks*740,23,34,3,24
rocky*144,9,11,,7,,2
rodeo*52,1
roger+2,3
rogue*3,1,1
roids
roils+
roily
roles*25,34,12,,,,2
rolls*113,10,9,,,,1
roman+,,1,4,,77
romps*
rondo+27
roods
roofs*63,5,8,,4
rooks*
rooky
rooms*209,54,48,1,2
roomy*5,1,3
roost*14,1,4
roots*444,23,24,,24,8,31
rooty
roped*13,1,2
roper+3
ropes*99,4,11,,17
roses*82,7,16,,2,1
rosin+9
rotor*20,6,14
rouge*4,2,1
rough*292,41,29,2,4,2,4
round*1076,77,335,10,262,37,14
rouse*12,2,6,,9
roust+
route*196,33,21,,2,3,2
routs*1
roved*,1
rover*5
roves*1,,1
rowan ,,1
rowdy+4,4,2
rowed*43,2,1,,2
rower+
royal*105,27,163,7,83,1
rubes+
ruble+
ruche
ruddy*6,3,2,,4
ruder*
ruffs*1,,1
rugby*2,,5
ruing
ruins*56,8,13,2,48
ruled*130,31,16,6,27,5
ruler*260,3,28,,82,4,3
rules*444,76,72,9,30,272,26
rumba
rumen+,2
rummy*,1
rumor*10,7,,,1
rumps*4
runes+2,1
rungs*4,,1
runic+3
runny+1
runts*2
runty+1
rupee+,6
rural*59,47,37,11,1
ruses*
rusks+,,1
russe ,1
rusts*4
rusty*55,7,1
rutty+
saber*6,1
sable*,2,1
sabra 1
sabre+2,2,1
sacks*35,1,7,,1
sadly*109,12,17,2,1
safer*50,5,8,4,,3
safes*1,,1
sagas*3,,1
sager
sages*,1,,,1
sahib
sails*110,2,11,,1
saint*17,10,13,,1
saith 4,4,3
sakes*19,,2,,1
salad*84,9,8
sales*100,130,66,1
sally+3,3,,,1
salon+4,1,3
salsa+
salts*14,6,2,,,1
salty*54,4,3,,1
salve*1,3,3,,1
salvo+2,2
samba+2
sands*43,7,11,,1
sandy*117,6,11,,1
saner*1,1,1
sappy+,1
saran
sarge+
saris*
sassy*8
sated+,,1,,1
sates
satin*17,5,3
satyr*,,,,1
sauce*62,20,5
saucy*8,1
sauna+
saute+1,1
saved*256,43,51,7,137,18,4
saver+4,1
saves*32,5,1,,16,17,1
savor*3,1
savvy+1,1
sawed*14,,,,1
sawer
saxes+,,1
sayer+
scabs*2,,,,2
scads*
scald*1,1
scale*737,55,115,4,6,12,4
scalp*34,4,4,,2
scaly*9
scamp*7
scams+
scans*4,2,,,,3
scant*10,5,1,2,4
scare*55,3,3,3,1
scarf*50,4,6
scarp 3
scars*18,10,4
scary*23,2,,1,,2,3
scats
scene*302,102,100,4
scent*69,6,15,1,5
schmo
schwa*79
scion+,1
scoff*7,,1,,7
scold*19
scone+
scoop*16,5
scoot*3
scope*24,27,42,12,,8,6
scops ,1
score*411,66,77,4,,1,8
scorn*18,4,6,,2
scour*2,1,2
scout*53,6,2
scowl*7,,1
scows*3
scram*2,,1
scrap*43,8,7,2
screw*100,20,9
scrim+,1
scrip+1,,6
scrod+
scrub*36,9,1,1
scrum ,,1
scuba*13
scudi
scudo
scuds
scuff*5,1
scull*,,1
scums*
scurf ,,3
scuse ,1
scuzz+
seals*49,4,7,,11
seams*82,9,4,,1
seamy+,,1
sears*
seats*143,15,27,49,8
sebum
secco ,3
sects*8,2
sedan*6,2
seder+,,3
sedge*
sedgy
sedum
seeds*560,42,19,1,9
seedy+1,,2
seeks*25,10,11,2,41
seems*638,258,297,101,42,18,43
seeps*11
seers*1,1,,,4
seest ,,,,4
seeth
segue+1
seine+,,9,,2
seize*40,5,4,,48
selah
selfs
sells*66,13,5,1,12
semen ,,,,6
semis+
sends*82,4,7,,26,5
sense*556,311,265,32,25,26,27
sepal+
sepia+,1
sepoy
septa ,6
serfs*14,1,2
serge*1,1,1
serif+,,,,,18
serum*10,18,22
serve*317,107,80,10,244,14,1
servo+,5
setup+7,8,,,,6,2
seven*687,106,116,15,488,40,9
sever*4,3,2,,1
sewed*33,1,2,,2
sewer*8,9
sexed
sexes*8,11,9,1
shack*40,1,6
shade*203,28,30,,18
shads ,,3
shady*30,1,4
shaft*69,11,13,,12
shags+
shahs+
shake*143,17,17,,32
shako
shaky*19,5,3,3
shale*19,,3,1
shall*1051,266,355,11,6976,113,7
shalt+22,,8,,2
shame*67,21,18,1,191
shams*,1
shank*13,1,2
shape*766,84,98,4,,71,2
shard+
share*354,96,91,25,53,3
shark*92
sharp*499,71,70,2,25,31,1
shave*15,6,1,,14
shawl*40,3,1
shawm
shays
sheaf*6,3,2,,7
shear*10,40,7,,3
sheds*19,4,1,,6
sheen*9,2
sheep*464,23,27,,205
sheer*50,14,27,,4,,1
sheet*367,45,46,,2,2,1
sheik*,4
shelf*150,12,12,1
shell*245,21,12
sherd ,,,,1
shews
shied+3,3,,1
shier
shies+1,1
shift*70,41,29,2,,29,17
shiki
shill+,1
shims+1,1
shine*157,4,5,,36
shins*3
shiny*123,3,15
ships*731,44,37,2,37,2
shire+
shirk*1,,1
shirr
shirt*222,26,29,,1
shish+,1
shits
shlep
shmoo
shnor
shoal*3,,5,,2
shoat
shock*100,31,53,1,2
shoed+
shoer
shoes*461,44,36,,1
shoji ,1
shone*119,5,12,,2
shook*420,57,53,1,14
shoos*
shoot*214,27,22,,16,,1
shops*126,16,64,7
shore*447,43,29,1,6
shorn*4,,1,,5
short*1534,208,228,16,23,58,1
shots*78,29,36,1
shout*143,9,10,,46
shove*15,2,2
shown*1490,166,230,16,79,84,4
shows*1184,94,146,6,25,68,52
showy*15,1,2
shred*1,3,3
shrew*4
shrub*25,1,1,,1
shrug*11,2,2
shuck*
shuns*1,2,1,,1
shunt+1,1
shush*2
shute ,,1
shuts*9,1,,,5,1
shyer*1
shyly*19,4,5
sibyl
sicko+
sicks
sided*4,1,,1,2
sides*827,99,87,20,35,18,85
sidle*2,1
siege*12,5,3,1,41
sieve*15,1,5,,3,,1
sifts*2
sighs*20,1,3,,1
sight*565,86,98,4,345,,1
sigma+,,,,,2,1
signs*352,68,52,5,101,89,2
silks*21,,9
silky*32,1,3
sills*6,,1
silly*150,15,29,1,3,4,1
silos*1,2,1
silts*2
silty+
since*2041,624,541,62,196,373,251
sines+,,,,,2,1
sinew*12,,,,3
singe*1
sings*127,10,5,,6
sinks*30,,6,1,5,1
sinus*1,1
sired+,3,2
siree+3
siren*12,1,1
sires*
sirup*1
sisal*11
sissy+5,,,1
sitar+1
sited 1,,6
sites*28,16,43,9,1,1
situs ,5
sixes*22
sixth*99,20,14,1,50,4,1
sixty*108,21,23,1,47
sized*10,4,3
sizer
sizes*201,12,14,,,39,9
skate*33,1,1
skeet+,2
skein*3
skews*
skids+,1
skied*2
skier*11,,1
skies*74,9,4,,14
skiff*14,9
skill*262,41,42,,19,1
skimp*1
skims*3
skins*168,7,5,,19
skint ,,2
skips*21,1,3,,1,6
skirt*123,21,24,,9
skits*2,1,1
skoal+1
skulk*,1
skull*77,3,4,,5
skunk*7
skyed+
slabs*23,,5
slack*13,8,5,,7,1,1
slags*1
slain*27,,,,165
slake*1
slams*6
slang*27,2,2
slant*91,3,4,,,34
slaps*11,1
slash*37,3,2,,,22
slate*20,6,3,,,1
slats*6,1,2
slave*175,30,4,,9
slaws
slays*1,,,,5
sleds*34
sleek*37,2,4,,6
sleep*717,65,61,6,88
sleet*22,1,4
slept*200,27,24,,49
slews*2
slice*88,13,17,,,,1
slick*28,7,1,1,,2,3
slide*182,20,7
slier
slily
slime*11,,4,,2
slims+
slimy*7,,3
sling*9,1,1,,9
slink*5,,1
slips*34,8,3,,8,1
slits*24,2,1
slobs+1
sloes ,,1
slogs+
slomo
sloop*11,1
slope*146,19,18,,2,8,2
slops*1
slosh*4
sloth*20,,1,,1
slots*17,5,2,,,1,1
slows*22,,,,,1
slued
slues*
sluff+
slugs*7,4,2
slump*4,8,6,3
slums*22,7,8
slung*22,2,5,,2
slunk*1
slurp+2
slurs*3,,,,,1
slush*2
sluts+
slyer*
slyly*9,2
smack+14,4,3,,,2
small*3555,529,522,21,101,86,9
smart*109,21,11,,3,1,3
smash*20,4,7
smear*,2,1
smell*378,34,30,1,12
smelt*7,3,4,,1
smile*281,58,76,,1
smirk*3,3,1
smite*3,,,,5
smith*7,,2,,4
smock*2,,8
smogs
smoke*328,40,42,1,62,20,1
smoky*12,2,2
smote*8,,1,,93
smurf
smuts
snack*29,6,2
snafu+
snags*5,1,6
snail*47,1,1,,1
snake*226,42,18,,2
snaky*1
snaps*25,,1,,1
snare*26,1,2,1,49
snarf
snark
snarl*15,,3
sneak*25,2,1,,,1
sneer*5,1,3
snide+
sniff*30,2,,,1
snipe*2
snips*13,1
snits+
snobs*2,1
snood
snook
snoop*,1
snoot+,,1
snore*8,,3
snort*14,3,5
snots+
snout*24,1,1,,1
snows*29,7,,,2
snowy*56,4,2,1
snubs*,,1
snuck+,1
snuff*10,,1
snugs
soaks*16
soaps*12,3
soapy*8,1
soars*5,,,,1
sober*24,16,7,1,8
socko+
socks*78,7,8
socle
sodas*5
sofas*2,3
softs
softy+
soggy*16,3,2
soils*54,15,26
solar*198,14,3,6
soled*,,1
soles*18,5,4,,8
solid*390,77,61,3,4,8,1
solon 1
solos*10,3
solum ,,1
solve*874,20,24,7,4,51,66
somas
sonar*25,7
songs*658,58,33,3,41
sonic*8,2
sonly+
sonny+2,1,2
sooth+
soots*
sooty+4,,1
soppy+
sorer*
sores*5,3,1,,8
sorry*282,47,70,1,7,6,1
sorta+3
sorts*76,12,25,1,14,16
souls*25,23,11,,48,,1
sound*4667,202,157,4,161,5
soups*11,,1
soupy+3
sours*,1
souse+
south*709,62,107,49,122,1,1
sowed*6,,2,,12
sower+1,,1,,9
soyas
space*1499,175,114,10,12,551,27
spacy+
spade*7,4,2
spake 8,,,1
spang
spank*4
spans*6,5,1,,,7
spare*94,23,26,3,51,1
spark*76,10,6,,4,6
spars*9
spasm*4,3,6
spate+1,2,2
spats*1,1
spawn*4,,8,,,2
spays+
spazz
speak*661,109,105,6,399,6,2
spear*99,6,4,,6
speck*27,7,3,,7
specs+1,,,,,1,1
speed*750,81,103,16,6,20,2
spell*1052,19,23,1
spelt+5,,9,,3
spend*418,53,86,17,36,4,2
spent*522,104,132,13,33,3
sperm*6
spews*1,,,,,1
spice*19,4,11,,3
spics
spicy*10,1
spied*31,,1,,8
spiel+1
spier
spies*15,2,4,,15
spiff
spike*18,2
spiky*4
spill*23,1,4,,,1
spilt*3,,4,,1
spina
spine*35,6,5
spins*52,,3,1
spiny*16
spire*4,5,2
spite*173,56,79,4,14,8,2
spits*5,,1,,2
spitz 1
spivs ,,1
splat+1
splay+
split*159,30,39,3,7,34,11
spoil*59,3,11,1,85,1,1
spoke*512,87,111,5,284,1
spoof+,1
spook*4
spool*26
spoon*92,6,7
spoor
spore*22
sport*117,17,20,3,17
spots*157,31,15,2,5,2,18
spout*29,1,5
sprat 1
spray*72,16,12,1
spree*2,4,3,1
sprig*2,1,,,1
sprit 5
sprog
sprue ,2
spuds*
spued 1
spume ,1,1
spumy
spunk*3,,,1
spurn+1,,1,,8
spurs*25,3,,,,1
spurt*12,2,2
sputa
squab*1
squad*16,17,2
squat*16,7,2
squaw*16,1
squib+1
squid*19,,1
stabs*3,1,2
stack*41,7,1,,,39,6
staff*164,104,129,10,46
stage*344,172,210,13,2,6,3
stags*2,1
stagy
staid+2,1,1
stain*19,6,5,,5
stair*18,2,3
stake*47,20,11,4,2,,6
stale*14,4,3
stalk*72,,1,,2
stall*95,17,8,1,2
stamp*76,8,46,3,2,2
stand*1081,147,158,19,279,41,11
stank*1,,1,,1
staph+
stare*70,14,10,,4,1
stark*10,4,6
stars*713,27,55,1,7
start*1087,153,184,14,1,58,36
stash+
state*1281,563,270,135,15,27,14
stats+
stave*5,2,3,1,,1
stays*90,5,7,,7,7,6
stead*8,5,3,,84
steak*30,8,1,1
steal*83,5,7,2,22
steam*340,17,31
steed*12,1,1,,1
steel*565,39,52,1,1
steep*184,13,6,3,6
steer*79,9,4,1,,,2
stein+,4,,,,,1
stela
stele
stems*171,32,30,3,,8,1
steno
steps*747,115,93,3,59,15,21
stern*75,22,17,4,9,1
stets
stews*3,1,1
stick*502,39,40,1,15,23,6
stied
sties*1
stiff*138,21,19,,1
stile*5,,2
still*3421,781,823,63,248,72,54
stilt*1
sting*34,5,6,1,3
stink*4,3,2,,1
stint*7,6,4,,1
stirs*10,3,4,,14
stoae
stoas
stoat 1,,1
stock*227,141,86,4,5,5,4
stogy
stoic+1
stoke+1
stole*52,10,17,,7
stoma
stomp*8
stone*593,47,75,,196
stony*24,5,2,,2
stood*1387,212,196,4,284
stool*58,8,7,,4
stoop*29,4,,,2
stops*132,7,13,2,5,19,5
store*681,72,42,4,14,8,1
stork*7,,,,5
storm*319,26,31,,34,,1
story*2237,149,201,8,26,60,7
stoup
stout*48,2,13,,3,2
stove*183,15,12,,1
stows*1
strap*41,2,3
straw*213,15,13,,21
stray*38,12,5,,3
strep+2
strew*2,,,,2
strip*230,29,34,2,21,,1
strop+
strum*26,,1
strut*4,2,6,,,14
stubs*6,2,1
stuck*186,23,20,2,3,1,2
studs*8,3,1
study*2581,241,146,11,4,50,25
stuff*129,31,37,2,37,12,8
stump*39,2,3,,7
stung*27,2,4,,1
stunk*,1
stuns*,,1
stunt*31,1,2
styes
style*356,97,101,13,2,140,2
styli
suave+,2,2
sucks*1
sudsy+
suede*3,,1
suers
suets*
suety+
sugar*574,34,61,,,3
suing*3,1
suite*26,21,10,,,,1
suits*108,25,9,4,3
sulfa*2
sulks*,1,1
sulky*3,4,1
sully+
sumac+6,1
summa 1
sumps+,,1
sunny*116,12,12,1
sunup*12
super*21,7,10,,,3,3
supes
supra ,3
suras+
surds+,,,,,1
surer*5,,1
surfs*
surge*20,9,5,1,2
surly*3,2,2
sushi+,2
sutra+
swabs*
swags
swain+1
swami+,1
swamp*64,5,1,1,1
swank+1,1
swans*15,1,,,1
swaps*1
sward 3,,1
sware
swarf ,,1
swarm*24,3,1,,9
swart 2,1
swash+4
swath+2,1,1
swats*
sways*2,,,,1
swear*34,10,11,,59
sweat*105,23,19,,5
swede
sweep*81,15,11,2,17
sweet*319,69,43,,49
swell*49,7,7,,7,1
swept*161,34,28,4,19,1
swift*131,14,19,,29,2
swigs+
swill*,,,1
swims*47,,1
swine*7,3,1,,15
swing*189,20,13,12,1
swipe*,2
swirl*14,2,3
swish*36,,2
swiss+36
swive
swoon*1,,1,,1
swoop*14,2,2,1,2,1
sword*93,6,13,,48
swore*28,14,9,,92
sworn*9,5,4,1,54
swung*164,48,29,2,3
sylph 1
synch+,,,,,1
syncs
synod+1
syrup*50,4,1
tabby+2,,1
table*1502,153,279,,110,108,67
taboo*3,3,1,1
tabor 5
tabus
tacet
tacit+2,2,1
tacks*25,,2
tacky+1
tacos*3
tacts
taels
taffy*3,1
tagua 1,1
tails*123,7,4,,5,,24
taint*3,1,1
taken*1015,281,423,36,350,31,11
taker+1
takes*835,86,98,12,87,81,28
talcs*1,,1
tales*140,19,19
talks*85,18,91,,,1
talky ,1
tally*16,4,3
talon*
talus+
tamed*32,,1,,2
tamer*1
tames*1,,,,1
tamps+
tango+2,2
tangs
tangy*6,1
tanks*110,18,35
tansy+,1
taped*7,1,2
taper*10,3,19,,,1
tapes*18,4,1
tapir*6
tapis ,1
tardy*3,1
tared ,,1
tares+1,,3
tarns+
taros
tarot+
tarps+
tarry*8,1,,,18
tarts*10,,1
tasks*63,29,18,1,4,6
taste*283,57,77,4,28,2
tasty*29,2,3
tater
tatty
taunt*3,4,,1,17
taupe+
tawny*6,4,4,,1
taxed*10,13,7,3,1
taxer+
taxes*104,44,20,15,11,1
taxis*21,3,2
taxol
taxon
teach*254,41,35,1,102,4
teaks
teals*
teams*150,22,10,1
tears*223,34,49,1,63,1
teary 1
tease*28,6,3
teats 1,2
techs+,,1
techy
tecum
teddy*4,2
teems*1,1,,,2
teens*35,5,1,1
teeny+2,,,,,2
teeth*538,103,62,,54
telex+
tells*808,34,47,1,21,51,91
telly+,,5
tempi+,,1
tempo*90,4,7
temps+,,4
tempt*4,2,3,1,4,1
tench ,,1
tends*62,34,15,2,4,7,4
tenet+1
tenon+7
tenor*18,6,6
tense*194,15,11,,1,2
tenth*73,5,12,,68,2,1
tents*85,10,5,1,6
tepee*8
tepid*2,1,3
terce
terms*425,162,235,36,19,51,306
terns+12
terra+3,1,1
terry ,1
terse*,2,2,,,2
tesla+
tests*185,61,105,4,6,16
testy+1
tetra
texas+
texts*9,4,26,,,23,3
thane+
thank*301,33,60,,34,5
thanx 1
thats 2
thaws*7
thees 1
theft*11,10,8,,5
their*13258,2667,2808,349,4889,216,9
theme*183,55,45,1,1,1,1
thens
there*15194,2713,3321,305,2124,427,353
therm
these*11611,1571,1504,100,1128,422,241
theta+,,,,,9,2
thews+
thick*540,67,69,,37,35,2
thief*68,8,7,,28
thigh*21,9,5,1,36
thine*13,1,3,,32
thing*1828,331,341,29,258,74,28
think*4746,433,575,17,81,64,24
thins*3
third*945,182,135,23,208,53,37
thong*6,1,,,4
thorn*27,3,5,,7
those*2647,850,957,136,1669,149,64
thous
three*4413,592,698,78,523,228,97
threw*342,46,54,1,61,,1
throb*6,,1
throe+
throw*281,41,19,6,52,,6
thrum 1
thuds*4,1
thugs+1,2,,2
thumb*150,11,13,,5,3
thump*62,3,2
thunk ,1
thwap 2
thyme*4,,1
tiara*,,3,,2
tibia+2
ticks*18,2,,,,1
tidal*59,1,7
tided
tides*110,4,12
tiers*,3,1,,2
tiffs+
tiger*112,6,3
tight*208,28,32,1,3,7,2
tikes
tikis
tilde+1,,,,,14
tiled*7,4,1
tiler
tiles*44,6,3,,1,,2
tills*,,,,2
tilth ,1,1
tilts*7,2
timed*17,9,2
timer+7
times*2051,264,262,25,207,91,68
timid*23,5,7,1,4
tines*5,2
tinge*2,,2
tings
tinny+4
tints*7,1,3
tippy*
tipsy+,2,1
tired*461,48,52,,2,1,2
tires*85,12,2
tiros ,,1
titan+1,,1
titer+,4
tithe*1,,6,,23
title*302,54,61,1,2,63,2
titre ,,1
titty
tizzy+
toads*89,,1
toady+
toast*57,18,17
today*1923,282,287,33,115,2,1
toddy+
toffs+
toffy
togas*
toile
toils*3,,2,,7
toked+
token*18,10,4,,2,427
toker+
tokes+
tolls*8,2
tombs*29,2,1,1,23
tomes+,1
tommy*1,7
tonal+99,9,2
toned*,,1
toner+,3
tones*275,15,12
tongs*12,1,,,3
tonic*26,1,4
tools*379,34,34,1,,3,9
toons+
tooth*91,20,10,,1
toots*4
topaz*2,,,,5
toped+
toper+
topes+
topic*370,9,12,1,,6,1
topoi
topos
toque ,,2
torah
torch*28,2,10,,6
toric
torsi
torso*1,7,2
torte+
torts+
torus+
total*531,211,197,17,3,45,62
toted*2
totem*11,,1
toter
totes*3
totty
touch*432,86,78,2,43,9,3
tough*173,36,38,13,,1,5
tours*7,9,3
touts+
toves 7
towed*18,1,4
towel*48,6,6,,2
tower*170,12,45,1,31,,15
towns*337,50,90,1,78
toxic*6,3,1,1
toxin+2,1
toyed*,,2
toyer
toyon
trace*151,23,27,,5,6
track*312,37,30,1,2,11
tract*24,15,10,,,4
trade*479,134,214,30,16,2
trail*317,29,13
train*556,77,90,8,6,4
trait*26,3,4
tramp*40,1,2
trams+1,,5,6
trans 9
traps*53,8,3,,1,1
trash*20,2,3
trawl*1,,19
trays*19,3,6,,3
tread*15,5,5,,3
treap
treat*113,26,20,,18,20,3
treed+2
trees*1645,96,99,3,146,,15
treks*2,,,1
trend*27,46,26,12
tress*2
trews
treys
triad+6,1,1
trial*130,119,40,11,15,20,1
tribe*169,4,15,1,253
tribs
trice+3,,2
trick*189,15,25,4,,25,41
tried*1077,170,162,16,42,22,8
trier
tries*123,13,20,2,1,25,1
trike+1
trill*2,3,2
trims*2,1,1
trios*1
tripe*2,,1
trips*152,29,8,4,,,3
trite*1,2,,1
troll*9
tromp+
troop*35,9,3,,3
troth+1,,,,1
trots*6
trout*53,1,3
trove+
trows
truce*8,5
truck*410,56,10,,,1
trued
truer*4,2,1
trues
truly*146,57,32,1,59,6,4
trump+,1
trunk*186,8,8,,1,3
truss*13
trust*103,50,74,3,96,,1
truth*215,124,151,9,191,18,11
tryst+
tsars*2
tuans ,,1
tubal
tubas*
tubby+,,2
tubed
tuber*3
tubes*177,24,14,,1
tucks*8,,2
tufas
tufts*12,1,1
tufty+,,1
tulip*20,3,4
tulle+1,1,1
tummy+3,,3,,,1
tumor*32,13
tunas*
tuned*65,3,2,,,,2
tuner+7
tunes*79,7,4,,1
tunic*4,1,1,,5
tunny
tuple+
turbo+1
turds
turdy
turfs*
turfy
turns*440,38,29,,68,73,87
turps ,,1
tusks*38,3,,,1
tusky ,,1
tutor*16,4,15
tutti ,,1
tutus+
tuxes+
twain*7
twang*8,,1
twats
tweak+,,,,,,1
tweed*6,5,4
tweet+
twerp+
twice*400,74,73,1,30,37,26
twigs*67,1,3,,2
twill+3,,2
twine*19,,,,1
twink
twins*67,8,3,,6
twiny
twirl*6
twirp
twist*81,17,18,,4
twits+
twixt+
tying*22,5,2
tykes+
typal
typed*9,3,4,,,71
types*334,116,92,5,1,75,2
typos+,,,,,1
tyres ,,2
tyros+
tzars+
udder*3
ukase ,,1
ulcer*5,5,2
ulnar
ulnas
ultra*2,,3
umbel
umber+,4
umbra*2
umiak
umped+2
umpty+
unapt
unarc
unarm
unary+1,,,,,2
unate
unban
unbar*1
unbox ,,,,,1
uncap ,1
uncle*196,25,24,1,11
uncut+3,,,,,1
under*2987,703,646,84,410,45,24
undid*8,1,3,,2
undue*5,13,9,2
unfed+1
unfit*9,1,8,,3,1
unfix
unhip
unhit
unify*4,2,3,,,1
union*104,83,148,45,3,2,2
unite*29,10,6,,2
units*483,87,79,2,1,122,1
unity*64,66,44,2,4,3,8
unjam+
unlit+,,1
unman
unmap
unmet+
unpeg
unpin+1
unrig
unsay
unsee ,1
unset+,,,,,3
unsew
unsex
untie*14,2,,,9
until*2596,461,478,47,497,115,56
unwed+,12
unwon
unzip+1
upend+
upped*1,2
upper*327,70,60,10,47,35,91
upset*108,14,21,1,1,3
urban*50,41,28,3
ureas
urged*74,35,27,4,37
urger
urges*7,8,2,,1
urine*20,1,7,,2
usage*42,14,9,4,,19
users*13,6,15,2,,46
usher*7,2,2
using*1825,143,167,9,5,249,123
usual*313,96,118,5,,45,16
usurp*,1
usury+
uteri
utero
utter*25,13,13,,47
uvula
vacua
vacuo
vague*30,25,27,4,,2
vagus+
vails
vales*2,,1
valet*3,2,1
valid*15,22,31,7,3,12,46
valor*5,1,,,31
value*512,200,239,29,22,409,247
valve*40,3,6
vamps+
vaned
vanes*14
vapes
vapid+
vapor*209,12,,,3
varia
vases*10,11,2
vault*16,2,3,,3
vaunt+,,1,,2
veals*
veeps+
veers*2,1,,,,1
vegan+
veils*7,3,4,,3
veins*79,6,16
veiny+
velar
velds
veldt+9,1
venal+
vends*
venom*5,2,1,,7
vents*2,4,2
venue+,,2
verbs*583,7,13
verge*4,2,5,,,,1
versa+8,6,3,1,,6,4
verse*117,28,51,,,4
verso+,,1
verst
verve+3,4,1
vests*1,1
vetch+1
vexed*5,2,4,2,6
vexes*,1
vials*1
viand+
vibes+,1
vicar*2,4,13
vices*,5,1,,1
video*2,2,,,,3
viers
views*117,51,83,12,1
vigil*8,1,1
vigor*27,14,,,6
viler*
villa*5,3,6
ville
villi
vinca
vined
vines*66,8,3,,15
vinyl*3,4
viola*24,2
viols+1
viper*3,,1,,5
viral+1
vireo*
vires ,,4
virus*48,13,5
visas*1
vised
vises*
visit*443,109,143,2,39
visor*6,,1
vista*1,2,2
vitae+2,,1
vital*65,56,40,7,,1
vitam
vitas+
vitro+,4
vivas+
vivid*67,25,14
vivre ,,1
vixen*2
vizor+1
vocab
vocal*94,14,26
vodka+4,,4
vogue*2,4,3,2,,,1
voice*1280,221,271,,463,1
voids*,1
voila+1
voile*2
volts*16,1,1
vomit*4,,1,,1
voted*60,27,11,9
voter*7,4,1,9
votes*54,20,23,27
vouch*2
vowed*8,5,1,,19
vowel*1484,7
vower
voxel
vroom 2
vulva
vying*1,3
wacko+
wacky+,1
waded*33,2,2
wader+
wades*5
wadis+1
wafer*,,1,,3
wafts*
waged*8,7,2,2,4
wager*8,3,,,2
wages*74,42,61,7,41
wagon*356,53,10,,1
wahoo 1
waifs*
wails*2,2,,,2
waist*80,11,16,,3
waits*28,2,,1,11,3
waive*,1,3
waked*8,2,,,1
waken*4,,2
waker
wakes*23,1,,,2
waled+
wales+1,2
walks*140,13,10,,39,2,1
walls*478,70,62,5,108
waltz*29,1,12
wands*2,,1,,1
waned*5,2,1,,1
wanes*2,,,,1
wanly*1,,1
wanna 6,5,3
wanta 6,1
wants*461,71,72,13,3,23,2
wards*3,3,4,1
wares*19,1,2,,14
warms*31,1,1,,2
warns*15,3,4,1,2,2
warps*
warts*4,5,1
warty+9,1
washy
wasps*15,,,,1
waspy
wassa 1
waste*191,35,55,9,120,2,2
watch*969,80,80,4,68,17,2
water*7194,436,387,10,518
watsa 1
watts*7
waved*173,16,13,,13,,1
waver*5,3,,,3
waves*647,51,22,1,41
waxed*173,4,6,,2
waxen*3,1,2
waxer+5
waxes*647
wazoo
weald
weals
weans*
wears*74,5,8,1,3,1
weary*104,16,18,,48,1
weave*53,4,3,,4
webby
weber
wedge*34,4,1,1,1
wedgy
weeds*105,5,8,,12
weedy*4,,3
weeks*526,142,127,24,22,,1
weeny
weeps*3,,,,2
weepy+
weest
wefts
weigh*199,4,9,1,1
weird*32,9,3,,,12,4
weirs ,1
welch+,5
welds*2,,2,,,1
wells*78,4,1,1,5
welsh ,1
welts*2,1,1
wench+3
wends*
wests
wetly 1,1
whack*11,1,1,,,2
whale*274,,8,,1
whams+
whang+
wharf*42,2,6,3
whats
wheal
wheat*342,9,12,10,53
wheee 1
wheel*418,54,32,,27,,4
whelk*1
whelm
whelp*1,,,,2
whens 2
where*5611,930,1041,86,474,300,321
whets*
whews
wheys*
which*14016,3560,4467,347,3083,756,362
whiff*6,1,2
while*2837,679,590,110,384,132,35
whims*1,1,1,1
whine*13,4,2
whiny+1
whips*17,1,1,,6
whipt 1
whirl*26,3,2,,2
whirr+5
whirs*1
whish+
whisk*12,,1
whist+5,,2
white*2085,260,261,14,71,36,1
whits*
whizz+3
whoas
whole*1886,309,435,28,384,52,17
whomp+1
whooo 1
whoop*19,1
whops+
whore 1,2,1
whorl+
whose*799,251,301,40,346,180,103
whoso
whump
wicks*
widen*14,5,3,2,,1
wider*83,17,43,10,,17,1
widow*54,24,28,,70,9
width*149,14,12,,2,250,2
wield*3,1,,,5
wifey
wilco
wilds*13
wiled
wiles*4,2,,,4
wills*8,1,2,,8
wilts*1,,1
wimps+
wimpy+
wince*4
winch+4,,2
winds*367,19,16,,32,,1
windy*53,2,4,1,2
wined+
wines*20,24,25,,1
winey
wings*514,25,29,2,102
winks*3,,,,4
winos+,1
wiped*79,19,14,1,16,2,3
wiper+3
wipes*4,,,,3,2,1
wired*22,11,1
wirer
wires*205,13,6
wised ,,1
wiser*21,7,6,2,9
wises
wisps*8,1,1
wispy*,2
wists
witch*109,5,2
withs
witty*4,10,8
wives*68,17,50,2,15
wizen
woken*,,4
wolds
woman*750,217,243,5,409
wombs*,,,,2
women*592,184,198,14,251,,1
wonks
wonky
wonts
woods*557,24,12,,4,1
woody*24
wooed*1,1,2,,1
wooer+
woofs*1
wools*
wooly*
woosh 3
woozy+1
words*11215,272,315,16,660,184,49
wordy*3,1
works*485,124,165,15,234,90,45
world*2799,687,586,81,353,12,11
worms*119,4,7,,13
wormy*1,1
worry*214,55,44,4,5,13,2
worse*180,50,90,9,37,8,1
worst*108,34,45,8,3,2,4
worth*327,91,118,19,18,13,1
worts
would*11188,2714,2799,460,696,450,102
wound*161,28,26,,31
woven*84,9,3,,13
wowed*1
wowee
wrack+1,1
wraps*12,2,,,1,,1
wrath*17,8,4,,278
wreak*1,1,,,1
wreck*75,8,8,1,1
wrens*8
wrest*4,1,,1
wrier
wring*6,2,1,,2
wrist*61,9,17
write*9846,106,101,3,89,105,125
writs+1,1
wrong*606,129,162,20,101,29,12
wrote*877,181,120,5,103,6,1
wroth ,,,,1
wrung*7,,1,1,2
wryer+
wryly*3,3,4
wurst+
xenon+3,1
xerox+
xored
xylem+1,4
yacht*16,1,17
yahoo
yanks*3,1,,1
yards*360,63,57,,1,1
yarns*63,6,2
yawed 1
yawls 2
yawns+1,,2
yawny 1
yawps 1
yearn*4,1,1,,1
years*3966,958,1086,185,594,9,15
yeast*47,3,1
yecch
yella 7
yells*26
yelps*2,1
yenta
yerba
yeses*
yield*47,35,42,5,52,16,17
yikes+
yipes+1
yobbo
yodel*2,1
yogas*
yogic
yogis+
yoked*2,,,,3
yokel+1,1
yokes*4,,,,2
yolks+12,,1
yolky
yores
young*1920,367,485,22,358
yourn 1
yours*170,25,48,,76,2
youse 1
youth*201,75,48,8,102
yowls*
yoyos+
yucca*12,1
yucky+
yukky
yules*
yummy+3
yurts
zappy
zayin
zeals*
zebra*25,1
zebus+1
zeros*42,2,7,,,2,3
zests*
zesty+
zetas+
zilch+
zincs
zings+
zingy+
zippy+
zloty
zombi
zonal+,,2
zoned*,1
zones*43,3,9,2
zonks
zooey
zooks
zooms*3,1
zowie 4
* End of file "words.dat"
</PRE></BODY></HTML>

\section{Goals and Approaches}
\label{goal}

This research will consider network control for optical TDM
point--to--point networks in multiprocessor environments. There are 
three primary goals of this work: (1) to develop and evaluate
dynamic network control mechanisms which establish connections
(virtual paths)
for the connection requests that arrive at the network dynamically, 
(2) to investigate the feasibility
of applying compiled communication technique in optical multiplexed
interconnection networks, and to address  relevant issues when applying 
compiled communication, and (3) to compare the communication performance
of these two schemes.  Shared memory
programs compiled for  distributed memory machines and 
programs using explicit message passing will be used for the
study of compiled communication. The torus topology will be assumed to be
the underlying physical topology in this research for performance
evaluations. This topology  
has been demonstrated to be an efficient
topology for point--to--point networks
 and is used in many commercial supercomputers. 
Note, however, that many  techniques developed in
this research can also be applied to other topologies.

Optical interconnection networks offer large bandwidth and
low error rates. To fully exploit the large bandwidth in an optical network,
the control overhead must be reduced.
Since dynamic network control must be performed in the electronic domain, 
it is crucial to 
develop efficient network control mechanisms that 
 establish all--optical paths
for connection requests without incurring too much control overhead. 
The study of dynamic network control schemes is 
the first goal of this research. The results of the study will give
general guidelines for the design of dynamic network control schemes
in optical multiplexed interconnection networks. The study will also
lead to the 
understanding of  the impact of system parameters on the control schemes.

One way to reduce the control overhead is to use compiled communication.
Compiled communication reduces runtime control overhead by
managing communications at compile time.  The study of compiled communication
 is the second focus of this
research. A number of issues must be addressed in order to apply 
compiled communication. How to efficiently 
obtain  communication patterns in 
a program?  How to manage network resources when communication patterns
are known? How to handle communication patterns that are 
unknown at compile time?
These are some questions to be answered among others.
The study of compiled communication will try to give solutions to
the problems encountered when applying compiled communication.
The results of the study will give insights into compiled 
communication in optical TDM point--to--point networks as well as the
potential performance gain by compiled communication.

In order to achieve the above goals, the steps outlined below are proposed.

\begin{enumerate}
\item 
  Study  efficient dynamic network control mechanisms for  optical TDM
  interconnection networks.
  \begin{enumerate}
    \item Develop distributed path reservation protocols for optical TDM
     point--to--point networks.
    \item Develop a network simulator that simulates the network behavior
          under all designed protocols.
    \item Study the performance of the protocols and 
          the impact of various
          system parameters, such as system size, message size, etc, on 
          the protocols. Latency and maximum throughput will be used 
          as performance measures.
  \end{enumerate}
\item Design and implement a tool that synthesizes  communication
          patterns from application programs.
   \begin{enumerate}
     \item Design a communication descriptor that can describe
           the communication requirement (logical communication patterns)
           in a program.
     \item Design and implement data flow analysis algorithms for
           communication optimization to obtain communication
           patterns in the optimized code of a program.
     \item Design and implement schemes that derive phyiscal 
                communication patterns from logical communication patterns.
   \end{enumerate}
\item 
  Study   compiled communication and evaluate its 
  performance in optical TDM networks.
  \begin{enumerate}
    \item Design communication scheduling algorithms that can be used by 
          the compiler to schedule the communication patterns that
          are known at compile time.
    \item Design efficient logical topologies that can be used to 
          route messages in the communication patterns that are unknown at
          compile time.
    \item Develop a network simulator that simulates  network behavior
          for networks
          with compiled communication. The simulator should include  
          off--line
          connection scheduling algorithms to establish connections
          for the communication patterns that are known at compile time. The 
          simulator should also be able to simulate  multi--hop
          communication for handling the  communication
          patterns that are unknown at compile time.
    \item Evaluate the performance of compiled communication. 
          This includes the performance  for 
          static patterns,  dynamic patterns, and the overall
          performance for all patterns in programs.
  \end{enumerate}
\item
  Carry out a comparison of dynamic communication and compiled communication
  using actual application programs.
\end{enumerate}
  
\subsection{Dynamic network control}
 
In order to fully explore the potential of optical communication,
optical signals should be transmitted in a pure circuit--switching 
fashion in the optical domain (all--optical communication). No buffering and 
optical-to-electronic or electronic-to-optical conversions should be needed
at intermediate nodes. Dynamic network control processes the connection
requests arriving at the network dynamically and negotiates with the
nodes in the network to establish virtual paths for 
connection requests before the communications can start. In this work, 
distributed path reservation algorithms for 
the establishment of VPs are to be designed and evaluated.
The major issues to be considered are the control overhead, 
the efficiency of the  virtual channel utilization  and the 
hardware overhead.  Some trade--offs among these issues may be  needed to
obtain an efficient algorithm. 

A cycle level network
simulator which simulates the network behavior for networks with
different  
protocols is needed. 
In order to  study all  variants of the protocols, the 
simulator must provide the following capabilities.
\begin{itemize}
\item It must be able to simulate all different protocols designed.
\item It must be able to simulate networks with different parameters, such as
      network size and multiplexing degree.
\item It must be able to simulate protocols with different protocol
      parameters.
\end{itemize}
Using the network simulator, the performance of the 
protocols will be studied, 
the impact of system parameters and protocol parameters
on the protocols will be determined. 
Based upon the results, the protocol parameters can be tuned to obtain 
good performance.


\subsection{A tool to extract communication patterns in a program}

To apply compiled communication, the compiler must be able to obtain the
communication requirements in a program. Traditional methods
analyze communication requirements on the logical processor 
array (logical communication patterns) 
\cite{Hinrichs95,Gupta92} and represent the communications in 
mathematical notations, such as matrix notation in \cite{Hinrichs95}.
Based on the form of the mathematical notation,  communication
patterns on  physical
processors (physical communication patterns)  may be obtained. 
Obtaining physical communication patterns
in this manner may not be sufficient for compiled communication, since
in compiled communication, we would like to know all  communication
patterns that can be synthesized by the compiler. Note that 
communication patterns that are unknown at 
compile time 
result in much larger overhead when 
using compiled communication. 

In this research, a descriptor, which can describe  logical communication 
patterns, is developed. The 
descriptor can be easily calculated from program structures. Traditional
communication optimization algorithms will  be implemented using
this descriptor to represent communications in programs. Note that,
since communication optimization are commonly implemented in compilers,
implementing traditional communication optimizations
will give  more realistic communication patterns in programs.
Once the communication optimizer is implemented,
 logical communication patterns in both optimized and 
non-optimized programs can be obtained and represented in the descriptor.
An efficient scheme will then be designed to derive  physical
communication patterns from the logical communication patterns. 
 Note that brute force methods can be used to derive physical communication
patterns. However, this may result in large
analysis overhead and must be avoided if possible.

\subsection{Compiled communication}

Under compiled communication, the compiler will manage the communications in
a program. For a static pattern, the compile can  insert codes in the 
program to set the network state such that, at runtime, a path will
exist before a communication starts without  dynamic path reservation.
For a dynamic pattern, the compile will route messages in the pattern
through a logical topology.
The following steps will be carried
out to study compiled communication.

\subsubsection{Connection scheduling for static patterns}

Once the compiler knows the communication patterns in a  program, it 
employs  off--line connection scheduling algorithms to manage the 
communications.  Since the communication time of a communication pattern
in an optical TDM system is proportional
 to the multiplexing degree required for 
the communication pattern,
the primary goal of these 
algorithms is to achieve the minimum multiplexing degree that contains all
the connections in the pattern. Different scheduling schemes may be needed for
different patterns. For example, connection scheduling for dense communication
patterns may be different from that for sparse communication patterns.

\subsubsection{Efficient logical topology design for dynamic patterns}

When a logical topology is to be used to route
messages in the communication patterns
that are unknown at compile time, 
 the communication performance depends on three 
factors, the multiplexing degree needed to realize the logical topology, the 
average number of intermediate hops and the processing time in each 
intermediate node. When the logical topology is the same as the physical
topology, no multiplexing is needed (multiplexing degree $= 1$). However,
messages may need to be routed through a large number of intermediate nodes.
When the logical topology is the complete graph, no intermediate hops are
needed for a connection. However, realizing a complete graph may require
a large multiplexing degree. Other choices between these two extremes
may result in better communication performance. 
All possible candidates for the logical topologies (on top of a physical
torus topology) will be examined and compared. 

\subsubsection{Network simulator for compiled communication}

A network simulator will be designed and implemented to study
the communication performance of compiled communication.
The simulator will take the trace and/or communication patterns generated
from compiler analysis of a program as inputs
and simulate compiled communication.
When communication patterns are known at compile time,  off--line 
algorithms will be used to perform the connection scheduling. When 
communication patterns are unknown at compile time, a pre--determined
communication pattern will be used to route the messages. Hence, the
simulator must be able to simulate  single--hop communication for
static patterns and multi--hop communication for dynamic patterns.
Again, it should provide sufficient flexibility similar to the flexibility
of  the simulator for
dynamic communications.

\subsubsection{Performance of compiled communication}

With the simulator, the performance of various off--line
connection scheduling algorithms can be studied and compared.
The performance of various logical topologies can be evaluated. The impact
of various system parameters on compiled communication can also be examined.
Synthesis communication patterns, as well as communication
patterns that come from real application programs, will be used in the 
evaluations.

\subsection{Performance comparison of dynamic communication and compiled 
communication}

Once the above steps are completed, I will be able to compare the 
communication performance of dynamic communication and compiled 
communication using benchmark programs including the floating point benchmark
in SPEC95 and other application programs. This study will quantify
the performance gained by compiling communication for patterns that are known
at compiled time and the performance lost by  routing messages over logical
topologies for  patterns that are unknown 
at compiled time.

\newpage






%% background seciton
\chapter{Background and related work}
\label{backg}

\section{Optical TDM networks}
\label{opnet}

An optical point--to--point network 
consists of switches with a fixed number of input and output ports.
One input port and one output port are used to connect 
the switch to a local processing element and all remaining 
input and output ports of a switch are
used for connections to other switches.   
An example of such networks is the $4 \times 4$ torus shown in 
Figure~\ref{TORI}.
In these networks, each link in the network is time--multiplexed to support 
multiple virtual channels.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/tori.eps,width=3.5in}}
\caption{A torus connected network}
\label{TORI}
\end{figure}

Two approaches can be used to establish connections in multiplexed networks,
namely {\em link multiplexing} (LM) and {\em path multiplexing} (PM) 
\cite{Qiao95}. PM uses the same channel on all
the links along the path to form a connection. On the other hand,
LM may use different 
channels on different links along the path, 
thus requiring time-slot interchange in TDM networks at each 
intermediate node. Fig.~\ref{pmlm} shows the PM and LM connections at a 
$2\times 2$ switch where each link supports two channels.
LM is similar to the multiplexing technique in electronic networks where
a data packet can change channels when it passes a switch. Using LM for 
communication has many advantages over using PM. For example, the path
reservation for a LM connection is simpler than that for a PM connection, and
LM results in better channel utilization. However,
optical devices for LM are still in the research stage and 
are very expensive using current technology. Hence,
this thesis is concerned only with  path multiplexing because
the enabling technology is more mature. 

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/pmlm.eps,height=2.2in}}
\caption{Path multiplexing and link multiplexing}
\label{pmlm}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/pm.eps,width=4.5in}}
\caption{Path multiplexing in a linear array}
\label{PM}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/ts.eps,width=3.5in}}
\caption{Changing the state of a switch in TDM}
\label{TS}
\end{figure}

In order to time--multiplex a network with path multiplexing, 
a {\em time slot} is defined to be a fixed period of time and the time
domain is divided into a repeated sequence of $d$ time slots, where $d$
is  the {\em multiplexing degree}. Different virtual channels on each link
occupy different time slots.
Figures \ref{PM} and \ref{TS} illustrate path multiplexing on a linear
array. In these two figures,  two virtual channels are supported
on each link by dividing the time domain into two time slots, 
and using alternating time
slots for the two channels $c0$ and $c1$.
Let us use $(u,v)$ to denote a connection from node $u$ to node $v$.
Figure \ref{PM} shows four established connections over the two channels,
namely connections
$(0, 2)$ and $(2, 1)$ that are established using channel $c0$, and
connections $(2, 4)$ and $(3, 2)$ that are established using channel $c1$.
The switches, called {\em Time--Multiplexed Switches} (TMS),
are globally synchronized at time slot boundaries, and each switch
is set to alternate between the two states that are needed to realize the
established connections.
For example, Figure~\ref{TS} shows the two states
that the $3 \times 3$ switch attached to PE 2 must realize for the 
establishment of the connections shown in Figure~\ref{PM}.
Note that each switch can be an electro-optical switch
(Ti:LiNbO$_3$ switch, for example
\cite{hinton}) which connects optical inputs to optical outputs without
E/O and O/E conversions. The state of a switch is controlled by 
setting electronic bits in a {\em switch state register}.

The duration of a time slot may be equal to the duration over which 
several hundred bits may be transmitted.
For synchronization purposes, a guard band at each end of a time slot
must be used to allow for changing the state of switches (shifting
a shift register) and to accommodate possible drifting or jitter. For example, 
if the duration of a time slot is $276ns$, which includes a guard band of
$10ns$ at each end, then $256ns$ can be used to transmit data. If the
transmission rate is $1Gb/s$, then a packet of 256 bits can be transmitted
during each time slot. 
Note that
the optical transmission rate is not affected by the relatively
slow speed of changing the state of  switches ($10 ns$)
since that change is performed only every $276 ns$.

Communications in TDM networks can either be {\em single--hop} or 
{\em multi--hop}. In single--hop communication, circuit--switching
style communications  are carried out. A path for a 
communication must be established before the communication starts. 
In general, any $N \times N$ network, other than a completely 
connected network, has a limited connectivity in the sense
that only  subsets, $C = \{ (x, y) | 0\le x, y < N\}$, of the possible
$N^2$ connections can be established simultaneously without conflict.
For single--hop communication the network
must be able to establish any possible connection in one hop, without
intermediate relaying or routing.
Hence,  the network
must be able to change the connections it supports at different times.
This thesis considers switching networks in which the set of connections 
that may be established simultaneously (that is, the state of the network) 
is selected by changing the contents of hardware registers. The single--hop
communication can be achieved in two ways. First, a path reservation
algorithm can be used to dynamically establish and tear down all--optical
connections for arbitrary communications. Second, compiled communication 
uses the compiler to analyze the communication requirement of a program
and insert code to establish all--optical 
connections (at phase boundaries) before communications start.  
Unlike the case in a single--hop system where connections 
are dynamically established and torn down, connections in 
a multi--hop system are fixed and a message may travel through 
a number of lightpaths to reach its destination.
Dynamic single--hop communication,
dynamic multi--hop communication and compiled communication will be discussed
in some details next.

\section{Dynamic single--hop communication with PM}

To establish a connection in an optical TDM  network,
a physical path, $PP$, from the source to the destination is first chosen. 
Then, a virtual path, $VP$, consisting of a virtual channel in each link 
in $PP$ is selected and the connection is established.  The selection 
of $PP$ has been studied extensively and is well understood \cite{Leighton92}.
It can be classified into {\em deterministic} routing, where
$PP$ can be determined from the source node and the destination node
(e.g., X--Y routing on a mesh), 
or {\em adaptive} routing, where  $PP$ is selected from
a set of possible paths. 
Once $PP$ is selected, a time slot is used
in all the links along  $PP$. 

The control in optical TDM networks is responsible for the establishment of a
virtual path for each connection request. 
Due to the similarity of TDM and WDM networks, many techniques for virtual channel assignment in one of these two types of networks can also apply to
the other type.
Network control for multiplexed optical networks can be classified into
two categories, centralized control and  distributed control. 
Centralized control assumes a central controller which maintains the 
state of the whole network and schedules all communication requests. 
Many time slot assignment  and wavelength assignment
algorithms have been proposed for  
centralized control. In \cite{Qiao94} a number
of time slot assignment algorithms are proposed for TDM multi-stage 
interconnection networks. In \cite{Chlamtac92} wavelength assignment for
wide area networks is studied. A time wavelength assignment algorithm for 
WDM star networks is proposed in \cite{ganz92}. Theoretical study
for optimal routing and wavelength assignment for arbitrary networks is
presented in \cite{Ramaswami94}. 

Distributed control does not assume a central controller and thus is more
practical for large  networks. 
Little work has been done on distributed control for 
optical multiplexed networks.
In \cite{Qiao94} a distributed path reservation scheme
for optical Multistage Interconnection  Networks (MIN) is
proposed. Distributed path reservation methods for both path multiplexing and 
link multiplexing are  presented in \cite{Qiao96}. 
This thesis proposes distributed path reservation algorithms that are more
efficient than the previous algorithms, investigates variations in 
channel reservation methods 
and studies the impact of the system parameters on the 
protocols.

\section{Dynamic multi--hop communication with PM}

By using path multiplexing, efficient logical topologies can be established
on top of the physical topology. The connections in the logical topologies
are lightpaths that may span a number of links. In such systems, the 
switching architecture consists of an optical component and an electronic
component. The optical component is an all--optical switch, which can 
switch the optical signal from some input channels to output channels in
the optical domain (i.e., without E/O and O/E conversions), and which can
locally terminate some other lightpaths by directing them to the node's 
electronic component. The electronic component is an electronic packet
router which serves as a store--and--forward electronics overlaid
on top of the optical virtual topology. 
Figure~\ref{ROUTER1} provides a schematic diagram of the architecture of the 
nodal switch in a physical torus topology.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/multiswitch.eps,width=1.5in}}
\caption{A nodal switching architecture}
\label{ROUTER1}
\end{figure}

Since the electronic processing is slow compared to the optical
data transmission, it is desirable to reduce the number of intermediate
hops in a multi--hop network. This can be achieved by having a logical topology
whose connectivity is high. However, realizing a logical topology with a large 
number of connections requires a large multiplexing degree. In a TDM system,
large multiplexing degree results in a large time to transmit a 
packet through a lightpath because every light path is established only
for a fraction of the time. Hence, there exists a performance trade--off
in the logical topology design between a logical topology with large 
multiplexing degree and high connectivity  and  a logical topology with
small multiplexing degree and low connectivity. As will be shown in 
this dissertation, both topologies have advantages for certain types
of communication patterns and system settings.

Multi--hop networks have been extensively studied in the area  of
WDM wide area networks. The works in 
\cite{Bannister90,chlamtac93,Labour91,Mukherjee94,Venkat96} 
consider the realization of logical topologies on optical multiplexed 
networks. These works consider wide area networks and focus on 
designing efficient logical topologies on top of irregular networks. 
Since finding an optimal logical topology on irregular networks
is an NP--hard problem, heuristics and 
simulated annealing algorithms are used to find suboptimal schemes.
This dissertation considers regular networks in multiprocessor 
environments and derives optimal connection scheduling schemes for 
realizing hypercube communications. Besides logical topology design,
connection scheduling algorithms can also be used to realize 
logical topologies. In \cite{Qiao96} message scheduling for permutation
communication patterns in mesh--like networks is considered. 
In \cite{Qiao94} optimal schemes for realizing
all--to--all patterns in multi-stage networks are presented. 
In \cite{Hinrichs94} message scheduling for all--to--all communication 
in mesh--like topologies is described. 

The performance of multi--hop
networks has also been previously studied. However,  
most previous performance studies for optical multi--hop networks assume 
a broadcast based underlying WDM network, such as an optical star network 
\cite{kovacevic95,Sivarajan91}, 
where the major concerns are the number of transceivers in each
node and  the tuning speed of the
transceivers. This thesis studies the logical topologies
on top of a physical torus topology in a TDM network, 
where the major focus is the 
trade--off between the multiplexing degree and the connectivity of a topology.

\section{Compiled communication}

Compiled communication has recently drawn the attention of several 
researchers \cite{Cappello95,Hinrichs95}. Compiled communication
has been used in combination with message passing in the iWarp system 
\cite{Gross89,Gross94,Hinrichs95a}, where
it is used for  specific subsets 
of static patterns. All other communications are handled using  
message passing. The prototype system described in \cite{Cappello95} 
eliminates the cost of supporting multiple communication models. It relies
exclusively upon compiled communication. However, the performance of this
system is severely limited due to frequent dynamic reconfigurations of 
the network. 
%This frequency can be reduced in  optical networks through
%the use of TDM. 
Compiled communication is more beneficial in optical multiplexed 
networks. Specifically, it reduces the control overhead, which is 
one of the major factors that limit the communication performance 
in optical networks. Moreover, multiplexing, which is natural in optical
interconnection networks, enables a network to support simultaneously more
connections  than  a non--multiplexed  network, which
reduces the reconfiguration overhead in compiled communication.

The communication patterns in an application program can be broadly
classified into two categories: {\em static patterns} that can be
recognized by the compiler and {\em dynamic patterns} that are only 
known at run-time. For a static pattern, compiled
communication computes a minimal set of network configurations that 
satisfies the connection requirement of the pattern and thus,  
handles  static patterns with high efficiency.
Recent studies \cite{Lahaut94} have shown that about 95\% of the 
communication patterns in scientific programs are static patterns.
Thus, using the compiled communication technique to improve the 
communication performance for the static patterns is likely to 
improve the overall communication performance. 
Some advantages of using compiled communication for
handling static patterns are as follows.

\begin{itemize}
\item Compiled communication totally eliminates the
path reservation and the large startup overhead associated with
the path reservation. 

\item The connection scheduling algorithm is executed 
off-line by the compiler. Therefore,  complex strategies 
can be employed to improve network utilization. 

\item No routing decisions are made at runtime which means that 
the packet header can be shortened causing the network 
bandwidth to be utilized more effectively.

\item Optical networks efficiently support multiplexing
which reduces the chance of network reconfigurations due to the lack of
network capacity. 

\item Compiled communication adapts to the 
communication requirement in each phase. For example, it can use different
multiplexing degrees for different phases in a program. In contrast,
dynamic communications always use the same configuration to handle 
all communications in a program which may not be optimal.

\end{itemize}

In order to apply compiled communication to a large scale multiprocessor 
system, three main problems must be addressed: 

\begin{description}
\item
{\bf Communication Pattern Recognition:}
This problem has been considered by 
many researchers since information on communication patterns has been
previously used to perform communication optimizations
\cite{Bromley91,Gupta92,Hinrichs95,Li91}. The stencil compiler 
\cite{Bromley91} for CM-2 recognizes {\em stencil} communication
patterns. Chen and Li \cite{Li91} incorporated a pattern extraction 
mechanism in a compiler to support the use of collective communication 
primitives. Techniques for recognizing a broad set of communication 
patterns were also proposed in \cite{Gupta92}. 
However, most of these methods determine a specific subset of static
communication
patterns, such as the broadcast pattern and the nearest neighbor pattern, 
which is  not sufficient for compiled communication. Since the 
communication performance of compiled communication relies heavily
on the precision of the communication analysis, it is desirable to 
perform more precise analysis that can recognize arbitrary communication
patterns. Furthermore, compiled communication
requires the partitioning of a program into phases, such that each 
phase contains communications that can be supported by the underlying 
network, and the scheduling of connections within each phase. These are
new problems that must be addressed.

\item
{\bf Compiling Static Patterns:} Once the compiler determines a 
communication pattern within each phase, which is called a 
{\em static pattern}, the compiler must be able to schedule the
communication pattern on the multiplexed network. 
In TDM networks, communication performance
is proportional to the multiplexing degree. Given a 
communication pattern, the smaller the multiplexing degree, the
less time the communication lasts. Thus, connection scheduling
algorithms that schedule all connection requests in a phase with
a minimal multiplexing degree must be designed to handle the static
patterns. It has been shown that optimal message scheduling 
for arbitrary topologies is NP-complete \cite{Chlamtac92}.
Hence, heuristic  algorithms that provide good performance need to be 
developed.

\item
{\bf Handling Dynamic Patterns:} A number of techniques can be used to
handle dynamic communication patterns. 
One approach is to setup all-to-all connections among 
all nodes in the system. This way each node has a time slot to 
communicate with every other node. However, establishing paths for 
the all-to-all communication can be prohibitively expensive for large systems.
An alternative is to perform dynamic single--hop or multi-hop communications. 
The dynamic communications are not as efficient as compiled communication. 
However, since this method is not used frequently, its
effect on the overall performance is limited. 

\end{description}

\section{Programming and machine model}

Compiled communication requires the compiler to extract  communication
patterns from application programs. 
The method to extract communication patterns in a program
depends on both programming model and  machine model.
The programming model includes the ones using
explicit communication primitives and the ones that require implicit
communication through remote memory references. There are two 
different machine architectures, the shared memory machine and the
distributed memory machine. Communication requirements for these two
machine models are different for a program.  In 
shared memory machines with hardware cache coherence, communications
result from cache coherence traffic, while 
in distributed memory machines,
communications  result from data movements  between
processors. 

{\bf Explicit communication}:
Most of the current commercial distributed memory
supercomputers support the explicit communication programming
model. In such programs, programmers explicitly use  communication 
primitives to perform the communication required in a  program.
The communication primitives can be high level library routines, such 
as PVM \cite{PVM94} or MPI \cite{MPI93}, or low level communication primitives
such as the shared memory operations in the CRAY T3D \cite{Numrich94} and
the CRAY T3E. Communication
patterns in a  program with explicit communication
primitives can be obtained from the
analysis of the communication primitives in the program.

{\bf Implicit communication}: Managing explicit communication is tedious
and error-prone. This has motivated considerable research towards 
developing compilers that relieve programmers from  the burden of 
generating communication
\cite{amarasinghe93,banerjee95,gupta95,hiran92,rogers89,zima88}.
Such compilers take sequential or shared memory 
parallel programs and generate Single Program Multiple Data (SPMD)
programs with explicit message
passing. This type of programs will be referred to as 
{\em shared memory programs}.
Shared memory programs can be compiled for execution on both
distributed memory machines and shared memory
machines. In the case when a program is to be run on a distributed memory
machine,
the communication requirements of the program can be obtained from 
memory references. 
If a program is to be run on a shared memory machine,
the communication requirements
depend on the cache behavior. However,
a superset of the communication patterns may be obtained by examining the
memory references in the program. 
This work will consider data parallel
shared memory programs compiled for execution on
distributed memory machines. 
Compilers that exploit task parallelism \cite{Gross94a,Subhlok93}
are not considered. However, similar techniques may also apply to 
task parallel programs.

\section{Compilation for distributed memory machines}
\label{comp}

While communication requirements of a shared memory
program can be obtained by analyzing
the remote memory references in the program, the actual communication patterns
in the program depend on the compilation techniques used. 
To obtain realistic communication patterns,
 compilation techniques for compiling 
shared memory programs for  distributed memory machines
 must be considered. 
The most important issues to be addressed
when compiling for distributed memory machines
are data partitioning,  code generation and 
communication optimization. This section surveys previous work
 on these issues.

{\em Data partitioning} decides the distribution of array elements
to processors. There are
two approaches for handling the data partitioning problem. 
The first approach is to add user directives to programming languages
and let the users  
specify the data distribution. This approach is used in Fortran D 
\cite{hiran92}, Vienna Fortran \cite{Chapman92} and High Performance
Fortran (HPF) \cite{HPF} among others. 
It uses  human knowledge of  
application programs and simplifies the compiler design. However, using this
approach requires programmers to work at a low level abstraction 
(understanding the detail of memory layout). Since the best placement 
decision will vary between different architectures, with explicit user
placement, the programmer must reconsider the data placement for each
new architecture. Hence, many algorithms have been developed to
perform automatic data distribution. An algorithm has been designed 
for the CM Fortran compiler that attempts to minimize and identify 
alignment communications in data parallel Fortran programs \cite{Knobe90}.
Similar algorithms have been proposed in 
\cite{Chatterjee93,Gupta92a,Hinrichs95,Li91a}.
Data partitioning directly affects the communication
requirements in a program running on a distributed memory machine. Once 
data partitioning is decided, the minimum requirement of data movements in
a program, which results in communications, is fixed.

{\em Code generation} generates the communication code to ensure the
correctness of a program. The {\em Owner computes} rule is generally used 
for distributing the computation onto processors. 
Under owner computes rule, the owner of the array
element on the
left hand side of an assignment statement executes the statement. Thus, the
owner of an array element on the right hand side of the assignment statement
must send the element to the owner of the left hand side, which results
in communication. Without considering efficiency, a simple scheme
can be used to generate the correct SPMD
 code by inserting guarded communication
primitives \cite{rogers89}. 
However, the communication and synchronization overhead of this scheme can 
be so
large that there may be 
no benefit for running the program on a multiprocessor system.
Several researchers have 
proposed techniques for generating efficient code for array statements, 
given {\em block}, {\em cyclic} and {\em block--cyclic} distributions. 
In \cite{Koelbel90,Koelbel91} compile time analysis of array 
statement with block and cyclic distribution is presented.
In \cite{Chatterjee93a} Chatterjee et al. present a framework for compiling
array assignment statements in terms of constructing a finite state machine. 
This method handles block, cyclic and block--cyclic distributions. Method
in \cite{Stichnoth93} improves Chatterjee's method in terms of buffer space and
communication code generation overheads. Other compilers
\cite{amarasinghe93,banerjee95,gupta95,hiran92,rogers89,zima88} use
communication optimization to generate efficient code for programs
on distributed memory machines. Different ways of code 
generation result in different communication patterns at runtime.
 For example, the compiler may 
decide to send/receive all elements in an array to speed up the 
communication. It may also decide to send/receive one element
 at a time to save buffer space. 

{\em Communication optimizations} reduce the cost of  communication 
in a program.
Communication performance not only
affects the performance of a  parallel application but also limits
its scalability. Therefore, communication optimization is 
crucial for the performance of programs compiled for a
distributed memory machine. Many communication optimizations are applied
within a single loop using data dependence information. Examples of such
optimizations include message vectorization \cite{hiran92,zima88}, 
collective communication \cite{Gupta92,Li91}, message coalescing
\cite{hiran92} and 
message pipelining
\cite{gupta95,hiran92}. Earlier methods are based on
 {\em location based} data dependence, which is not precise since it 
only determines whether two references refer to the same memory 
location. Later schemes refine the information and use 
{\em value based} data dependence \cite{amarasinghe93}.  
In value based data dependence, a read
reference depends on a write reference only if the write provides 
the value
for the read reference.

Communication optimizations based only on  data dependence information
usually result in redundant communications \cite{Chakrabarti96}. 
The more recently developed
optimizations use data flow information to reduce redundant communication
and perform other optimizations. In \cite{Gong93} 
a data flow framework which can integrate
a number of communication optimizations is presented. However, the method
 can only apply to a very small subset of programs 
which are constrained
in the forms of loop nests and  array indices. In \cite{Gupta96} a unified 
framework which uses global array data flow analysis for communication 
optimizations is 
described. Since only a very simplified version of the analysis algorithm
is implemented, it is not clear whether this approach is practical for large
programs. In \cite{Chakrabarti96,Kennedy95} methods 
that combine traditional data flow analysis 
techniques with data dependence analysis for performing global
communication optimizations are described. 
These schemes are very efficient in terms of their 
analysis cost since bit vectors are used to represent  data flow 
information. However, they cannot obtain the array data flow information 
that is as 
precise as the information computed using
array data flow analysis approaches.
Communication optimization changes the communication behavior of a program.
Since many communication optimizations are commonly used in production
compilers, these optimizations
must be considered to obtain realistic communication patterns in a program.

%Notice that most of the communication optimizations
%described above  assume the traditional communication model, 
%which might not be true in compiled communication. 
%Therefore, it is interesting to investigate
%useful communication optimizations under the assumption of
%compiled communication.
%\newpage

%\section{Chapter summary}

%In this chapter, I describe the multiplexing techniques used in optical
%interconnection networks, discuss the issues in dynamic single--hop
%communication, dynamic multi--hop communication and compiled 
%communication and survey previous results in these areas. 
%I have also summarized the related research in the 
%compilation for distribution memory machines. This chapter gives 
%the background and surveys previous research related to the 
%thesis. In the next chapter, I will discuss the techniques for 
%dynamic single--hop communication. 



\chapter{Compiled communication}
\label{compiled}

%Recently many researchers have shown that communication performance
%of dense matrix  applications can be greatly improved 
%by allowing network resources to be managed by compiler and using 
%the {\em compiled communication} technique\cite{Cappello95,Hinrichs95,Yuan96}.
In compiled communication, the compiler analyzes a program to determine
its communication requirement. The compiler can then
use the knowledge of the underlying
architecture, together with the knowledge of the communication requirement,
to manage network resources statically. As a result, 
runtime communication overheads, such as the path reservation overhead 
and the buffer allocation overhead, can be reduced or eliminated, 
and the communication performance can be improved.
Due to the limited resources, the underlying network  
cannot support arbitrary communication patterns. 
Thus, compiled communication
requires the compiler to analyze a program and partition
the program into phases such that each phase has a fixed, pre-determined 
communication pattern that the underlying network can support.
The compiler inserts code to reconfigure the network at  
phase boundaries, uses the knowledge of the 
communication requirement within each
phase to manage network resources directly, 
and optimizes the communication 
performance. 

A number of compiler issues must be addressed 
in order to apply the compiled communication technique 
to optical TDM networks.  Specifically, 
given a multiplexing degree, the compiler must
partition a program into phases such that each phase contains connections that
can be realized by the underlying network with the given multiplexing degree.
To obtain good performance, each phase must contain as much communication
locality as possible so that less reconfiguration overhead will be incurred
at runtime. A compiler, called the E-SUIF (extended SUIF) compiler, 
is implemented to support compiled communication.
The structure of the compiler is shown in Figure~\ref{overall}. 
There are four major components in the 
system. The first component is the {\em communication analyzer} that
analyzes a program and obtains
its communication requirement on virtual processor
grids. The second component is the {\em virtual to physical processor 
mapping} subsystem that computes the 
communication requirement of a program on physical processors.
The third component is the {\em communication phase analysis} subsystem that
partitions the program into phases such that each phase contains 
communications that the underlying network
can support. The communication phase analysis utilizes a fourth component
of the system, the {\em connection scheduling algorithms}, to realize 
a given communication pattern with a minimal number of channels. 


\begin{figure}
\centerline{\psfig{figure=fig/overall.eps,height=3.5in}}
\caption{The major components in the E--SUIF compiler}
\label{overall}
\end{figure}
 
Next, the programming model of the compiler will be discussed, followed
by the four components needed to support compiled communication.

\section{Programming model}

The E--SUIF compiler considers structured HPF--like programs 
that contain conditionals 
and nested loops, but no arbitrary goto statements. The programmer
explicitly specifies the data alignments and distributions. 
For simplicity, this chapter assumes that all arrays are aligned to a 
single virtual processor grid template, and the data distribution is 
specified through the distribution of the template.
However, the implementation of the communication analyzer
handles multiple virtual processor grids. 
Arrays are aligned to the virtual processor grid by 
simple affine functions. The alignments allowed are scaling, 
axis alignment and  offset alignment.  The mapping from a point 
$\vec{d}$ in data space to the
corresponding point $\vec{e}$ on the virtual processor grid is specified by
an alignment matrix $M$ and an alignment offset vector $\vec{v}$. 
$\vec{e} = M \vec{d} + \vec{v}$. 
The alignment matrix $M$ specifies the scaling and the axis alignment, thus
it is a permutation of a diagonal matrix. 
The distribution of the virtual 
processor grid can be cyclic, block or block--cyclic. Assuming that there
are $p$ processors in a dimension, and the block size of that 
dimension is $b$, the virtual processor $e$ is in physical processor
$e\ mod\ (p*b) / b$. For cyclic distribution, $b=1$. For block distribution,
$b=n/p$, where $n$ is the size of the virtual processes along the dimension. 


The communication analyzer performs communication optimizations
on each subroutine. 
A subroutine is represented by an {\em
interval flow graph} $G = (N, E)$, with nodes N and edges E.
The communication optimizations are 
based upon a variant of Tarjan's intervals \cite{Tarjan74}.
The optimizations require that there are no {\em critical edges}
which are edges that 
connect a node with multiple outgoing edges to a node with multiple 
incoming edges. The critical edges can be eliminated by edge splitting 
transformation\cite{Gupta96}. Figure~\ref{EXAMPLE} shows
an example code and its corresponding interval flow graph.
%Notice that intervals can be easily identified
%in SUIF's hierarchical intermediate representation.


\begin{figure}[tbph]
%\begin{subfigRow*}
\begin{minipage}{10cm}
%\small
%\footnotesize
\begin{tabbing}
\hspace{0.5in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
\hspace{0.5in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
\hspace{0.5in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
\hspace{0.5in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
\hspace{0.5in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
\hspace{0.5in}(s4)\hspace{0.1in}\>enddo\\
\hspace{0.5in}(s5)\hspace{0.1in}enddo\\
\hspace{0.5in}(s6)\hspace{0.1in}do i = 1, 100\\
\hspace{0.5in}(s7)\hspace{0.1in}\>do j = 1, 100\\
\hspace{0.5in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
\hspace{0.5in}(s9)\hspace{0.1in}\>enddo\\
\hspace{0.5in}(s10)\hspace{0.1in}enddo\\
\hspace{0.5in}(s11)\hspace{0.1in}do i = 1, 100\\
\hspace{0.5in}(s12)\hspace{0.1in}\>do j = 1, 100\\
\hspace{0.5in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
\hspace{0.5in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
\hspace{0.5in}(s15)\hspace{0.1in}\>end do\\
\hspace{0.5in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
\hspace{0.5in}(s17)\hspace{0.1in}end do\\
\end{tabbing}

\end{minipage}

\begin{minipage}{10cm}
\centerline{\psfig{figure=fig/3.eps,width=4in}}
\end{minipage}
%\end{subfigRow*}
\normalsize
\caption{An example program and its interval flow graph}
\label{EXAMPLE}
\end{figure}

\section{The communication analyzer}
\label{commlab}

The communication analyzer analyzes the communication
requirement on virtual processor grids and performs a number of common
communication optimizations. This section
presents the data flow descriptor used in the analyzer 
to describe communication, the general data flow algorithms to propagate
the data flow descriptor, and the communication optimizations
performed by the analyzer. 

\subsection{Section communication descriptor (SCD)}

In order for the compiler to analyze the communication requirement of a 
program, data structures must be designed for the compiler to 
represent the communications in the program. 
The data structures must both be powerful enough to represent 
the communication requirement and simple enough to be  manipulated easily.
%A communication descriptor, call {\em Section Communication Descriptor} (SCD),
%is designed for the analyzer.
%It can be used for both communication 
%optimization and to derived the communication pattern in physical processor
%space. 

\subsubsection*{The descriptor}

The communication analyzer represents communication using
{\em Section Communication Descriptor} (SCD).
% This subsection
%describes the format of the descriptor.
A $SCD = <A, D, CM, Q>$ consists of
 three components.
The first component is the array region that is involved in the
communication. This includes the array name $A$ and the array region
descriptor $D$. The second component is the communication mapping descriptor
$CM$, which describes the source--destination relationship of 
the communication.
The third component is a qualifier descriptor $Q$, which specifies the time
when the communication is performed.

The {\em bounded regular section descriptor} (BRSD)\cite{callahan88} is used
as the region descriptor. The region $D$ is a vector of subscript values.
Each element in the vector is either
(1) an expression of the form $\alpha*i + \beta$, where
$\alpha$ and $\beta$ are invariants and i is a loop
index variable, or (2) a triple
$l:u:s$, where $l$, $u$ and $s$ are invariants. The triple,
 $l:u:s$, defines a set of values, $\{l$, $l+s$, $l+2s$, ..., $u\}$, as used
in the array statement in HPF.

The source--destination mapping $CM$ is denoted as
$<src, dst, qual>$. The source, $src$, is a vector whose elements are
of the form $\alpha*i + \beta$, where $\alpha$ and $\beta$ are
invariants and $i$ is a loop index variable. The destination,
$dst$, is a vector whose elements are of the form
$\gamma*j + \delta$, where
 $\gamma$ and $\delta$ are
invariants and $j$ is a
loop index variable. The {\em mapping qualifier} list,
$qual$, is a list of range descriptors. Each range descriptor
is  of the form $i = l:u:s$, where $l$, $u$ and $s$ are invariants and
$i$ is a loop index variable.
The notation $qual=NULL$ and $qual = \perp$ denote that
no mapping qualifier is needed.
The mapping qualifier specifies the range of a variable in $dst$ that
does not occur in $src$ to express the broadcast effect.

The qualifier $Q$ is a range descriptor of the form $i = l:u:s$,
where $i$ is the loop index variable of the loop that directly
encloses the SCD. This qualifier is used to indicate the
iterations of the loop in which the SCD should be performed.
If the SCD is to be performed in every iteration in the loop,
$Q=NULL$ or $Q = \perp$. $Q$ will be referred to as the
{\em communication qualifier}.
Notice that the qualifiers in most SCDs are NULL.

\subsubsection*{Operations on SCD}

Operations, such as intersection, difference and union, on SCD descriptors
are defined next. Since in many cases, operations do not have sufficient
information to yield  exact results, {\em subset} and {\em superset} versions 
of these operations are implemented. The analyzer uses a proper
version to obtain conservative approximations. These operations
are extensions of the operations on BRSD. 
%Since SCD 
%descriptors are composed of three parts, their operations usually reduce to
%operations on their components. 

\noindent
{\bf Subset Mapping testing}. Testing whether a mapping is a subset of another
mapping is one of the most commonly used operations in the analyzer.
Testing that a mapping relation $CM_1$ ($= <s_1, d_1, q_1>$) is a subset of
another mapping relation $CM_2$ ($= <s_2, d_2, q_2>$) is
done by checking for a solution of equations $s_1 = s_2$ and $d_1 = d_2$, 
where variables in $CM_1$ are treated as constants and variables in $CM_2$ 
as variables,  and
subrange testing $q_1 \subseteq q_2$. 
%The equations 
%$s_1 = s_2$ and $d_1 = d_2$ can easily be solved by 
%treating variables in $M_1$ as constants and 
%variables in $M_2$ as variables. 
Note that since the elements
in $s_1$ and $s_2$ are of the form $\alpha*i+\beta$, the equations can
generally be solved efficiently. Two mappings, $CM_1$ and $CM_2$ are 
{\em related}
if $CM_1 \subseteq CM_2$ or $CM_2 \subseteq CM_1$. Otherwise, they are
unrelated.

\noindent
{\bf Subset SCD  testing}. Let
$S_1=<A_1, D_1,CM_1, Q_1>$, $S_2=<A_2, D_2,CM_2, Q_2>$,
$SCD_1 \subseteq SCD_2 \Longleftrightarrow A_1 = A_2 \wedge
D_1 \subseteq D_2\wedge CM_1 \subseteq CM_2 \wedge Q_1 \subseteq Q_2$.

\noindent
{\bf Intersection Operation}. The intersection of two SCDs represents the 
elements constituting the common part of their array sections that have the 
same mapping relation. The following algorithm describes the subset version of
the intersection operation. 
%The superset version can just return any of the two SCDs.
Note that the operation requires the qualifier $Q_1$ to be equal to $Q_2$ to 
obtain a non empty result. $\phi$ denotes an empty set.
This approximation will not hurt the 
performance significantly since most 
SCDs have $Q=\perp$.

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> \cap <A_2, D_2, CM_2, Q_2>$\\ 
\hspace{0.2in}= $\phi$,  if $A_1 \ne A_2$ or $CM_1$ and $CM_2$ are 
                             unrelated or 
                            $Q_1 \ne Q_2$\\
\hspace{0.2in}= $<A_1, D_1\cap D_2, CM_1, Q_1>$,  if $A_1 = A_2$ and $CM_1 
                                            \subseteq CM_2$ and $Q_1 = Q_2$\\
\hspace{0.2in}= $<A_1, D_1\cap D_2, CM_2, Q_1>$,  if $A_1 = A_2$ and $CM_1 
                                            \supseteq CM_2$ and $Q_1 = Q_2$
\end{tabbing}

\noindent
{\bf Difference Operation}. The difference
 operation causes a part of the array 
region associated with the first operand to be invalidated at all the 
processors where it was available. In the analysis, the difference operation
is only used to subtract  elements killed (by a statement, or by a 
region), which means that the SCD to be subtracted always has 
$CM=\top$ and  $Q=\perp$. 

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> - <A_2, D_2, \top, \perp>$\\
\hspace{0.2in}= $<A_1, D_1, CM_1, Q_1>$,  if $A_1 \ne A_2$\\
\hspace{0.2in}= $<A_1, D_1 - D_2, CM_1, Q_1>$, if $A_1 = A_2$.
\end{tabbing}


\noindent
{\bf Union operation.} The union of two SCDs represents the 
elements that can be in either part of their array section.
This operation is given by:

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> \cup <A_2, D_2, CM_2, Q_2>$\\
\hspace{0.2in}= $<A_1, D_1\cup D2, CM_1, Q_1>$, if $A_1 = A_2$ and 
                                            $CM_1 = CM_2$ and $Q_1=Q_2$\\
\hspace{0.2in}= list($<A_1, D_1, CM_1, Q_1>$, $<A_2, D_2, CM_2, Q_2>$), 
                otherwise.
\end{tabbing}

\subsection{A demand driven array data flow analysis framework}
\label{arraydflow}

Many communication optimization opportunities can be uncovered by
propagating SCDs globally. For example, if a SCD
can be propagated from a loop body to the loop header without being killed
in the process of propagation, the communication represented by the SCD
can be hoisted out of the loop body, that is,
 the communication can be vectorized.
Another example is the redundant communication elimination.
While propagating $SCD_1$,  if $SCD_2$ is encountered such that $SCD_2$ is 
a  subset of the $SCD_1$, then the communication represented by $SCD_2$ can
be subsumed by the communication represented by $SCD_1$ and can be eliminated.
Propagating  SCDs backward can find the earliest point to place the 
communication, while propagating SCDs forward can find the latest point where
the effect of the communication is destroyed. Both these
two propagations are useful in communication optimizations. 
Since forward and backward propagation
are quite similar, only backward propagation will be presented next.

Generic demand driven algorithms are developed to propagate
SCDs through interval flow graph. The analysis technique is
the reverse of the interval-analysis \cite{gupta93}.
Specially, by reversing the information flow associated with program points,
a system of request propagation rules is designed.
SCDs are propagated 
until they cannot be propagated any further, 
that is, all the elements in the SCDs are
killed. However, in practice, the compiler may choose to 
terminate the propagation prematurely to
save analysis time while there are still elements in SCDs. 
In this case, since the analysis starts from the 
points that contribute to the  optimizations,
the points that are textually close to the starting points, where
most of the optimization opportunities are likely to be 
present, are considered.
This gives the demand driven algorithm the ability to trade precision for
time. In the propagation, at a given time, only a single interval
is under consideration. Hence, the propagations are logically done in
an acyclic flow graph. During the propagation, a 
SCD may expand when it is propagated out of a loop. When a set of
elements of SCD is killed inside a loop, the set is propagated into the loop
to determine the exact point where the elements are killed. There are 
two types of propagations,
{\em upward} propagation, in which SCDs may need to be 
expanded, and {\em downward} propagation, in which SCDs may need to be 
shrunk. 

The format of a data flow {\em propagation request}
is  $<S, n, [UP|DOWN], level, cnum>$, where S is a SCD, n is a node
in the flow graph, constants $UP$ and $DOWN$ indicate whether the request is  
upward propagation  or downward propagation, $level$ indicates
at which level is the request and the value $cnum$ 
indicates which child node of 
$n$ has triggered the request. A special value $-1$ for $cnum$ is used as
the indication of the beginning of downward propagation.
The propagation request triggers 
some local actions and causes the propagation of a SCD from the node n. 
The propagation of SCDs follows the following rules. It is  assumed that node 
$n$ has $k$ children.

\subsubsection*{Propagation rules}

\noindent
{\bf RULE 1: upward propagation: regular node}. 
The request on a regular node takes an action based
 on SCD set $S$ and the local
information. It also propagates the information upward.
The request stops when S become empty. The rule is shown in the following 
pseudo code. In the code, functions $action$ and $local$
are depended on the type of optimization to be performed.
The $pred$ function finds all the nodes that are  predecessors in the 
interval flow graph and the 
set $kill_n$ includes all the elements defined in node
$n$. Note that $kill_n$ can be represented as an SCD.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S_1, n, UP, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, UP, level, k>$) : \\
\hspace{0.2in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.2in}\>action(S, local(n))\\
\hspace{0.2in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - kill_n, m, UP, level, j>$)\\
\end{tabbing}

A response to requests in a node $n$ 
occurs only when all its successors have been processed. This 
guarantees that in an acyclic flow graph
each node will only be processed once. The side effect is that 
the propagation will not pass beyond a branch point.
A more aggressive scheme can 
propagate a request through a node without checking whether
all its successors are processed. In that scheme, however, a nodes may need 
to be processed multiple times to obtain the final solution.    

\noindent
{\bf RULE 2: upward propagation: same level loop header node}.
The loop is contained in the current level. The request needs to obtain the
summary information, $K_n$, for the interval, perform the action
based on $S$ and the summary information, propagate the information past 
the loop and trigger a downward propagation to propagate the information
into the loop nest. Here, 
the summary function $K_n$, summarizes all the elements defined
in the interval. 
It can  be calculated either before hand or in a 
demand driven manner. The method to 
calculate the summary in a demand driven manner will be described later. 
Note that a loop header can only have one successor besides the 
entry edge into the loop body. The $cnum$ of the downward request
is set to -1 to indicate that it is the start of the downward propagation.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{0.2in}\>action(S, $K_n$) \\
\hspace{0.2in}\>if\=\ ($S - K_n \ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - K_n, m, UP, level, j>$)\\
\hspace{0.2in}\>if ($S\cap K_n \ne \phi$) then\\
\hspace{0.2in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$)\\
\end{tabbing}

\noindent
{\bf RULE 3: upward propagation: lower level loop header node}.
The relative level between the propagation request
and the node can be determined by 
comparing the level in the request and the level of the node. Once a request
reaches the loop header. The request will need to be expanded to be 
propagated in the upper level. At the same time, this request triggers 
a downward propagation for the set of elements that are killed in the loop. 
Assume that the loop index variable is $i$ with bounds $low$ and $high$.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{0.2in}\>calculate the summary of loop $n$\\
\hspace{0.2in}\>outside = $expand(S, i, low:high) - 
                     \cup_{def}expand(def, i, low:high)$\\ 
\hspace{0.2in}\>inside = $expand(S, i, low:high) \cap 
                     \cup_{def}expand(def, i, low:high)$\\
\hspace{0.2in}\>if\=\ (outside $\ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<outside, m, UP, level -1, j>$)\\
\hspace{0.2in}\>if (inside $\ne \phi$) then\\
\hspace{0.2in}\>\>request($<inside,  n, DOWN, level, -1>$)\\
\end{tabbing}

The variable $outside$ contains the elements that can be propagated out of
the loop, while the variable $inside$ contains the elements that are killed
within the loop. The expansion function has the same definition as in 
\cite{gupta93}. For a SCD descriptor S, expand(S, k, low:high) is a 
function which replaces
all single data item references $\alpha*k+\beta$ used in any
array section descriptor D in S by the triple ($\alpha*low+\beta:
\alpha*high+\beta:\alpha$). 
The set $def$ includes all the definitions that are the source of a
flow-dependence.

\noindent
{\bf RULE 4: downward propagation: lower level loop header node}.
This is the initial downward propagation. The loops index variable, $i$, 
is treated as a constant in the downward propagation. 
Hence, SCDs that are propagated into the loop body 
must be changed to be the initial
available set for iteration $i$, that is, subtract all the variables 
killed in the iteration i+1 to high and propagate the information from the 
tail node  to the head node. This propagation prepares the downward 
propagation into the loop body by shrinking the SCD for each iteration.

\begin{tabbing}
\hspace{0.2in}qu\=ery($<S, n, UP, level,cnum>$): \\
\hspace{0.2in}\>if\=\ $(cnum = -1)$ then\\
\hspace{0.2in}\>\>calculate the summary of loop $n$;\\
\hspace{0.2in}\>\>request($<S - \cup_{def}expand(def, k, i+1:high), 
                    l, DOWN, level-1, 1>$);\\
\hspace{0.2in}\>else\\
\hspace{0.2in}\>\> STOP /* interval processed */\\
\end{tabbing}

\noindent
{\bf RULE 5: downward propagation: regular node}.
For regular node, the downward propagation is similar to the upward
propagation.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S_1, n, DOWN, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, DOWN, level, k>$) : \\
\hspace{0.2in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.2in}\>action(S, local(n))\\
\hspace{0.2in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - kill_n, m, DOWN, level, j>$)\\
\end{tabbing}

\noindent
{\bf RULE 6: downward propagation: same level loop header node}.
When downward propagation reaches a loop header (not the loop header
whose body is being processing), it must generate further downward
propagation request to go deeper into the body.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, DOWN, level, 1>$): \\
\hspace{0.2in}\>action(S, summary(n)); \\
\hspace{0.2in}\>if\=\ ($S-K_n \ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - K_n, m, DOWN, level, j>$);\\
\hspace{0.2in}\>if\=\ ($S\cap K_n \ne \phi$) then\\
\hspace{0.2in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$);\\
\end{tabbing}

\subsubsection*{Summary calculation}

During the request propagation, the summary information of an interval is
needed when a loop header is encountered. 
An algorithm is described 
to obtain the summary information in a demand driven manner.
The calculation of kill set of the interval is used as an example. Let 
$kill(i)$ 
be the variables killed in node $i$, $K_{in}$  and $K_{out}$ 
be the variables killed before and after the node respectively.
Figure.~\ref{KILL} depicts the demand driven algorithm. The 
algorithm propagates the data flow information from the tail node to the 
header node in the interval using the following data flow equation:\\

\centerline{$K_{out}(n) = \cup_{s\in succ(n)}K_{in}(s)$}
\centerline{$K_{in}(n) = kill(n)\cup K_{out}(n)$}

When an inner loop header is
encountered, a recursive call is issued to get the summary information
for the inner interval. Once a loop header is reached, the kill set needs
to be expanded to be used by the outer loop.


\begin{figure}[htbp]
\begin{tabbing}
\hspace{0.2in}(1)\hspace{0.5in}Su\=mmary\_kill(n)\\
\hspace{0.2in}(2)\hspace{0.5in}\>$K_{out}(tail)$ = $\phi$\\
\hspace{0.2in}(3)\hspace{0.5in}\>fo\=r all $m\in T(n)$ and 
                               level(m) = level(n)-1 in backward order\\
\hspace{0.2in}(4)\hspace{0.5in}\>\>if\=\ m is a loop header then\\
\hspace{0.2in}(5)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                    K_{in}(s)$\\
\hspace{0.2in}(6)\hspace{0.5in}\>\>\>$K_{in}(m)$ = summary\_kill(m) $\cup
                                   K_{out}(m)$\\
\hspace{0.2in}(7)\hspace{0.5in}\>\>else\\ 
\hspace{0.2in}(8)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                   K_{in}(s)$\\
\hspace{0.2in}(9)\hspace{0.5in}\>\>\>$K_{in}(m)$ = $kill(m) \cup K_{out}(m)$\\
\hspace{0.2in}(10)\hspace{0.5in}\>return (expand($K_{in}(header)$, i, low:high))\\
\end{tabbing}
\caption{Demand driven summary calculation}
\label{KILL}
\end{figure}


\subsection{The analyzer}

The analyzer performs message vectorization, redundant communication 
elimination and communication scheduling using algorithms based upon
the demand driven algorithms described in the previous section. 
The analyzer performs the following steps:

\begin{enumerate}
\item {\em Initial SCD calculation}. 
Here the analyzer calculates the 
communication requirement for each statement that contains remote
memory references. Communications required
by each statement
are called {\em initial SCDs} for the statement and 
are placed preceding the statement.

\item {\em Message vectorization and available communication summary 
calculation}. The analyzer propagates
initial SCDs to the outermost loops in which they
can be placed. 
In addition to message vectorization optimization, 
this step also calculates
the summary of communications that are available after each loop. This 
information is used in the next step
for redundant communication elimination.

\item {\em Redundant communication elimination}. 
The analyzer performs redundant communication elimination using a demand 
driven version
of availability communication analysis \cite{gupta93}, which computes 
communications that are available before each statement. A communication
in a statement 
is redundant if it can be subsumed by available communications at
the statement. The analyzer also eliminates partially redundant 
communications. 

\item {\em Message scheduling}. 
The analyzer schedules messages
within each interval by placing messages with the same communication patterns
together and combining the messages to reduce the number
of messages. 
\end{enumerate}

\subsubsection*{Initial SCD Calculation}

The {\em owner computes} rule is assumed which 
requires each remote item referenced on the right handside of an 
assignment statement to be sent to the processor that owns the left
handside variable.
Initial SCDs for each statement represent this data movement.
Since the ownership of array elements determines  communication patterns,
the ownership of array elements will be described before the initial
SCD calculation step is presented.

\subsubsection*{Ownership.}

All arrays are aligned to a single virtual processor grid by 
affine functions. 
The alignments allowed are scaling, axis alignment and 
offset alignment.  The mapping from a point $\vec{d}$ in data space to a 
corresponding point $\vec{e}$ on the virtual processor grid (the owner of 
$\vec{d}$) can be specified by
an alignment matrix $M$ and an alignment offset vector $\vec{v}$
such that
$\vec{e} = M \vec{d} + \vec{v}$.  Using the alignment matrix and the 
offset vector, the owner of a data element
can be determined. Consider the 
 array $w$ in the example program in 
Figure~\ref{EXAMPLE}, the alignment matrix and the offset vector
 are given below.
\begin{center}
\small
\[ M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{v}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
\end{center}

\subsubsection*{Initial SCD Calculation.}

Using the ownership information, the initial SCDs are calculated
as follows. Let us consider each component in an 
initial $SCD = <A, D, CM, Q>$. $A$ is the array to be communicated.
The  region 
$D$ contains a single index given by the array subscript expression. 
The qualifier $Q = \perp$ since initial communications
must be performed in every iteration. Let $CM = <src, dst, qual>$. Since
initially communication does not perform broadcast, 
$qual = \perp$. Hence, the calculation of  $src$ and $dst$, 
which will be discussed in the following text, 
 is the only non-trivial computation in the calculation of initial
SCDs. 

Let $\vec{i}$ be the vector of loop induction variables. When  subscript
expressions are affine functions, an array reference can be
expressed as $A(G\vec{i} + \vec{g})$, where $A$ is the array name, 
$G$ is a matrix and $\vec{g}$ is  a vector. 
$G$ is called the {\em data access matrix}
and $\vec{g}$ the {\em access offset vector}.  The data access
matrix, $G$, and the access offset vector, $\vec{g}$, describe
a mapping from a point in the iteration space to a point in the data space.
Let $G_l$, $\vec{g}_l$, $M_l$, $\vec{v}_l$ be the data access matrix,
the access offset vector, the alignment matrix and the alignment vector 
for the {\em lhs} array reference, and  
$G_r$, $\vec{g}_r$, $M_r$, $\vec{v}_r$ be the corresponding quantities
for the {\em rhs} array reference. 
The source processor $src$ and destination processor $dst$
are given by:

\centerline{$src = M_r(G_r\vec{i} + \vec{g}_r) + \vec{v}_r,$ \hspace{1in}
            $dst = M_l(G_l\vec{i} + \vec{g}_l) + \vec{v}_l$}

Consider the communication of $w(i,j)$ in statement $s13$ 
in Figure~\ref{EXAMPLE}.
The analyzer can obtain from the program the data 
access matrices, access offset vectors, alignment matrices 
and alignment vectors and from them the  SCD for the communication 
given below.
As an indication of the complexity of a SCD, the structure for this 
communication required 524 bytes to store.
\begin{center}
\small
\[ M_z  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{v}_z = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{v}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
\[ G_l  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_l = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   G_r  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_r = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right)
\]
\end{center}
\centerline{$<A=w, D=(i, j), CM=<(2*j, i+1), (i, j), 
            \perp>, Q= \perp>$}

\subsubsection*{Message Vectorization and Available Communication Summary}

In this phase, the analyzer computes 
{\em backward exposed} communications, which 
are SCDs that can be hoisted out of a loop,
and  {\em forward exposed} communications, which are SCDs that are
available after the loop.
Backward exposed communications represent 
actual communications vectorized from inside the loop. When a SCD is 
vectorized, the initial SCD at the assignment statement
are replaced by SCDs
for  backward exposed communications at loop headers. 
Forward exposed communications
represent the  communications that are performed inside a loop and 
are still alive after the loop. 
Hence they can be used to subsume communications appearing
after the loop. By using data dependence information,  backward and
forward exposed communications are calculated by
propagating  SCDs from inner loop bodies to loop headers using a 
simplified version of the rules discussed in previous section.
% The rule is simplified in that the DOWN propagate of SCDs back into
%the loop body is no longer performed. 

%The calculations of forward exposed communications do not effect the
%SCDs in the original statement, since  the available communication
%only represents the effect
%of communications. 
Algorithms for the forward and backward exposed communication calculation
are  described in Figure~\ref{phase1}~(a) and (b). 
%Notation 
%$Request(S, n, UP|DOWN)$ denotes placing an UP or DOWN propagation of S after
%node n.  
Since only UP propagation
is needed,  $Request(S, n)$ is used to denote placing a
propagation of $S$ after node $n$.
In the algorithms,  $S$ is a
SCD occurring inside the interval whose header is node $n$ and whose induction
variable is $i$ with lower bound $1$ and upper bound $h$, 
$anti\_def$ is the set of definitions in the interval that have 
anti--dependence relation with the original array reference that causes  the
communication $S$, 
$flow\_def$ is the set of definitions in the interval that have 
flow--dependence relation with the original array reference that causes the
communication $S$. 
%The expansion function has the similar definition as in 
%\cite{gupta93}. 
For a SCD, S, $expand(S, i, 1:h)$ first 
determines which portion of the $S=<A, D, CM, Q>$ to be expanded.
If $D$ is to be expanded, that is, $i$ occurs in D, 
the function will replace
all single data item references $\alpha*i+\beta$ used in D
by the triple $\alpha+\beta:\alpha*h+\beta:\alpha$. 
If $D$ cannot be expanded, that is, after expansion $D$ is not in the 
allowed form, then the communications will stay inside the loop.
If $CM=<src, dst, qual>$ 
is to be expanded, that is,  $i$ occurs in $dst$ but not in $src$ and $D$,
the function will add $i=1:h:1$ 
into the mapping qualifier list $qual$. 

The algorithms determine the part of communications $Outside$, that 
can be hoisted out of a loop, and $Inside$, that cannot be hoisted
out of the loop. In forward exposed communication calculation, 
the analyzer makes $Outside$ as the forward exposed communication and ignores
the $Inside$ part. 
%Note again that forward exposed 
%communication calculation only calculates the communications that are alive
%after a loop and does not actually change the communications in a program.
In backward exposed communication calculation, the analyzer
makes $Outside$ as backward exposed communication. In addition, the analyzer
must also change the original SCD according to contents of $Inside$. In
the case when the SCD can be fully vectorized, 
%$Inside$ will be empty and 
%all communications in the SCD inside the loop are summarized by $Outside$ and 
%are hoisted out of the loop. 
the SCD in the original statement is 
removed. In the case when the SCD cannot be fully vectorized, part of the
communication represented by 
$Outside$ is hoisted  out of the loop, while other part represented by
$Inside$ stays at the original statement. Thus, the SCD in the original
statement must be modified by a  communication qualifier to indicate that
the SCD only remains in iterations that generate communications 
in $Inside$.
%Note that in forward exposed communication calculation,
%communications are killed by anti--dependence while in backward exposed 
%communication calculation,  communications are killed by flow--dependence.

\begin{figure}[htbp]
\small
\footnotesize
\begin{subfigRow*}
\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}re\=qu\=es\=t$(S, n)$ : \\
\>$Outside = expand(S, i, 1:h) -$\\  
\>\>$\cup_{anti\_{def}}expand(anti\_def, i, 
             1:h)$\\
\>if $(Outside \ne \phi)$ then\\
\>\>record $Outside$ as\\ 
\>\>\>forward exposed in node n \\
\>\>Let m be the header of the\\
\>\>\>interval including node $n$\\
\>\>$request(Outside, m)$;\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
(a) Forward exposed communication
\end{tabbing}
\end{minipage}

\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}re\=qu\=es\=t$(S, n)$ : \\
\>$Outside = expand(S, i, 1:h) -$\\
\>\>$\cup_{flow\_def}expand(flow\_def, i, 
             1:h)$\\
\>$Inside = expand(S, i, 1:h) \cap$\\
\>\>$\cup_{flow\_def}expand(flow\_def, i, 1:h)$\\
\>if $(Outside \ne \phi)$ then\\
\>\>convert $Inside$ in terms of $S$\\
\>\>\>with qualifier, denoted as $D$\\
\>\>if (conversion not successful) then\\
\>\>\>stop /* fail */\\
\>\>else\\
\>\>\>ch\=ange the S into D\\
\>\>\>record $Outside$ as backward\\
\>\>\>\>exposed comm. at node $n$.\\
\>\>\>Let m be the header of the\\
\>\>\>\>interval including node $n$\\
\>\>\>$request(Outside, m)$;\\
\\
(b) Backward exposed communication
\end{tabbing}
\end{minipage}
\end{subfigRow*}
\caption{Algorithms for the forward and backward exposed communication}
\label{phase1}
\end{figure}


%\begin{figure}
%\begin{subfigRow*}
%\small
%\footnotesize
%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} DO\=\ i = 1, 100\\
%              \>COMM: $C_1= <A, (1), <(1), (i), \perp>,\perp>$\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2= <B, (i-1), <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (a)
%\end{tabbing}
%\end{minipage}
%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} FORWARD COMM: $C_1^f=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} FORWARD COMM: $C_2^f=<B, [0:99], <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in} DO\=\ i = 1, 100\\
%              \>COMM: $C_1= <A, (1), <(1), (i), \perp>,\perp>$\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2= <B, (i-1), <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (b)
%\end{tabbing}
%\end{minipage}

%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} FORWARD COMM: $C_1^f=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} FORWARD COMM: $C_2^f=<B, [0:99], <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in} BACKWARD COMM: $C_1^b=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} BACKWARD COMM: $C_2^b=<B, (0), <(i-1), (i), \perp>, 
%                               \perp>$\\
%\hspace{0.2in} DO\=\ i = 1, 100\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2^s= <B, (i-1), <(i-1,1), (i,1), \perp>, i=2:100:1>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (c)
%\end{tabbing}
%\end{minipage}
%\end{subfigRow*}
%\caption{Message vectorization}
%\label{VEC}
%\end{figure}

\begin{figure}[tbph]
\centerline{\psfig{figure=fig/vexam.eps,width=3.5in}}
\caption{Calculating backward exposed communications}
\label{VEC}
\end{figure}

Consider communications in the loop in 
Figure~\ref{VEC}. Assume that arrays $a$, $b$ and $d$ are identically aligned
to the virtual processor grid, initial SCDs, $C1$ and $C2$,  are shown in 
Figure~\ref{VEC}.
$C3$, $C4$ and $C5$ are the communications after the backward exposed 
communication calculation. 
Calculating the backward exposed communication for $C1$
results in  communication $C3$ 
in the loop header and the removal of  the communication $C1$ from its
original statement.
Calculating  the backward exposed communication for $C2$
puts $C4$ in the loop header and changes $C2$ into $C5$. 
Note that, there is a  flow--dependence relation from b(i) to 
 b(i-1). In calculating the 
backward exposed communication for SCD $C2$, 
$Inside = <b, (1:99:1), <(i-1,1), (i,1), \perp>, \perp>$. Converting $Inside$
back in terms of $C2$ results in $C5$.


%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{1.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{1.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{1.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{1.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{1.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{1.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_2=<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_3=<w, (2:101:1, 1:99:1), <(2*j, i+1), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_4=<y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 100)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{1.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{1.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{1.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{1.2in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
%\hspace{1.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after vectorization}
%\label{EXAMVEC}
%\end{figure}


%\begin{figure}[htbp]
%\small
%\footnotesize
%\begin{tabbing}
%\hspace{1in}\hspace{0.5in}Do\=\ i=1, 100\\
%\hspace{1in}s1: \>D[i] = A[1]\\
%\hspace{1in}s2:\>B[i] = B[i-1] + 1\\
%\hspace{1in}\hspace{0.5in}End Do\\
%\hspace{1in}\hspace{0.5in}Do i=1, 100\\
%\hspace{1in}s3:\>C[i] = B[i-1] + 1\\
%\hspace{1in}\hspace{0.5in}End Do
%\end{tabbing}
%\caption{backward and forward exposed communication}
%\label{BACKWARD}
%\end{figure}

\subsubsection*{Redundant Communication Elimination}

This phase calculates  available communications before each statement,
and eliminates a communication at the statement if the communication
is available. This optimization is done by propagating SCDs forward until
all elements are killed. During the propagation, if another SCD that can be
subsumed is encountered, that SCD is redundant and can be eliminated.

\begin{figure}[htbp]
\small
\footnotesize
\begin{subfigRow*}
\begin{minipage}{10cm}
\begin{tabbing}
\hspace{0.1in}re\=quest($S_1, n, UP$) $\wedge$ ... \\
\hspace{0.1in}$\wedge$request($S_k, n, UP$) : \\
\hspace{0.1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.1in}\>if\=\ (SCDs in n is a subset of S) then\\
\hspace{0.1in}\>\>remove the SCDs\\
\hspace{0.1in}\>if $(S-kill_n \ne \phi)$ then\\
\hspace{0.1in}\>\>fo\=r all $m\in succ(n)$\\
\hspace{0.1in}\>\>\>request($S - kill_n, m, UP$)\\
\\
\hspace{0.1in}{(a) Actions on nodes within an interval}\\
\end{tabbing}
\end{minipage}

\begin{minipage}{10cm}
\begin{tabbing}
\hspace{0.1in}re\=quest($S, n, UP$): \\
\hspace{0.1in}\>calculate the summary of loop $n$\\
\hspace{0.1in}\>In\=side= $expand(S, i, 1:i-1) \cap$\\
\hspace{0.1in}\>\> $(\cup_{def}expand(def, i, 1:i-1))$\\
\hspace{0.1in}\>if\=\ (inside $\ne \phi$) then\\
\hspace{0.1in}\>\>Let $l$ be the first node.\\
\hspace{0.1in}\>\>request($Inside,  l, DOWN$)\\
\\
\\
\hspace{0.1in}{(b) Actions on a loop header}\\
\end{tabbing}
\end{minipage}
\end{subfigRow*}
\vspace{-0.15in}
\caption{Actions in forward propagation}
\label{FORWARD1}
\vspace{-0.15in}
\end{figure}

Using the interval analysis
technique \cite{gupta93}, two passes are needed to obtain
the data flow solutions in an interval. 
Initially, UP propagations are performed.
Once the UP propagations reach interval headers, summaries of the SCDs
are calculated and  DOWN propagations of the summaries
are triggered. Note that since 
the data flow effect of propagating SCDs between
intervals is captured in the message vectorization
 phase of the analyzer, both the UP
and DOWN propagations are
performed within an interval in this phase.

Assuming that node $n$ has $k$ predecessors. 
When propagating SCDs within an interval in forward propagation, 
actions in a node
will be triggered only when all its predecessors place requests. The nodes
calculate the SCD available by performing intersection on all
SCDs that reach it, check whether communications within the node can 
be subsumed, and propagate the live communications  forward. 
 Figure~\ref{FORWARD1}~(a) describes actions 
on the nodes inside the interval in an UP forward propagation.
When the UP propagation reaches an interval boundary,
the summary information is calculated by obtaining all the elements that
are available in iteration $i$, and a DOWN propagation is triggered.
Note that in forward propagation,   communications
can be safely assumed to be performed in every iteration ($Q=\perp$), 
since the effect of the communication
must guarantee that the valid values are at the proper processors for the 
computation. Figure~\ref{FORWARD1}~(b)
shows actions at interval boundaries. The
propagation of a DOWN request is similar to that of an UP request except that
a DOWN propagation stops at interval boundaries.
% The results of redundant 
%communication elimination in the program in Figure~\ref{EXAMVEC} is shown in 
%Figure~\ref{EXAMRED}. As can be seen from the figure, the communication
%$C_3$ in Figure~\ref{EXAMVEC} is subsumed by communication $C_1$.

%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{1.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{1.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{1.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{1.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{1.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{1.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_2=<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_4=<y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 100)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{1.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{1.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{1.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{1.2in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
%\hspace{1.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after redundant communication elimination}
%\label{EXAMRED}
%\end{figure}


\subsubsection*{Global Message Scheduling}

After the redundant communication elimination phase, the analyzer further
reduces the number of messages using a global message scheduling
algorithm  proposed by Chakrabarti et al. in \cite{Chakrabarti96}.
The idea of this optimization 
is to combine messages that are of the same communication
pattern into a single message to reduce the number of messages in 
a program.  In order to perform message scheduling, the analyzer first
determines the earliest and latest points for each communication.
Placing the communication in any point between the earliest and 
the latest points that dominates the latest point 
always yields correct programs.
Thus, the analyzer can 
schedule the placement of messages  such that messages of 
same communication
patterns are placed together and are combined to reduce the number of
messages.

The latest point for a communication is the place of the SCD 
after redundant communication elimination.
Note that after message vectorization, SCDs are  placed
in the outermost loops that can perform the communications.
The earliest point for a SCD can be found by propagating
the SCD backward. 
As in \cite{Chakrabarti96}, it is  assumed 
that communication
for a SCD is performed at a single point. Hence, the backward
propagation will stop after an assignment statement, a loop header or a
branch statement where part of the SCD is killed. Since the propagation of
SCDs stops at a loop header node, only the UP propagation is needed.
Once the earliest and latest points for each communication are known, 
the greedy heuristic in \cite{Chakrabarti96} is used to perform the 
communication scheduling.
% The greedy algorithm
%propagates SCDs from their earliest points to the latest points, 
%along the path,
%if some communications are of the same pattern, the communications will
%be combined. The combined communication will be further propagated to the 
%latest point common to all communications combined.
%Consider communications $C_2$ and $C_4$ in  Figure~\ref{EXAMRED}.
%The earliest and the latest points for $C_2$
%are before statement $s6$ and $s11$,
%respectively. The earliest and the latest points for $C_4$ are before 
%statement $s11$. Since these two communications are of the same communication
%pattern, they can be placed before statement $s11$ and combined.


%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{0.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{0.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{0.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{0.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{0.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{0.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{0.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{0.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{0.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{0.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{0.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{0.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{0.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{0.4in}COMM: $C_{24} =\{<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>,  $\\
%\hspace{0.7in}  $ <y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>\}$\\
%\hspace{0.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 200)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{0.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{0.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{0.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{0.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{0.2in}(s16)\hspace{0.1in}\>w(i+1, 200) = ...\\
%\hspace{0.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after message scheduling}
%\label{EXAMCOM}
%\end{figure}

\subsection{Evaluation of the analyzer}
\label{evalanalyzer}

The analyzer is implemented as part of the E--SUIF compiler
which is developed to support compiled communication on optical TDM networks.
The E--SUIF compiler is based on the Stanford SUIF compiler \cite{SUIF}.
The generation of a program used for evaluations is carried out in
the following steps. First,
a sequential program is compiled using SUIF frontend, $scc$, 
to generate the SUIF intermediate representation. Next, 
the SUIF transformer, $porky$, is used to perform 
a number of scalar optimizations including
copy propagation, dead code elimination and induction variable
elimination. The communication preprocessing phase is used to annotate
global arrays with data alignment information. 
The analyzer is then invoked to analyze and optimize  communications 
in the program. After communication optimizations, the backend of the
compiler inserts a library call into the SUIF intermediate representation
for each SCD remaining in the program. Finally, the $s2c$ tool
is used to convert the SUIF intermediate representation into C program, which
is the one that is executed for evaluation.

To evaluate performance of the analyzer, 
a communication emulation system is developed. 
The system takes SCDs as input, emulates the 
communications described by the SCDs and collects statistics about 
the required communications, such as
the total number of elements communicated and the 
total number of messages communicated.
The emulation system provides an interface to C program in the form of 
a library call whose arguments include all information in a SCD.
The compiler backend in E--SUIF 
automatically generates the library call for each 
SCD remaining in the program. 
In this way, the communication performance of a program
can be evaluated in the emulation system
by running  programs generated by the E--SUIF compiler.

%Once this program is generated, it can be
%compiled with the emulation library and run to collect the communication 
%performance statistics. 
%In our experiments, we used the number of elements
%to be communicated as the performance measure.

Six programs, L18, ARTDIF, TOMCATV, SWIM, MGRID and ERHS
are used in the experiment. Programs ARTDIF, TOMCATV, SWIM, MGRID and ERHS
are from the SPEC95 benchmark suite. 
The descriptions of the programs are as follows.
\begin{enumerate}
\item L18 is the explicit hydrodynamics kernel in livermore loops (loop 18).
\item ARTDIF is a kernel routine obtained from  HYDRO2D program, 
which is an astrophysical program for the computation of galactical jets
using hydrodynamical Navier Stokes equations. 
\item TOMCATV does the mesh generation with Thompson's solver. 
\item SWIM is the SHALLOW weather prediction program.
\item MGRID is the simple multigrid solver for computing a three
dimensional potential field. 
\item ERHS is part of the APPLU program, which is the solver for five 
coupled \\
parabolic/elliptic partial differential equations. 
\end{enumerate}

Table~\ref{analysis} shows  the analysis cost of the analyzer. 
The analyzer, which implements all the optimization
algorithms on all SCDs in the programs,  was run on a 
SPARC 5 machine with 32MB memory. 
Row 2 and Row 3 shows the program sizes.
Row 4 shows the
cumulative memory requirement, which is the sum of number of SCDs passing 
through each node. This number is approximately equal 
to the memory requirement of
traditional data flow analysis.
The value in parenthesis
is the maximum number of cumulative SCDs in a node, which is the extra memory
needed by the analyzer.
In the analyzer, the size of a SCD ranges from 0.6 to about 3 kbytes.
The results show that traditional analysis method will require large
amount of memory when a program is large, while the analyzer uses
little extra memory. 
Row 5 gives the raw analysis times and row 6 shows the rate at which
the analyzer operates in units of  $lines/sec$. On an average,
the analyzer compiles 172 lines per second for the six programs. 
Row 9 shows the total time,
which includes analysis time and the time to load and store the
SUIF structure, for reference. In most cases, the analysis time is only
a fraction of the load and store time.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Program &            L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
size(lines) &         83  &    101 &     190 &  429 &   486 & 1104\\
%\hline
%\# of tree nodes   &  85  &    110 &     337 &  543 &   660 &  958\\
\hline
\# of initial SCDs &  35  &     12 &     108 &   76 &   125 &  403\\
\hline
accu. memory req.  & 348(1) &  175(1) & 5078(3) &  767(1) &  1166(1) & 
                                                                    6029(5)\\
\hline
analysis time(sec) & 0.62 &   0.32 &    3.47 & 1.87 &  1.92 & 20.92\\
\hline 
lines / sec        & 133  &   316  &      54 & 229  &  253  & 52 \\
\hline
total time(sec)    & 2.00 &   1.75 &    6.95 & 6.65 & 12.52 & 35.42\\
\hline
\end{tabular}
\end{center}
\caption{Analysis time}
\label{analysis}
\end{table}

Table~\ref{elements} and Table~\ref{messages} show the effectiveness
of the optimizations in the analyzer. 
Table~\ref{elements} shows the reduction of
the 
total number of elements to be communicated and Table~\ref{messages} shows
the reduction of the total number of messages. 
Both cyclic and block distributions on 16 PE systems are considered.
This experiment is conducted using the test input  provided by
the SPEC95 benchmark for programs TOMCATV, SWIM, MGRID ERHS. The outermost
iteration number in MGRID is reduced to 1 (from 40). Problem 
sizes of $6\times 100$ for L18 and $402\times 160$ for ARTDIF are used.
The number of elements and number of messages
communicated after all optimizations is compared
to those after message vectorization optimization.
Table~\ref{elements} shows that for cyclic distribution, an
average reduction of 31.5\% of the total communication elements is achieved.
The block distribution greatly reduces the number of elements to be
communicated and affects the optimization performance of the analyzer.
For block distribution, the average reduction is 23.1\%. 
Table~\ref{messages} shows that the analyzer reduces the 
total number of messages
by 36.7\% for cyclic distribution and by 35.1\% for block
distribution. These results indicate
that global communication optimization opportunities are quite common and
the analyzer developed is effective in finding these opportunities.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dist. & Opt. &        L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
      &      &       $\times10^4$& $\times10^5$ & $\times10^8$  & 
$\times10^7$ & $\times10^7$ & $\times10^6$\\
\hline
      & Vector. &    1.38  & 7.01   & 1.38    & 6.38   & 5.69 & 3.62\\
\cline{2-8}
cyclic & Final  &   0.96  & 5.73   & 0.34 &  4.58   & 5.69 & 2.29 \\
\cline{3-8}
       &        &   69.6\% & 81.7\% & 24.6\% & 71.8\% & 100\%& 63.3\%\\
\hline
\hline
       &        &   $\times10^3$ & $\times10^4$ & $\times10^6$ & 
$\times10^6$ & $\times10^6$ & $\times10^6$\\
\hline
       & Vector. &  3.26    & 7.17 & 5.74 & 3.38 & 8.49 & 3.11\\
\cline{2-8}
block  & Final &  2.57 &  6.97 & 5.12 & 1.08 & 8.49 & 1.65\\
\cline{3-8}
       &       &  78.8\% & 97.2\% & 89.1\% & 32.0\% & 100\% & 53.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Total number of elements to be communicated}
\label{elements}
\end{table}

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dist. & Opt. &        L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
   & Vector. &    368  & 400   & 68555  & 3892   & 17662 & $1.14\times10^6$\\
\cline{2-8}
cyclic & Final  & 96  &  336   & 41075  & 1807   & 17662 & $0.72\times10^6$\\
\cline{3-8}
       &        &   26.1\% & 84.0\% & 59.9\% & 46.4\% & 100\%& 63.1\%\\
\hline
\hline
       & Vector. &  330  & 185 & 16750 & 3894 & 14650 & $9.20\times10^5$\\
\cline{2-8}
block  & Final &  90 &  161 & 10915 & 2209 & 14650 & $4.89\times10^5$ \\
\cline{3-8}
       &       &  27.3\% & 87\% & 65.2\% & 56.7\% & 100\% & 53.2\%\\
\hline
\end{tabular}
\end{center}
\caption{Total number of messages}
\label{messages}
\end{table}

%\begin{figure}
%\centerline{\psfig{figure=fig/size.eps,width=6in}}
%\caption{The reduction of the number of array elements in communications}
%\label{size}
%\end{figure}
%\vspace{-0.2in}

\section{Virtual to physical processor mapping}

%The communication analyzer analyzes the communication requirement on
%virtual processor grids. 
In order to support compiled communication,
communication patterns on physical processors must be computed.
This section assumes that the physical processor grid has the same
number of dimensions as the logical processor grid. Notice that this is
not a restriction because a dimension in the physical processor grid
can always be collapsed by assigning a single processor to that dimension.
This section presents algorithms to compute  communications
on physical processors from SCDs. The computation
may not always be precise due to  symbolic 
constants in the SCD that are unknown at compile time.
The algorithms employ multi--level approximation schemes to 
obtain best information. 
%In this case, approximations are needed.

Given a $SCD = <A, D, CM = <src, dst, qual> , Q>$, let us first consider
the case where $A$ is an one-dimensional array and the virtual processor
grid is also one-dimensional. Let $src = \alpha*i+\beta$ and
$dst = \gamma*i + \delta$, $\alpha \ne 0$, $\gamma \ne 0$, and $qual = NULL$. 
$qual \ne NULL$ will be considered later
when multi-dimensional arrays and multi-dimensional virtual processor
grids are discussed. 
Let the alignment matrix and the offset vector be $M_A$ and $v_A$, that 
is, element A[n] is owned by virtual processor $M_A*n + v_A$. 
Let us assume that the number
of physical processors is $p$ and the block size of the distribution of
virtual processor grid is $b$. For an element A[n], the 
physical source processor of the communication can be computed as follows.
 
\centerline{$(M_A*n+ v_A)\ mod\ (p*b) / b$}

\noindent
The virtual destination processor can be computed by
first solving the equation \\
\centerline{$(M_A*n + v_A) = \alpha*i+\beta$ 
to obtain $i = (M_A*n+v_A-\beta) / \alpha$}

\noindent
and then replacing the value of $i$ in  $dst$ to obtain the 
virtual destination processor $\gamma*(M_A*n+v_A-\beta)/\alpha + \delta$. 
Thus, the
physical destination processor is given by\\
\centerline{$ (\gamma*(M_A*n+v_A-\beta)/\alpha + \delta)\ mod\ (p*b) / b$.} 

\noindent
The physical communication pattern for the SCD can be obtained
by considering all elements in $D$. 
However, there are situations that the exact
region $D$ cannot be determined at compile time. It is desirable to 
have a good approximation scheme that computes
the communication patterns when $D$ cannot be determined at compile time.

Before the approximation scheme is presented, let us first examine
the relation between communications on physical processors and 
that on virtual processors. Let us use notation $src\rightarrow dst$ 
to represent
a communication from $src$ to $dst$. Given a data region 
$D=l:u:s$, the communications on virtual processors can be derived as follows.
By mapping $D$ to the virtual processor grid, the source processors
of the communications can be obtained. Since the mapping from data space to 
the virtual processor grid is linear, the set of source processors can 
be represented as a triple $vs_l:vs_u:vs_s$, that is, the 
source processors on the virtual processor grid are $vs_l$, $vs_l+vs_s$, 
$vs_l+2*vs_s$, ..., $vs_u$. Due to the way in which 
$CM.src = \alpha*i + \beta$ 
is computed, 
equations $vs_l+i*vs_s = CM.src$, $i = 0, 1, 2, ...$,  
always have integer solutions.
Since $CM.dst$ is of the form $\gamma*i+\delta$, 
where $\gamma$ and $\delta$ are constants, 
the destination processors on the virtual processor grid can 
also be represented
as a triple $vd_l:vd_u:vd_s$, where 
$vd_l = \gamma*((vs_l-\beta)/\alpha) + \delta$, 
$vd_u = \gamma*((vs_u-\beta)/\alpha) + \delta$ and $vd_s = \gamma*vs_s/\alpha$. Notice that because of the way in which
$CM$ is computed, all the division operations in the formula result in 
integers. Thus, communications on the virtual processor grid can be
represented as $vs_l\rightarrow vd_l:vs_u\rightarrow vd_u:vs_s\rightarrow vd_s$, meaning the set\\
\centerline{$\{vs_l\rightarrow vd_l, vs_l+vs_s\rightarrow vd_l+vd_s, ..., vs_u\rightarrow vd_u\}$.}

\noindent
Communications on physical processors are obtained by mapping virtual 
processors onto  physical processors.  Given a block--cyclic distribution 
with block size $b$ and processor number $p$, a sequence of processors
on the virtual processor grid $l, l+s, l+2*s, ...$ will be mapped
to a sequence of physical processors repeatedly. For example, assuming that 
$p=2$ and $b=2$, the sequence of virtual processors 
$2, 2+3=5, 2+2*3=8, 2+3*3=11, ....$ will be mapped to physical processors
$1,0,0,1$ repeatedly as shown in Figure~\ref{virtualspace}. As will be seen
later, this characteristic can be utilized to develop an approximation 
algorithm for the cases when $D$ is unknown at compile time.  
A point $e$ in the virtual processor grid can be represented by 
two components $(pp, o)$, where
$pp = e\ mod\ (p*b) / b $ is the physical processor that contains 
$e$ and $o = e\ mod\ b$ is the offset of $e$ within the processor.
Let $(pp_k, e_k)$ correspond to $l+k*s$, $k = 0, 1, ...$.
It can be easily shown that \\
\centerline{$pp_i = pp_j \wedge e_i=e_j$ implies 
            $pp_{i+1} = pp_{j+1} \wedge e_{i+1}=e_{j+1}$}

\noindent
Since in the $(pp, o)$ space, there are $p$ choices for $pp$ and $b$ choices
for $o$, Thus, there exists a $k$, $k \le p*b$, such that $pp_k=p_0$ and 
$e_k=e_0$, which determines a repetition point. In the previous 
example, consider the 
sequence\\
\centerline{$2 =(1,0), 5=(0,1), 8=(0,0), 11=(1,1), 14=(1,0) ...$.}

\noindent
Thus, the physical processors repeat the sequence $1,0,0,1$.

Communications on physical processor contains two processors, the source
processor and the destination processor. Thus, in order for the 
communications to repeat, both source and destination processors must
repeat. Following the above discussion, the communication on the virtual 
processor grid, $src\rightarrow dst$, can be represented by four components 
$(spp, so, dpp, do)$, where $spp$ is the physical processor that
contains $src$, $so$ is the offset of $src$ within the processor, 
$dpp$ is the physical processor that contains $dst$, $do$ is the 
offset of $dst$ within the processor. Assuming that the source array and the 
destination array are mapped to the same virtual processor grid, 
there are $p$ choices for $spp$ and $dpp$, and  $b$ choices for $so$ and $do$.
Thus, there exists $k$, $k\le p^2b^2$, such that both source and destination 
processors, and thus the communication pattern, will repeat themselves.
The following lemma summarizes these results. Using this lemma,
communication patterns can be obtained by considering the elements in $D$ 
until the repetition point or the end of $D$, whichever occurs first.


\begin{figure}
\centerline{\psfig{figure=fig/virtualspace.eps,width=3.5in}}
\caption{Virtual processor space}
\label{virtualspace}
\end{figure}

\noindent
{\bf Lemma: } Assume that the virtual processor grid is distributed over $p$
processors with block size $b$.
Let $SCD = <A, D=l:u:s, CM = <src, dst, qual> , Q>$, 
assuming u is infinite, there exist a value
$k$, $k\le p^2b^2$, such that the communication for all 
$m \ge k$, $A[l+m*s]$ has the same
source and destination as the communication for $A[l+(m-k)*s]$.

\noindent
{\bf Proof}: Follows from above discussions. $\Box$

The implication of the lemma is that the algorithm to determine the
communication pattern for the SCD can stop when the
repetition point occurs. In other words, when the upper bound of 
$D$ is unknown, the communication pattern can be approximated by using
the repetition point. Figure~\ref{algo1} shows the algorithm to compute
the physical communication pattern for a 1--dimensional array and 
a 1--dimensional virtual processor grid. 
The algorithm first checks the SCD.
Let $D=l:u:s$ and $CM=<\alpha+\beta*i, \gamma+\delta*i, \perp>$.
If $l$ contains variables or the mapping is not clean ($\alpha$, $\beta$, 
$\gamma$ or $\delta$ are symbolic constants), the communication is 
approximated  with all--to--all connections.
Note that
by the semantics of array sections, when $l$ is unknown, the compiler
cannot determine the actual sequence of elements in an array section.
%The algorithm then
%deals with the situation when array region $D$ contains
%variables.  
When $s$ contains variables, it will be 
approximated by 1, that is, $D$ is approximated by a superset $l:u:1$.  
When  $u$ contains variables, 
the physical communication is approximated by
considering all elements until the repetition point. Note that when  
$u$ contains a variable, the sequence in $D$ is 
$l$, $l+s$, $l+2*s$, .... Although
the upper bound of the sequence is unknown to the compiler, 
the repetition point can be used
to approximate the communication pattern. 

\begin{figure}
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Co\=mpute\_1--dimensional\_pattern($D$, $CM.src$, 
                                                       $CM.dst$)\\
\\
              \>Let $D=l:u:s$, $CM.src=\alpha*i+\beta$, $CM.dst=\gamma*i+\delta$\\
              \>{\bf if}\=\ ($l$ contains variables) {\bf then}\\ 
              \>\>{\bf return} all--to--all connections\\
              \>{\bf end if}\\
              \>{\bf if}\=\ ($\alpha$, $\beta$, $\gamma$ or $\delta$ 
                  are variables) {\bf then}\\ 
              \>\>{\bf return} all--to--all connections\\
              \>{\bf end if}\\
              \>$pattern = \phi$\\
              \>{\bf for each} element $i$ in $D$ {\bf do} \\
              \>\>$pattern = pattern + communication\ of\ i$\\
              \>\>{\bf if}\=\ (communication repeated) {\bf then}\\
              \>\>\>{\bf return} $pattern$\\
              \>\>{\bf end if}\\
              \>{\bf end for}
\end{tabbing}
\end{center}
\caption{Algorithm for 1-dimensional arrays and 1-dimensioanl virtual processor grid}
\label{algo1}
\end{figure}

Now let us consider multi-dimensional arrays and multi--dimensional
virtual processor grids. In an 
n--dimensional virtual processor grid,  a
processor is represented by a $n$--dimensional coordinate
$(p_1, p_2, ..., p_n)$. The algorithm to compute the communication pattern
finds all pairs of source and destination processors that require
communication. This is done by considering the dimensions in virtual 
processor grid one at a time.
A set of $src=(sp_1,sp_2,...,sp_n)\rightarrow dst=(dp_1,dp_2,...,dp_n)$ 
pairs is used to represent the communications. 
A wild--card, $*$, is used to represent the dimension within a tuple
that has not been considered. Initially the communication set 
contains a single element where 
all dimensions are wild--cards. When one dimension is considered, 
it generates a 1-dimensional communication pattern for a specific dimension
in the source and the destination, denoted as $src\_dim$ and $dst\_dim$
respectively. This 1-dimensional pattern may degenerate to contain only source
processors or destination processors.
A cross product operation is defined to merge the 1-dimensional 
communication patterns into the $n$-dimensional communication. 
This operation is similar to the cross product of  sets
except that  specific dimensions are involved in the operation.
For the degenerate form of the 1-dimensional pattern,
the operation only involves source processors or destination processors.

For example, consider the communication for\\ 
\centerline{$SCD = <y, (1:4:1,1:4:1), <src = (i,j), dst = (j,i), qual =NULL>, NULL>$.}

\noindent
Further assume
that the virtual processor grid is distributed on 2 processors
with block size of 2 in each dimension 
and array $y$ is identically mapped to the virtual 
processor grid. Initially, the communication set contains a single element
$(*,*) \rightarrow  (*,*)$, indicating that all dimensions in the source and destination
processor have not been considered. Considering the first dimension 
in the data space, which is identically mapped to the first dimension
of the virtual grid. Hence, $src\_dim = 1$. From the mapping relation
$CM.src$ and $CM.dst$, it is can found that dimension 2 in the destination
processor correspond to dimension 1 in the source processor. 
Hence, $dst\_dim = 2$. Applying the algorithm for the 1--dimensional 
communication pattern obtains the communication to be $\{0\rightarrow 0, 1\rightarrow 1\}$ with
$src\_dim=1, dst\_dim=2$.
Taking the cross product of this pattern 
with the 2-dimensional communication set 
$\{(*,*) \rightarrow  (*,*)\}$ yields $\{(0,*)\rightarrow (*,0), (1,*)\rightarrow (*,1)\}$. Considering the
second dimension of the data space, the 1--dimensional communication set
is $\{0\rightarrow 0, 1\rightarrow 1\}$ with $src\_dim=2, dst\_dim=1$. 
Taking the cross product of this pattern set
to the 2--dimensional communication set gives 
$\{(0,0)\rightarrow (0,0), (0, 1)\rightarrow (1, 0), (1, 0)\rightarrow (0, 1), (1, 1)\rightarrow (1, 1)\}$, which is
the physical communication for the $SCD$. 

The above example does not take constant mappings and non--NULL 
qualifiers into consideration.
The algorithm to compute communication patterns for multi-dimensional
arrays that is  shown in Figure~\ref{algo2} considers all these situations.
The algorithm first checks whether the mapping relation can be processed.
If one loop induction variable occurs in two or more dimensions 
in $CM.src$ or $CM.dst$, the algorithm cannot
find the correlation between dimensions in source and destination processors,
and the communication pattern for the SCD is 
approximated by all--to--all connections. 
If the SCD passes the mapping relation test, the algorithm determines for each
dimension in the data space the corresponding dimension $sd$ in the source 
processor grid.
If it does not exist, the data dimension is not distributed
and need not be considered. 
If there exists such a dimension, the algorithm
then tries to find the corresponding dimension $dd$ in the destination
processor grid by checking whether there is a dimension $dd$ such that
$CM.dst[dd]$ contains the same looping index variable as the source 
dimension $CM.src[sd]$.
If such dimension exists, 
the algorithm computes 1-dimensional communication pattern
between dimension $sd$ in the source processor and dimension $dd$ in 
the destination processor, then cross--products the 1-dimensional 
communication pattern into the $n$-dimensional communication pattern.
When $dd$ does not exist, the algorithm determines a degenerate 1-dimensional
pattern, where only source processors are considered, and cross-products
the degenerate 1-dimensional pattern into the communication pattern.
After all dimensions in the data space are considered, there may still
exist dimensions in the source processor (in the virtual processor grid)
that have not been considered. These dimensions should be constants and
are specified by the alignment matrix and the alignment offset vector.
The algorithm fills in the constants in the source processors.
Dimensions in destination processor may not be fully considered, either.
When $CM.qual \ne NULL$, the algorithm  finds for each
item in $CM.qual$ the corresponding dimension, computes all possible 
processors in that dimension and cross--products the list into the 
communication list. Finally, the algorithm fills in all constant
dimensions in the destination.


%\begin{figure}
%\begin{center}
%\begin{tabbing}
%\hspace{0.1in}Co\=mpute communication pattern\\
%              \>{\bf if}\=\ (the mapping relation is not good) {\bf then}\\
%              \>\> approximate with all-to-all connections and stop.\\
%              \>{\bf for each} dimension in the array {\bf do}\\
%              \>\>determine the corresponding dimension in source and
%                  destination processor grids.\\ 
%              \>\>compute 1-dimensional communication pattern.\\
%              \>\>cross product this one-dimensional mapping to previous
%                    mappings.\\
%              \>fills in all other dimensions in the source processor if
%                necessary.\\
%              \>{\bf for each} elements in the mapping qualifier {\bf do}\\
%              \>\>determine the corresponding destination dimension.\\
%              \>\>compute all possible processors in dimension.\\
%              \>\>cross product the destination processors to 
%                  the previous mappings.\\
%              \>fills in all other dimensions in the destination processor if
%                necessary.\\
%\end{tabbing}
%\end{center}
%\caption{Algorithm for multi--dimensional array}
%\label{algo2}
%\end{figure}

\begin{figure}
\begin{center}
\begin{tabbing}
\hspace{0.1in}Co\=mpute communication pattern(SCD)\\
              \>Let $SCD=<A,D,CM,Q>$\\
              \>{\bf if}\=\ (the format of $CM$ is not good) {\bf then}\\
              \>\> {\bf return}  all-to-all connections\\
              \>{\bf end if}\\
              \>$pattern = \{(*,*,..., *)\}$\\
              \>{\bf for each} dimension $i$ in the array {\bf do}\\
              \>\>Let $sd$ be the corresponding dimension in source processor
                  grids.\\
              \>\>Let $dd$ be the corresponding dimension in destination
                  processor grids.\\ 
              \>\>1dpattern = compute\_1-dimensional\_pattern($D[i]$,
                  $CM.src[sd]$, $CM.dst[dd]$)\\
              \>\>pattern = cross\_product(pattern, 1dpattern)\\
              \>{\bf end for}\\
              \>pattern = source\_processor\_constants(pattern)\\
              \>{\bf for each} element $i$ in the mapping qualifier {\bf do}\\
              \>\>Let $dd$ be the corresponding destination processor 
                  dimension.\\
              \>\>1dpattern = compute\_1-dimensional\_pattern($CM.qual[i]$,
                              $\perp$, $CM.dst[dd]$)\\
              \>\>pattern = cross\_product(pattern, 1dpattern)\\
              \>{\bf end for}\\
              \>pattern = destination\_processor\_constants(pattern)\\
              \>{\bf return} pattern
\end{tabbing}
\end{center}
\caption{Algorithm for multi--dimensional array}
\label{algo2}
\end{figure}

\begin{figure}
\small
\footnotesize
\begin{center}
\begin{tabbing}
\hspace{1.5in}\=ALIGN (i, j) with VPROCS(2*j, i+2, 1) :: x\\
\>ALIGN (i) with VPROCS(1, i+1, 2) :: y\\
\>DO i = 1, 5\\
\>DO\=\ j = 1, 5\\
\>\>x(i, j) = y(i) + 1\\
\>END DO\\
\>END DO\\
\end{tabbing}
\end{center}
\caption{An example}
\label{ANEXAM}
\end{figure}


An example in Figure~\ref{ANEXAM} illustrates how 
communications on physical processors are derived.
In the program, the virtual processor grid is $3$-dimensional and 
the alignment array and 
the alignment offset vector
for arrays $x$ and $y$ are as follows:

\begin{center}
\[ M_x  = \left(
       \begin{array}{c} 0 \\ 1 \\ 0 \end{array}
       \begin{array}{c} 2 \\ 0 \\ 0\end{array} \right),\
   \vec{v}_x = \left(
       \begin{array}{c} 0 \\ 2 \\ 1 \end{array} \right)
%\]
%\[
   M_y  = \left(
       \begin{array}{c} 0 \\ 1 \\ 0 \end{array}
        \right),\
   \vec{v}_y = \left(
       \begin{array}{c} 1 \\ 1 \\ 2\end{array} \right)
\].
\end{center}

Let us assume that the virtual processor grid, $VPROCS$, is distributed as 
$p= (2,2,1)$,
which means 2 processors in dimension 0, 2 processors in dimension 1 and 
1 processor in dimension 2, and $b= (2,2,1)$, which means 
the block size 2 in dimension 0,  2 in dimension 1 and 1 in 
dimension 2. After communication analysis, the SCD to represent
the communication is as follows:\\
\centerline{\small $SCD = <y, (1:5:1), <src = (1, i+1, 2), 
dst = (2*j, i+2, 1), qual = \{j=1:5:1\}>, NULL>$.}

The communication on physical processors is computed as follows. 
First consider
the dimension 0 in the array $y$. From the alignment, the algorithm 
knows that dimension 1
in the virtual processor grid corresponds to this dimension in the data space. 
Checking $dst$ in $M$, the algorithm can find that dimension 1 in destination 
corresponds to dimension 1 in source processors. 
Applying the 1-Dimensional
mapping algorithm, an 1--dimensional communication pattern
$\{0\rightarrow 1, 1\rightarrow 0\}$ with $src\_dim=1$
and $dst\_dim=1$ is obtained. 
Thus the communication list becomes $\{(*, 1, *)\rightarrow (*, 0, *),
(*, 0, *)\rightarrow (*, 1, *)\}$ after taking the cross product with the 
1--dimensional pattern. 
Next, the other dimensions in source
processors, including dimension 0 that is always mapped to processor 0 and
dimension 2 that is always mapped to processor 1 are considered.
After filling in the physical processor in these dimensions in 
source processors, the communication pattern 
becomes $\{(0, 1, 1)\rightarrow (*, 0, *), (0, 0, 1)\rightarrow 
(*, 1, *)\}$. Considering the $qual$ in $M$,  the dimension 0 of
the destination processor can be either 0 or 1. Applying the cross product
operation, the new communication list 
$\{(0, 1,1)\rightarrow (0, 0, *),\ (0, 1, 1,)\rightarrow (1, 0, *),\ (0, 0, 1)\rightarrow (0, 1, *),\
   (0, 0, 1)\rightarrow (1, 1, *)\}$ is obtained. Finally, 
the dimension 2 in the destination processor is always mapped to processor 0,
Thus, the final mapping is  
$\{(0, 1,1)\rightarrow (0, 0, 0), (0, 1, 1s)\rightarrow (1, 0, 0), (0, 0, 1)\rightarrow (0, 1, 0),
(0, 0, 1)\rightarrow (1, 1, 0)\}$.


There are several levels of approximations in the algorithm. 
First, 
when the algorithm cannot correlate the source and destination processor
dimensions from the mapping relation, the algorithm
uses an  approximation
of all--to--all connections. If the mapping relation contains
sufficient information to distinguish the relation of 
the source and destination processor dimension, 
computing the communication pattern for 
a multi-dimensional array reduces to computing 1-dimensional 
communication patterns, thus the approximations within each 
dimension are isolated to that dimension and will not affect the 
patterns in other dimensions. Using this multi-level approximation
scheme, some information is obtained when the compiler does not have
sufficient information for a communication. 

\section{Connection scheduling algorithms}
\label{cscheduling}

Once the communication requirement on physical processors is obtained,
the compiler uses off--line algorithms to perform
connection scheduling and determines the communication phases in a program. 
This section presents the connection scheduling 
algorithms and their performance evaluation. These algorithms assume
a torus topology.
%However, the fundamental principles of the algorithms
%can be extended to other topologies. 

For a given network, a set of connections that do not share any link is called
a configuration. In an optical TDM network with path multiplexing, 
multiple configurations can be supported simultaneously. Specifically, for 
a network with multiplexing degree $d$, $d$ configurations can be 
established concurrently. Thus, for a given communication pattern,
realizing the communication 
pattern with a minimum multiplexing degree is equivalent to  determining the
minimum number of configurations that contain all the connections in 
the pattern. Next,
 some definitions will be presented to formally state 
the problem of
connection scheduling. A connection  from a source $src$ 
to a destination $dst$ is denoted as $(src, dst)$.

\begin{description}
\item A pair of connections  $(s_1, d_1)$ and $(s_2, d_2)$ are said
to {\bf conflict}, if they cannot be simultaneously established because
they use the same link.

\item A {\bf configuration} is a set of connections
$\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ such that
no connections in the set conflict.

\item Given a set of connections  
$Comm = \{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$,
the set $MC =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} is a
{\bf minimal configuration set} for $Comm$ iff: \\
$\bullet$ 
each $C_i \in MC$ is a configuration and each connection 
$(s_{i}, d_{i}) \in R$ 
is contained in exactly one configuration in $MC$; and \\
$\bullet$
each pair of configurations $C_i,C_j \in MC$ contain connections 
$(s_i, d_i) \in C_i$ and $(s_j, d_j) \in C_j$ such that
$(s_i, d_i)$ conflicts with $(s_j, d_j)$.
\end{description}

It has been shown that optimal message scheduling 
for arbitrary topologies is NP-complete \cite{Chlamtac92}.
Therefore these algorithms are heuristics that are demonstrated to
provide good performance. Three 
connection scheduling heuristic algorithms that  compute a
minimal configuration set for a given connection set $Comm$ are described next.

%
\subsection{Greedy algorithm}
%
In the greedy algorithm, a configuration is created by repeatedly 
putting connections into the configuration until no additional 
connection can be established in that configuration.
If additional connections remain, another configuration is created
and this process is repeated till all connections have been processed.
This algorithm is a modification of an algorithm proposed in \cite{Qiao94}.
The algorithm is shown in Figure~\ref{SIMPLE}. The time complexity of
the algorithm is $O(|Comm|\times max_i(|C_{i}|)\times d)$, where $|Comm|$ 
is the number of the connections, $|C_{i}|$ is the number of connections
in configuration $C_{i}$ and $d$ is the number of configurations
generated.

\begin{figure}[htmb]
%\small
\begin{tabbing}
\hspace{0.5in}(1)\hspace{0.3in}MC = $\phi$, k = 1\\
\hspace{0.5in}(2)\hspace{0.3in}{\bf re}\={\bf peat}\\
\hspace{0.5in}(3)\hspace{0.3in}\> $C_{k} = \phi$\\
\hspace{0.5in}(4)\hspace{0.3in}\>{\bf for}\= {\bf each} $(s_{i}, d_{i}) 
                                              \in Comm$\\
\hspace{0.5in}(5)\hspace{0.3in}\>\>{\bf if}\=\ $(s_{i}, d_{i})$ does 
                                   not conflict with any 
                                   connection in $C_{k}$ {\bf then}\\
\hspace{0.5in}(6)\hspace{0.3in}\>\>\>$C_{k} = C_{k} \bigcup$ { $(s_{i}, d_{i})$ }\\
\hspace{0.5in}(7)\hspace{0.3in}\>\>\>Comm = Comm $-$ { $(s_{i}, d_{i})$ }\\
\hspace{0.5in}(8)\hspace{0.3in}\>\>{\bf end if}\\
\hspace{0.5in}(9)\hspace{0.3in}\>{\bf end for}\\
\hspace{0.5in}(10)\hspace{0.3in}\>MC = MC $\bigcup$ { $C_{k}$ }\\
\hspace{0.5in}(11)\hspace{0.3in}{\bf until} $Comm = \phi$\\
\end{tabbing}
\normalsize
\caption{The greedy algorithm.}
\label{SIMPLE}
\end{figure}

For example consider the linearly connected nodes shown in Figure~\ref{EXAM}. 
The result for applying the greedy algorithm to schedule connections 
set \{(0, 2), (1, 3),(3, 4), (2, 4)\} is shown in Figure~\ref{EXAM}(a). 
In this case, (0, 2) will be in time slot 1, (1, 3) in time slot 2, (3, 4) 
in time slot 1 and (2, 4) in time slot 3. 
Therefore, multiplexing degree 3 is needed to establish the paths for the 
four connections.  However,  as shown in Figure~\ref{EXAM} (b), 
the optimal scheduling for the four connections, which can be obtained
by considering the connection in different order, is to schedule (0, 2) in 
slot 1, (1, 3) in slot 2, (3, 4) in slot 2 and (2, 4) in slot 1. 
The second assignment only use 2 time slots to establish all the connections. 

\begin{figure}[htbp]
\begin{center}
\begin{picture}(0,0)%
\special{psfile=../962SC96/fig/961.3.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0050in}%
\begin{picture}(920,120)(75,640)
\end{picture}

\end{center}
\caption{Scheduling connections (0, 2), (1, 3),(3, 4), (2, 4)}
\label{EXAM}
\end{figure}


\subsection{Coloring algorithm}

The greedy algorithm  processes the connections in an arbitrary order.
This subsection describes an algorithm that applies a heuristic 
to determine the order to process the connections.
The heuristic assigns higher priorities to connections with fewer
conflicts. By giving the connections with less conflicts higher priorities, 
each configuration is likely to accommodate more connections and thus the
multiplexing degree needed for the patterns is likely to decrease. 

The problem of computing the minimal configuration set is formalized
as a graph coloring problem. A coloring of a graph is an assignment of 
a color to each node of the graph in such a manner that no two nodes 
connected by an edge have the same color. A conflict graph for a set of
connections is built in the following manner, (1) 
each node in the graph 
corresponds to a connection and (2) an edge
is introduced between two nodes if the connections represented by the 
two nodes are conflicted.
As stated by the theorem given below,
the number of colors used to color the graph is equal to the number of 
configurations needed to handle the connections. 

%\begin{description}
%\item
\noindent
{\bf Theorem:} Let $Comm=\{(s_{1}, d_{1}),(s_{2}, d_{2}),...,(s_{m}, d_{m})\}$
be the set of connections and $G = (V, E)$ be the conflict graph for $Comm$. 
There exists a configuration set $M = \{C_{1}, C_{2}, ..., C_{t}\}$
for $R$ if and only if $G$ can be colored with $t$ colors.
%\end{description}

\noindent
{\bf Proof}: Since connections that correspond to the nodes with the same 
color do not conflict with each other, they can be placed in 
one configuration. $\Box$

%Prove: ($\Rightarrow$) Assuming R has 
%configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. Let 
%$(s_{i}$, $d_{i}) \in C_{k}$, node $n_{i}$ can be colored by color $k$. 
%Therefore, there are totally $t$ colors in the graph. Now, we need to prove
%that for any two node $n_{i}$, $n_{j}$ such that $(i, j) \in E$, the two nodes
%are colored by different color. By the construction of conflict graph,
%if $(i, j) \in E$, node $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ share same links,
%hence, by the construction of configuration, $(s_{i}, d_{i})$ and
%$(s_{j}, d_{j})$  is in different configuration, thus $n_{i}$ and $n_{j}$ is
%colored by different colors. Hence, G can be colored by $t$ colors.
%
%($\Leftarrow$) Assuming G can be colored by $t$ colors. Let 
%$C_{i}$ = {$( s_{j}, d_{j})$ : $n_{j}$ is colored by color j}, 
%$M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. To prove 
%M is a configuration for R, we need to 
%prove 1) for any $(s_{i}, d_{i}) \in R$,
%there exists a $C_{k}$ such that $(s_{i}, d_{i}) \in C_{k}$, and 2) $C_{k}$
% must
%be a configuration. The first condition is trivial. Now, let us consider
%the second condition. Let $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ belong to 
%$C_{k}$, by the construction the G, $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$
%do not share any links. Therefore, $C_{k}$ is a configuration. Hence, there
%exist configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} for R. $\Box$
%
%\begin{description}
%\item
%{\bf Corollary:} The optimal multiplexing degree for establishing 
%connections in $R$ is equivalent to the minimum number colors to 
%color graph $G$.
%\end{description}


Thus, the
coloring algorithm attempts to minimize the number of colors used in 
coloring the graph. Since the coloring problem is known to be NP-complete, 
a heuristic is used for graph coloring. The heuristic determines the order 
in which nodes are colored using the node priorities.
The algorithm is summarized in Fig~\ref{COLOR}. It should be noted that
after a node is colored, the algorithm updates the priorities of uncolored 
nodes. This is because in computing the degree of an uncolored node, 
only  the edges that connect the node to other uncolored nodes are 
considered. 
The algorithm finds a solution in linear time (with respect to the 
size of the conflict graph). The time complexity of the algorithm is 
$O(|Comm|^2\times max_i(|C_{i}|)\times d)$, where $|Comm|$ is the 
number of the 
connections, $|C_{i}|$ is the number of connections
 in configuration $C_{i}$ and 
$d$ is the total number of configurations generated.


\begin{figure}[htbp]
%\small
\begin{tabbing}
\hspace{0.5in}(1)\hspace{0.3in} Construct conflict graph G = (V, E)\\
\hspace{0.5in}(2)\hspace{0.3in} Calculate the priority for each node\\
\hspace{0.5in}(3)\hspace{0.3in} MC = $\phi$, k = 1\\
\hspace{0.5in}(4)\hspace{0.3in} NCSET = V\\
\hspace{0.5in}(5)\hspace{0.3in} {\bf re}\={\bf peat}\\
\hspace{0.5in}(6)\hspace{0.3in} \>Sort NCSET by priority\\
\hspace{0.5in}(7)\hspace{0.3in} \> WORK = NCSET\\
\hspace{0.5in}(8)\hspace{0.3in} \> $C_{k} = \phi$\\
\hspace{0.5in}(9)\hspace{0.3in} \>{\bf wh}\={\bf ile} (WORK $\ne \phi$)\\
\hspace{0.5in}(10)\hspace{0.3in} \>\> Let $n_{f}$ be the first 
                                      element in WORK\\
\hspace{0.5in}(11)\hspace{0.3in} \>\>$C_{k} = C_{k} \bigcup \{<s_{f}, d_{f}>\}$\\
\hspace{0.5in}(12)\hspace{0.3in} \>\>NCSET = NCSET $- \{n_{f}\}$\\
\hspace{0.5in}(13)\hspace{0.3in} \>\>{\bf fo}\={\bf r} {\bf each}  $n_{i} \in NCSET$ 
                                      and $(f, i) \in E$ {\bf do} \\
\hspace{0.5in}(14)\hspace{0.3in} \>\>\> update the priority of $n_{i}$\\
\hspace{0.5in}(15)\hspace{0.3in} \>\>\> WORK = WORK - $\{n_{i}\}$\\
\hspace{0.5in}(16)\hspace{0.3in} \>\>{\bf end for}\\
\hspace{0.5in}(17)\hspace{0.3in} \>{\bf end while}\\
\hspace{0.5in}(18)\hspace{0.3in} \>MC = MC + $\{C_{k}\}$\\
\hspace{0.5in}(19)\hspace{0.3in} {\bf until} NCSET = $\phi$
\end{tabbing}
\normalsize
\caption{The graph coloring heuristic.}
\label{COLOR}
\end{figure}

For torus and mesh networks, a suitable choice for priority for a
connection is the ratio of the number of links in the path 
from the source to the destination and the degree of the node 
corresponding to the connection in $G$. 
Applying the coloring algorithm to the example in Figure~\ref{EXAM},
in the first iteration, the connections are reordered as 
$\{(0, 2), (1, 3), (2, 4), (3, 4)\}$ and connections (0, 2), (2, 4) will be
put in time slot 1. In the second iteration, connections (1, 3), (3, 4) are
put in time slot 2. Hence, applying the 
coloring algorithm will use 2 time slots
to accommodate the connections.

\subsection{Ordered AAPC algorithm}

The graph coloring algorithm has better performance than the greedy heuristic.
However, for dense communication patterns the heuristics cannot guarantee that
the multiplexing degree found would be bounded by the minimum multiplexing 
degree needed to realize the all-to-all pattern. The algorithm described in 
this section targets dense communication patterns. By grouping the connections
in a more organized manner, better performance can be achieved for dense 
communication.

The worst case of arbitrary communication is the {\em all-to-all personalized 
communication} (AAPC)  where each node sends a message to every 
other node in the system. Any communication pattern can be embedded in AAPC. 
Many algorithms \cite{Hinrichs94,Horie91} have been designed to 
perform AAPC efficiently for different topologies.
Among these algorithms, the ones that are of 
interests to us are the phased AAPC algorithms, in which the AAPC connections 
are partitioned into contention--free phases. A phase in this kind of AAPC 
corresponds to a configuration. Some phased AAPC algorithms are optimal in
that every link is used in each phase and every connection follows the
shortest path. Since all the connections in each AAPC phase are 
contention--free,
they form a configuration that uses all the links in the system. 
Each phase in the phased AAPC communication forms an {\em AAPC configuration}.
The set of {\em AAPC configurations} for AAPC communication pattern is 
called {\em AAPC configurations set}. 
The following theorem states the property 
of connection scheduling using AAPC phases.

%\begin{description}
%\item 

\noindent{\bf Theorem: } Let $Comm =
\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ be the 
set of connections, if $Comm$ can be partitioned into $K$ phases 
$P_1 = \{(s_{1}, d_{1}), ... , (s_{i_{1}}, d_{i_{1}})\}$,\\ 
$P_2 = \{(s_{i_{1} + 1}, d_{i_{1} + 1}), ... , (s_{i_{2}}, d_{i_{2}})\}$,
... ,
 $P_K = \{(s_{i_{K-1} + 1}, d_{i_{K-1} + 1}), ... , 
(s_{i_{K}}, d_{i_{K}})\}$, such that $P_i$, $ 1 \le i \le K$, is a subset
of an AAPC configuration. Using the greedy algorithm to schedule the
connections $(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})$
results in a multiplexing degree less than or equal to K.

\noindent
{\bf Proof}: The theorem is proven  by contradiction 
that for any $\alpha$, 
$1\le \alpha \le m$, let $(s_\alpha, d_\alpha) \in P_\beta$, 
$1\le \beta \le K$, connections $(s_1, d_1)$, ..., $(s_\alpha, d_\alpha)$
can be scheduled by the greedy algorithm using a multiplexing degree
less than or equal to $\beta$.

Let $(s_\alpha, d_\alpha) \in P_\beta$ be the first connection that does
not satisfy the above proposition. That is,  
$(s_1, d_1)$, ..., $(s_{\alpha-1}, d_{\alpha-1})$ are scheduled using 
a multiplexing degree of $\beta$ and $(s_\alpha, d_\alpha)$ cannot be
accommodated in configuration $\beta$. Since the connections 
in $P_\beta$ do not conflict with each other, 
another connection that belongs to  $P_\gamma$, $\gamma < \beta$ must be
scheduled in configuration $\beta$. Hence, $(s_\alpha, d_\alpha)$ is not the
first connection that does not satisfy the proposition, which contradicts
the assumption. $\Box$

The theorem states that if the connections
are reordered by the AAPC phases,
at most all AAPC 
phases are needed to realize arbitrary pattern using the
greedy scheduling algorithm. For example, following the algorithms in 
\cite{Hinrichs94}, $N^3/8$ phases are needed for a $N\times N$ torus. 
Therefore, in a $N\times N$ torus, $N^3/8$ degree is enough to satisfy
any communication pattern.

To obtain better performance on dense communication patterns, it is 
better to keep the connections in their AAPC format as much as possible. 
It is therefore better to schedule the phases with higher link 
utilization first. This heuristic is used in the ordered AAPC algorithm.
In ordered AAPC algorithm, the rank of the AAPC phases is calculated so 
that the phase that has higher utilization has higher rank. The phases 
are then scheduled according to their ranks. The algorithm is depicted in 
Figure~\ref{ORDAAPC}. The time complexity of this algorithm is
$O(|Comm|(lg(|Comm|) + max_i(|C_{i}|)\times K))$, where $|Comm|$ 
is the number of 
the connections, $|C_{i}|$ is the number of connections in configuration
$C_{i}$ and $K$ is the number of configurations needed. The advantage 
of this algorithm is that for this algorithm the multiplexing degree 
is bounded by $N^3/8$. Thus, in situations where the greedy or coloring
heuristics fail to meet this bound, AAPC can be used.  

\begin{figure}[ht]
%\small
\begin{tabbing}
\hspace{1in}(1)\hspace{0.3in}PhaseRank[*] = 0\\
\hspace{1in}(2)\hspace{0.3in}{\bf for}\= $(s_{i}, d_{i}) \in Comm$ {\bf do}\\
\hspace{1in}(3)\hspace{0.3in}\>let $(s_{i}, d_{i}) \in A_{k}$\\
\hspace{1in}(4)\hspace{0.3in}\>PhaseRank[k] = PhaseRank[k] + length($(s_{i}, d_{i})$)\\
\hspace{1in}(5)\hspace{0.3in}{\bf end for}\\
\hspace{1in}(6)\hspace{0.3in}sort phase according to PhaseRank\\
\hspace{1in}(7)\hspace{0.3in}Reorder $Comm$ according the sorted phases.\\
\hspace{1in}(8)\hspace{0.3in}call greedy algorithm\\
\end{tabbing}
\caption{Ordered AAPC scheduling algorithm}
\label{ORDAAPC}
\normalsize
\end{figure}

\subsection{Performance of the scheduling algorithms}

In this section,  the performance of the connection scheduling
algorithms on $8\times 8$ torus topology is studied. 
The performances of the algorithms 
are evaluated using randomly generated communication patterns, patterns
encountered during data redistribution, and some frequently used 
communication patterns. The metric used to compare the algorithms is the 
multiplexing degree needed to establish the connections.
It should be noted that a dynamic scheduling algorithm will not perform
better than the greedy algorithm since it must establish the connections 
by considering the connections in the order that they arrive. 

A {\em random communication pattern} consists of a certain number of 
random connections. A random connection is obtained
by randomly generating a source and a destination. Uniform probability
distribution is used to generate the sources and destinations.
The {\em data redistribution communication patterns} are obtained by
considering the communication results from array redistribution. In this
study,   data redistributions of a 3D array are considered. The array
has block--cyclic distribution in each dimension. The distribution of a
dimension can be specified by the block size and the number of processors
in the dimension.  A distribution is denoted  as {\em p:block(s)}, where
$p$ is the number of processors in the distribution and $s$ is the block size.
When the distribution of an
array is changed (which may result from the changing of the value $p$ or 
$s$), communication may be needed. 
Many programming
languages for supercomputers, such as CRAFT FORTRAN, allow an array to be
redistributed within a program. 

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
number of  & greedy  & coloring & AAPC & combined &improvement\\
connections. & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
100  & 7.0  & 6.7 & 6.9 & 6.6 & 6.3\%\\
\hline
400 & 16.5  & 16.1 & 16.5 & 15.9 & 3.8\%\\
\hline
800 & 27.2  & 25.9 & 26.5 & 25.6 & 6.3\%\\
\hline 
1200 & 36.3 & 34.5 & 35.3 & 34.2 & 6.1\%\\
\hline
1600 & 45.0  & 43.5 & 43.4 & 42.8 & 5.1\%\\
\hline
2000 & 53.4  & 50.4 & 50.4 & 49.7 & 7.4\%\\
\hline
2400 & 60.8  & 57.5 & 57.4 & 56.7 & 7.2\%\\
\hline
2800 & 68.8  & 64.4 & 62.4 & 62.4 & 10.2\%\\
\hline
3200 & 76.3  & 70.8 & 64 & 64 & 19.2\%\\
\hline
3600 & 83.9  & 76.8 & 64 & 64 & 31.1\%\\
\hline
4000 & 91.6  & 83 & 64 & 64 & 43.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Performance for random patterns}
\label{RANDOM}
\end{table}

Table~\ref{RANDOM} shows the multiplexing degree required to establish
connections for random communication patterns using the algorithms
presented.
The results in each row are the averages obtained from scheduling 100 different
randomly generated patterns with the specific number of connections.
The results in the column labeled {\em combined algorithm} are obtained 
by using 
the minimum of  the coloring algorithm and
the AAPC algorithm results.
Note that in compiled communication, more time can be spent
to obtain better runtime network utilization. Hence,  the
combined algorithm can be used to obtain better result by the compiler. The 
percentage  improvement shown in the sixth column
is achieved by the combined algorithm over the
dynamic scheduling. 
It is  observed that the coloring algorithm is always
better than the greedy  algorithm
and the AAPC algorithm is better than 
the other algorithms when the communication is dense. 
It can be seen that for sparse random patterns (100 - 2400
connections), the 
improvement range varies from 3.8\% to 7.2\%. Larger improvement
results for dense communication.  For example, the combined algorithm
uses 43.1\% less multiplexing degree than that of the greedy algorithm
for all--to--all pattern. 
This result confirms the result in \cite{Hinrichs94} that
it is desirable to use compiled communication for dense communication.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
No. of  & No. of  & greedy   & coloring & AAPC & combined &improvement\\
connections & patterns & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
0 - 100 & 34  & 1.2 &  1.2 & 1.2 & 1.2 & 0.0\%\\
\hline
101 - 200 & 50 & 5.9 & 4.9 & 4.8 & 4.6 & 28.3\%\\
\hline
200 - 400 & 54 & 10.6 &  9.7 & 10.0 &  9.5 & 11.6\%\\
\hline 
401 - 800 & 105 & 17.7 & 15.9 & 16.0& 15.5 & 14.2\%\\
\hline
801 - 1200 & 122 & 31.7 & 28.7 & 28.6 & 27.6 & 14.9\%\\
\hline
1201 - 1600 & 0  & 0      & 0    & 0    &0     &    0\%\\
\hline
1601 - 2000 & 15 & 46.3 &  42.8 & 35.1 & 35.1 & 31.9\%\\
\hline
2001 - 2400 & 77 & 55.5 &  51.5 & 51.9 & 50.4 & 10.1\%\\
\hline
2401 - 4031 & 0  & 0       & 0    & 0     &   0   &  0\% \\
\hline
4032     & 43 & 92  & 83 & 64 &  64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for data distribution patterns}
\label{REDIST}
\end{table}

To obtain more realistic results,  the performance is also evaluated using
the communication patterns for data redistribution and some
frequently used communication patterns which occurs in the programs 
analyzed by the E--SUIF compiler.
Table~\ref{REDIST} shows the performance of the algorithms for data 
redistribution patterns. The communication patterns
are  extracted from the communication resulting from 
the random data redistribution of a 3D array of size
$64 \times64 \times 64$. 
The  random data redistribution is created by randomly generating
the source data distribution and
the destination data distribution with regard to
the number of processors allocated to each dimension and the block size
in each dimension. Precautions are taken to make sure that the 
total processor number is 64 and the block size is not too large so that
some processors do not contain any part of the array.
The table lists the results for 500 random data redistributions. The first
column lists the range of the number of connections in each pattern.
The second column lists the number of data redistrictions whose number of 
connections fell into the range. For example, the second column in the
last row indicates that among the 500 random data redistributions, 43
results in 4032 connections. Columns three to six
list the multiplexing degree required by the greedy algorithm, the coloring
algorithm, the AAPC algorithm and the 
combined algorithm respectively. The seventh 
column lists the percentage improvement
 by the combined algorithm over the greedy
algorithm.
The result shows that the 
 multiplexing degree required to establish connections resulting
from data redistribution is less than that resulting 
from the random communication patterns. 
For the data redistribution pattern, the percentage improvement 
obtained by using
the combined algorithm  ranges from
10.1\% to 31.9\%, which is larger than the improvement for the random 
communication patterns.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Pattern & No. of conn. & greedy  & coloring & AAPC & comb & percentage\\
\hline
ring    & 128 &  3  &  2 & 2  &  2   &   50\%\\
\hline
nearest neighbor & 256 & 6  & 4 & 4  &  4 & 50\%   \\
\hline
hypercube & 384 &  9 & 7 & 8 &  7 & 28.6\%\\
\hline
shuffle--exchange & 126 &  6& 4 & 5  & 4 & 50\% \\
\hline 
all--to--all & 4032 &  92 & 83 & 64 & 64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for frequently used patterns}
\label{FUSED}
\end{table}

Table~\ref{FUSED} shows the performance for some
frequently used communication patterns. In the ring and the
nearest neighbor patterns, no conflicts arise in the links. 
However, there are conflicts in the communication switches.
The performance gain is higher for these 
specific patterns when the combined algorithm is used.

\section{Communication Phase analysis} 

Armed with the connection scheduling algorithms, the compiler can
determine when two communication patterns can be combined so that
the underlying network can support both patterns simultaneously and
thus, can partition a program into phases such that each phase contains
connections that can be supported by the underlying network. 
This section considers the compiler algorithm to partition a program.
%into phases so that each phase will contains communications
%that can be supported by the underlying network. 
%Besides using  the 
%algorithms described in the previous section to obtains communication 
%patterns (on physical processors), we also use the algorithms in 
%\cite{Yuan96} to obtain the number of channels needed to 
%support a communication pattern on torus networks.

The communication phase analysis is carried out in a recursive manner on 
the high level SUIF representation of a program, 
which is similar to an abstract syntax tree. 
%Here, the high level SUIF
%will be briefly introduced. 
%Before I present the communication phase analysis algorithm, I 
%will first briefly describe the high level SUIF representation. 
%Details about 
%the SUIF representation can be found in \cite{SUIF}.
SUIF represents a program in a hierarchical manner. 
A procedure contains a list of SUIF nodes, where each
node can be of different types and can contain sub--lists. Some important
SUIF node types include TREE\_FOR, TREE\_LOOP,
TREE\_IF, TREE\_BLOCK and TREE\_INSTR. 
A TREE\_FOR node represents a for--loop structure. 
It contains four sub--lists, $lb\_list$ which contains the SUIF
to compute the 
lower bound, $ub\_list$ which contains the nodes to compute the upper bound,
$step\_list$ which contains the nodes to compute the step, and 
$body$ which contains the 
loop body. A TREE\_LOOP node represents a 
while--loop structure. It contains two sub--lists, $test$ and $body$.
A TREE\_IF node represents an if--then--else structure. It contains
three sub--lists, $header$ which is the test part, $then\_part$ which contains
the nodes in the then part, and the $else\_part$. A TREE\_BLOCK node
represents a block of statements, it contains a sub--list $body$.
A TREE\_INSTR nodes represents a statement.

%Each structure has two variables associated with it, the 
%communication pattern, $pattern$, which is the communication patterns 
%expose in the header before any, and the phase indicator, $phase$ to indicate
%whether there are phases (network reconfiguration) within the structure. 

%\subsection{Phase analysis algorithms}

Given a SUIF representation of a program, which contains a list of nodes,
the communication phase analysis algorithm determines the communication
phases for each sub--lists in the list and then determines the 
communication phases of the list. 
In addition to the annotations for communications, a {\em composite node}, 
which contains sub--lists, is associated with
two variables, $pattern$, which is the communication pattern that is exposed
from the sub--lists, and the $kill\_phase$, which has a boolean value, 
indicating whether its sub--lists contain phases.

\begin{figure}[tbph]
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Communication\_Phase\_Analysis(list)\\
\hspace{0.5in}Input: $list$: a list of SUIF nodes\\
\hspace{0.5in}Output:\=\ $pattern$: communication pattern exposed out of the 
                      list\\ 
              \>$kill\_phase$: whether there are phases within the list\\
\\
\hspace{0.5in}\=An\=al\=yze communication phases for each node in the list.\\
       \>$c\_pattern = NULL, kill\_phase = 0$\\
       \>{\bf For each} node $n$ in list in backward order {\bf do}\\
       \>\> {\bf if} ($n$ is annotated with $kill\_phase$) {\bf then}\\
       \>\>\>Generate a new phase for $c\_pattern$ after $n$.\\
       \>\>\>$c\_pattern = NULL, kill\_phase = 1$\\
       \>\>{\bf end if}\\
       \>\> {\bf if} ($n$ is annotated with communication pattern $a$) 
           {\bf then}\\
       \>\>\>$new\_pattern = c\_pattern + a$\\
       \>\>\>{\bf if}\=\ ($multiplexing\_degree(new\_pattern) \le d$) 
             {\bf then}\\
       \>\>\>\>$c\_pattern$ = new\_pattern\\
       \>\>\>{\bf else}\\
       \>\>\>\>Generate a new phase for $c\_pattern$ after $n$.\\
       \>\>\>\>$c\_pattern = a, kill\_phase = 1$\\
       \>\>\>{\bf end if}\\
       \>\>{\bf end if}\\
       \>{\bf end for}\\
       \>{\bf return} $c\_pattern$ and $kill\_phase$
\end{tabbing}
\end{center}
\caption{Communication phase analysis algorithm}
\label{PHASE}
\end{figure}





\begin{figure}[tbph]
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Communication\_Phase\_Analysis for TREE\_IF\\
\\
\hspace{0.5in}\=Analyze the $header$ list.\\
             \>Analyze the $then\_part$ list.\\
             \>Analyze the $else\_part$ list.\\
             \>Let $comb =$ the combined communications from the three 
               sub--lists.\\
             \>{\bf If}\=\ (there are phase changes in the sub--lists) 
               {\bf then}\\
             \>\>Generate a phase in each sub--list for the 
                 communication exposed.\\
             \>\>$pattern = NULL, kill\_phase = 1$\\
             \>{\bf if} ($multiplexing\_degree(comb) > d$) {\bf then}\\
             \>\>Generate a phase in each sub--list for the 
                 communication exposed.\\
             \>\>$pattern = NULL, kill\_phase = 1$\\
             \> {\bf else}\\
             \>\> $pattern = comb, kill\_phase = 0$\\
             \>{\bf end if}\\
             \>Annotate the TREE\_IF node with $pattern$ and $kill\_phase$.
\end{tabbing}
\end{center}
\caption{Communication phase analysis for TREE\_IF nodes}
\label{treeif}
\end{figure}

The algorithm to analyze communication phases in a program
for a node list is shown in Figure~\ref{PHASE}. 
The algorithm assumes that the multiplexing degree for the 
system is $d$. It also uses one of the algorithms discussed in 
section~\ref{cscheduling}, denoted as\\
$multiplexing\_degree(Comm)$,
to compute the multiplexing degree required to 
realize communication pattern $Comm$. Given a node list, the algorithm 
first recursively examines the sub--lists of all nodes 
and annotates the nodes with $pattern$ and $kill\_phase$. 
This post--order traversal of the
SUIF program accumulates the communications in the innermost loops first,
and thus can capture the communication locality when it exists and is 
supported by the underlying
network. Figure~\ref{treeif} describes the operations for
TREE\_IF nodes. The algorithms for TREE\_IF node computes
the phases for the three sub--lists. In the cases when there are phases 
within the sub--lists and  when the network does not have enough capacity
to support the combined communication, a phase is created in 
each of the sub--list to accommodate the corresponding communication
from that sub--list. Otherwise, the TREE\_IF node is annotated with 
the combined communication indicating the communication requirement of
the IF statement. Algorithms for processing other node types are 
similar. After all sub--lists in all nodes in the list are analyzed, 
the node list contains a straight line program, whose nodes are annotated with
communication, $pattern$ and $kill\_phase$.
The algorithm examines all these annotations in each node
from back to front. A variable $c\_pattern$ is
used to maintain all communications currently accumulated. 
There are two cases when a phase is generated. First, 
once a $kill\_phase$ annotation is encountered, which indicates there are 
phases in the sub--lists,  thus, it does not make sense to maintain 
a phase passing the node since there are phase changes during the
execution of the sub--lists, a new
phase is created to accommodate the connection requirement after the node. 
Second, in the cases when adding a new communication pattern 
into the current (accumulated) pattern exceeds the network capacity, 
a new communication phase is needed.
% Notice that a SCD can represent a communication pattern that is
%as complex as all to all connections. It might require 
% several communication phases for a single communication.


\begin{figure}
\centerline{\psfig{figure=fig/phaseexam.eps,width=5.9in}}
\caption{An example for communication phase analysis}
\label{phaseexam}
\end{figure}

Figure~\ref{phaseexam} shows an example for the communication phase analysis.
The program in the example contains six communications, $C0$, $C1$,
$C2$, $C3$, $C4$, $C5$ and $C6$, an IF structure and a DO structure.
The communication phase analysis algorithm first analyzes
the sub--lists in the IF and DO structures. Assuming the combination
of  $C1$ and $C2$ can be supported by the underlying network, while 
combining communications $C1$, $C2$ and $C3$ exceeds the network 
capacity, which results in the two phases in the IF branches and the 
$Kill\_phase$ is set for the IF header node. Assuming that all communications
of $C5$ within the DO loop can be supported by the underlying network,
Figure~\ref{phaseexam}~(a) shows the results after the sub--lists are analyzed.
The algorithm then analyzes the list by considering each node from back
to forth, it combines communications $C4$ and $C5$. Since the IF header
node is annotated with $kill\_phase$. A new phase is generated for 
communications $C4$ and $C5$ after the IF structure. The algorithm then
proceeds to create a phase for communication $C0$. Figure~\ref{phaseexam}~(c)
shows the final result of the communication phase analysis for this example.

%Here let me construct a communication phase analysis example.
%Let me think of a good example.

\subsection{Evalutation of the communication phase analysis algorithm}
\label{evalphase}

This section  presents the performance evaluation of the E--SUIF compiler
for compiled communication.
The compiler is evaluated with respect to the analysis time and 
the runtime performance.  The E--SUIF compiler 
analyzes the communication requirement of a program
and partitions the program into phases 
such that each phase contains a communication pattern that can be realized 
by a multiplexing degree of $d$, where $d$ is a parameter. In addition, the
compiler also gives channel assignments for connections in each phase.
It is  assumed that the underlying network is a $8\times 8$ torus.

\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|}
\hline
Prog. & Description & Distrib.\\
\hline
0001 & Solution of 2-D Poisson Equation by ADI & (*, block)\\
\hline
0003 & 2-D Fast Fourier Transform & (*, block)\\
\hline
0004 & NAS EP Benchmark - Tabulation of Random Numbers & (*, block)\\
\hline
0008 & 2-D Convolution & (*, block)\\
\hline
0009 & Accept/Reject for Gaussian Random Number Generation & (block) \\
\hline
0011 & Spanning Percolation Cluster Generation in 2-D & (*, block)\\
\hline
0013 & 2-D Potts Model Simulation using Metropolis Heatbath & (*, block)\\
\hline
0014 & 2-D Binary Phase Quenching of Cahn Hilliard Cook Equation & (*, block) \\
\hline
0022 & Gaussian Elimination - NPAC Benchmark & (*, cyclic) \\
\hline
0025 & N-Body Force Calculation - NPAC Benchmark & (block, *) \\
\hline
0039 & Segmented Bitonic Sort & (block) \\
\hline
0041 & Wavelet Image Processing & (*, block)\\
\hline
0053 & Hopfield Neural Network & (*, block) \\
\hline
\end{tabular}
\end{center}
\caption{Benchmarks and their descriptions}
\label{desc}
\end{table}

Programs from the HPF benchmark suite
at Syracuse University are used to evaluate the algorithms. 
The benchmarks and their descriptions are listed in Table~\ref{desc}. 
The table also shows the data
distribution of the major arrays in the programs. These 
distributions are obtained from the original benchmark programs.
Table~\ref{analysistime} breaks down the analysis
time. The table shows the time for overall analysis, the logical
communication analysis and the communication phase analysis.
The overall analysis includes
the time to load and store the program, 
the time to analyze communication requirement on the
virtual processor grid, 
the time to derive communication requirement on the
physical processor grid and the time for communication phase analysis.
The communication phase analysis time accounts for a significant portion
of the overall analysis time for all the programs. 
This is because the communication phase operates on large sets of data
(communication pattern). %Hence, to apply this technique for large programs,
%the analysis cost must be improved by carefully designing the analysis 
%algorithms and data structures. 
However, for medium size programs, such as the benchmarks used,
the analysis time is not significant. 

\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|c|c|}
\hline
benchmarks & size (lines) & overall & logical communication & phase analysis\\
\hline
0001 & 545 & 11.33 & 0.45 & 8.03 \\
\hline
0003 & 372 & 24.83 & 0.50 & 11.80\\
\hline
0004 & 599 & 19.08 & 0.42 & 15.02\\
\hline
0008 & 404 & 27.08 & 0.68 & 13.28\\
\hline
0009 & 491 & 46.72 & 4.45 & 19.65\\
\hline
0011 & 439 & 14.78 & 0.57 & 11.37\\
\hline
0013 & 688 & 23.08 & 1.07 & 17.30\\
\hline
0014 & 428 & 15.58 & 1.03 & 11.38 \\
\hline
0022 & 496 & 22.57 & 0.77 & 18.35 \\
\hline
0025 & 295 & 5.77 & 0.78 & 3.35 \\
\hline
0039 & 465 & 16.08 & 0.38 & 13.13 \\
\hline
0041 & 579 & 9.93 & 0.28 & 6.62\\
\hline
0053 & 474 & 7.39 & 0.35 & 4.33\\
\hline
\end{tabular}
\end{center}
\caption{Communication phase analysis time}
\label{analysistime}
\end{table}


\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|c|c|c|c|}
\hline
benchmark & \multicolumn{3}{c|}{connections per phase} &  \multicolumn{3}{c|}{
channels per phase}\\
\cline{2-7}
programs  & actual & compiled & percentage & actual & compiled & percentage\\
\hline
0001 & 564.4 & 564.4 & 100\% & 9.1 & 9.1 &100\% \\
\hline
0003 & 537.6 & 537.6 & 100\% & 8.6 & 8.6 & 100\% \\
\hline
0004 & 116.3 & 116.3 & 100\% & 5.5 & 5.5 & 100\% \\
\hline
0008 & 562.6 & 562.6 & 100\% & 8.9 & 8.9 & 100\% \\
\hline
0009 & 91.2 & 230.7 & 39.6\% & 4.3 & 6.6 & 65.1\% \\
\hline
0011 & 126.3 & 126.3 & 100\% & 5.2 & 5.2 & 100\% \\
\hline
0013 & 67.3 & 67.3 & 100\% & 3.1 & 3.1 & 100\% \\
\hline
0014 & 126.4 & 126.4 & 100\% & 4.0 & 4.0 & 100\% \\
\hline
0022 & 13.1 & 413.2 & 3\% & 4.6 & 8.9 & 52.7\% \\
\hline
0025 & 80.0  &80.0 & 100\% & 3.0 & 3.0 & 100\% \\
\hline
0039 &125.7 & 125.8 & 99.9\% & 8.8 & 8.8 & 99.9\%\\
\hline
0041 & 556.1 & 556.1 & 100\% & 8.8 & 8.8& 100\% \\
\hline
0053 & 149.2& 575.2 & 25.9\% & 9.0 & 9.1 & 98.9\\
\hline
\end{tabular}
\end{center}
\caption{Analysis precision}
\label{precision}
\end{table}


Table~\ref{precision} shows the precision of the analysis. It compares
the average number of channels and connections per phase obtained from
our algorithms with  those in  actual executions. The number of channels and 
connections per phase in actual executions is obtained by accumulating the
connections within each phase, which is determined by the compiler. When 
a phase change occurs, the statistics about the number of connections within
each phase is collected and the connection scheduling algorithm is invoked
to compute the number of channels needed for the connections in that phase.
For most programs, the analysis results match the actual program executions,
which indicates that approximations are seldom used. 
For the programs where
 approximations occur, the channel approximation is better
than the connection approximation as shown in benchmark 0022. This is mainly
due to the approximation of the communications that are not vectorized.
For such communications, if the underlying network can support all connections
 in a loop, the phase will contain the loop and use the channels
for all communications in the loop. However, for the connections, the 
compiler approximates each individual communication inside the loop with
all communications of the loop. Since the number of channels for a
communication pattern determines the communication performance, this 
type of approximation does not hurt the communication performance.

\section{Chapter summary}

This chapter addressed the compiler issues for applying compiled 
communication. 
In particular, algorithms for communication analysis were presented
which take into consideration common communication optimizations
including message vectorization, redundant communication elimination and 
message scheduling. A demand
driven array data flow framework, which improves over previous communication
optimization algorithms by reducing the analysis cost and improving the 
analysis precision,  was developed 
for the communication optimizations. 
Three off-line connection
scheduling algorithms were described that realize 
a given communication pattern with a minimal multiplexing degree. 
A communication phase analysis algorithm, which partitions a program 
into phases such that each phase contains communications that can be
supported by the underlying network, was developed. 
The algorithm also exploits 
communication locality to reduce the amount of reconfiguration overhead
during program execution.

A compiler, called the E--SUIF compiler, implements all the above
algorithms and thus, supports  compiled communication.  
The E--SUIF compiler 
compiles a HPF--like program, analyzes its communication requirement,
partitions the program into phases such that each phase contains 
connections that can be supported by the underlying network,
assigns channels for connections in each phase, and outputs a C program
with the communication and phase annotations such that when 
the program is executed, the communications (and phases) 
in the program can be simulated.
All the algorithms were evaluated in the compiler. It was found that
the communication optimization algorithms are efficient in terms of 
the analysis cost and are  effective in finding the optimization 
opportunities. The communication phases analysis algorithm 
generally captures the program runtime behavior accurately. 

In the last three chapters, techniques for the 
three communication schemes are discussed.
Next chapter evaluates the three 
communication schemes and compares their performance using real application
programs. 










\documentstyle[11pt,psfig,setspace,subfigureh,psubfigure,fancyheadings]{thesis}
\newcounter{DefCount} 

\begin{document}

\maketitlepage

\makesignaturepage

\makecopyrightpage

\abstract
{
%With the increasing computation power of parallel computers, 
%interprocessor communication has become an important factor that
%limits the performance of supercomputing systems.
Optical interconnection networks are promising networks for future 
supercomputers due to their large bandwidths. However, the speed mismatch
between the fast optical data transmission and the relatively
slow electronic control components poses challenges for 
designing an optical network whose large bandwidth can be utilized 
by end users. The Time--Division--Multiplexing (TDM) technique 
alleviates this mismatch problem by sacrificing part of the large
optical bandwidth for efficient network control. 
This thesis studies efficient control mechanisms for optical
TDM point--to--point networks. Specifically, 
three communication schemes are considered, 
dynamic single--hop communication, dynamic 
multi--hop communication and compiled communication. 

%\thispagestyle{fancy}
%\setlength{\headrulewidth}{0pt}
%\rhead{Dr. Rami Melhem, Dr. Rajiv Gupta}
%\markright{\hspace{3.5in}Dr. Rami Melhem, Dr. Rajiv Gupta}

Dynamic single--hop communication uses a path reservation protocol to 
establish all--optical paths for connection requests that arrive at
the network dynamically. 
%In this 
%scheme, once a message is converted into the optical domain, 
%it remains there until it reaches the destination.
%No electronic/optical (E/O) and optical/electronic (O/E) conversions, 
%or electronic processing and buffering are performed 
%at intermediate nodes during data transmission. 
%In this communication scheme, 
%the electronic processing
%is isolated in the path reservation process. Hence, 
An efficient path 
reservation protocol is essential for this scheme to achieve high 
performance. In this thesis, a number of efficient distributed path
reservation protocols are designed and the impact of 
system parameters on these protocols is studied. 

Dynamic multi--hop communication allows intermediate hops to 
route messages toward their destinations.
% and thus, requires
%E/O and O/E conversions and electronic processing at intermediate
%hops. 
%
% Since the routing at 
%intermediate hops requires E/O and O/E conversions, it is
%important to reduce the number of intermediate hops in this 
%scheme. 
In optical TDM networks,
efficient dynamic multi--hop communication can be achieved
by routing messages through a logical topology that is more efficient than 
the physical topology. 
This thesis studies efficient schemes to realize logical topologies 
on top of physical torus topologies, presents an analytical model
for analyzing the maximum throughput and the average packet delay  
for multi--hop networks, and evaluates the performance of the optical 
communication on the logical 
topologies.

%In compiled communication, the compiler analyzes the communication
%requirement of a program and  partitions the program into phases 
%such that each phase contains communications 
%that can be supported by the underlying network. Using the 
%knowledge of the underlying network and the communication requirement, 
%the compile manages network resources, such as virtual
%channels, statically. By using the compiled communication technique, 
%runtime communication overheads, such as the path reservation overhead, 
%can be reduced or eliminated, and the communication 
%performance is improved. This technique is particularly attractive to 
%optical interconnection networks since optical networks have large control 
%overheads compared to electronic networks. 

Compiled communication eliminates the runtime communication overheads
of the dynamic communications by managing network resources 
at compile time. 
%Many compiler issues must be addressed in order to apply
%this technique. 
This thesis considers
issues for   applying  the compiled communication technique to
optical TDM networks, including communication analysis, connection scheduling
and communication phase analysis. 
%Specifically, it describes algorithms to 
%analyze the communication requirement of a program, proposes off--line 
%connection scheduling schemes that schedule connections using 
%a minimal multiplexing degree, and describes a 
%communication phase analysis algorithm which 
%partitions the program into phases such that each phase 
%contains communications that can be supported by the underlying network. 
A compiler, called the E--SUIF compiler, is implemented to support
compiled communication on optical TDM networks.
%All the algorithms are implemented and evaluated in a compiler based on the 
%Stanford SUIF compiler. 

%Communication in optical interconnection networks can be carried out 
%using any of the three communication schemes. 
Each communication scheme has its 
advantages and limitations
and is more efficient for some types of
communication patterns. 
%Dynamic single--hop communication 
%achieves all--optical communication during data transmission, 
%however, it requires extra hardware support
%to exchange control messages and results in large startup overhead.
%Dynamic multi--hop communication does not have large startup overhead,
%yet it requires electronic processing during data transmission. Compiled
%communication accomplishes all--optical communication without extra
%hardware support and large startup overhead. However, the performance
%of compiled communication relies heavily on the precision of compiler 
%analysis. It cannot be applied to applications whose communication patterns
%are known only at runtime time. 
This thesis compares the performance 
of the three communication schemes using a number of benchmarks and real
application programs and identifies the situations where each communication 
scheme out--performs the other schemes. 
}

\tableofcontents
\listoffigures
\listoftables

\bibliographystyle{plain}

\input{intro}

\input{background}

\input{singlehop}
\input{multihop}
\input{compiled}
\input{perform}

%\newpage

\begin{thebibliography}{9}

\bibitem{Acampora89}
A.S. Acampora and M.J. Karol, ``An Overview of Lightwave Packet Network.''
{\em IEEE Network Mag.}
3(1), pages 29-41, 1989.

\bibitem{amarasinghe93}
S. P. Amarasinghe and M. S. Lam ``Communication Optimization and Code
Generation for Distributed Memory Machine.'' In {\em Proceedings ACM SIGPLAN'93
Conference on Programming Languages Design and Implementation}, June 1993.

\bibitem{Amar95}
S. P. Amarasinghe, J. M. Anderson, M. S. Lam and C. W. Tseng,
``The SUIF Compiler for Scalable Parallel Machines.'' 
{\em Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing}, February, 1995.

\bibitem{As94}
H.R. As, ``Media Access Techniques: the Evolution towards Terabit/s LANs
and MANs.'' {\em Computer Networks and ISDN Systems},
26(1994) 603--656.

\bibitem{banerjee95}
P. Banerjee, J. A. Chandy, M. Gupta, E. W. Hodges IV, J. G. Holm, A. Lain, D. J. Palermo, S. Ramaswamy, and E. Su.
``The PARADIGM Compiler for Distributed-Memory Multicomputers.''
     in {\em IEEE Computer}, Vol. 28, No. 10, pages 37-47, October 1995.

\bibitem{Bannister90}
J.A. Bannister, L. Fratta and M. Gerla ``Topological Design of the
Wavelength--Division Optical Network.'' {\em IEEE INFOCOM'90}, 
pages 1005---1013, 1990.


\bibitem{Barry95}
R.A. Barry and P.A. Humblet. ``Models of Blocking Probability in All--optical
Networks with and without Wavelength Changers.'' In {\em Proceeding of
IEEE Infocom}, pages 402-412, April 1995.

\bibitem{beauquier97} B. Beauquier, J. Bermond, L. Gargano, P. Hell,
S. Perennes and U. Vaccaro ``Graph Problems Arraying from Wavelength--Routing
in All--Optical Networks.'' {\em Workshop on Optics and 
Computer Science}, 1997. 

\bibitem{Brackett90}
C. A. Brackett, ``Dense wavelength division multiplexing networks:
Principles and applications,'' {\em IEEE Journal on Selected Areas of 
Communications}, Vol. 8, pp. 948-964, Aug. 1990.

\bibitem{Brassil94} 
J. Brassil, A. K. Choudhury and N.F. Maxemchuk, ``The Manhattan Street 
Network: A High Performance, Highly Reliable Metropolitan Area Network,''
{\em Computer Networks and ISDN Systems},
26(6-8), pages 841-858, 1994.


\bibitem{Bromley91} 
M. Bromley, S. Heller, T. McNerney and G. L. Steele, Jr. ``Fortran at
Ten Gigaflops: the Connection Machine Convolution Compiler'.'' In
{\em Proc. of SIGPLAN'91 Conf. on Programming Language Design and 
Implementation}. June, 1991.

\bibitem{callahan88}
D. Callahan and K. Kennedy ``Analysis of Interprocedural Side Effects in a 
Parallel Programming Environment.'' {\em Journal of Parallel and Distributed
Computing}, 5:517-550, 1988.

\bibitem{Cappello95}
F. Cappelllo and C. Germain. ``Toward high 
communication performance through compiled communications on a 
circuit switched interconnection network.'' In {\em Proceedings of the Int'l
Symp. on High Performance Computer Architecture}, pages 44-53, Jan. 1995.

\bibitem{Chakrabarti96}
S. Chakrabarti, M. Gupta and J. Choi ``Global Communication 
Analysis and Optimization.'' {\em Proceedings of the ACM SIGPLAN'96
Conference on Programming Language Design and Implementation} (PLDI),
Pages 68 --- 78, Philadelphia, PA, May, 1996.

\bibitem{Chapman92}
B. Chapman, P. Mehrotra and H. Zima ``Programming in Vienna Fortran.''
{\em Scientific Programming}, 1:31--51, Fall 1992.

\bibitem{Chatterjee93}
S. Chatterjee, J. R. Gilbert, R. Schreiber and S. Teng ``Automatic Array 
Alignment in Data--Parlllel Programs.'' {\em Proceedings of the 20th Annual
ACM Symposium on Principles of Programming Languages}, Charleston, SC, Jan.
 1993.

\bibitem{Chatterjee93a}
S. Chatterjee, J. Gilbert, F. J. E. Long, R. Schreiber and S. Teng 
``Generating local addresses and communication sets for data--parallel
programs.'' In {\em Proc. of PPoPP}, pages 149--158, San Diego, CA, May 1993.


\bibitem{chen96}
C. Chen and s. Banerjee, ``A New Model for Optimal Routing and Wavelength
Assignment in Wavelength Division Multiplexed Optical Networks,'' {\em 
Proc. IEEE Infocom'96}, pages 164--171, 1996.

\bibitem{Chlamtac92}
I. Chlamtac, A. Ganz and G. Karmi. ``Lightpath 
Communications: An Approach to High Bandwidth Optical WAN's'' {\em 
IEEE Trans. on
 Communications}, Vol. 40, No. 7, July 1992.

\bibitem{chlamtac93}
I.Chlamtac, A. Ganz and G. Karmi ``Lightnets: Topologies for High--Speed
Optical Networks.'' {\em Journal of Lightwave Technology}, Vol. 11,
No. 5/6, pages 951---961, May/June 1993.


\bibitem{Dally87}
W. Dally and C. Seitz, ``Deadlock--Free Message Routing in Multiprocessor
Interconnection Networks.'' {\em IEEE trans. on Computers}, Vol. C--36,
No. 5, May 1987.

\bibitem{Dowd93}
P. Dowd, K. Bogineni and K. Ali,
``Hierarchical Scalable Photonic Architectures for High-Performance Processor Interconnection'',
{\em IEEE Trans. on Computers},
vol. 42, no. 9, pp. 1105-1120, 1993.

\bibitem{ganz92}
A. Ganz and Y. Gao,
``A Time-Wavelength assignment algorithm for WDM Start Networks'',
{\em Proc. of IEEE INFOCOM},
1992.

\bibitem{Gong93}
C. Gong, R. Gupta and R. Melhem. ``Compilation Techniques for
Optimizing Communication on Distributed-Memory System''. {\em International
conference on Parallel Processing}. Vol. II, pages 39-46, 
August 1993.

\bibitem{Gross89}
T. Gross. ``Communication in iWarp Systems.'' In {\em Proceedings 
Supercomputing}'89, pages 436--445, ACM/IEEE, Nov. 1989.

\bibitem{Gross94} 
T. Gross, A. Hasegawa, S. Hinrichs, D. O'Hallaron, and T. Stricker
``Communication Styles for Parallel Systems.'' {\em IEEE Computer}, 
vol.27, no. 12, December, 1994, pp. 34-44.

\bibitem{Gross94a}
T. Gross, D. O'Hallaron, and J. Subhlok ``Task parallelism in a High 
Performance Fortran framework.'' {\em IEEE Parallel \& Distributed Technology},
 vol 2, no 2, 1994, pp 16-26.

\bibitem{Gupta92}
M. Gupta and P. Banerjee. ``A Methodology for High--Level Synthesis of 
Communication on Multicomputers.'' In {\em International Conference on
Supercomputing}, Pages 357--367, 1992.

\bibitem{Gupta92a}
M. Gupta and P. Banerjee. ``Demonstration of Automatic Data Partitioning
Techniques for Parallelizing Compilers on Multicomputers.'' {\em IEEE
Trans. on Parallel and Distributed Systems}, 3(2)179-193, 1992.

\bibitem{gupta93}
M. Gupta and E. Schonberg ``A Framework for Exploiting Data Availability to
Optimize Communication.'' In {\em 6th International Workshop on Languages
and Compilers for Parallel Computing}, LNCS 768, pp 216-233, August 1993.

\bibitem{gupta95}
M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K.Y. Wang, D. Shields,
W.M. Ching and T. Ngo. ``An HPF compiler for the IBM SP2.'' In 
{\em proc. Supercomputing'95}, San Diego, CA, Dec. 1995.

\bibitem{Gupta96}
M. Gupta, E. Schonberg and H. Srinivasan ``A Unified Framework for
Optimizing Communication in Data-parallel Programs.'' In {\em IEEE trans.
on Parallel and Distributed Systems}, Vol. 7, No. 7, pages 689-704,
July 1996.

\bibitem{Hinrichs94}
S. Hinrichs, C. Kosak, D.R. O'Hallaron, T. Stricker and 
R. Take. ``An Architecture for Optimal All--to--All Personalized
Communication.'' In {\em 6th Annual ACM Symposium on Parallel Algorithms and
Architectures}, pages 310-319, June 1994.

\bibitem{Hinrichs95}
S. Hinrichs. ``Compiler Directed Architecture--Dependent Communication
Optimization.'' Ph.D dissertation, School of Computer Science, 
Carnegie Mellon University, 1995.

\bibitem{Hinrichs95a}
S. Hinrichs ``Simplifying Connection--Based Communication.'' {\em IEEE Parallel
and Distributed Technology}, 3(1)25--36, Spring 1995.

\bibitem{hinton}
H. Scott Hinton,
``Photonic Switching Using Directional Couplers'',
{\em IEEE Communication Magazine},
Vol 25, no 5, pp 16-26, 1987.

\bibitem{hiran92}
S. Hiranandani, K. Kennedy and C. Tseng ``Compiling Fortran D for MIMD
Distributed--memory Machines.'' {\em Communications of the ACM}, 35(8):66-80,
August 1992.

\bibitem{Horie91}
T. Horie and K. Hayashi. ``All--to--All Personalized 
Communication on a wrap--around Mesh.'' In {\em Proceedings of CAP Workshop},
November, 1991.

\bibitem{HPF}
High Performance Fortran Forum. {\em High Performance Fortran Language 
Specification Version 1.0.}, May 1993.

\bibitem{Ikegami97}
T. Ikegami ``WDM Devices, State of the Art.'' {\em Photonic Networks},
Springer, pages 79--90, 1997.

\bibitem{Kennedy95}
K. Kennedy and N. Nedeljkovic ``Combining dependence and data-flow analyses
to optimize communication.'' In {\em Proceedings of the 9th International
Parallel Processing Symposium}, Santa Barbara, CA, April 1995.

\bibitem{Knobe90}
K. Knobe, J.D. Lukas and G.L. Steele, Jr. ``Data Optimization:
Allocation of Arrays to Reduce Communication on SIMD Machines.''
{\em Journal of Parallel and Distributed Computing}, 8:102-118, 1990.

\bibitem{Koelbel90}
C. Koelbel ``Compiling Programs for Nonshared Memory
Machines.'' Ph.D thesis, Purdue University, August 1990.

\bibitem{Koelbel91}
C. Koelbel and P. Mehrotra ``Compiling global name--space parallel loops
for distributed execution.'' {\em IEEE Trans. on Parallel and Distributed 
Systems}, 2(4):440-451, Oct. 1991.

\bibitem{kovacevic95}
M. Kovacevic, M. Gerla and J.A. Bannister, ``On the performance of 
shared--channel multihop lightwave networks,'' {\em
Proceedings IEEE INFOCOM'95}, Boston, MA, pages 544--551, April 1995.

\bibitem{Kumar92}
M. Kumar. ``Unique Design Concepts in GF11 and Their Impact on
Performance''. {\em IBM Journal of Research and Development}. Vol. 36
No. 6, November 1992.

\bibitem{Labour91}
J. P. Labourdette and A. S. Acampora ``Logically Rearrangeable Multihop
Lightwave Networks.'' {\em IEEE Trans. on Communications}, Vol. 39, No. 8,
pages 1223---1230, August 1991.


\bibitem{Lahaut94}
D. Lahaut and C. Germain, ``Static Communications in 
Parallel Scientific Programs.'' In {\em 
Parallel Architecture \& Languages}, Europe,
pages 262--274, Athen, Greece, July 1994.

\bibitem{lee95}
S. Lee, A. D. Oh and H.A. Choi ``Hypercube Interconnection in TWDM
Optical Passive Star Networks'', {\em Proc. of the 2nd  International 
Conference on Massively Parallel Processing Using Optical Interconnections.}
San Antonio, Oct. 1995.

\bibitem{Leighton92}
F. Leighton, {\em Introduction to parallel algorithms and architecture: arrays,
trees, hypercubes.} Morgan Kaufmann, 1992.

\bibitem{Li91}
J. Li and M. Chen. ``Compiling  Communication --efficient Programs for 
Massive Parallel Machines.'' {\em IEEE Trans. on Parallel and Distributed 
Systems}, 2(3):361-376, July 1991.

\bibitem{Li91a}
J. Li and M. Chen ``The Data Alignment Phase in Compiing  Programs for
Distributed Memory Machines.'' {\em Journal of Parallel and Distributed 
Computing}, 13(2):213--221, October 1991.


\bibitem{Melhem95}
R. Melhem, ``Time--Multiplexing Optical Interconnection
Network; Why Does it Pay Off?'' In {\em Proceedings of the 1995 ICPP 
workshop on Challenges for Parallel Processing}, pages 30--35, August 1995.

\bibitem{MPI93} 
``The Message Passing Interface Forum''. {\em Draft Document for
a Standard Message Passing Interface}, November 1993.

\bibitem{Mukherjee92a}
 B. Mukherjee, ``WDM--based local lightwave networks --- Part I: Single--hop 
systems,'' {\em IEEE Network Magazine}, vol. 6, no. 3, pp. 12--27, May 1992.

\bibitem{Mukherjee92b} 
 B. Mukherjee, ``WDM--based local lightwave networks --- Part II: Multihop
systems,'' {\em IEEE Network Magazine}, vol. 6, no. 4, pp. 20--32, July 1992.

\bibitem{Mukherjee94}
B. Mukherjee, S. Ramamurthy, D. Banerjee and A. Mukherjee ``Some Principles
for Designing a Wide--Area Optical Network.'' {\em IEEE INFOCOM'94}, Vol. 1,
pages 1d1.1---1d1.10, 1994.



\bibitem{Nugent88} 
S. Nugent, ``The iPSC/2 direct--connect communications technology.'' In
{\em Proceedings of the 3rd conference on Hypercube Concurrent Computers and 
Application}, Volume 1, Jan. 1988.

\bibitem{Numrich94} R. W. Numrich, P.L. Springer and J.C. Peterson, 
``Measuerment of Communication Rates on the CRAY-T3d Interprocessor
Network''. In {\em Proceedings of High Performance Computing and Networking},
LNCS 797.

\bibitem{PVM94}
R. Manchek, ``Design and Implementation of PVM version 3.0'',
Technique report, University of Tennessee, Knoxville, 1994.


\bibitem{Qiao94}
C. Qiao and R. Melhem, ``Reconfiguration with Time Division Multiplexed 
MIN's for Multiprocessor Communications.'' {\em IEEE Trans. on Parallel and
Distributed Systems}, Vol. 5, No. 4, April 1994.

\bibitem{Qiao95}
C. Qiao and R. Melhem. ``Reducing Communication Latency with Path Multiplexing
in Optically Interconnected Multiprocessor Systems.'' In {\em Proceedings
of the International Symposium on High Performance Computer Architecture},
pages 34-43, January 1995.

\bibitem{Qiao96}
C. Qiao and Y. Mei, ``On the Multiplexing Degree Required to Embed 
Permutation in a Class of Networks with Direct Interconnects.'' 
In {\em IEEE Symp. on High Performance Computer Architecture}, Feb. 1996.

\bibitem{Ramaswami94}
R. Ramaswami and K. Sivarajan, ``Optimal Routing and Wavelength Assignment
in All--Optical Networks.'' {\em IEEE INFOCOM'94}, vol. 2, pages 970--979,
June 1994.

\bibitem{rogers89}
A. Rogers and K. Pingali ``Process decomposition through locality of
reference.'' In {\em Proc. SIGPLAN'89 conference on Programming Language
Design and Implementation}, pages 69-80, June 1989.

\bibitem{Salisbury97}
C. Salisbury and R. Melhem ``Modeling Communication Costs in Multiplexed 
Optical Switching Networks'', The {\em 
International Parallel Processing Symposium}, Geneva, 1997.

\bibitem{Sivarajan91}
K.Sivarajan and R. Ramaswami, ``Multihop networks based on de bruiji graphs,''
{\em Proceedings IEEE INFOCOM'91}, Bal Harbour, FL, pages 1001--1011, 
April 1991.

\bibitem{Sivalingam93}
K.M. Sivalingam and P.W. Dowd, ``Latency hiding strategies of pre--allocation
based media access protocols for WDM phontic networks,'' in {\em Proc. 26th
IEEE Simulation Symposium}, pages 68 -- 77, Mar. 1993.


\bibitem{Stichnoth93}
J. Stichnoth, D. O'Hallaron, and T. Gross ``Generating communication for array
    statements: Design, implementation, and evaluation,''
{\em Journal of Parallel and
    Distributed Computing}, vol. 21, no. 1, Apr, 1994, pp. 150-159. 

\bibitem{Subhlok94}
J. Subhlok, D. O'Hallaron, T. Gross, P. Dinda, J. Webb ``Communication and
    memory requirements as the basis for mapping task and data parallel 
    programs.'' {\em Proc. Supercomputing '94}, Washington, DC, Nov. 1994, 
    pp. 330-339. 

\bibitem{Subhlok93}
J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross ``Exploiting task and 
data  parallelism on a multicomputer,'' {\em Proceedings of the ACM 
SIGPLAN Symposium on Principles and Practice of Parallel Programming}, 
San Diego, CA, May, 1993, pp 13-22. 

\bibitem{Subramanian96}
S. Subramanian,  M. Azizoglu and A. Somani, ``Connectivity and Sparse
Wavelength Conversion in Wavelength-Routing Networks.''
{\em Proc. of INFOCOM'96}, pages 148--155,
1996.

\bibitem{SUIF}
Stanford Compiler Group ``The SUIF Library'', Stanford University.

\bibitem{Sussman92}
A. Sussman, G. Agrawal and J. Saltz, ``PARTI primitives for unstructured
and block structured problems.'' {\em Computing Systems in Engineering},
Vol. 3, No. 4, pages 73--86, 1992.

\bibitem{Tarjan74}
R.E. Tarjan ``Testing flow graph reducibility.'' 
{\em Journal of Computer and System Sciences}, 9:355-365, 1974.

\bibitem{Vengsarkar97}
A.M. Vengsarkar ``Optical Fiber Devices.'' {\em Photonic Networks}, Springer,
Pages 133--140, 1997.

\bibitem{Venkat96}
A. Venkateswaran and A. Sengupta ``On a Scalable Topology for Lightwave 
Networks.'' {\em IEEE INFOCOM'96}, Vol. 2, pages 4a.4.1---4a.4.8, 1996.

\bibitem{Yuan96b}
X. Yuan, R. Gupta and R. Melhem, ``Distributed Control in Optical 
WDM Networks,'' {\em
IEEE Conf. on Military Communications}(MILCOM), pages 100-104, McLean, VA, Oct.
21-24, 1996. 

\bibitem{Yuan96}
X. Yuan, R. Gupta and R. Melhem, "Demand-driven Data Flow Analysis for 
Communication Optimization," {\em Workshop on Challenges in Compiling for 
Scalable Parallel Systems}, New Orleans, Louisiana, Oct. 23-26, 1996.

\bibitem{Yuan96a}
X. Yuan, R. Melhem and R. Gupta ``Compiled Communication for All--optical
TDM Networks'', {\em Supercomputing'96}, Pittsburgh, PA, 
Nov. 1996.

\bibitem{Yuan97}
X. Yuan, R. Melhem and R. Gupta ``Distributed Path Reservation Algorithms
for Multiplexed All-optical Interconnection networks'' {\em the 
Third International
Symposium on High Performance Computer Architecture(HPCA 3)}, San Antonio,
Texas, Feb.1-5, 1997

\bibitem{Yuan97a}
X. Yuan, R. Gupta, and R. Melhem " An Array Data Flow Analysis based
Communication Optimizer," {\em Tenth Annual Workshop on Languages 
and Compilers for Parallel Computing} (LCPC'97), 
Minneapolis, Minnesota, August 1997 

\bibitem{Yuan97b}
X. Yuan, R. Gupta, and R. Melhem " Does Time Division Multiplexing 
Close the Gap Between Memory and Optical Communication Speeds?" 
{\em Workshop on Parallel Computing, Routing, and Communication} (PCRCW'97), 
Atlanta, Georgia, June 1997.

\bibitem{Yuan97c}
X. Yuan, C. Salisbury, D. Balsara and R. Melhem, ``A Load Balancing
Package on Distributed Memory System and its Application the
Particle-Particle Particle-Mesh (P3M) Methods.'' {\em Parallel Computing},
Vol. 23, No.19, pages 1525-1544, Oct. 1997.

\bibitem{Yuan98}
X. Yuan, R. Melhem and R. Gupta ``Performance of Multihop Communication
Using Logical Topologies on Torus Networks.'' {\em The Seventh 
International Conference on Computer Communications and Networks} (IC3N'98),
Lafayette, Louisiana, 1998.

\bibitem{Yuan98a}
X. Yuan and R. Melhem "Optimal Routing and Channel Assignment for 
Hypercube Communication on Optical Mesh-like Processor Arrays." 
{\em the Fifth International Conference on Massively Parallel
      Processing Using Optical Interconnections}(MPPOI'98), Las Vegas, 
      June 1998

\bibitem{Yuan98b}
X. Yuan, R. Melham, R. Gupta, Y, Mei and C. Qiao "Distributed Control
Protocols for Wavelength Reservation and Their Performance Evaluation"
Submitted to {\em IEEE trans. on Communications}, 1998.

\bibitem{zima88}
H. Zima, H. Bast and M. Gerndt. ``SUPERB: A tool for semi--automatic 
MIMD/SIMD parallelization.'' {\em Parallel Computing}, 6:1-18, 1988.

\bibitem{zhang94}
Z. Zhang and A. Acampora, ``A Heuristic Wavelength Assignment Algorithm for 
Multihop WDM Networks with Wavelength Routing and Wavelength Reuse.'' 
{\em Proc. IEEE Infocom'94}, pp 534-543, June 1994.

\end{thebibliography}
\end{document}










\chapter{Introduction}

% \subsection{Optical interconnection networks}

With the increasing computation power of parallel computers, 
interprocessor communication has become an important factor that
limits the performance of parallel computers.
Due to their capability of offering large bandwidth and low latency, 
optical interconnection networks are considered promising networks for future 
massively parallel computers. In all--optical networks,
communications are carried out in a pure circuit--switching 
fashion in order to avoid electronic/optical or optical/electronic
conversions at intermediate nodes. 
Specifically, packet switching techniques, which are usually used in electronic
multicomputer and multiprocessor interconnection networks are at a
disadvantage when
optical transmission is used. The absence of suitable photonic logic
devices makes it extremely inefficient to process packet routing information
in the photonic domain.
Moreover, conversion of this information into the electronic domain
increases the latency at intermediate nodes relative to the internode
propagation delay. Although  optical/electronic conversions may
be acceptable for large distributed networks \cite{Chlamtac92}, it represents a
disadvantage for multiprocessor networks in which internode propagation delays
are very small.

With the maturing of optical technology, transmission costs, and in 
particular the cost of high speed data links, has dropped tremendously in the 
last few years. 
The electronic processing capability of computers cannot match
the potentially very
high speed of  optical data transmission. Although in electronic
networks, the  processing speed is faster than 
the transmission speed, the transmission speed in optical networks
may be  much
higher than  the 
electronic processing speed which is typically limited to a few
gigabits per second. Thus,  
the communication bottleneck has  shifted from 
the transmission medium to the processing  needed to control that
medium in optical networks. This requires redesign
of network protocols to efficiently utilize the high speeds in optical
networks. 

Multiplexing techniques can be  used in optical
networks to fully utilize the large bandwidth of optics. 
Using multiplexing, 
multiple virtual channels are supported on a single link.
It is possible to concurrently establish multiple connections
using a single fiber in a multiplexed network.  Therefore, 
for a given topology, multiplexing increases the connectivity of the network.
Many research efforts have focussed on two
multiplexing techniques for optical interconnection networks,
namely, {\em time--division multiplexing} (TDM)
\cite{Melhem95,Qiao94,Qiao95,Qiao96} and {\em wavelength--division
multiplexing} (WDM) 
\cite{Brackett90,Chlamtac92,Ramaswami94}.
In TDM, each link is multiplexed 
by having different virtual channels on the link use different
{\em time slots}. In WDM each link is multiplexed by
having different virtual channels on the link use different 
{\em wavelengths}.

Regardless of the multiplexing technique, two approaches
can be used to establish connections in  multiplexed networks,
namely {\em link multiplexing} (LM) and {\em path multiplexing} (PM) 
\cite{Qiao95}.
In LM a connection which spans more than one communication
link is established by using possibly different channels on different links.
In PM a connection which spans more than one
communication link uses the same channel on all the links.
In other words, PM uses the same time-slot or the same wavelength on all
the links of a connection. On the other hand, 
LM can use different time-slots or different
wavelengths on different links, thus requiring time-slot interchange or
wavelength conversion capabilities at each intermediate node. 
Fig.~\ref{pmlm} shows the PM and LM connections at a switch.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/pmlm.eps,height=2.2in}}
\caption{Path multiplexing and link multiplexing}
\label{pmlm}
\end{figure}

Point--to--point networks, such as meshes, tori, rings and hypercubes,
are commonly used in commercial supercomputers. By exploiting space diversity
and traffic locality, they offer larger aggregate throughput and 
better scalability than shared media networks such as buses and stars.
Optical point--to--point networks can use either multi-hop packet routing 
(e.g.Shuffle Net
\cite{Acampora89})
or {\em deflection routing} \cite{Brassil94}. 
The performance of packet routing 
is limited by the speed of electronics since buffering and address decoding 
are usually performed in the electronic domain.
Thus, packet routing cannot efficiently utilize the potentially high bandwidth
that optics can provide.
With deflection routing, buffering of the optical data signals at intermediate
nodes is avoided by routing a packet through alternate
paths when multiple packets compete for the same output link.
While deflection routing requires simple network nodes and minimal 
buffering, a mechanism is necessary to guarantee bounded transfer
delays within the network.
As pointed out in \cite{As94}, although
point--to--point optical networks have intrinsically high aggregate throughput,
this advantage comes at the expense of additional control complexity 
in the form of routing and congestion control. New solutions should
exploit the inherent flexibility of dynamic reconfiguration of logical
topologies. 

While an all--optical network has the potential of providing large bandwidth,
network control for the all--optical network must not incur too much overhead
in order for the large bandwidth to be efficiently utilized.
In order for a data transmission 
to remain in the optical domain from the source node 
to the destination node, control mechanisms must be provided to establish
an all--optical path prior to  the data transmission. 
The efficiency of these path establishment mechanisms
is crucial for the communication performance.
This research
proposes to investigate network control schemes that establish
all--optical paths efficiently in optical TDM point--to--point networks
through architectural and compiler support. 
Specifically, two options for network control will be considered, 
the dynamic {\em distributed path reservation} scheme and 
the {\em compiled communication} scheme. 




To dynamically establish connections, either centralized 
or distributed control mechanisms can be used.
Centralized control mechanisms, such as 
wavelength assignment \cite{Ramaswami94} and 
time--slot assignment \cite{Qiao94},  collect all 
communication requests in a central controller and perform connection
scheduling in the controller. When the network size is large, 
the central controller becomes a  bottleneck. 
Thus, centralized control mechanisms 
are not scalable to large networks. Distributed path reservation protocols
distribute the work load of  path reservations to all the  nodes in the 
system. During distributed path reservation, a connection request, which 
arrives
at the network dynamically, will negotiate with each node along the path for
the establishment of a connection. The communication can  start after a
successful path reservation.
This research
 will focus on studying distributed path reservation protocols.
The protocols are generalizations of control protocols in 
non-multiplexed circuit--switching networks \cite{Nugent88}.
Multiplexing, however, introduces an additional complexity which requires
a careful consideration of many factors and parameters that affect the
efficiency of the protocols.
A distributed path reservation scheme can generally
be partitioned into two parts, selecting a physical
path and assigning virtual channels along the physical path to form 
a virtual path. To select a physical path, either deterministic routing
or adaptive routing can be used. Once a physical path is determined, a
path reservation scheme must decide how to select virtual channels on
the links along the physical path for an all--optical connection.
In addition, the signal propagation overhead must also be considered
in the designing of distributed reservation protocols. Many system paramters 
(system size, multiplexing degree, etc),
as well as  protocol parameters, contribute to the performance
of the protocols. The problem to be addressed is that, for a given
system, how to reserve paths without incurring too much protocol overhead.

\begin{figure}[htbp]
\begin{tabbing}
\hspace{1in} (1) ...\\
\hspace{1in} (2) barrier\\
\hspace{1in} (3) set network state to support the connections
                 used in $C_1$ and $C_2$\\
\hspace{1in} (4) barrier\\
\hspace{1in} (5) DO\=\ i = 1, 1000\\
\hspace{1in} (6) \> $C_1$: computation that uses communication pattern 1\\
\hspace{1in} (7) \> $C_2$: computation that uses communication pattern 2\\
\hspace{1in} (8) END DO
\end{tabbing}
\caption{An Example}
\label{CCOMM}
\end{figure}

Using distributed path reservation protocols, the
network control is performed in the electronic domain,
which is relatively slow in comparison to
the large bandwidth supported by optical data paths. Thus, the network
control overhead might become the bottleneck  in
all--optical networks, especially for communications with small message
sizes. Compiled communication is proposed to improve communication
performance by shifting the runtime control overhead 
into compile time processing. 
In compiled communication, the compiler determines the 
communication patterns in a program and inserts 
instructions that set the state of the network to support the 
communication patterns. 
Hence,
at runtime, an all--optical path for each communication in the program is  
established without  dynamic path reservation. 
Fig.~\ref{CCOMM} shows an example of compiled
communication. In the example, the compiler determines that two communication
patterns are needed inside the loop and inserts
codes in lines (2) --- (4) to set the network state to establish connections
in patterns $C_1$ and $C_2$. Hence, at runtime, the communications
inside the loop need not perform path reservation.

Beside eliminating dynamic control overhead, 
compiled
communication also offers a number of advantages over traditional dynamic
network control. First,   
off-line algorithms can be used to manage network
resources more efficiently. 
For example, off--line connection scheduling algorithms result in
better channel utilization than  simple on--line routing algorithms.
Since off--line algorithms are executed at compile time, 
the complexity of the algorithms will
not effect the runtime efficiency of the program. 
Second, compiled communication may simplify the 
software communication protocol since 
communications are deterministic
under compiled communication
\cite{Kumar92}. Third, 
since  routing decisions are made at compile time, 
compiled communication uses simpler hardware in the 
router and does not incur routing delay.
The major limitation of
 compiled communication is that it cannot efficiently handle  the
communication patterns that are unknown at compile time. 

A number of issues must be addressed
in order to apply compiled communication techniques. First, the compiler must
be able to synthesize communication patterns from application programs.
This task can be complex depending on the programming model and the 
machine model. Second, the compiler must
have  knowledge of the underlying networks. Many compilers generate code
using high level communication libraries, such as PVM and MPI, which
provide a very simple model for communications. However, this simple model
also hinders  compiled communication. A different
communication model
must be provided for the compiler to manage communications statically.
Third, efficient off--line algorithms must be designed for the compiler 
to manage network resources effectively. 
Further, if compiled 
communication is to be the only means for network control, mechanisms to
perform correct communications for the communication
patterns that are unknown at compile time
must be provided. One way to handle compiled time unknown communication
patterns in compiled communication is to use a pre--determined pattern
to route messages and emulate multi-hop communication.

%When dealing with compile time unknown patterns, 
%routing messages through a pre--determined
%communication pattern (logical topology) is an alternative
%to providing  distributed path reservation protocols. 
%Data transmissions are no longer all--optical, instead, there are 
%intermediate nodes at which routing must be performed.  
%Three factors contribute to the communication performance: 
%the number of intermediate hops, the processing time at an 
%intermediate hop and the multiplexing degree required to realize the
%logical topology.
%Consider an 
%$n\times n$ mesh connected network. A message has to be relayed, on average,
%at $n$
%intermediate routing hops. If a  hypercube logical topology is 
%realized on the mesh using virtual circuits, then a message will be relayed, 
%on average, at
%$\frac{lg(n)}{2}$ intermediate routing hops.
%This research will study 
%efficient logical topologies for  point--to--point physical networks.
%Notice that  two
%extremes for the choices of logical topologies are (1) 
%having no logical topology, which
%minimizes the multiplexing degree  while requiring a large number of 
%intermediate relays and (2) realizing
%a complete graph,  which eliminates all the intermediate relays 
%at the cost of a large multiplexing degree. A good logical topology may be
%a compromise between these two extremes.

This research will consider both dynamic network control and compiled 
communication in optical TDM point--to--point networks using path 
multiplexing.
To support this research,  a detailed 
network simulator that simulates all  proposed
control schemes will be 
designed and implemented. 
The simulator should be able to simulate communication with three different
control mechanisms,  (1) various dynamic path reservation protocols, 
(2) compiled communication for   patterns
known at compile time  
where communication paths are assumed to 
be realized using off--line algorithms, and (3)  compiled communication 
for  patterns unknown at compile time
where messages are routed through a logical topology.
Beside the network simulator, a tool
 must be developed to extract communication 
patterns from application programs. 
Traditional communication traces are not sufficient for
the study of compiled communication. Instead,  static communication
patterns must be distinguished from dynamic communication patterns. 
In order to obtain this
information, A communication analysis tool that analyzes
communication requirements at compiler time will be developed. 

In the rest of the proposal,  background information and previous work
related to this research will be presented in  Chapter 2. 
The research goals and  approaches will be described in
Chapter 3. Preliminary work that has been done will be reported in 
Chapter 4, and Chapter 5 concludes the proposal.

\newpage











\chapter{Introduction}

Fiber--optic technology has advanced significantly over the past
few years, so have the development of tunable lasers, filters, high--speed
transmitter and receiver circuits, optical amplifiers and photonic
switching devices \cite{Ikegami97,Vengsarkar97}. 
With the maturing of optical technology, 
transmission cost, and in particular the cost of high speed data links, 
has dropped tremendously. The electronic processing capability of 
computers cannot match the potentially very
high speed of  optical data transmission.
The communication bottleneck has shifted from 
the transmission medium to the processing  needed to control that
medium. 

In optical interconnection networks, each physical link can offer very high
bandwidth. In order to fully utilize the available bandwidth, an optical
link can be shared through {\em time--division multiplexing} (TDM)
\cite{ganz92,Melhem95,Qiao94} and/or {\em wavelength--division 
multiplexing} (WDM) \cite{Brackett90,Chlamtac92,Dowd93,Subramanian96}.
In TDM, optical links are multiplexed 
by assigning different virtual communication channels to different
{\em time slots}, while in WDM, optical  links are multiplexed by
assigning different virtual communication channels to different 
{\em wavelengths}. By using TDM, WDM or TWDM (a combination of TDM and WDM), 
each link can support multiple channels.

Point--to--point networks, such as meshes, tori, rings and hypercubes,
are used in commercial supercomputers. By exploiting space diversity
and traffic locality, they offer larger aggregate throughput and 
better scalability than shared media networks such as buses.
Optical point--to--point networks can be implemented by 
replacing electronic links
with optical links and operating in a packet switching fashion just like 
electronic networks. The performance 
of such networks is limited by the speed of electronics since 
buffering and address decoding are performed in the electronic domain.
Thus, these networks cannot efficiently utilize the potentially high bandwidth
that optics can provide. New generation optical point--to--point networks
exploit the {\em channel--routing} capability in optical switches.
In TDM networks, time--slot routing\cite{Qiao94} is used, while in 
WDM networks, wavelength--routing\cite{chlamtac93} is used. The 
channel routing in an optical switch routes messages from a channel of an 
input port to a channel of an output port without converting the messages
into the electronic domain. The switch states, however, are usually 
controlled by electronic signals.

Using channel routing, two approaches can be used to establish connections 
in  multiplexed optical networks, namely {\em link multiplexing} (LM) and 
{\em path multiplexing} (PM). These connections are called {\em lightpaths}
since the light signal travels through the connections that may span a number
of optical links and switches without being converted into the electronic 
domain. In LM, a connection which spans more 
than one communication link is established by using possibly different 
channels on different links. In PM, a connection which spans more than one
communication link uses the same channel on all the links.
In other words, PM uses the same time-slot or the same wavelength on all
links of a connection, while LM can use different time-slots or different
wavelengths, thus requiring time-slot interchange or
wavelength conversion capabilities at each intermediate optical switch. 
Since the technology to support PM is more mature than the one to support LM,
this thesis focuses on PM.

Since the electronic processing speed is relatively
slow compared to the optical data transmission speed,
optical point--to--point networks should ideally employ
{\em all--optical} communication in data transmission.
In all--optical communication, no electronic processing and
no electronic/optical (E/O)
or optical/electronic (O/E) conversions are performed
at intermediate nodes. Once converted into the 
optical domain, the signal remains there until it reaches the destination.
All--optical communication eliminates the electronic processing
bottleneck at intermediate nodes 
during data transmission and thus, exploits the large
bandwidth of optical links. This thesis considers all--optical 
networks where a  lightpath is established before a communication 
starts and the data transmission is carried out in a pure 
circuit--switching fashion. This type of communication is referred to 
as the {\em dynamic single--hop communication}. 
In such networks, electronic processing occurs only in 
the path reservation process and hence, using an efficient path reservation
protocol is crucial to obtain high performance. In this work, 
a number of path reservation algorithms that dynamically
establish lightpaths are designed and the impact of system parameters on the
algorithms is studied. These algorithms 
use a separate control network to exchange 
control messages and allow all--optical communication in the optical
data network.
 
Although dynamic single--hop networks achieve all--optical communication
in data transmission, the path reservation algorithms require
extra hardware support to exchange control messages
and result in large startup overhead, especially for small messages. 
An alternative is to use {\em dynamic multi--hop
communication}. In multi--hop networks, intermediate nodes are responsible
for routing packets  such that a packet sent from a 
sender will eventually reach its destination, possibly after being routed
through a number of intermediate nodes. Clearly,  multi--hop networks 
require E/O and O/E conversions at intermediate nodes. Thus, it is
important to reduce the number of hops that a packet visits. This reduction
may be achieved in an optical TDM network by 
combining the channel--routing technique and the 
packet switching technique. Specifically, packets may be routed 
through a  logical topology which has a small diameter 
as opposed to  the physical topology which may have a large diameter. 
The major issue in the multi--hop 
communication is to design appropriate logical topologies.
This thesis considers efficient schemes for realizing logical topologies
on top of physical mesh and torus networks using path multiplexing. 
Realizing logical
topologies on optical networks is different from traditional embedding 
techniques in that both routing and channel assignment 
options must be considered. An analytical model that 
models the maximum throughput and average package latency of 
multi--hop networks is developed and is used to evaluate the performance of 
logical topologies and identify the advantages of each logical topology.

While dynamic (single--hop or multi--hop) communications handle arbitrary
communication patterns,  their performance can be limited
by the electronic processing which occurs during path reservation 
in single--hop communication and during packet routing in 
multi--hop communication. 
{\em Compiled communication} overcomes this limitation for communication 
patterns that are known at compile time.
In compiled communication, the compiler analyzes a program and determines
its communication requirement. The compiler then
uses the knowledge of the underlying
architecture, together with the knowledge of the communication requirement,
to manage network resources statically. As a result,
runtime communication overheads, such as path reservation
and buffer allocation overheads, are reduced or eliminated,
and the communication performance is improved. However,
due to the limited network resources, the underlying network
cannot support arbitrary communication patterns.
Compiled communication
requires for the compiler to analyze a program and partition
it into phases such that each phase has a fixed, pre-determined
communication pattern that the underlying network can support.
The compiler inserts codes for performing network reconfigurations at
phase boundaries to support all connections in the next phase.
At runtime, a lightpath is available
for each communication without path reservation. Therefore,  
compiled communication accomplishes all--optical communication
without incurring extra hardware support and large start--up overheads.
This thesis studies the application of compiled communication 
to optical interconnection networks. Specifically, it considers
the communication analysis techniques needed to analyze the communication
requirement of a program. These analysis techniques are 
general in that
they can be applied to other communication optimizations and 
can be used for compiled communication in electronic networks. This thesis
also develops a number of  connection scheduling schemes which realize a given
communication pattern with a minimal multiplexing degree. Note that in 
optical TDM networks, communication time is proportional to the 
multiplexing degree. Finally, a communication phase analysis algorithm is 
developed to partition a program into phases so that each 
phase contains connections that can be supported by the underlying network.
All the algorithms are implemented in a compiler which is 
based on the Stanford SUIF
compiler\cite{Amar95}. This thesis evaluates the performance of the algorithms
in terms of both analysis cost and runtime efficiency.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
  & Single--hop & Multi--hop & Compiled\\
\hline
All--optical comm. & Yes & No & Yes\\
\hline
Startup overhead  & Yes & No & No\\
\hline
Extra hardware    & Yes & No & No\\
\hline
Arbitrary comm.   & Yes & Yes & No\\
\hline
\end{tabular}
\end{center}
\caption{General characteristics of the three schemes}
\label{generalchar}
\end{table}

Table~\ref{generalchar} summarizes the general characteristics of the three
schemes. In optical interconnection networks, 
the central problem to be addressed
is the reduction of  the amount of electronic processing needed for
controlling the 
communication. In dynamic single--hop networks, this problem is addressed 
by having efficient path reservation algorithms. In multi--hop networks,
this problem is tackled by designing efficient logical topologies to 
route messages. Compiled communication totally eliminates the electronic
processing in communications. However, it only applies to the communication
patterns that are known at compile time. While communications in optical
TDM point--to--point networks can be carried out by any of the three
communication schemes, it is necessary to understand the strengths and 
the limitations of each communication scheme in order to make appropriate
choices when
designing an optical interconnection network. 
In addition to considering the options within each communication scheme,
this thesis compares the performance of the three communication schemes 
using a number of benchmarks and real applications and identifies 
the situations in which each communication scheme has advantage over 
other schemes.

% ollowing section summarizes the contributions of the thesis.

%\section{Thesis contributions}
%
%This thesis makes contributions in the {\em design of control mechanisms}
%for time--multiplexed 
%optical interconnection networks. The contributions are in two areas:
%optical interconnection networks and compiler analysis techniques. In the 
%optical interconnection
%networks area, this thesis introduces efficient control schemes 
%for dynamic single--hop communication and dynamic multi--hop communication. 
%This thesis also 
%proposes and validates the idea of applying the 
%compiled communication technique 
%to optical interconnection networks.  In the compiler area, 
%this thesis addresses all the issues needed
%to apply the compiled communication paradigm 
%to optical interconnection networks,
%including communication optimization, 
%communication analysis, connection scheduling and 
%communication phase analysis. Some of the research presented in this 
%thesis has appeared in \cite{Yuan96,Yuan96a,Yuan97,Yuan97a,Yuan98,Yuan98a}.
%The main contributions of the thesis are detailed as follows.
%
%\begin{itemize}
%\item Dynamic single--hop communication.
%\begin{enumerate}
%  \item  Develop distributed path reservation protocols for optical TDM
%         point--to--point networks.
%  \item  Study the performance of the protocols and 
%         the impact of various
%         system parameters, such as system size, message size, etc, on 
%         the protocols. 
%\end{enumerate}
%\item Dynamic multi--hop communication.
%\begin{enumerate}
%  \item Develop efficient schemes to realize logical topologies on
%        top of physical optical torus and mesh networks.
%  \item Develop an analytical model to model the maximum throughput and the 
%        packet delay for multi--hop networks.
%  \item Study the performance of various logical topologies and identify
%        the advantages and the limitations of each logical topology.
%\end{enumerate}
%\item Compiled communication.
%\begin{enumerate}
%  \item Develop a communication analyzer that analyzes the communication 
%        requirement of a program.
%  \begin{itemize}
%    \item  Design a communication descriptor that describes
%           the communication requirement on virtual processor grids.
%    \item  Design data flow analysis algorithms for
%           communication optimizations to obtain communication
%           patterns in an optimized program.
%    \item  Design schemes that derive 
%           communication patterns on physical processors from the
%           communication descriptors.
%  \end{itemize}
%  \item Design communication scheduling algorithms that can be used by 
%        the compiler to schedule the communication patterns that
%        are known at compile time.
%  \item Design  a communication phase analysis algorithm
%        that partitions a program into phases such that the connections
%        within each phase can be supported by the underlying network.
%  \item Implement all the algorithms and evaluate these algorithms in terms
%        of both analysis cost and runtime efficiency.
%\end{enumerate}
%\item Simulators and Performance evaluation.
%\begin{enumerate}
%  \item Implement network simulators that simulate dynamic single--hop 
%        communication with various control protocols, dynamic multi--hop
%        communication with different logical topologies, and compiled
%        communication.
%  \item Study the performance of the three communication schemes via
%        benchmark and real application programs and identify the 
%        advantages and the limitations of each scheme.
%\end{enumerate}
%\end{itemize}
%
%Although all the techniques presented for optical TDM networks, many techniques
%may also be applied to optical WDM networks or to non--optical networks.

%\section{Thesis organization}

The remainder of the thesis is organized as follows.
Chapter 2 begins by describing background and thesis assumptions. This chapter
presents an overview of optical interconnection networks, discusses the 
TDM technique and introduces the {\em path multiplexing} (PM) and 
{\em link multiplexing} (LM) techniques
for establishing connections. This chapter also surveys the research related to
the three communication schemes. Finally, this background chapter surveys 
the traditional compilation techniques for distributed memory machines and
communication optimizations and discusses the 
difference between the traditional communication optimizations and the 
compiled communication technique.

Chapter 3 discusses the techniques used in dynamic single--hop communication.
Two types of distributed path reservation protocols, the {\em forward
reservation protocols} and the {\em backward path reservation protocols},
are described. This chapter also describes a  network simulator for
dynamic single--hop communication, evaluates the performance of 
the two types of protocols and  studies the impact of system
parameters on these protocols.

Chapter 4 discusses dynamic multi--hop communication. This chapter presents
efficient schemes for realizing logical topologies  on top of the 
physical torus networks,
describes an analytical model that models the maximum throughput and 
the average packet delay for the logical topologies and verifies 
the model with 
simulation. In addition, this chapter also describes the simulator for
dynamic multi--hop communication, evaluates 
the multi--hop communication with the logical topologies and identifies 
the advantages and the limitations of each logical topology.

Chapter 5 considers compiled communication. This chapter describes
the communication analyzer and discusses the communication descriptor used 
in the analyzer, the 
data flow analysis algorithms for communication optimizations, and
the actual communication optimization performed in the analyzer.
The chapter also describes off-line connection 
scheduling algorithms and a communication
phase analysis algorithm. Using these algorithms, the compiler analyzes 
the communication requirement of a program, partitions the program
into phases such that each phase contains connections that can be 
supported by the underlying network, and schedules the connections within 
each phase.  The chapter also presents the evaluation
of the compiler algorithms and studies their runtime efficiency.   

Chapter 6 compares the communication performance of the three
communication schemes. Three sets of application (benchmark) programs, 
including hand--coded parallel programs, HPF benchmark programs
and sequential programs from SPEC95, are used to evaluate the 
communication performance of the communication schemes. Different sets
of programs exhibit different communication characteristic. For example, 
the hand--coded programs are highly optimized for parallel execution, while
the programs from SPEC95 are not optimized for parallel execution.
This chapter compares the communication performance of the three
communication schemes and 
identifies the advantages of each scheme.

Finally, Chpater 7 summarizes the dissertation and suggests some directions
for future research.










\chapter{Dynamic multi--hop communication}
\label{multi}

Since by using time--division
multiplexing multiple channels are supported on an optical link,
more sophisticated logical topologies can be realized on top of 
a simpler physical network to improve the communication performance. These
logical topologies reduce the number of intermediate hops that a packet travels
at the cost of a larger multiplexing degree. 
On the one hand, the large multiplexing degree increases the packet 
communication time between hops. On the other hand, reducing the number 
of intermediate hops reduces the time spent at intermediate nodes. 
This chapter studies the trade--off between the  multiplexing degree
and the number of intermediate hops needed in logical topologies implemented
on top
of physical torus networks. Specifically,  four logical
topologies ranging from the most complex logical all--to--all 
connections to the simplest logical torus topology are examined.  
An analytical model for the
maximum throughput and the average packet delay is developed and verified
through simulations.
The performance 
and the impact of system parameters on the performance for
these four topologies are studied. Furthermore, the performance of 
the multi--hop communication  using an efficient logical topology is 
compared with that of the single--hop communication using a 
distributed path reservation protocol, and the advantages and 
the drawbacks of these two communication schemes are identified.

To perform multi--hop communication, packets may be routed through
intermediate nodes. Specifically, a communication
module at each node, which will be referred to as the {\em router} in this
chapter, is needed to route packets toward their destinations.
It is  assumed that each router 
contains a {\em routing buffer} that buffers all
incoming packets. For each
packet, the router determines whether to deliver the packet
to the local PE or to
the next link toward the packet destination.
A separate {\em output path buffer} is used 
for each outgoing path that buffers the packets
to be sent on that path and thus accommodates the speed mismatch between the 
electronic router and the optical path. 
Figure~\ref{ROUTER} depicts the structure of a router 
(see also Figure~\ref{ROUTER1}).
Note that the output paths are multiplexed in time over the physical links 
that connect the local PE to its corresponding switch. 
In the rest of the chapter, {\em routing delay} will be used 
to denote the time a packet spends in the routing buffer 
and the time for the router to make
a routing decision for the packet (packet routing time).
{\em Transmission delay} will be used to denote the time 
a packet spends on the path
buffer and the time it takes for the packet to be transferred on the path.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/queue.eps,width=5in}}
\caption{A router}
\label{ROUTER}
\end{figure}

\section{Realizing logical topologies on physical torus topology}

Four logical topologies are considered in this section, 
the logical torus topology, the logical hypercube topology, 
the logical all--to--all topology, and 
the logical allXY topology where all--to--all connections are established
along each dimension. Let us consider an example in which a packet 
is transmitted from node 0 to node 11 in 
the $4\times 4$ torus shown in Figure~\ref{DEF1}. Using the  
logical all--to--all topology,
the packet will go directly from node 0 to node 11. Using the logical allXY
topology, the packet will go from node 0 to node 3 to node 11. Using the
logical hypercube topology,
the packet will go from node 0(0000) to node 1(0001)
to node 3(0011) to node 11(1011). Using the logical torus topology, the packet
will go from node 0 to node 3 to node 7 to node 11. 


\begin{figure}
\centerline{\psfig{figure=fig/def1.eps,width=2in}}
\caption{Node numbering in a torus topology}
\label{DEF1}
\end{figure}

Traditional embedding techniques that
minimize the {\em congestion}  for a given communication 
pattern are not adequate for minimizing the number of virtual channels
needed to realize the communication in an optical network with path 
multiplexing. The congestion is usually not equal to the number of 
channels needed to realize a communication pattern. 
Consider the example in Figure~\ref{notequal} in which
the congestion in the network is 2, while 3 channels
are needed to realize the three connections. To efficiently realize a 
logical topology in an optical network, both routing and channel assignment
(RCA) options must be taken into consideration. 
Schemes to realize these four logical topologies on the physical 
torus topology will be discussed next. 


\begin{figure}
\centerline{\psfig{figure=fig/notequal.eps,width=2.4in}}
\caption{Difference between embedding and RCA}
\label{notequal}
\end{figure}


\subsection{Logical hypercube topology}

This subsection considers the optimal schemes to realize the logical
hypercube topology on top of the physical torus topologies. Since the
algorithm to realize the hypercube topology on the physical torus topologies
utilizes the algorithms to realize
the hypercube on top of physical mesh, ring and array topologies, 
algorithms to realize the hypercube topology on 
top of all these mesh--like topologies are discussed. 

Given networks of size $N$, it will be proven that 
$\lfloor\frac{2N}{3}\rfloor$ and
$\lfloor\frac{N}{3} + \frac{N}{4} \rfloor$ channels are the minimum required
to realize hypercube communication on array and ring topologies, 
respectively. Routing and 
channel assignment schemes that achieve these minimum 
requirements are developed, 
indicating that the bounds are tight and the  schemes are optimal.
These schemes are extended to mesh and torus topologies and it is proven 
that for a $2^k\times 2^{r-k}$ ($k\ge r-k$) mesh or torus, 
$\lfloor\frac{2\times2^k}{3} \rfloor$ and 
$\lfloor\frac{2^k}{3} + \frac{2^k}{4} \rfloor$ channels are the minimum 
required for realizing hypercube communication on these two topologies, 
respectively. Routing and channel assignment schemes 
are designed 
that use at most two more channels than the optimal to realize hypercube
communication on these topologies. In the following sections, first
the problem of routing and channel assignments for the hypercube
communication on the physical mesh--like topologies is formally defined, 
and then the algorithms are described.

\subsubsection{Problem definition} 

A network is modeled as a directed graph G(V, E), where nodes in 
V are  switches and edges in E are links.
Each node in a network is assigned
a node number starting from 0. It is assumed that
in arrays and rings the nodes are numbered
from left to right in ascending order, and that
the nodes are numbered in row major order for meshes and tori of size 
$n\times m$.  Thus, 
the node in the $i$th column
and the $j$th row is numbered as $j \times m + i$. 
%Fig.~\ref{DEF1} shows a $4\times 4$ torus topology.
This subsection focuses on studying the optimal RCA schemes for 
these traditional numbering schemes. 
Optimal node numbering (and its RCA) is a
much more complex problem and is not considered in this dissertation.
The number of nodes in a network is assumed to be $N=2^r$. 
For a mesh or a torus to contain $2^r$ nodes, each row and column must
contain a number of nodes that is a power of two. 
Hence, the size
of meshes and tori is denoted as $N=2^k\times 2^{r-k}$.
Without losing generality, it is always assumed that $k \ge r-k$.
The notations $ARRAY(N)$ and
$RING(N)$ are used to represent arrays and rings  of size N respectively,
and $MESH(2^k\times 2^{r-k})$ and $TORUS(2^k\times 2^{r-k})$ for
meshes and tori of size $2^k\times 2^{r-k}$ respectively. 

The connection from node $src$ to
node $dst$ is denoted as $(src, dst)$. A {\em communication pattern}
is a collection of connections. 
The {\em hypercube} communication pattern contains a connection $(src,dst)$ 
if and only if the binary representations of 
 $src$ and $dst$ differ in precisely one bit. A connection in the hypercube
communication pattern is called a {\em dimension l connection} if it connects
two nodes that differ in the $l$th bit position. In a network of 
size $N=2^r$, the set, $DIM_l$, where
$0\le l\le r-1$, is defined as  the set of all dimension $l$ connections
and ${H_r}$ is defined as the hypercube communication pattern. That is

\vspace{-0.1in}
\begin{tabbing}
\hspace{0.3in}$DIM_l=$\=$\{(i, i+2^l)\ |\ i\ mod\ 2^{l+1}\ < \ 2^l\} \cup$
                     $\{(i, i-2^l)\ |\ i\ mod\ 2^{l+1}\ \ge\ 2^l\}$\\
\\
\hspace{0.3in}${H_r} = \cup_{l=0}^{r-1}DIM_l $
\end{tabbing}
%\centerline{$DIM_l=\{(i, i+2^l)\ |\ i\ mod\ 2^{l+1}\ < \ 2^l\} \cup
%                   \{(i, i-2^l)\ |\ i\ mod\ 2^{l+1}\ \ge\ 2^l\}$}
%\vspace{0.15in}
%\centerline{${H_r} = \cup_{l=0}^{r-1}DIM_l $ }
%\vspace{0.15in}

\vspace{-0.1in}
It can be easily proven that removing any $DIM_l$, for any $l\le r-1$,
from ${H_r}$ leaves two disjoint sets of connections, each of which being
a hypercube pattern on $\frac{N}{2}$ nodes. 
For example, removing $DIM_0$ from ${H_r}$ results in an ${H_{r-1}}$
on the 
even--numbered nodes and another ${H_{r-1}}$ on the odd--numbered nodes once
the nodes are properly renumbered. Next, 
some definitions are introduced and the results of this section are 
summarized.

\begin{description}

\item
{\bf Definition:} $P(x, y)$ is a {\em directed path}
 in G from node x to node y. It
 consists of a set of consecutive edges beginning at x and ending at y.

\item
{\bf Definition:} Given a network G and a communication pattern  $I$, 
a {\em routing R(I)} of $I$
is a set of directed paths $R(I) = \{P(x, y) | (x, y) \in I\}$.

\item
{\bf Definition:} Given a network G, a communication pattern I and a 
routing R(I) for the communication pattern, 
the congestion of an edge $\alpha\in E$, denoted as $\pi(G, I, R(I), \alpha)$, 
is the number of
paths in R(I) containing $\alpha$. The {\em congestion}
of G in the routing R(I), denoted as $\pi(G, I, R(I))$, is the maximum congestion 
of any edge of G in the routing R(I), that is, 
$\pi(G, I, R(I)) = max_{\alpha}\{\pi(G, I, R(I), \alpha)\}$. 
The {\em congestion} 
of G for 
a communication pattern I, 
denoted as $\pi(G, I)$, is the minimum congestion of G in any 
routing R(I) for I, that is, $\pi(G, I) = min_{R}\{\pi(G, I, R(I))\}$.

\item
{\bf Definition:} Given a network G and a routing $R(I)$ for communication
 pattern I, an
{\em assignment function} $A: R\rightarrow INT$,
is a mapping from the set of paths to the set of integers $INT$, 
where an integer
corresponds to a channel.
A {\em channel assignment} for a routing $R(I)$ is an assignment
function $A$ that satisfies the following conditions: 

\begin{enumerate}
\item  If 
$P(x_1, y_1)$, $P(x_2, y_2)$ are different paths that share a common edge,
then\\ $A(P(x_1, y_1)) \ne A(P(x_2, y_2))$. This condition 
ensures that each channel on one link can only be assigned
to one connection (i.e., there are no link conflicts). 

\item $A(P(x, y_1)) \ne A(P(x, y_2))$ and $A(P(x_1, y)) \ne A(P(x_2, y))$.
This condition ensures that 
each node can only use
one channel at a time to send to or receive from 
other nodes (i.e., there are
no node conflicts). 
\end{enumerate}


%A channel assignment
%that violates condition (1) is said to have {\em link conflicts}, and a
%channel assignment that violates condition (2) is said to have
%{\em node conflicts}. 
$A(R)$ denotes the set of 
channels assigned to the paths in R and $|A(R)|$ is the size of $A(R)$. 
Let $w(G, I, R)$ denote  the 
minimum number of channels for the routing R, that is,
$w(G, I, R) = min_A\{|A(R)|\}$.  $w(G,I)$ denotes the smallest 
$w(G, I, R)$ over all R, i.e. $w(G,I) = min_R\{w(G, I, R)\}$

\item
{\bf Lemma 1:} $w(G,I) \ge \pi(G, I)$.\\
{\bf Proof: } Follows directly from the above definitions. $\Box$
\end{description}

\noindent
The following sections show that\\

%I further give a lower bound for  $\pi(MESH(2^k\times 2^{r-k}), H_r)$
%and $\pi(TORUS(2^k\times 2^{r-k}), H_r)$
%and show that

\begin{tabbing}
\hspace{0.3in}$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$\\
\\
\hspace{0.3in}$w(RING(N), H_r) = \pi(RING(N), H_r) = 
\lfloor \frac{N}{3} + \frac{N}{4}\rfloor$\\
\\
\hspace{0.3in}$w(MESH(2^k\times 2^{r-k}), H_r)
                \le \lfloor \frac{2\times 2^k}{3} \rfloor + 2$
              $\le \pi(MESH(2^k\times 2^{r-k}), H_r) + 2$\\
\hspace{0.3in}\\
\hspace{0.3in}$w(TORUS(2^k\times 2^{r-k}), H_r) 
              \le \lfloor \frac{2^k}{3} + \frac{2^k}{4}\rfloor + 2 $
              $\le \pi(TORUS(2^k\times 2^{r-k}), H_r) + 2$
\end{tabbing}

%\centerline{$w(MESH(2^k\times 2^{r-k}), H_r) \le 
%\lfloor \frac{2\times 2^k}{3} \rfloor + 2 \le 
%\pi(MESH(2^k\times 2^{r-k}), 
%H_r) + 2$}
%\centerline{ $w(TORUS(2^k\times 2^{r-k}), H_r) \le
%\lfloor \frac{2^k}{3} + \frac{2^k}{4}\rfloor + 2 \le
%\pi(TORUS(2^k\times 2^{r-k}), H_r) + 2$}

\subsubsection{Hypercube on linear array}

Since routing in a linear array is fixed, the RCA
problem is reduced to a channel assignment problem. 
Given a linear array of size $N=2^r$, it is proven that
 $\lfloor\frac{2N}{3}\rfloor$ channels is the lower
bound for realizing the hypercube communication  by showing  that 
$\pi(ARRAY(N), H_r) \ge \lfloor\frac{2N}{3}\rfloor$.
A channel assignment scheme is developed that uses
$\lfloor\frac{2N}{3}\rfloor$ channels for the hypercube communication. 
This proves that the bound is a tight lower 
bound and that the channel assignment scheme is optimal. 
%Array topology is a 
%relatively simple topology, there exist general 
%optimal channel assignment schemes on this topology\cite{qiao96}. 

\vspace{0.12in}
\noindent
{\bf A lower bound}
\vspace{0.12in}

Using Lemma 1, a lower bound is obtained
by proving that there exists a link in the linear
array that is used $\lfloor\frac{2N}{3}\rfloor$ times when realizing
${H_r}$. The following 
lemmas establish the bound.

\noindent
{\bf Lemma 2}: In a linear array of size $N=2^r$, where $r \ge 2$, 
there are $2^{r-1}$ connections in $DIM_{r-1}\cup DIM_{r-2}$ that
use the link $(n, n+1)$ for any specific $n$ satisfying 
$2^{r-2} \le n \le 2^{r-1}-1$.

\noindent
{\bf Proof}: The connections in $DIM_{r-1}$ and $DIM_{r-2}$
can be represented by

%\small
% \footnotesize
\begin{tabbing}
\hspace{0.1in}$DIM_{r-1}$\= =$\{(i, i + \frac{N}{2}) | 0\le i <\frac{N}{2}\}$ 
                 $\cup\{(i, i - \frac{N}{2}) | \frac{N}{2}\le i < N\}$\\
\\
\hspace{0.1in}$DIM_{r-2} =\{(i, i+\frac{N}{4})|0 \le i < \frac{N}{4}$ or $
                         \frac{N}{2}\le i < \frac{3N}{4}\}$\\
\hspace{0.1in}\>$\cup$\hspace{0.05in} $\{(i, i-\frac{N}{4})| \frac{N}{4} \le i < \frac{N}{2}$ or 
                         $\frac{3N}{4}\le i < N\}$
\end{tabbing}

\noindent
Consider the 
connections in $DIM_{r-1}$. All connections $(i, i+\frac{N}{2})$ with
$0 \le i\le n$ use link $(n, n+1)$, where $2^{r-2} \le n \le 2^{r-1}-1$. 
Hence, as shown in Fig.~\ref{LEMMA1}~(a),
there are  n+1 connections in $DIM_{r-1}$ that use link 
$(n, n+1)$. Similarly, in $DIM_{r-2}$,
all connections  $(i, i+\frac{N}{4})$, where
$n < i+\frac{N}{4} < \frac{N}{2}$, use  link $(n, n+1)$.
As shown in Fig.~\ref{LEMMA1}~(b), there are $2^{r-1} - n - 1$
such connections.
Hence, there are a total of $n+1 + 2^{r-1} - n - 1 = 2^{r-1}$ 
connections in $DIM_{r-1}$ and $DIM_{r-2}$ that use link $(n, n+1)$. $\Box$


\begin{figure*}
\centerline{\psfig{figure=fig/l1.eps,width=5.4in}}
\caption{Dimension $r-1$ and $r-2$ connections}
\label{LEMMA1}
\end{figure*}

\noindent
{\bf Lemma 3}: In a linear array of size $N=2^{r}$, there exists a link 
$(n, n+1)$ such that at least $\lfloor\frac{2N}{3}\rfloor$ connections in 
${H_r}$ use that link.

\noindent
{\bf Proof}: Let $T_i(2^r)$ be the number of connections in ${H_r}$
that use link $(i, i+1)$ and let $T(2^r) = max_i (T_i(2^r))$.
Thus $T(2^0) = 0$ and $T(2^1) = 1$.
From Lemma 2, one knows that
for $2^{r-2} \le n \le 2^{r-1}-1$, link $(n, n+1)$ is used
$2^{r-1}$ times  by connections in $DIM_{r-1}$ and $DIM_{r-2}$. Thus,
the links in the second quarter of the array (from node $2^{r-2}$ to node 
$2^{r-1}-1$) are used $2^{r-1}$ times by  dimension $r-1$ and 
dimension $r-2$ connections.
By the definition of hypercube communication, it is known that 
dimension 0 to dimension $r-3$ connections 
form a hypercube on this quarter of the array. Thus, 
Lemma 2 can be recursively applied 
 and the following inequality is obtained.\\

\centerline{$T(2^r) \ge 2^{r-1} + T(2^{r-2})$}

\noindent
It can be proven by induction that the above  inequality and the 
boundary conditions $T(2^0) = 0$, $T(2^1) = 1$, imply that 
$T(N) = T(2^r) \ge \lfloor \frac{2N}{3}\rfloor$.
%
%Base case: $T(2^0) = 0 \ge \lfloor \frac{2\times 0}{3} \rfloor$ and 
%           $T(2^1) = 1 \ge \lfloor \frac{2\times 2}{3} \rfloor$.
%
%Induction case: assuming $T(\frac{N}{4}) = T(2^{r-2}) \ge \lfloor 
%                          \frac{2\times 2^{r-2}}{3} \rfloor$,
%
%$T(N) = T(2^r) \ge 2^{r-1} + T(2^{r-2})
%           \ge 2^{r-1} + \lfloor \frac{2^{r-1}}{3} \rfloor
%           \ge \lfloor \frac{3\times 2^{r-1} + 2^{r-1}}{3} \rfloor
%           \ge \lfloor  \frac{2\times 2^{r}}{3} \rfloor
%           = \lfloor \frac{2N}{3} \rfloor$
%\noindent
Hence, there exists a link which is used at least
$\lfloor\frac{2N}{3}\rfloor$ times by connections in ${H_r}$. $\Box$

The proof of Lemma 3 is constructive in the sense that
the link that is used at least 
$\lfloor\frac{2N}{3}\rfloor$ times can be found. 
By recursively considering the second quarter of the linear array, 
one can conclude that the source node, $n$, of the link $(n, n+1)$ that is 
used at least $\lfloor\frac{2N}{3}\rfloor$ times in ${H_r}$ is 
$n = \frac{N}{4}+\frac{N}{16} + \frac{N}{64} + .. = 
 \lfloor \frac{N}{3} \rfloor$.
Hence, the link that is used at least 
$\lfloor\frac{2N}{3}\rfloor$ times in ${H_r}$ is 
$(\lfloor \frac{N}{3} \rfloor, \lceil \frac{N}{3} \rceil)$.

\noindent
{\bf Corollary 3.1} Give an array of size $N=2^r$, if the nodes in the 
array are partitioned into 2 sets $S_1=\{i|0\le i \le n\}$ and 
$S_2=\{i | n+1 \le i \le N\}$, where $n=\lfloor \frac{N}{3}\rfloor$,
then there are at least $\lfloor\frac{2N}{3}\rfloor$
connections  in ${H_r}$ 
from  $S_1$ to  $S_2$ and 
$\lfloor\frac{2N}{3}\rfloor$
connections
from $S_2$ to $S_1$. $\Box$

\noindent
{\bf Theorem 1}: $\pi(ARRAY(N), H_r) \ge \lfloor \frac{2N}{3} \rfloor$.

\noindent
{\bf Proof}: Directly from Lemma 3. $\Box$

\vspace{0.12in}
\noindent
{\bf An optimal channel assignment scheme}
\vspace{0.12in}

By the definition of hypercube communication, connections  
in ${H_r}$ can be partitioned
into three sets, $DIM_0$, $EVEN_r$ and $ODD_r$. $DIM_0$ contains the 
dimension 0 connections, $EVEN_r$ contains connections 
between nodes with even node numbers, and $ODD_r$ contains 
connections between nodes with odd node numbers. 
Each of $EVEN_r$ and $ODD_r$
forms a $r-1$ dimensional 
hypercube communication, ${H_{r-1}}$, if only the nodes involved in 
communications are considered
and that the nodes are renumbered accordingly. 
%Fig.~\ref{LEMMA4} shows the
%partition of ${H_3}$.  
Thus, channel assignment schemes for 
${H_{r-1}}$  can be extended to realize ${H_r}$ as shown in the 
following lemma.

%\begin{figure}
%\centerline{\psfig{figure=fig/l4.eps,width=4in}}
%\caption{$H_3 = EVEN_3 \cup ODD_3 \cup DIM_0$}
%\label{LEMMA4}
%\end{figure}

%A straight forward channel assignment scheme can be obtained from 
%the above partitioning of ${H_r}$.
%Assuming we know how to assign channels for 
% ${H_{r-1}}$ on an array of size $2^{r-1}$,
%we can assign channels for ${H_r}$ by 
%scheduling $EVEN_r$ on the $2^{r-1}$ even numbered nodes, $ODD_r$
%on the $2^{r-1}$ odd numbered nodes and using one more configuration to 
%schedule $DIM_0$
%(it can be easily  proven that $DIM_0$ forms a configuration).
%Let
%$D(N)$ be the number of configurations
% needed to realize hypercube communication 
%for array of size $N$. It can be expressed in the following equation.\\
%\centerline{$D(N) = 2D(N/2) + 1$}

\noindent
{\bf Lemma 4}: Assuming that ${H_{r-1}}$ can be
realized on an array of size $2^{r-1}$
using  $K$ channels, then ${H_r}$
can be realized on an array of size $2^{r}$ 
using $2K+1$ channels.

\noindent
{\bf Proof}: ${H_r} = EVEN_r\cup ODD_r \cup DIM_0$. From the above 
discussion and the
assumption, $EVEN_r$ and $ODD_r$ are ${H_{r-1}}$ (when nodes are properly
renumbered), $K$ channels can be used to realize 
$EVEN_r$ or $ODD_r$. Since it can be easily proven that 
$DIM_0$ can be realized with one channel,
a total of $2K+1$ channels can be used to realize ${H_r}$. $\Box$

Let $D(N)$ be the number of channels
needed for ${H_r}$ on an array of size $N = 2^r$. 
If a channel assignment scheme is used that is in accordance with the proof
of  Lemma 4, it can be shown that the  equation,
$D(N) = 2D(N/2) + 1$.
Given that no channel is needed to realize hypercube communication on 
a 1--node array, D(1) = 0. Solving for $D(N)$ results in 
$D(N) = N-1$, which is not optimal.
%means that using this simple scheme, 
%$N-1$ channels are needed to realize ${H_r}$ on a linear array of size 
%$N=2^r$. This  does not reach the lower bound. 
%Fig.~\ref{NOOPT} shows the 
%channel assignment for a 16 node array using this simple scheme.
The following lemma improves this simple channel assignment scheme.


%\begin{figure}
%\centerline{\psfig{figure=fig/noopt.eps,width=4in}}
%\caption{A non-optimal channel assignment
%         for ${H_4}$ on array uses 15 channels.}
%\label{NOOPT}
%\end{figure}

\begin{figure}
\centerline{\psfig{figure=fig/l6.eps,width=3in}}
\caption{Realizing $DIM_0\cup DIM_1$ of $H_3$}
\label{LEMMA6}
\end{figure}

\noindent
{\bf Lemma 5}: Assuming that ${H_{r-2}}$ can be realized on an array of size 
$2^{r-2}$ using $K$ channels, then ${H_r}$ can be realized on an 
array of size $2^r$ using $4K+2$ channels.

\noindent
{\bf Proof}: Consider ${H_r}$
without dimension 0 and dimension 1 connections. 
By the definition of ${H_r}$,
${H_r} - (DIM_0\cup DIM_1) = DIM_2\cup ... \cup DIM_{r-1}$ 
forms four hypercube patterns, each being an ${H_{r-2}}$ pattern on 
nodes $\{n\ |\ n\ mod\ 4 = i\}$ (with proper node renumbering), denoted
by $subarray_i$, for $i =$ 0, 1, 2 or 3. From 
the hypothesis, ${H_{r-2}}$ can be realized on an array of size $2^{r-2}$ 
using  $K$ channels.  The 
four sub--cube patterns can be realized in $4K$ channels.
The remaining connections to be considered are those in $DIM_0$ and $DIM_1$.
It can easily be proven that connections in $DIM_0$ and $DIM_1$
can be assigned to 2 channels as shown in Fig.~\ref{LEMMA6}.
%by
%mixing half the connections in $DIM_0$ with half the connections in $DIM_1$
%in one channel.
%Fig.~\ref{LEMMA6} shows an example for an 8--node array. As can be seen
%from the figure, similar channel assignment can be used to assign 2 channels
%to all $DIM_0$ and $DIM_1$ connections for any array of size $N=2^r$.
Hence, the hypercube communication ${H_r}$ can be realized using
a total of $4K+2$ channels. $\Box$

\begin{figure}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}\=Algorithm 1: Assign\_array($N = 2^r$) \\
\>(1)\hspace{0.1in}\=If $(r = 0)$ then return $\phi$\\
\>(2)\>If\=\ (r is odd) then\\
\>(3)\>\>/* applying Lemma 4 */\\
\>(4)\>\>recursively apply Assign\_array($N/2=2^{r-1}$)  for $EVEN_r$. \\
\>(5)\>\>recursively apply Assign\_array($N/2=2^{r-1}$)  for $ODD_r$.\\
\>(6)\>\>assign connections in  $DIM_0$ to one channel.\\
\>(7)\>Else /* r is even, apply Lemma 5 */\\
\>(8)\>\>Fo\=r i = 0, 1, 2, 3\\
\>(9)\>\>\>apply Assign\_array($N/4 = 2^{r-2})$
           for $subarray_i$. \\
\>(10)\>\>assign connections in $DIM_0 \cup DIM_1$ to 2 channels.
\end{tabbing}
\caption{The channel assignment algorithm}
\label{ALGO1}
\end{figure}

The channel assignment algorithm, {\em Algorithm 1}, 
is depicted in Fig.~\ref{ALGO1}. 
%Notice that 
%while the algorithm is described using a recursive notation for simplicity,
%it is easier to perform the channel assignment in a bottom-up fashion.
For the base case, when $N=2^0=1$, the hypercube pattern 
contains no connection.
To assign channels to connections in an array of size $N=2^r$, 
$r > 0$, there are two 
cases. If $r$ is even, then Lemma 5 is applied to use
$4K+2$ channels for the hypercube pattern, where $K$ is the 
number of channels needed
 to realize a hypercube pattern on an array of size $2^{r-2} = N/4$. 
If $r$ is odd, Lemma 4 is applied to use
$2K+1$ channels to realize the hypercube pattern, where $K$ is the
number of channels needed
to realize a hypercube pattern in an array of size $2^{r-1} = N/2$.
The example of using this algorithm to schedule ${H_4}$ in an array of size 
16 is shown in Fig.~\ref{OPT}.


\begin{figure}
\centerline{\psfig{figure=fig/opt.eps,width=3.6in}}
\caption{Optimal channel assignment for ${H_4}$}
\label{OPT}
\end{figure}

\noindent
{\bf Theorem 2}: {\em Algorithm 1}  uses $\lfloor\frac{2N}{3}\rfloor$
channels for  ${H_r}$ on a
linear array with $N=2^r$ nodes, thus 
$w(ARRAY(N), H_r) \le \lfloor\frac{2N}{3}\rfloor$.

\noindent
{\bf Proof}: Let $D_{odd}(2^r)$ and $D_{even}(2^r)$
denote the number of channels needed 
when $r$ is  odd and even, respectively.
The number of channels for the hypercube pattern using 
{\em Algorithm 1}
can be formulated as follows,

\hspace{0.1in}$D_{odd}(2^r) = 2D_{even}(2^{r-1}) + 1$, when $r$ is odd.

\hspace{0.1in}$D_{even}(2^r) = 4D_{even}(2^{r-2}) + 2$, when $r$ is even.

\noindent
Using the  boundary condition $D_{even}(1)= D_{even}(2^0) = 0$,
it can be proven by induction that  $D_{odd}(N) = \frac{2N}{3} - \frac{1}{3}$
and $D_{even}(N) = \frac{2N}{3} - \frac{2}{3}$.
%
%Base case: $D_{even}(2^0) = \frac{2\times 1}{3} - \frac{2}{3} = 0$.
%
%Induction case: Assuming $D_{even}(N) = \frac{2N}{3} - \frac{2}{3}$
%and $D_{odd}(N) = \frac{2N}{3} - \frac{1}{3}$.
%
%$D_{odd}(2N) = 2D_{even}(N) + 1 
%            = 2\times (\frac{2N}{3} - \frac{2}{3}) + 1
%            = \frac{2\times(2N)}{3} - \frac{1}{3}$
%$D_{even}(2N) =  4D_{even}(N/2)+2
%              = 4 (\times \frac{2\times N/2}{3} - \frac{2}{3}) + 2
%              = \frac{2\times (2N)}{3} - \frac{2}{3}$
%
%\noindent
Hence, $D_{odd}(N)$ and $D_{even}(N)$ are equal 
to $\lfloor\frac{2N}{3}\rfloor$. 
$w(ARRAY(N), H_r) \le \lfloor\frac{2N}{3}\rfloor$. $\Box$
%, which is equal to the lower bound in 
%Theorem 1. $\Box$

%Notice that for a linear array of size $2^r$, where $r$ is an odd number,
%Lemma 5 can also apply recursively without applying Lemma 4 first. However, 
%this will not lead to an optimal channel assignment. 
%Specifically, the number of channels required in this case can be determined
% from
% the formula $D(N) = 4D(N/4) + 2$, with
%boundary condition $D(2) = 1$. The solution
%of this equation is $D(N) = \frac{5N}{6}- \frac{2}{3}$, which is not optimal.

\noindent
{\bf Theorem 3}:\\ 
$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$, and Algorithm 1 is optimal.

\noindent
{\bf Proof: } Follows from Theorem 1, Theorem 2 and Lemma 1.$\Box$
%From Theorem 1, we have 
%$\pi(ARRAY(N), H_r) \ge \lfloor \frac{2N}{3} \rfloor$, from Theorem 2, 
%we have $w(ARRAY(N), H_r) \le \lfloor \frac{2N}{3} \rfloor$, and
%from Lemma 1, we have $w(ARRAY(N), H_r) \ge \pi(ARRAY(N), H_r)$.
%Combining these results, we obtain 
%$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$.
%Since the channel assignment algorithm, {\em Algorithm 1}, uses 
%$\lfloor \frac{2N}{3} \rfloor$ channels for ${H_r}$, it is optimal.
%$\Box$

\subsubsection{Hypercube connections  on rings}

By having links between node 0 and node $N-1$, two paths can be 
established from any node to any other node on a ring. It has been shown
\cite{beauquier97}
that even for a fixed routing, general optimal channel assignment problem is 
NP--complete. This section  focuses on the specific
problem of optimal RCA for ${H_{r}}$ on ring topologies, obtaining
a lower bound on the number of channels needed to realize ${H_{r}}$ and
developing an optimal routing and channel assignment algorithm 
that achieves this lower bound.

%Assuming that 
%a virtual circuit between two nodes is always established using a
% shortest path, the flexibility provided by 
% the extra links between node 0 and node $N-1$ affects only 
%the dimension $r-1$ connections that span $\frac{N}{2}$ nodes.  
%The following 
%lemma establishes  
%a lower bound for the multiplexing degree needed to 
%realize ${H_r}$ in a ring with $N=2^r$ nodes.

\noindent
{\bf Lemma 6}: $\pi(RING(N), H_r) \ge 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$.

\noindent
{\bf Proof}: This lemma is proven by showing that there exist 
two cuts on a ring
that partition the ring into two sets, $S_1$ and $S_2$, such that 
$ 2\times \lfloor\frac{N}{3} + \frac{N}{4}\rfloor$  connections in 
${H_r}$  originate at nodes  in $S_1$ and terminate at nodes in $S_2$.
Since there are only 2 links connecting  $S_1$ to
$S_2$, one of the 2 links must be used at least
$\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$ times, regardless of which routing
scheme is used.
Consider ${H_r}$ on a ring of size $N=2^r$.
The connections in $DIM_0\cup...DIM_{r-2}$
form two $r-1$ dimensional
hypercube patterns in two {\em arrays} of size $2^{r-1}$.
The first  array, denoted by $subarray_1$, contains  
nodes 0, ..,  $2^{r-1}-1$ and the second  array, denoted by 
$subarray_2$,  contains nodes $2^{r-1}$,..,  $2^r-1$.
From Corollary 3.1, it follows that
there exists a link in each $2^{r-1}$ node array such that  
$\lfloor\frac{N}{3}\rfloor$ connections in the hypercube pattern
 use that link in each
direction. From the discussion in previous section, the link is  
$(\lfloor \frac{N}{3} \rfloor, \lceil \frac{N}{3} \rceil)$ in $subarray_1$
 and $(\lfloor \frac{N}{3} \rfloor+2^{r-1}, 
\lceil \frac{N}{3} \rceil + 2^{r-1})$ in 
$subarray_2$. 
These two links partition  the ring 
into two sets 
$S_1 = \{i| 0\le i\le \lfloor \frac{N}{3} \rfloor\} 
\cup \{i|2^{r-1}+\lfloor \frac{N}{3} \rfloor+1\le i\le 
2^r-1\}$ and 
$S_2 = \{i| \lfloor \frac{N}{3} \rfloor+1 \le i 
\le 2^{r-1}+\lfloor \frac{N}{3} \rfloor\}$.
Hence, there are $\lfloor\frac{N}{3}\rfloor$ connections from 
$S_1\cap subarray_1$ to $S_2\cap subarray_1$ and 
$\lfloor\frac{N}{3}\rfloor$ connections from 
$S_1\cap subarray_2$ to $S_2\cap subarray_2$ in $DIM_0\cup...DIM_{r-2}$.
Thus, there are $2\times \lfloor\frac{N}{3}\rfloor$ connections 
in $DIM_0\cup..\cup DIM_{r-2}$ originating at nodes in $S_1$ and
terminating at nodes in $S_2$.
Fig.~\ref{LEMMA8} 
shows the cuts on a 16--node ring. 
The remaining connections of ${H_r}$
are in $DIM_{r-1}$. By partitioning
the ring into $S_1$ and $S_2$, each node in $S_1$ has a dimension $r-1$ 
connection to a node in $S_2$. Hence, there are  $N/2$ 
connections in $DIM_{r-1}$ between  $S_1$ and $S_2$. Therefore, a total of 
$2\times \lfloor\frac{N}{3}\rfloor + N/2 = 2\times 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$ connections in ${H_r}$ are 
from  $S_1$ to 
 $S_2$. Thus, $\pi(RING(N), H_r) \ge 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$.  $\Box$

\begin{figure}
\centerline{\psfig{figure=fig/l8.eps,width=3.2in}}
\caption{Hypercube on a ring}
\label{LEMMA8}
\end{figure}


%In the following, we show that using deterministic 
%odd--even shortest path
%routing and a channel assignment scheme derived from lemma 6, 
%this lower bound can be achieve. 
The RCA scheme uses an odd--even shortest path
routing. Given a ring of size $N=2^r$,
an odd--even shortest path routing  works as follows. 
A connection between two nodes is  established using a
shortest path. Connections that have two shortest
paths are of the forms $(i, i+2^{r-1})$  and $(i, i-2^{r-1})$. For these
connections, the clockwise path is used if  $i$ is even and the 
counter--clockwise path if $i$ is odd. 
%Note that using odd--even 
%shortest path routing, the extra links between node 0 and node 
%$N-1$ affect only 
%the dimension $r-1$ connections that span $\frac{N}{2}$ nodes in
%${H_r}$.

The channel assignment algorithm is derived from Lemma 6. There are
two parts in the algorithm, channel assignment for
connections in $DIM_{r-1}$ and 
channel assignment for connections in $DIM_0\cup .. \cup DIM_{r-2}$. 
%Using odd--even short path routing, 
Channel assignment for 
connections in $DIM_0\cup .. \cup DIM_{r-2}$
is equivalent to channel assignment for  two ${H_{r-1}}$ 
in two disjoint arrays, 
thus, using the channel assignment 
scheme (for array) described in the 
previous section, $\lfloor \frac{N}{3} \rfloor$ channels 
can be used to realize
these connections. For the connections in $DIM_{r-1}$,
using  odd--even shortest path routing, 
four connections in $DIM_{r-1}$,
$(i, i+2^{r-1})$, $(i+2^{r-1}, i)$, $(i+1,i+2^{r-1}+1)$, $(i+2^{r-1}+1, i+1)$,
can be realized using one channel. We denote by $CONFIG_i$ these four 
connections. Since
the union of all $CONFIG_i$, where $i = 0, 2, 4, ..., N/2-2$ is equal to 
$DIM_{r-1}$,  $N/4$  channels are 
sufficient to realize $DIM_{r-1}$. Fig.~\ref{ALGO2} shows  the channel
assignment
algorithm for ring topologies.



\begin{figure}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}\=Algorithm 2: Assign\_ring($N=2^r$)\\
\>\\
\>(1)\hspace{0.1in}\=Apply Assign\_array($N/2=2^{r-1}$) on $subarray_1$.\\
\>(2)\hspace{0.1in}Apply Assign\_array($N/2=2^{r-1}$) on  $subarray_2$.\\
\>\>Since $subarray_1$ and $subarray_2$ are disjoint, \\
\>\>channels can be reused in steps (1) and (2).\\
\>(3)\>fo\=r i = 0, N/2-2, step 2\\
\>\>\>Assign a channel to connections $(i, i+2^{r-1})$, $(i+2^{r-1}, i)$,\\
\>\>\>         $(i+1,i+2^{r-1}+1)$ and $(i+2^{r-1}+1, i+1)$\\
\end{tabbing}
\caption{The channel assignment for rings}
\label{ALGO2}
\end{figure}

\noindent
{\bf Theorem 4}: {\em Algorithm 2} uses 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$ channels
to realize ${H_r}$ in a ring of size $N=2^r$.

\noindent
{\bf Proof}: Follows from above discussion. $\Box$

\noindent
{\bf Theorem 5}: 
$w(RING(N), H_r) = \pi(RING(N), H_r) = 
\lfloor \frac{N}{3 } + \frac{N}{4} \rfloor$, and the odd--even shortest path
routing with Algorithm 2 is an optimal RCA scheme for hypercube
connection  on rings.

\noindent
{\bf Proof: } Follows from Lemma 1, Lemma 6 and Theorem 4.$\Box$
%From Lemma 6, we have 
%$\pi(RING(N), H_r) \ge \lfloor \frac{N}{3} + \frac{N}{4} \rfloor$,
% from Theorem 4, 
%we have $w(RING(N), H_r) \le \lfloor \frac{N}{3} + \frac{N}{4} \rfloor$,
%and from Lemma 1, we have $w(RING(N), H_r) \ge \pi(RING(N), H_r)$.
%Combining these results, we obtain
%$w(RING(N), H_r) = \pi(RING(N), H_r) = 
%\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$.
%Since with the odd--even shortest path routing, the 
%channel assignment algorithm, {\em Algorithm 2}, uses 
%$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$ channels 
%for ${H_r}$, the routing scheme together with the algorithm forms an 
%optimal RCA scheme for hypercube communication on rings. $\Box$

\subsubsection{Hypercube connections on meshes}

Given a $2^k\times 2^{r-k}$  mesh,
realizing the hypercube connections on the mesh is equivalent 
to realizing $H_{k}$ in each row and $H_{r-k}$ in each column. 
The following lemma gives the lower bound on the number of channels
required
to realize  hypercube communication patterns on  meshes. 
%Unlike the lower bound for the linear array
%and ring, we cannot prove that this lower bound is tight.

\noindent
{\bf Lemma 7}: $\pi(MESH(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2\times2^k}{3}\rfloor$, assuming  $k\ge r-k$.

\noindent
{\bf Proof}:  
The hypercube pattern on the mesh contains $2^{r-k}$  $k$--dimensional
hypercube patterns on  $2^k$ arrays in the $2^{r-k}$ rows. 
Consider a cut in edges
$(\lfloor \frac{2^k}{3} \rfloor, \lceil \frac{2^k}{3} \rceil)$ in every
row, which partitions the mesh into two parts. 
From Corollary 3.1, we know that for each row there are 
$\lfloor \frac{2\times 2^k}{3} \rfloor$ connections from the left of the
cut to the right of the cut, hence, there are a total of 
$2^{r-k} \times \lfloor \frac{2\times 2^k}{3} \rfloor$ connections
crossing
 the cut. Since there are $2^{r-k}$ edges in the cut, 
there exists at least one edge that is used at least 
$\lfloor\frac{2\times2^k}{3}\rfloor$ times. Thus,
 $\pi(MESH(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2\times2^k}{3}\rfloor$. $\Box$

%
%Without losing
%generality, we assume that      $k \ge r-k$ in the following discussions.
%The following lemma states the properties of this type of mesh. 
%
%\noindent
%{\bf lemma 8}: The hypercube pattern pattern in an $2^k\times 2^{r-k}$ mesh
%is equivalent to the hypercube pattern for array of size $2^k$ in every row in 
%x direction and hypercube pattern for array of size $2^{r-k}$ in every column
%in y direction.
%
%\noindent
%Proof: The lemma is proven by showing that connections sourced at
%any node in $i$th column and $j$th row forms the hypercube connections
%in $i$th column and hypercube connections in $j$th row that sourced at this
%node. Let number $i$ be represented in binary with $k$ bits,
%${k-1}i_{k-2}...i_0$ and $j$ be represented in binary with $r-k$ bits
%$j_{r-k-1}j_{r-k-2}...j_0$. Since the node in $i$th column and $j$th row
%are numbered as $n = i\times 2^{r-k} + j$, the binary representation of $n$
%is ${k-1}i_{k-2}...i_0j_{r-k-1}j_{r-k-2}...j_0$. By definition, the 
%hypercube connections sourced at this node in $i$th row are
%
%\centerline{\{${k-1}i_{k-2}...i_0a_{r-k-1}a_{r-k-2}...a_0 | $
%               there is one and only one $a_l$, where $0\le l\le r-k-1$,
%               differ from $j_l$\}}
%
%\noindent
%the hypercube connections source at this node in $j$th column are
%
%\centerline{\{$a_{k-1}a_{k-2}...a_0j_{r-k-1}j_{r-k-2}...j_0 | $
%               there is one and only one $a_l$, where $0\le l\le k-1$,
%               differ from $l$\}}
%\noindent
%By definition of hypercube communication, the hypercube pattern in the mesh
%sourced node\\
% ${k-1}i_{k-2}...i_0j_{r-k-1}j_{r-k-2}...j_0$ is equivalent to 
%the union of these two sets. Hence, the hypercube  pattern in 
%an $2^k\times 2^{r-k}$ mesh
%is equivalent to the hypercube pattern for array of size $2^k$ in every row in 
%x direction and hypercube pattern for array of size $2^{n-k}$ in every column
%in y direction. $\Box$
%
%lemma 8 states that establishing hypercube on 

Given a mesh of size 
$2^k\times 2^{r-k}$, the hypercube communication pattern 
in each 
row is denoted by ${H_k^{row}}$ 
and  the hypercube communication pattern in 
each column by ${H_{r-k}^{col}}$.
The RCA scheme uses X--Y shortest path routing.
Since we already know the optimal channel assignment for ${H_k^{row}}$ and 
${H_{r-k}^{col}}$, the challenge here is  to reuse channels on 
connections in two dimensions efficiently. 
Let us define an {\em array configuration} as the set of connections 
in a linear array that are assigned to the same channel. {\em Ring}, 
{\em mesh} and {\em torus configurations} are defined similarly.
Using the definition of configurations, a mesh configuration can be obtained
by combining array configurations in  the rows and the columns.
For example,
if an array configuration in x dimension and an array configuration in 
y dimension can be combined into a mesh configuration, the two array 
configurations can be realized in the mesh topology using one channel.
Notice that,  while there is no link conflict when assigning channels to 
 row and column connections, 
%${H_k^{row}}$ uses only x direction links while ${H_{r-k}^{col}}$
%uses only y direction links. However, 
node conflicts may occur and must be avoided.

\begin{figure}
\centerline{\psfig{figure=fig/l13.eps,width=3.0in}}
\caption{a Mesh configuration}
\label{LEMMA13}
\end{figure}

%Since we will try to combine array configurations
%into mesh configurations, 
Let us first take a deeper look at the 
array configurations for arrays of size $N=2^k$. Following the channel
 assignment algorithm, {\em Algorithm 1}, array
configurations can be classified into three categories;
 $E$--configurations that contain only connections
between even--numbered nodes,  $O$--configurations that contain  only
connections  between odd--numbered nodes,
and $EO$--configurations that contain  
dimension 0 (and/or) dimension 1 connections 
%(thus may connect  even
%number nodes and odd number nodes). 
As discussed in Section 3,
if $k$ is odd, there is only one 
$EO$--configuration for  connections in $DIM_0$,  
$(\lfloor \frac{2N}{3}\rfloor -1) / 2$ $E$--configurations for
connections in $EVEN_k$,
and $(\lfloor \frac{2N}{3}\rfloor -1) / 2$ $O$--configurations for
connections in $ODD_k$. Similarly, 
if  $k$ is even, there are two $EO$--configurations, 
 $(\lfloor \frac{2N}{3}\rfloor -2) / 2$ $E$--configurations 
and 
 $(\lfloor \frac{2N}{3}\rfloor -2) / 2$ $O$--configurations. 
The following lemma shows that $E$--configurations  and $O$--configurations 
in  rows and columns of the mesh 
can be combined.

\noindent
{\bf Lemma 8}: Given an  $E$--configuration, $E_x$, and an  
$O$--configuration, $O_x$, in the x direction and 
an $E$--configuration, $E_y$, and an $O$--configuration, $O_y$, in the 
y direction, 
$E_x$ and $O_x$ in all rows and $E_y$ and $O_y$ in all  columns can be
realized in two mesh configurations.

\noindent
{\bf Proof}: The proof is by constructing the two mesh 
configurations. In the first mesh configuration,
let all odd numbered rows realize $O_x$
and all even numbered row realize $E_x$.  
In this case, no connection starts or terminates at an 
odd numbered node in an even column or at 
an even numbered node in an odd column. Thus, in the same mesh configuration,
$E_y$ can be realized in odd
columns and $O_y$ can be realized in even columns.
The second mesh configuration realizes $E_x$ on odd numbered rows, 
$O_x$ on even numbered rows,
$E_y$ on even numbered columns and $O_y$ on odd numbered columns. These
two mesh configurations
 realize  $E_x$ and $O_x$ in all rows
 and $E_y$ and $O_y$ in all columns.
 Fig.~\ref{LEMMA13} shows the construction of a 
mesh configuration. $\Box$

Lemma 8 lays the foundation for the channel assignment algorithm. 
Let $a$ be the number of $E$--configurations 
and $O$--configurations in ${H_k^{row}}$,
$b$ be the  number of $EO$--configurations in ${H_k^{row}}$,
$c$ be the number of $E$--configurations
 and $O$--configurations in ${H_{r-k}^{col}}$,
and $d$ be the  number of $EO$--configurations in ${H_{r-k}^{col}}$.
From assumptions, it follows that  $k \ge r-k$, $a\ge c$, 
$a+b = \lfloor \frac{2\times 2^k}{3}\rfloor$ and 
$d \le 2$.
By combining $E$--configurations and $O$--configurations in rows and 
columns into mesh 
configurations, all the $E$--configurations and $O$--configurations 
in each row and
all the $E$--configurations and $O$--configurations 
in each column can be realized
using $a$ mesh configurations. 
Using an individual mesh 
configuration for each EO  configuration in the rows and the columns,
a total of 
$a + b + d \le \lfloor \frac{2\times 2^k}{3}\rfloor + 2$
configurations are sufficient 
to  realize the hypercube connections.
%communication pattern on the mesh. 
%This is stated in the following theorem.

\noindent
{\bf Theorem 4}: ${H_r}$ can be realized on a $2^k\times 2^{r-k}$ mesh, 
 where $k \ge r-k$, using 
$\lfloor \frac{2\times 2^k}{3}\rfloor + 2$ channels. $\Box$

%\noindent
%{\bf Proof:} Straight forward from the above discussion. $\Box$

\noindent
{\bf Corollary 4.1:} $w(MESH(2^k\times 2^{r-k}), H_r) \le 
\lfloor \frac{2\times 2^k}{3} \rfloor + 2 \le 
\pi(MESH(2^k\times 2^{r-k}), 
H_r) + 2$. $\Box$

\subsubsection{Hypercube connections on tori}
\label{hypercubeontori}

%Hypercube communication on torus topologies is 
%obtained from the hypercube communication on the ring topology.
As in the case of realizing $H_r$ on a mesh, 
${H_r}$ can be realized on a  $2^k\times 2^{r-k}$
torus by realizing ${H_{k}^{row}}$ in each row and ${H_{r-k}^{col}}$
in each column. The following lemma gives a lower bound
on the number of channels required to realize ${H_r}$
on a torus.

\noindent
{\bf Lemma 9}: $\pi(TORUS(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$, assuming  $k\ge r-k$.

\noindent
{\bf Proof:} 
The hypercube pattern on the torus contains $2^{r-k}$  $k$--dimensional
hypercube patterns on  $2^k$ rings in the $2^k$ rows. 
Considered two cuts in edges
$(\lfloor \frac{2^{k-1}}{3} \rfloor, \lceil \frac{2^{k-1}}{3} \rceil)$
and 
$(\lfloor \frac{2^{k-1}}{3} \rfloor + 2^{k-1}, 
\lceil \frac{2^{k-1}}{3} \rceil + 2^{k-1})$ in every
row which partition the torus into two parts. 
Following the same reasoning as in the proof of lemma 6, 
it is known that for each row there are 
$2 \times \lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor$ 
connections from one part to the other part, hence, there are a total of 
$2^{r-k} \times 2 \times \lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor$ 
connections
crossing the two parts. 
Since there are $2\times 2^{r-k}$ edges in the cut, regardless of the
routing scheme used, there exist at least one edge that is used at least 
$\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$ times. Thus,
 $\pi(TORUS(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$. $\Box$

X--Y routing between dimensions and odd--even shortest path routing
within each dimension are used to develop the RCA scheme. 
Next, the combination of 
ring configurations into torus configurations is considered.
As in the case of rings,
given a $2^k\times 2^{r-k}$ torus,  
the connections in ${H_r}$ are partitioned into two sets. The first
set includes all connections in $DIM_0\cup..\cup DIM_{k-2}$ in each row and 
all connections in $DIM_0\cup..\cup DIM_{r-k-2}$ in each column. The second
set includes the connections in $DIM_{k-1}$ in each row and 
the connections in $DIM_{r-k-1}$ in each column. The connections  
in $DIM_0\cup..\cup DIM_{k-2}$ in each row and the
connections in $DIM_0\cup..\cup DIM_{r-k-2}$ in each column form four 
hypercube patterns on  four disjoint $2^{k-1}\times 2^{r-k-1}$ 
sub--meshes in the torus.
A straight forward extension of
the channel assignment scheme
 in the previous section can be used to assign channels to these
connections with at most
$\lfloor \frac{2^k}{3} \rfloor + 2$ channels. 

To realize the connections in $DIM_{k-1}$ in each row and 
the connections in 
$DIM_{r-k-1}$ in each column, 
%we assume that $r-k \ge 3$, and thus, $k \ge 3$.
%Note that if $r-k < 3$, then the hypercube pattern 
%on each row have 1, 2 or 4 nodes
%and a multiplexing degree of 2 is sufficient to realize all $H_{r-k}^{col}$. 
%In this case, a simple scheme that realizes $H_k^{row}$ and 
%$H_{r-k}^{col}$
%individually yields a connections scheduling that results in at most
%2 more multiplexing degree than the minimum required (the multiplexing degree
%to realize $H_k^{row}$). 
%For connections in $DIM_{k-1}$ in each row and $DIM_{r-k-1}$ 
%in each column, 
The same partitioning for the 
ring topology discussed in section 4 is followed. Specifically, 
the following configurations are constructed 
in rows and columns respectively

\noindent
$row_i = \{(i, i+2^{k-1}), (i+2^{k-1}, i), (i+1, i+1+2^{k-1}), 
          (i+1+2^{k-1}, i+1)\}$

\noindent
$column_j = \{(j, j+2^{r-k-1}), (j+2^{r-k-1}, j), (j+1, j+1+2^{r-k-1}), 
          (j+1+2^{r-k-1}, j+1)\}$

\noindent
$DIM_{k-1}$ is  composed of
 the configurations
$row_i$, for  $i = 0, 2, ..., 2^{k-1}-2$ and $DIM_{r-k-1}$ 
 is composed of  the configurations 
$column_j$ for $j = 0, 2, ..., 2^{r-k-1}-2$.

\noindent
{\bf Lemma 10} For any $i_1$, $i_2$, 
where $i_1 \ne i_2$,  
$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
$column_{i_2}$ in each column can be realized in two torus configurations.

\noindent
{\bf Proof}: Similar to the proof of Lemma 8, omitted. $\Box$
%This lemma is proven by constructing the two torus configurations,
%while avoiding  node conflicts. 
%The first torus configuration is constructed in the following way.
%For each 
%row $i_1$, $i_1+1$, $i_1+2^{r-k-1}$ and $i_1+1+2^{r-k-1}$, configuration
%$row_{i_1}$ is realized, while in all other rows, $row_{i_2}$ is realized.
%Now, consider the columns. In each column $i_1$, $i_1+1$,
%$i_1+2^{k-1}$ and $i_1+1+2^{k-1}$, the $(i_2)$th, $(i_2+1)$th, 
%$(i_2+2^{r-k-1})$th
%and $(i_2+1+2^{r-k-1})$th nodes are not used. Hence, 
%$column_{i_2}$ can be established
%in these columns. A similar argument is used for establishing 
%$column_{i_1}$ on 
%all other columns. This completes the construction of the first torus
% configuration.
%The second torus configuration can be obtained from the first 
%torus configuration by swapping $row_{i_1}$ and
%$row_{i_2}$ in the rows and $column_{i_1}$ and $column_{i_2}$ in the
%columns. These two configurations include   
%$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
%$column_{i_2}$ in each column. Thus,  
%$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
%$column_{i_2}$ in each column can be realized in two torus configurations.
%$\Box$

%\begin{table}[htbp]
%\\small
%\footnotesize
%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%columns \& rows & configuration \\
%\hline
%row $i_1$ & $row_{i_1}$ \\
%\hline
%row $i_1+1$ & $row_{i_1}$ \\
%\hline
%row $i_1+2^{r-k-1}$ & $row_{i_1}$ \\
%\hline
%row $i_1+1+2^{r-k-1}$ & $row_{i_1}$ \\
%\hline
%other rows & $row_{i_2}$ \\
%\hline
%column $i_1$ & $column_{i_2}$ \\
%\hline
%column $i_1+1$ & $column_{i_2}$ \\
%\hline
%%column $i_1+2^{k-1}$ & $column_{i_2}$ \\
%\hline
%column $i_1+1+2^{k-1}$ & $column_{i_2}$ \\
%\hline
%other columns & $column_{i_1}$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Configuration 1}
%\label{CONF1}
%\end{table}


%\begin{table}[htbp]
%\small
%\footnotesize
%%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%columns \& rows & configuration \\
%\hline
%row $i_1$ & $row_{i_2}$ \\
%\hline
%row $i_1+1$ & $row_{i_2}$ \\
%\hline
%row $i_1+2^{r-k-1}$ & $row_{i_2}$ \\
%\hline
%row $i_1+1+2^{r-k-1}$ & $row_{i_2}$ \\
%\hline
%other rows & $row_{i_1}$ \\
%\hline
%column $i_1$ & $column_{i_1}$ \\
%\hline
%column $i_1+1$ & $column_{i_1}$ \\
%\hline
%column $i_1+2^{k-1}$ & $column_{i_1}$ \\
%\hline
%column $i_1+1+2^{k-1}$ & $column_{i_1}$ \\
%\hline
%other columns & $column_{i_2}$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Configuration 2}
%\label{CONF2}
%\end{table}
%
%\noindent
%{\bf Lemma 11}: Assuming that $r-k \ge 3$, 
%the dimension $k-1$ connections in each row and 
%dimension $r-k-1$ connections in each column can be realized in 
%$2^{k-2}$ torus configurations.
%
%\noindent
%{\bf Proof}: From Lemma 10,  configurations
%$row_i$, $i = 0, 2, ..., 2^{r-k-1}-2$ and configurations 
%$column_j$, $j = 0, 2, ..., 2^{r-k-1}-2$ can be realized in $2^{r-k-2}$
%torus configurations. Since $2^{k-2} - 2^{r-k-2}$ torus configurations
% can be
%used to realize $row_i$,  $ i = 2^{r-k-1}, .. 2^{k-1}$, $2^{k-2}$ 
%torus configurations can realize all the dimension $k-1$ connections 
%in each row and 
%dimension $r-k-1$ connections in each column. $\Box$

\noindent
{\bf Theorem 5}:  ${H_r}$ can be realized on a $2^k\times 2^{r-k}$ torus, 
where $k \ge r-k$, using
$\lfloor \frac{ 2^k}{3} + \frac{2^k}{4}\rfloor + 2$ channels.

\noindent
{\bf Proof}: As discussed above,  
$\lfloor \frac{ 2^k}{3}\rfloor + 2$ channels 
are sufficient to realize all 
connections in ${H_r}$, except the connections in  $DIM_{k-1}$ in each row and 
$DIM_{r-k-1}$ in each column, by realizing  four hypercube
communication patterns on the four disjoint sub--meshes. From Lemma 10, 
configurations
$row_i$, $i = 0, 2, ..., 2^{r-k-1}-2$ and configurations 
$column_j$, $j = 0, 2, ..., 2^{r-k-1}-2$ can be realized in $2^{r-k-2}$
torus configurations. Since $2^{k-2} - 2^{r-k-2}$ torus configurations
can be
used to realize $row_i$,  $ i = 2^{r-k-1}, 2^{r-k-1}+2, .., 2^{k-1}-2$, 
all the dimension $k-1$ connections in each row and 
dimension $r-k-1$ connections in each column can be 
realized in $2^{k-2}$ 
torus configurations. Hence, ${H_r}$ can be realized by  a total of 
$\lfloor \frac{ 2^k}{3}\rfloor + 2 + 2^{k-2}
= \lfloor \frac{ 2^k}{3} + \frac{2^k}{4}\rfloor + 2$ configurations.
$\Box$

\noindent
{\bf Corollary 5.1:} $w(TORUS(2^k\times 2^{r-k}), H_r) \le 
\lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor + 2 \le 
\pi(TORUS(2^k\times 2^{r-k}), 
H_r) + 2$. $\Box$

%\end{doublespace}

%\subsubsection{Conclusion}

%In this subsection, I studied optimal schemes to realize 
%hypercube connections on  mesh--like optical networks.
%I prove that 
%$\lfloor \frac{2N}{3}\rfloor$ and $\lfloor \frac{N}{3} + \frac{N}{4}\rfloor$
%are tight lower bounds of the number of channels needed to realize hypercube
%connections on linear arrays and rings of size $N$, respectively. I develop
%optimal RCA algorithms that achieve these lower bounds. Also,
%I study the mesh and torus topologies and develop RCA
%algorithms that use at most 2 more channels than the 
%optimal. 

\subsection{Logical torus, all--to--all and allXY topologies}
\label{otherontori}

The logical torus topology coincides with the physical network. Thus, when 
realizing logical torus topology, there are no link conflicts 
since the physical network can support all links in the logical 
network simultaneously. However, node conflicts may occur. 
Under our network model, each node in the network can only access
one channel at any given time slot. Hence, to support 4 out--going links at 
each node, at least 4 channels are needed. Using 4 channels, the logical 
torus topology can be realized as follows. All links in a torus can be
classified into four categories, the UP links, the DOWN links, the LEFT 
links and the RIGHT links. Each category can be realized using 1 channels
without incurring node conflicts and link conflicts as shown in 
Figure~\ref{logicaltorus}. Notice that all nodes can be sending and receiving 
messages in the figure. Hence, 4 channels are sufficient and necessary
to realize the logical torus topology on top of the physical torus 
topology.    

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/logicaltorus.eps,width=5in}}
\caption{Realizing logical torus topology}
\label{logicaltorus}
\end{figure}

Optimal schemes to realize all--to--all communication on ring and torus 
topologies can be found in \cite{Hinrichs94}. It is shown in 
\cite{Hinrichs94} that for an $N$ node ring, $N\ge 8$, 
the all--to--all communication can be realized with $N^2/8$ channels without
node conflicts. For
an $N\times N$ torus, the all--to--all communication can be realized with
$N^3/8$ channels. The connections on each channel  
to realize the all--to--all communication will be called 
an {\em AAPC configuration}.
Details about the connection scheduling can be found in 
\cite{Hinrichs94}.

The logical allXY topology realizes  all--to--all connections in
each dimension in the physical torus. For an $N\times N$ torus, each
node in the logical allXY topology logically connects to $2N-2$ nodes.
Using the AAPC configurations for rings,
techniques similar to the ones in section~\ref{hypercubeontori}
can be used to combine the ring configurations to form torus configurations
and realize the allXY on an $N\times N$ torus, where $N\ge 16$, resulting in 
a multiplexing degree of $N^2/8$. For an $N\times N$ torus with $N\le 8$,
$2N-2$ channels can be used to realize the allXY topology.
For example, 
using the 8 AAPC configurations for 8--node rings in \cite{Hinrichs94},
6 configurations along each dimension cannot be combined because
of node conflicts, while 2 configurations in each dimension can be
combined in the torus, resulting a multiplexing degree 
of $14=2\times 8-2$ for realizing the allXY topology.

\section{Performance of the logical topologies under light load}

This section  considers the communication performance of the 
logical topologies under light load such that the network contentions
on both channels and switches are negligible.
An analytical model will be described that takes the network 
contention effect into consideration later in this chapter.

Let us assume that a packet can be
transferred from source to destination on a path 
in one time slot and that the network has a multiplexing degree of  $d$.
It takes on average $\frac{d + 1}{2}$ time slots to transfer a packet from
a router to the next router. Thus, assuming that the packet routing time in 
each router (including the E/O, O/E conversions) is $\gamma$, the average
number of intermediate routing hops per packet is $h$, and the network
contention is negligible, 
the average delay time for each packet can be expressed as follows: 

\vspace{-0.15in}
\begin{center}
\[delay = (h + 2) * \gamma + (h + 1) * \frac{d + 1}{2}.\]
\end{center}

The first term, $(h+2) * \gamma$, is the average routing time that a packet 
spends at the  $h$ intermediate
routers and the 2 routers at the sending and receiving nodes.
The second term, $(h+1) * \frac{d + 1}{2}$, is the
average packet transmission time on paths plus the time that a packet waits 
in the output path buffers.
Thus, the average delay time is determined by three parameters, 
the multiplexing degree $d$, the packet routing time 
$\gamma$, and the average number of hops per 
packet transmission $h$.
We can assume that the packet routing 
time $\gamma$ is the same for all topologies.
Different logical topologies result in  
different number of intermediate hops, $h$, 
and different multiplexing degree, $d$.
Next, the performance of the four logical topologies is discussed.

Given an $N\times N$ torus, the logical all--to--all topology
establishes direct connections  between all pairs of nodes and thus,
totally eliminates the intermediate hops, resulting in 
$h= 0$. Using the algorithm in \cite{Hinrichs94},  a multiplexing degree
of $\frac{N^3}{8}$ can be used to realize the logical all--to--all topology. 
Thus $d = \frac{N^3}{8}$, and the delay time is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{all-to-all} = 2\times \gamma + (\frac{N^3}{8} + 1) / 2 = 
             O(\gamma + N^3). \]
\end{center}

Given an $N\times N$ torus, a  logical torus topology can be realized using 
a multiplexing degree of 4 (i.e., $d = 4$).
For a logical $N\times N$ topology, the average number of intermediate
hops is $h = \frac{N}{2} - 1$. Hence the delay time for the logical torus
topology is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{torus} = (\frac{N}{2} + 1)\times \gamma + 
                              \frac{N}{2} \times (4+1)/2 = O(N\times \gamma).\]
\end{center}

For $N = 2^r$,  the algorithm in section~\ref{hypercubeontori} can  realize 
a logical hypercube topology  on an $N\times N$ torus
using a multiplexing degree of 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 2$, if $r$ is odd, and 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 1$, 
if $r$ is even.
For a logical $N^2$ node hypercube, the average number of intermediate
hops is $h = \frac{lg(N^2)}{2} - 1 = lg(N) - 1$.
Hence,  the delay time (for an even r) is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{hypercube} = (lg(N) + 1)\times \gamma + 
     \lg(N) \times (\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 2) / 2
     = O(\gamma lg(N) + Nlg(N)).
\]
\end{center}

Finally, let us consider the logical {\em allXY} topology.
As discussed in section~\ref{otherontori}, 
when $N \le 8$,  the logical topology can be realized
using a multiplexing degree of $2N-2$. For $N > 8$, 
a multiplexing degree of  
$\frac{N^2}{8}$ is needed. Since 
for two nodes in the same column or row,
no intermediate hop is needed, while in other cases, 
one intermediate hop is required,
the average number of intermediate hops on the logical allXY topology is
given by:

\centerline{$\frac{2N-2}{N^2-1}\times 0 + \frac{(N^2-1) - (2N-2)}{N^2-1} 
            \times 1 = \frac{N^2-2N + 1}{N^2-1}.$}

Therefore, for $N > 8$, the average delay can be expressed as follows:

\vspace{-0.15in}
\begin{center}
\[delay_{all\_XY} = (2+\frac{N^2-2N+1}{N^2-1})\times \gamma + 
                    (1+\frac{N^2-2N+1}{N^2-1})\times 
                    (\frac{N^2}{8} + 1) / 2 = O(\gamma + N^2).\]
\end{center}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Logical  & Number of   & multiplexing & total number \\
topology & intermediate hops (h) & degree (d) & of connections (P)\\
\hline
all--to--all & $0$ & $\frac{N^3}{8}$ & $N^2(N^2-1)$ \\
\hline
all\_XY  & $\frac{N^2-2N+1}{N^2-1}$ & 
$\frac{N^2}{8}$\ $\dagger$ & $N^2(2N-2)$\\ 
\hline
hypercube & $lg(N) -1$ & $\lfloor \frac{N}{3}+\frac{N}{4} \rfloor + 1$\ $\ddagger$ &
$N^2lg(N)$\\
\hline
torus &  $\frac{N}{2} -1 $  &  4 & $N^2\times 4$\\
\hline
\end{tabular}
\end{center}

$\dagger$ Assuming that $N>8$. If $N < 8$, the value is $2N-2$.

$\ddagger$ Assuming that $r$ is even. If $r$ is odd, the value is 
$\lfloor \frac{N}{3}+\frac{N}{4} \rfloor + 2$.

\caption{Summary of logical topologies}
\label{toposummary}
\end{table}

Table~\ref{toposummary} summarizes the average number of intermediate 
hops ($h$),
the multiplexing degree ($d$) and the total number of logical 
connections ($P$) 
for the four topologies. 
%Since we will use physical $8\times 8$ and $16\times 16$ tori as our
%underlying networks, we evaluate $h$, $d$ and $P$ for these two
%physical topologies in Table~\ref{specifictopology}. 
Figure~\ref{DELAY} plots  the average delay as a function of the packet
routing time $\gamma$, for the four logical
topologies on a physical $16\times 16$ torus. When
$\gamma$ is very small compared to data transmission time ($\gamma\le 0.5$),
the logical torus topology
achieves the smallest delay time. When $0.5\le \gamma \le 4.25$, the 
logical hypercube
has the best performance. When $4.25\le \gamma \le 256.25$, the allXY topology
gives the best performance. When $\gamma
 > 256.25$, the all--to--all topology has the
smallest packet delay. 

The characteristics exhibited in Figure~\ref{DELAY} are true for any
network size. Specifically, for a given $N$, there is a value of $\gamma$ below
which routing on the torus is more efficient than routing on the logical
hypercube. Similarly, 
there is a value of $\gamma$, above which
routing on the allXY topology
is more efficient than routing on the logical hypercube.
Finally, there is a value of $\gamma$, above which
routing on the all--to--all  topology
is more efficient than routing on the allXY topology.
In Figure~\ref{BEST} these special values are plotted for different $N$
and the ($N, \gamma$) parameter space is divided into four regions.
Each region is labeled by the logical topology that results in the
lowest average packet delay.

These results are obtained by ignoring network traffic contention,
and thus  are valid only  under light load. 
In the next section, a queuing model is used to study the network
performance under high load.

%\begin{table}[htbp]
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
%physical & logical  & h & d & P \\
%topology & topology &  &  & \\
%\hline
% & all--to--all & 0 &  64 & $64\times 63$ \\
%\cline{2-5}
%$8\times 8$ & all\_XY  & 0.78 & 14 & $64\times 14$\\ 
%\cline{2-5}
%torus & hypercube & 2 & 6 & $64\times 6$\\
%\cline{2-5}
% & torus &  1  &  4 & $64\times 4$\\
%\hline
% & all--to--all & 0 &  512 & $256\times 255$ \\
%\cline{2-5}
%$16\times 16$ & all\_XY  & 0.88 & 32 & $256\times 30$\\ 
%\cline{2-5}
%torus & hypercube & 3 & 10 & $256\times 8$\\
%\cline{2-5}
% & torus &  1  &  4 & $256\times 4$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Logical topologies on $8\times 8$ and $16\times 16$ tori}
%\label{specifictopology}
%\end{table}

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}
{\psfig{figure=fig/delay.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}
{\psfig{figure=fig/delay1.eps,width=2.95in}}
\end{subfigure}
\end{subfigRow*}
\caption{Performance for logical topologies on $16\times 16$ torus}
\label{DELAY}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range.eps,width=3in}}
\caption{Logical topologies giving lowest packet delay for given $\gamma$ and $N$}
\label{BEST}
\end{figure}

\section{An analytical model and its verification}

This section describes an approximate 
analytical model that takes network contention into consideration.
This model is used to study the effect of the network load on 
the maximum throughput and the packet delay.
It is assumed that in each time slot, a 
packet can be sent from the source to the destination on a path. 
For example, if a 1Gbps channel is used with a 53--byte packet (or cell)
as defined in the ATM standard, then the slot duration is $0.424\mu s$.
All other delays in the system are normalized with respect 
to this slot duration.

The routers and the paths in a network are modeled as a network of queues.
As shown in Figure~\ref{ROUTER},  each router has a routing queue that 
buffers the packets to be processed. The  router places packets
either into one of the output path queues that buffer packets
waiting to be transmitted, or into the local processor. 
Both a router
and a path have a constant service time. The exact model for such network
is very difficult to obtain. The network is approximated by making the 
following assumptions: 1) each queue is independent of each other and 
2) each queue has a Poisson arrival and constant service time. These 
assumptions enable the derivation of  expressions for 
the maximum throughput and the average packet delay 
of the four logical topologies
by dealing with the M/D/1 queues independently. The simulation results
confirm that these approximations are reasonable.  The following 
notation is used in the model:

\begin{itemize}
\item $N$. Size of each dimension of the torus. Thus,
 the network has a total of 
$N^2$ nodes.

%\item $d$. Multiplexing degree in the network. Different logical topologies
%require different multiplexing degrees. The multiplexing degrees for the four
%logical topologies are summarized  in 
%Table~\ref{toposummary}. A {\em frame} consists of $d$ time slots.
%
%\item $P$. Number of connections in the logical topology. Different
%logical topologies contain different numbers of connections. 
%The numbers of connections in the four logical topologies  are summarized
%in Table~\ref{toposummary}.
%
%\item $h$. Average number of intermediate hops per packet transmission.
%This number depends on the logical topology and is summarized in 
%Table~\ref{toposummary}. 
%The average number of paths a packet goes
%through is equal to $h+1$. The average number of routers a packet goes
%through is $h+2$, intermediate hops plus the sending and receiving nodes.

\item $d$, $h$ and $P$ are defined in the previous section. 
A {\em frame} consists of $d$ time slots. Within a frame, one time slot
is allocated to each path. As discussed earlier,
the average number of paths that a packet traverses
is equal to $h+1$. The average number of routers that a packet traverses
is $h+2$.
 

\item $\lambda$. Average packet generation rate at each node per
time slot. This implies that the average generation rate of packets to the 
entire network is $N^2\lambda$. It is  assumed 
that the arrival process is Poisson
and is independently and identically distributed on all network nodes. 
Furthermore, it is  assumed that all packets are equally likely to be destined
to any one of the network nodes. At each router, the newly generated packets
and the packets arriving from other nodes
are maintained in an infinite routing buffer before being processed
as shown in Figure~\ref{ROUTER}.

\item ${\lambda}_s$. Average rate of packet arrival at a router per time slot,
including both generated packets and  packets received from other nodes. 
This composite arrival
rate, ${\lambda}_s$, may be derived as follows. In any time slot the total 
number of generated
packets that arrive at all the routing buffers is $\lambda N^2$. 
On average, each of 
these packets traverses  $h+2$ routers within the network. 
Therefore, under steady state condition, there will be $\lambda N^2 (h+2)$
packets in all the routers of the network 
in each time slot. Under the assumption that each packet is 
equally likely to be in each router, the total arrival rate is given by 
${\lambda}_s = {\lambda} (h+2)$.

\item ${\lambda}_p$. Average rate of packet arrival at a path buffer
per time slot.
This arrival rate, ${\lambda}_p$, can be derived as follows. 
Under steady state condition, in any time slot, 
the total number of  packets in all the routers in the network is  
$\lambda N^2 (h+2)$. 
Of all these packets, $\lambda N^2$ packet will exit the network and 
$\lambda N^2  (h+2) - \lambda N^2  = \lambda N^2  (h+1)$ packets will be 
transmitted through paths in the network. 
Under the assumption that sources and destinations are uniformly
distributed in the network, the average arrival
rate is given by ${\lambda}_p = \frac{\lambda N^2 (h+1)}{P}$. 

\item ${\gamma}$. The routing time per packet at a router. 
Since packets are
of the same length, the routing time is a constant value. The 
average packet departure rate from the routing buffer, denoted by
${\mu}_s$,  is ${\mu}_s = \frac{1}{{\gamma}}$.

\item ${\mu}_p$. The average packet departure rate from each path buffer
per time slot. Since in the model used,
each path will be served once in every frame, ${\mu}_p = \frac{1}{d}$. 
The average service time in each path is $S_p = \frac{1}{{\mu}_p} = d$.

\end{itemize}

\subsection*{Maximum throughput}

With the above notation, the maximum throughput and average
packet delay of the logical topologies can now be studied. First the theoretical
maximum throughput is considered and then the average packet delay. 
Two bottlenecks can
potentially limit the maximum throughput.

\begin{itemize}
\item If the average packet arrival rate at a routing buffer
 is larger than the average
packet departure rate, that is  if ${\lambda}_s \le {\mu}_s$, then 
the throughput will be limited by the router processing bandwidth.
The maximum packet generation rate allowed by the router bandwidth, 
${\lambda}^{max}_s$, can be derived as follows: ${\lambda}_s \le {\mu}_s$, or 
$(h+2)\lambda \le \frac{1}{{\gamma}}$, or $\lambda \le \frac{1}{{\gamma} (h+2)}$. Thus,

\vspace{-0.15in}
\begin{center}
\[{\lambda}^{max}_{s} = \frac{1}{{\gamma}(h+2)}\] 
\end{center}

\item If the average packet arrival rate at a path buffer
is larger than the average 
packet departure rate, that is ${\lambda}_p \le {\mu}_p$, then
the throughput will be limited by the path bandwidth. The maximum fresh packet
generation rate allowed by the path bandwidth, ${\lambda}^{max}_{p}$, can be 
derived as follows: ${\lambda}_p \le {\mu}_p$, or 
$\frac{(h+1)\lambda N }{P} \le \frac{1}{d}$, or 
$\lambda \le \frac{P}{(h+1)Nd}$. Thus, 

\vspace{-0.15in}
\begin{center}
\[{\lambda}^{max}_{p} = \frac{P}{(h+1)Nd}\]
\end{center}

\end{itemize}

The theoretical maximum throughput is the minimum of ${\lambda}^{max}_{s}$
and ${\lambda}^{max}_{p}$, that is, 
${\lambda}^{max} = min ({\lambda}^{max}_{s},  {\lambda}^{max}_{p})$. Given
a topology, ${\lambda}^{max} =  {\lambda}^{max}_{s}$ indicates that
the router speed is the bottleneck, while
${\lambda}^{max} =  {\lambda}^{max}_{p}$ indicates that the path speed is 
the bottleneck. 

\subsection*{Average packet delay}

As mentioned before, the packet delay is divided into the 
{\em routing delay}, 
which includes the time a packet spends on routing buffers and the time 
for routers to process the packets, and the {\em transmission delay}, which 
includes the time a packet spends on path buffers and the actual
packet transmission time on the paths. 

Let us first
consider the routing delay in each router.
It takes ${\gamma}$ timeslots for a
router to process the packet when the packet reaches the front of the 
routing buffer. As for the packet waiting time in the routing buffer,
since the routing buffer is modeled as an $M/D/1$
queue, the average queuing delay depends on the arrival rate
${\lambda}_s$ and is given by:
\vspace{-0.15in}
\begin{center}
\[ Q = \frac{{\lambda}_s ({\gamma})^2}{2(1-\frac{{\lambda}_s}{{\mu}_s})} \]
\end{center}
 
where ${\lambda}_s$ is the average packet arrival rate, ${\gamma}$ is the
expected service time, ${\mu}_s$ is the average packet departure rate.
Given that ${\mu}_s = \frac{1}{{\gamma}}$,  the total time
that a packet spends in each router is given by:
\vspace{-0.15in}
\begin{center}
\[ routing\ delay = {\gamma} + \frac{{\lambda}_s({\gamma})^2}{2(1 - {\lambda}_s{\gamma})} 
\hspace{1in}(1)\]
\end{center}

Consider the two components of 
the transmission delay on each path. 
The first component is 
the delay required by a packet to synchronize with
the appropriate outgoing slot in the frame 
on which the node transmits and the actual packet transmission time. 
The average value of this delay is 
$\frac{1 + 2 + ... + d}{d} = \frac{d+1}{2}$. The second component is the  
$M/D/1$ queuing delay that a packet 
experiences at the buffer before it reaches the head of the buffer. 
This follows the same formula as in the 
routing delay case, and is given by,
\vspace{-0.15in}
\begin{center}
\[ \frac{{\lambda}_p S^2_p}{2 (1 - \frac{{\lambda}_p}{{\mu}_p})} = 
   \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)}\]
\end{center}

The two components are combined to obtain 
the total delay a packet encounters on a path as follows,
\vspace{-0.15in}
\begin{center}
\[ transmission\ delay = \frac{d+1}{2} + \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)} \hspace{1in}(2) \]
\end{center}

As discussed earlier, each packet
takes $h+2$ hops and $h+1$ paths on average. Thus, given that on average,
a packet spends {\em  routing delay} in each router and 
{\em transmission delay} on each 
path, the average packet delay can be expressed as follows:

\centerline{$delay = (h+2) \times routing\ delay + (h+1) \times transmission\ delay.$}

Using formula (1) and (2),
the following average delay encountered by a packet from the source
to the destination is obtained.

\vspace{-0.15in}
\begin{center}
\[delay = (h+2)\times ({\gamma} + \frac{{\lambda}_s({\gamma})^2}{2(1 - {\lambda}_s{\gamma})})
           +(h+1)\times (\frac{d+1}{2} + 
                  \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)})\]
\end{center}

\subsection*{Model verification}

To verify the analytical model and to further study the performance of these
logical topologies, a network simulator was developed that simulates all four 
logical topologies on top of the torus topology. 
The simulator takes the following parameters.

\begin{itemize}
\item  {\em system size}, $N\times N$: This specifies the size of the network. Based on 
the logical topology, the system size also determines the multiplexing degree
in the system.

\item {\em packet generation rate}, ${\lambda}$: This is the rate at 
which fresh packets are 
generated in each node. 
It specifies the traffic on the network. The inter--arrival 
of packets follows a Poisson distribution. 
When a packet is generated at a node,
the destination is generated randomly among all other nodes in the system
with a uniform distribution.

\item {\em Packet routing time}, ${\gamma}$.

\end{itemize}

Fig~\ref{twothroughput} shows the maximum throughput obtained from the 
analytical model and from simulations.  Both $8\times 8$ and 
$16\times 16$ physical torus networks with different packet routing time
are examined.  
As can be seen from the figure, the analytical results and the simulation
results almost have a perfect match for all cases.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/thro.64.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/thro.256.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{predicted and simulated maximum throughput}
\label{twothroughput}
\end{figure}


Figure~\ref{twodelay1} and Figure~\ref{twodelay2} 
show the average packet delays obtained from the analytical model and from
simulations. Here, the packet routing time, $\gamma$,
 is equal to 1 time slot.
For $8\times 8$ torus, the analytical model matches the
simulation results fairly well 
for all topologies except when the generation rate is close
to saturation. The difference between the 
results from the analytical model and 
those from simulations is around 10\%. 
For the $16\times 16$ physical topology, 
the analytical model matches the simulations results for 
the all--to--all, allXY
and hypercube topologies. For the torus topology, the difference 
 is about 20\% due to the 
approximation.  Studies using other values of $\gamma$ have 
also been conducted. The 
analytical model and the simulation results on those studies
match slightly better than those shown in Figures \ref{twodelay1} and 
\ref{twodelay2}. Thus, overall the analytical model gives a good indication of
the actual performance.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/alltoall64.1.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/alltoall256.1.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delays for logical all--to--all topology ($\gamma = 1$)}
\label{twodelay1}
\end{figure}

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/three64.1.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/three256.1.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delays for logical allXY, hypercube and torus 
         topologies ($\gamma = 1$)}
\label{twodelay2}
\end{figure}

\section{Performance of the logical topologies}

In the previous section, an analytical model for performance
study for the logical topologies was developed and verified.
This section focuses on studying 
the performance of the logical topologies.
Since the simulation and the analytical model match reasonably well,
only the analytical model is used in this section 
to study the performance. 

Figure~\ref{throughput1} shows the impact of packet routing time 
on the maximum throughput. 
The underlying topology is a $32\times 32$
torus. For all logical topologies, increasing the speed of routers
increases the maximum throughput up to a certain limit.
For the all--to--all topology, the
router speed of 1 packet per 4 time slots is sufficient to overcome the
router performance bottleneck. 
Using faster router will not 
further 
improve the maximum throughput. For the allXY and hypercube topologies, the 
threshold is 1 packet per 2 time slots, and for the
 torus topology, the threshold
is 1 packet per time slot. When the routing speed is faster than the
threshold value, the maximum throughput is bound by the link speed
and the maximum throughput will not increase along with the increase in
 router speed.
Table~\ref{throughput2} shows the bandwidth limits
of routers and links for $N = 32$.

Figure~\ref{throughput1} also shows that the all--to--all
topology achieves higher maximum throughput
than the allXY topology, which in turn
achieves higher maximum throughput than the hypercube topology. The logical
torus has the worst maximum throughput. This observation holds for all
packet routing speeds. Under high workload, all paths
in the all--to--all and allXY topologies are utilized. The algorithms
to realize the all--to--all and allXY topologies guarantee that in each time
slot all links are used if all connections scheduled for that time slot are
in use, while the hypercube and torus topologies can not achieve this effect. 
Thus, it is expected that the all--to--all topology and the allXY topology
will outperform the hypercube and torus topologies in terms of maximum
throughput.
%This result indicates that
%using time division multiplexing to establish more connections improves the 
%link utilization under high workload.
 
\begin{figure}[htbp]
\centerline{\psfig{figure=fig/throughput1.eps,width=5in}}
\caption{Maximum throughput .vs. packet routing time $(N= 32)$}
\label{throughput1}
\end{figure}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
topology & bottleneck &  ${\gamma} = 0.5$ & ${\gamma} = 1$ &  ${\gamma} = 2$& ${\gamma}=4$\\
\hline
    &  ${\lambda}^{max}_{s}$ &  2.0     & 1.0     &    0.5   & 0.25\\
\cline{2-6}
all--to--all & ${\lambda}^{max}_{p}$ & 0.25  & 0.25 & 0.25  & 0.25\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.25  & 0.25  & 0.25 & 0.25\\
\hline
    &  ${\lambda}^{max}_{s}$ &  1.36     & 0.68  & 0.34   & 0.17\\
\cline{2-6}
allXY & ${\lambda}^{max}_{p}$ & 0.25  & 0.25 & 0.25  & 0.25\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.25  & 0.25  & 0.25 & 0.17\\
\hline
    &  ${\lambda}^{max}_{s}$ &  0.67     & 0.33     &    0.17   & 0.09\\
\cline{2-6}
hypercube & ${\lambda}^{max}_{p}$ & 0.1  & 0.1 & 0.1  & 0.1\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.1  & 0.1  & 0.1 & 0.09\\
\hline
    &  ${\lambda}^{max}_{s}$ &  0.24     & 0.12     &    0.06   & 0.03\\
\cline{2-6}
torus & ${\lambda}^{max}_{p}$ & 0.06  & 0.06 & 0.06  & 0.06\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.06  & 0.06  & 0.06 & 0.03\\
\hline
\end{tabular}
\end{center}
\caption{Maximum throughput for the logical topologies on $32\times 32$ torus}
\label{throughput2}
\end{table}

Figure~\ref{throughput3} shows the impact of network size on the maximum
throughput. The results in this figure are based 
upon a  packet routing time of one time slot.
Different packet routing times were also studied and 
similar trends were found.
In terms of maximum throughput, the all--to--all topology scales the best, 
followed
by the allXY topology, followed by the hypercube topology. 
The logical torus topology
scales worst among all these topologies. Figures~\ref{throughput1}
and \ref{throughput3} show that by using time--division multiplexing
to establish complex logical topology, the large aggregate
bandwidth in the network can be exploited to deliver higher throughput when the network
is under high workload. 
%In term of maximum throughput, the performance
%of the logical topologies are ordered as follows.
%\centerline{$all--to--all > allXY > hypercube > torus$}


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/throughput2.eps,width=5in}}
\caption{Maximum throughput .vs. network size (${\gamma} = 1$)}
\label{throughput3}
\end{figure}

%From the study of maximum throughput of the logical topologies, 
%we draw the conclusion
%that the complex logical topologies, such as the all--to--all topology
%and  the allXY topology
%are more efficient to exploit the bandwidth in the network. 

Although the all--to--all topology is the 
best in terms of the maximum throughput,
it suffers from large packet delay when the network is  not saturated.
Packet delay
is another performance metric to be considered.
For a network to be efficient, 
it must also be able to deliver packets with a small
delay. 
It is well known that time--division multiplexing results in larger
average packet delay due to the sharing of the links.
However, as discussed earlier, while using time--division multiplexing 
techniques
to establish logical topologies increases the per hop transmission time,
it reduces the average number of hops that a packet travels. Thus, the overall
performance   depends on  system parameters. Next, 
this effect for the logical topologies is studied.

Figure~\ref{latency1} shows the delay with regard to the fresh
packet generation 
rate. The underlying topology is a $16\times 16$
torus and $\gamma$  is  1 time slot. 
The figure shows that the all--to--all topology incurs 
very large delay
compared  to other logical topologies.  This is because of the large 
multiplexing degree needed to realize the logical all--to--all topology. 
Other 
topologies have similar delay when the generation rate is small, that is,
under low 
workload. However, the allXY topology has a larger saturation point than
the hypercube and torus topologies  and thus
has a small delay even when the network load is reasonably high (e.g. 
$\lambda = 0.25$). These results also hold for larger packet
routing times.
 
\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}
{\psfig{figure=fig/ss1.0.eps,width=3in}}
\end{subfigure}
\begin{subfigure}
{\psfig{figure=fig/ss1b.0.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delay as a function of packet generation rate 
         $({\gamma} = 1.0, N=16)$}
\label{latency1}
\end{figure}

Figure~\ref{latency2} shows the impact of packet routing time on the 
average packet delay. The results are based upon a $16\times 16$ 
torus network and a packet generation rate of 0.005. 
The packet routing speed
has an impact on the delay for all topologies. For very small packet
routing time
(${\gamma} = 0.25$), the torus topology has the smallest delay.
When the packet routing time increases, the delay in torus increases 
drastically, 
while the delays 
in the all--to--all and allXY topologies increase
slightly. In the all--to--all and allXY topologies a packet
travels through fewer number of routers
 than it does in the torus topology. Hence the
contention at  routers does not affect the delay in the all--to--all and 
allXY topologies as much as it does in the torus and hypercube topologies.
This study also implies
that to achieve good packet delay for logical torus topology, 
fast routers are crucial, while a fast router
is not as important in the all--to--all
and allXY topologies.
 
\begin{figure}[htbp]
\centerline{\psfig{figure=fig/delay2.eps,width=5.5in}}
\caption{Impact of  packet routing time on packet delay 
         ($\lambda = 0.005, N=16$)}
\label{latency2}
\end{figure}

Figure~\ref{latency3} shows the impact of network size on the packet delay
for the topologies. The results are based upon a packet routing time of
1 time slot and a packet generation rate of 0.01.
This figure shows the manner  in which the delay time grows with regard
to the network size. As discussed in section 2, ignoring network contention
for a physical $N\times N$
torus, the all--to--all topology results in
a packet delay of $O(\gamma+N^3)$, 
 the allXY topology has a delay of $O(\gamma +N^2)$,
hypercube has a delay of $O(\gamma lg(N) + Nlg(N))$, and torus has 
a delay of $O(\gamma N)$.
Thus, the all--to--all topology has  very large delay when the network size
is large. The delay differences among the other
three topologies are relatively 
small for reasonably large sized networks. When the packet
routing time is small (${\gamma}= 1.0$), the hypercube topology 
scales slightly 
better than the torus and the allXY topologies as shown in
Figure~\ref{latency3}~(a). When ${\gamma}$ is large (${\gamma}=4.0$), 
the hypercube topology and the allXY topology
are better than the other two topologies as shown in 
Figure~\ref{latency3}~(b). 
%We should note that the figure plots delays for network sizes 
%up to 1024 nodes. As the network size grows larger (given a certain
%${\gamma}$) the torus would have the best delay, since its asymptotic
%delay grows in the  order of  
%$O(N)$ which is the best among all these topologies. While theoretically
%the logical torus should scales better than other topologies,  
%realizing the logical hypercube
%and allXY topology is usually better than the logical torus for a network
%of practical size.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[${\gamma} = 1.0$]
{\psfig{figure=fig/delay3.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[${\gamma}= 4.0$]
{\psfig{figure=fig/delay3b.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{impact of network size on the delay ($\lambda = 0.01$)}
\label{latency3}
\end{figure}

From the above discussions, three parameters, $N$, $\gamma$ and 
$\lambda$ affect the average packet delay for all the logical topologies. 
Next, 
the regions in the $(N, \gamma, \lambda)$ parameter space, where a logical
topology has the lowest packet delay are identified. 
Figure~\ref{best1} shows the best topologies in 
the parameter space $(N, \gamma)$ with fixed $\lambda$.
Comparing Figure~\ref{BEST}, where the network contention
is ignored, with Figure~\ref{best1}, where the contention is taken into 
consideration, it  can be seen
 that the logical topologies with less connectivity
suffer more from network contention. As can be seen from 
Figure~\ref{best1}~(a), with small packet generation rate, 
all four logical topologies occupy part of the 
$(N, \gamma)$ parameter space, which indicates that under certain 
conditions, each of the four topologies out--performs the other three
topologies. While in the case of large packet generation rate as shown in 
Figure~\ref{best1}~(b), the logical torus topology is pushed out 
of the best topology picture.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[${\lambda} = 0.01$]
{\psfig{figure=fig/range1.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[${\lambda}= 0.06$]
{\psfig{figure=fig/range2.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Best logical topology for a given packet generation rate}
\label{best1}
\end{figure}

Figure~\ref{best2} shows the best logical topologies on the 
$(\gamma, \lambda)$ parameter space. Here, the underlying network
is a $16\times 16$ torus. Networks of different size exhibit similar
characteristics. The majority of the $(\gamma, \lambda)$ parameter space
is occupied by the logical hypercube and allXY topologies. 
The logical torus topology is good only when the 
$\lambda$ is small and $\gamma$ is small. The logical all--to--all 
topology out--performs other topologies only when the network is almost
saturated, that is, large $\lambda$ or large $\gamma$. This indicates
that in general, 
the logical hypercube and allXY topologies are better topologies 
than the logical torus and all--to--all topologies in terms of packet delay. 

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range4.eps,width=4in}}
\caption{Best logical topology for a $16\times 16$ torus}
\label{best2}
\end{figure}

Figure~\ref{best3} compares the performance of the logical
hypercube and allXY topologies.  Given a fixed $\gamma$, there
is a packet generation rate, $\lambda$, above which the 
allXY topology out--performs the logical hypercube topology.
When  $\gamma$ increases, the line in the figure moves down.
In other words, the hypercube topology is more sensitive to the
packet routing time $\gamma$.


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range3.eps,width=4in}}
\caption{Best logical topology for a given packet routing 
time ($\gamma = 1.0$)}
\label{best3}
\end{figure}


\section{Multi--hop communication vs single--hop communication}
\label{multisingle}

Previous sections considered the logical topologies that can be used 
to route packets and perform {\em multi--hop} communications.
As discussed in Chapter 3, another way to perform dynamic 
communication on multiplexed optical networks is to use a path 
reservation algorithm which reserves an optical path
from the source to the destination 
and then perform {\em single--hop} communications. 
The performance of these two communication schemes 
on a physical $16\times 16$ torus is compared in this
section. The logical allXY topology is used as the
logical topology for multi--hop communication since it offers large maximum
throughput and reasonably small average package delay for this 
size of networks. To obtain a fair comparison, the following 
assumptions are made:

\begin{itemize}
\item Both networks have the same multiplexing degree. For a $16\times 16$ 
torus, this means that both networks have a multiplexing degree of 32, which
is required for the logical allXY topology.
\item The data packet processing time in the multi--hop communication is 
equal to the control packet processing time in the path reservation
algorithm, since electronic processing is involved in both cases.
{\em Packet processing time}, $\gamma$, is used
 to represent both the data packet processing
time in the multi--hop communication and the control packet processing time
in the single--hop communication.
\item Control packet propagation time between two neighboring nodes 
is equal to data packet propagation time between the source and the 
destination, which is equal to 1 time slot.
\item It is assumed that a data {\em message} contains $s$ packets. 
Accordingly, the
{\em average message delay}, which is defined as the difference between the 
time the message is generated and the time when the whole message is received,
is measured 
instead of the {\em average packet delay}. The notation $\lambda_{msg}$ in 
this section represents the message generation rate per node per time slot. 
Since messages can be of different sizes, the {\em network load} is defined
to be\\
\centerline{$network\ load = s \times \lambda_{msg} 
            \times number\ of\ nodes,$} 
which is equal to the total number of packets injected into the network.
For the same reason, the throughput is measured
in terms of  packets delivered per time slot. 
\end{itemize}

The analytical model for the multi--hop communication 
cannot model the communication performance when packets in a message 
are sent to the same destination
since  destinations of packets are no longer distributed
uniformly among all nodes.
In some sense the number of packets in a message reflects the locality
of the communication traffic. All results in this section are obtained through
simulations. 


%\begin{figure}[htbp]
%\begin{subfigRow*}
%\begin{subfigure}[$throughput$]
%{\psfig{figure=fig/comp1b.eps,width=3in}}
%\end{subfigure}
%\begin{subfigure}[$delay$]
%{\psfig{figure=fig/comp1a.eps,width=3in}}
%\end{subfigure}
%\end{subfigRow*}
%\caption{Throughput and delay for message of size 8 ($\gamma = 1$)}
%\label{comp1}
%\end{figure}


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/allthro.eps,width=5.5in}}
\caption{Maximum throughput}
\label{comp1}
\end{figure}

Figure~\ref{comp1} shows the maximum throughput of the two schemes
with different message sizes, $s$, and packet processing times, $\gamma$. 
The packet processing time affects both the single--hop communication and the 
multi--hop communication, while the message size affects only single--hop
communication (larger message size leads to higher maximum throughput).
When the packet processing speed is fast, e.g.
$\gamma = 1$, such that the path bandwidth is the bottleneck in the 
communication, the multi--hop communication offers larger maximum throughput
than the single--hop communication. The reason is that multi--hop
communication utilizes the links in the network more efficiently 
when the network is saturated and 
does not incur additional control overhead. However, the multi--hop 
communication is more  sensitive to the packet processing time 
and the maximum throughput of the 
multi--hop communication decreases drastically when the packet 
processing time increases. In the single--hop communication,
the packet processing is only involved in the control network, thus, 
preserving the large bandwidth in the data network when the packet
processing time is large. This effect manifests itself when the message size 
is reasonably large and the extra control overhead is amortized over the
length of a message. Thus, 
the single--hop communication offers larger maximum throughput when
the packet processing time is large and the message size is sufficiently
large.
%In the multi--hop communication, message size does not affect the maximum
%throughput too much, while in the single--hop communication,
%large maximum throughput can only be achieved for large message sizes.
%This also implies that the multi--hop communication performs much 
%better then the single--hop communication when the message size is small.
Figures \ref{comp1a}~(a) and \ref{comp1a}~(b) show the maximum throughput 
with different message sizes for packet processing times of 1 and 4 
respectively. As can be seen from the figures, when the packet 
processing time is small ($\gamma = 1$), the multi--hop communication offers
larger maximum throughput for all message sizes. When the packet 
processing time is large ($\gamma = 4$), the single--hop communication
has a larger maximum throughput when the message size is sufficiently large.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[$\gamma = 1$]
{\psfig{figure=fig/comp4b.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[$\gamma = 4$]
{\psfig{figure=fig/comp4b.n.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Maximum throughput for different message sizes}
\label{comp1a}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/multisize.eps,width=5.5in}}
\caption{Impact of message size on the average message delay ($\gamma = 1$)}
\label{delay1}
\end{figure}

When the network is under light load, it is more meaningful to compare
the message delay. 
Figure~\ref{delay1} shows the  impact of the network load and the message size 
on the average message delay. In this figure,  $\gamma = 1$.
When the message size is small ($size = 4$), the multi--hop communication
has smaller message delay. When the message size is large ($size = 64$), the
single--hop communication offers smaller message delay. In both cases, 
the large network load amplifies the difference between single--hop and 
multi--hop communications. For  messages of medium size
($size = 16$), the multi--hop communication has smaller delay when the network
load is below a certain point. In general, small messages favor the multi--hop
communication while large messages  favor the  single--hop communication.


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/routing.eps,width=5.5in}}
\caption{Impact of packet processing time 
         on the average message delay ($\gamma = 1$)}
\label{delay2}
\end{figure}

The packet processing time affects the average message delay for both 
the single--hop communication and the multi--hop 
communication. In the single--hop 
communication, the packet processing time affects the path reservation
time only. Thus, given a fixed packet processing time, the extra
control overhead is almost the same for all message sizes. In 
the multi--hop communication, the extra overhead applies to each packet in a 
message, and thus the larger the message size, the larger the overhead. 
Figure~\ref{delay2} shows the impact of the packet routing time on the 
average packet delay. In this figure,  
the same network load of 10.24 is considered for different message sizes
(e.g. a generation rate of 0.01 for messages of size 4, 
$0.01\times 4\times 256 = 10.24$) with different message sizes. 
As can be seen from the figure, when the 
message size is small, the single--hop communication incurs larger 
message delay while for large message sizes, the multi--hop 
communication incurs larger message delay. 
The large packet processing time amplifies these effects. 

\section{Chapter summary}

This chapter considered the logical topologies for routing message
on top of torus topologies. Schemes for realizing the logical
torus, hypercube, allXY (where all--to--all connections along each dimension
are established) and all--to--all topologies on top of physical torus
networks were discussed. Optimal schemes for realizing hypercube
on top of physical arrays and rings were designed. Schemes that use
at most 2 more channels than the optimal for realizing 
hypercube on top of meshes and tori were presented.

An analytical model for the maximum throughput and the packet latency 
for multi--hop networks was developed and verified through simulations.
This analytical model was used to study the performance of the logical
topologies and to identify the cases where each logical topology 
out--performs the other topologies. 
In general, the performance of the logical
topologies with less connectivity, such as the torus and 
hypercube topologies,
are more sensitive to the network load and the router speed while
the logical topologies with more connectivity, such as the all--to--all and 
allXY topologies, are more sensitive to network size.
Logical topologies with dense connectivity achieve higher
maximum throughput than the topologies with less connectivity.
In addition, they also scale better 
with regard to the network size. In terms of the maximum throughput, 
the topologies can be ordered as follows:

\centerline{{\em all--to--all} $ > allXY > hypercube > torus$.}

In term of the average packet delay, the logical torus topology achieves best 
results only when the router is fast and the network is under light load,
while the logical all--to--all topology is best only when the router
is slow and the network is almost saturated. In all other cases, logical
hypercube and allXY topologies out--perform logical torus and 
all--to--all topologies. Comparing the logical allXY to the logical hypercube,
the allXY topology is better when the network is under high load. 
These results hold for all network sizes.

This chapter further compared  multi--hop communication with 
single--hop communication and identified the advantages and the limitations
of each communication scheme. The study in this chapter used randomly
generated communication traffic. Performance evaluation of these two 
schemes using communication patterns from real application programs, which 
confirms the results in this chapter, will be presented in Chapter 6.
Multi--hop communication is more efficient than 
single--hop communication in terms of maximum throughput when the packet 
processing speed is not a bottleneck in the system and 
when the message size is small.  When packet processing speed is slow, 
the single--hop  communication has higher maximum throughput when the 
message size is sufficiently large. In terms of the average message delay
when the network is under light load, large messages favor single--hop 
communication, while small messages favor multi--hop communication. 
The large packet processing time amplifies these effects.
Table~\ref{summary1} and Table~\ref{summary2} summarize these conclusions.


\begin{table} [htbp]
\begin{center}
\small
\caption{Maximum throughput on a $16\times 16$ torus}
\label{summary1}

\begin{tabular}{|c|c|c|}
\hline
       &  Small message size($4$) & Large message size($64$)\\
\hline
Small packet processing time & Multi--hop & Multi--hop\\
\hline 
Large packet processing time & Multi--hop & Single--hop\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table} [htbp]
\begin{center}
\small
\caption{Average message delay on a $16\times 16$ torus}
\label{summary2}

\begin{tabular}{|c|c|c|c|c|}
\hline
Network  & Packet processing   & \multicolumn{3}{c|}{Message size}\\
\cline{3-5}
load & time & Small(4)  & Medium(16) & Large(64)\\
\hline
Small & Small & Multi--hop & Multi--hop & Single--hop\\
\cline{2-5}
                    &  Large  & Multi--hop & Single--hop & Single--hop \\
\hline 
Large & Small & Multi--hop & Single--hop & Single--hop\\
\cline{2-5}
                    &  Large & Multi--hop & Single--hop & Single--hop\\
\hline
\end{tabular}
\end{center}
\end{table}

Both communication schemes suffer from the bottleneck of electronic processing,
which occurs in the path reservation in single--hop communication and
in the packet routing at intermediate nodes in multi--hop communication. 
Using the compiled communication technique discussed in the next chapter, 
this bottleneck can be removed. 

 



\chapter{Performance comparison}

This chapter evaluates the relative performance of the three
communication schemes presented in Chapters 3, 4, and 5
using real application programs. Three
sets of programs are used in the evaluation. The first set of programs
includes three hand--coded parallel programs, where communications 
are well defined and highly optimized for parallel execution. 
The second set of programs includes a number of HPF benchmark programs which
are tuned for parallel execution.
% and are concerned about
% the ease of programming. 
The third set of programs
includes a number of programs from SPEC95 which are not optimized for parallel
execution. 

The performance measurement is the communication time 
in the unit of time slots. A packet, 
which contains a number of words,
can be transmitted through a lightpath in a time slot.
In addition, {\em normalized time} is also used to compare the 
performance of the schemes. 
In normalized time, the best communication time
among all schemes is assigned a value of $1.0$
and communication times of all schemes are normalized with respect to 
the best communication time. The normalized time  shows
the best scheme for each program and how other schemes perform compared
to the best scheme. It is assumed that the  communication in  each pattern 
is performed in a synchronized manner. That is, the program synchronizes
before and after each communication pattern and thus no interleaving of 
communications and computations is allowed.  

Because the E--SUIF compiler does not handle the message passing paradigm,
the first set of experiment is carried out manually by extracting 
the communication patterns in the programs by hand.
The programs in the second and third sets are 
generated automatically by the E--SUIF compiler for the experiment. 
As discussed in Chapter 5,
the E--SUIF compiler first analyzes and optimizes the communications in a 
program and represents the communications using Section Communication 
Descriptors (SCDs). It then performs the communication phase analysis
and partitions the program into phases and schedules the communication 
pattern within each phase. Finally, the backend of the E--SUIF compiler
generates a library call, $lib\_comm$, for each SCD and another library
call, $lib\_phase$ for each phase. The $lib\_comm$ takes a SCD with all
runtime information as 
parameter. When the program is 
executed, the $lib\_comm$ procedure invokes a network 
simulator which simulates dynamic single--hop communication, 
dynamic multi--hop communication or compiled communication to obtain 
the communication time of the communication 
using one of the three communication schemes. 
The $lib\_phase$ is useful only when simulating compiled 
communication. It accesses to the 
communication requirement of each phase (that is obtained by the compiler),
and performs channel assignment for connections within each phase. Thus,
the communication performance of a program is obtained by running the
program generated by the E--SUIF compiler. 

The experiments use the following system settings.

\begin{itemize}
\item Physical network: $8\times 8$ torus.
\item Packet size: 4 words.
\item Routing algorithm: 
XY routing between dimensions and Odd--Even shortest--path routing within
each dimension.
\item Dynamic single--hop communication.
  \begin{itemize}
  \item Control protocol: Conservative backward reservation protocols ($cset$
        size is 1). As discussed in Chapter 3, the conservative backward
        reservation protocol almost has the best performance among all the
        path reservation protocols.  
  \item Control packet processing time: 1 time slot.
  \item Control packet propagation time: 1 time slot.
  \item Maximum control packet retransmission time: 5 time slot.
  \item Multiplexing degree: 1, 4, 14, 20.
  \end{itemize}
\item Dynamic multi--hop communication. 
  \begin{itemize}
  \item Logical topologies: torus, hypercube, allXY and all--to--all.
  \item packet switching time: 1 time slot.
  \end{itemize}
\item Compiled communication.
  \begin{itemize}
  \item Connection scheduling algorithms: combined algorithm for the first
        set of experiment, AAPC algorithm for the second and third 
        experiments.
  \end{itemize}
\end{itemize}  

\section{Hand--coded parallel programs}

This set of program includes three
hand--coded parallel programs, namely $GS$, $TSCF$ and $P3M$.
The $GS$ program uses Gauss--Siedel
iterations to solve Laplace equation on a discretized unit 
square with Dirichlet boundary conditions. It contains 
a nearest neighbor communication pattern with fairly large message size 
(64 packets messages).
The $TSCF$ program simulates the evolution of a
self--gravitating system using a self consistent field approach. It 
contains a hypercube communication pattern with small message 
size (1 packet message). $P3M$ performs particle--particle particle--mesh 
simulation \cite{Yuan97c}. This program contains five static communication
patterns.  Table~\ref{PATT}
describes the static communication patterns that arise in these programs.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Pattern & Type & Description\\
\hline
GS   & shared array ref. & PEs are logically linear array, Each PE \\
      &     & communicates with two PEs adjacent to it.\\
\hline
TSCF   & explicit send/recv & hypercube pattern\\
\hline
P3M  1 & data redistrib. & (:block, :block, :block) $\rightarrow$ (:, :, :block)\\
\cline{2-3}
P3M 2 & data redistrib. & (:, :, :block) $\rightarrow$ (:block, :block, :)\\
\cline{2-3}
P3M 3 & data redistrib. & (:block, :block, :) $\rightarrow$ (:, :, :block)\\
\cline{2-3}
P3M 4 & data redistrib. & (:, :, :block) $\rightarrow$ (:block, :block, :block)\\
\cline{2-3}
P3M 5 & shared array ref. & PEs are logically 3--D array, each PE\\
      &                   & communicates with 6 PEs surrounding it\\
\hline
\end{tabular}
\caption{Communication pattern description.}
\label{PATT}
\end{center}
\end{table}

Table~\ref{TP3M} shows the communication
time for these patterns in one main loop step in the programs. 
Table~\ref{TP3MN} shows the normalized time where
the best communication time is normalized to $1.0$.
In this experiment, it is assumed that there is sufficient multiplexing 
degree to support all the patterns in compiled communication. 
Thus, each phase contains one communication pattern and no
network reconfiguration is required to within each pattern. 
For dynamic single--hop communication, the communication time for
fixed multiplexing degrees of 1, 4, 14 and 20 is evaluated. 
For dynamic multi--hop
communication, the logical  torus, hypercube, allXY and
all--to--all topologies are considered. 
The following observations can be made from the results in Table~\ref{TP3MN}.

\begin{itemize}
\item Compiled communication out--performs
dynamic single--hop communication in all cases. The average
communication time for dynamic single--hop communication 
is 4.5 to 8.0 times greater than that
for compiled communication, depending on the multiplexing degree used
in dynamic single--hop communication.
Larger performance gains are observed for communications with
small message sizes (e.g., the $TSCF$ pattern) and dense communication 
(e.g., the $P3M\ 2$ pattern). Large multiplexing degree does not 
always improve the communication
performance for dynamic single--hop communication. For example, 
a multiplexing 
degree of 1 results in the best performance (for dynamic single--hop
communication) for the pattern in GS while a
degree of 14 has the best performance for the $P3M\ 5$ pattern.
\item Compiled communication out--performs  
dynamic multi--hop communication in all cases except for 
the $TSCF$ program where dynamic multi--hop communication has better 
communication time when using the logical hypercube topology. 
The reason is that
the $TSCF$ program only contains hypercube communication with message size 
equal to 1. Multi--hop communication 
achieves good communication performance when communication patterns in
a program matches the logical topology. However, on average, the
communication time for multi--hop communication is 3.0 to 7.6 times
larger than the communication time for compiled communication, depending 
on the logical topology used. 
\item Compiled communication achieves an 
average normalized time of 1.1 for all the communication patterns, which
indicates that compiled communication almost delivers optimal 
communication performance.
\item Comparing dynamic  multi--hop
communication with dynamic single--hop communication,
multi--hop communication has better performance
when the message size is small (e.g. $TSCF$, $P3M\ 5$), 
and when the communication requires dense connections 
(e.g. $P3M\ 2,3$), while
single--hop communication is better when the message size is large
(e.g. $GS$).
\end{itemize}

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & GS  & TSCF & P3M 1& P3M 2,3 & P3M 4& P3M 5\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 131 & 19 & 831 & 382 & 457 & 40 \\
\hline
     & torus & 404 & 30 & 3366 & 1656 &1632 & 127\\
%\cline{2-8}
Multihop & hypercube & 792 & 13 & 3371& 1338 &1499 & 74\\
%\cline{2-8} 
comm. & allXY & 990  & 17 & 3157 & 1058 &960 &121\\
%\cline{2-8}
      & alltoall & 4159 & 70 & 1326 &749 &1326 & 276\\
\hline
      & $d=1$  & 209 & 215 & 3194& 6655 &2091 & 378\\
Single--hop & $d=4$&  296 & 118 &2029 &2998 &1302 & 213\\
comm &        $d=14$ & 924 & 107 & 1713 &2171 &1508 & 196\\
  &          $d=20$ & 1296 & 108 &1702 & 2096 &1314 & 231\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Communication time (timeslots) for the hand--coded programs}
\label{TP3M}
\end{table} 

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & GS  & TSCF & P3M 1& P3M 2,3 & P3M 4& P3M5 & Average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 1.0 & 1.5 & 1.0 & 1.0 & 1.0 & 1.0 & 1.1 \\
\hline
     & torus & 3.1 & 2.3 & 4.1 & 4.3 &3.6 & 3.2 & 3.4\\
%\cline{2-8}
Multihop & hypercube & 6.0 & 1.0 & 4.1& 3.5 &3.3 & 1.9 & 3.3\\
%\cline{2-8} 
comm. & allXY & 7.6  & 1.3 & 3.8 & 2.8 &2.1 &3.0 & 3.4\\
%\cline{2-8}
      & alltoall & 31.7 & 5.4 & 1.6 &2.0 &2.9 & 6.9 & 8.4\\
\hline
      & $d=1$  & 1.6 & 16.5 & 3.9 &17.4 & 4.6 & 9.5 & 8.9\\
Single--hop & $d=4$&  2.3 & 9.1 &2.4 &7.8 &2.8 & 5.4 & 5.0\\
comm &        $d=14$ & 7.1 & 8.2 & 2.1 &5.7 &3.3 & 4.9 & 5.2\\
  &          $d=20$ & 9.9 & 8.2 &2.0 & 5.5 &2.9 & 5.8 & 5.7\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Normalized communication time for the hand--coded programs}
\label{TP3MN}
\end{table} 

  
In this study, two types of communication patterns are observed in 
a well designed parallel program, 
fine grain communications resulted from shared array references
and coarse grain communications resulted from data redistributions.
The fine grain communication  causes sparse connections with 
small message sizes, while the coarse grain communication results in 
dense connections with large message size.
For a communication system to efficiently support the 
fine grain communication,
the system must have small latency. Optical single--hop networks that use 
dynamic path reservation algorithms have a large 
startup overhead, thus cannot support this type of communication
efficiently. As shown in our simulation results, 
compiled communication where the startup overhead is eliminated
and dynamic multi--hop communication perform this type of 
communications efficiently.
For the coarse grain communication, the control overhead in the dynamic
communications  is not significant.
However, dense communication
results in a large number of conflicts in the system (path reservation in 
dynamic single--hop communication and packet routing in dynamic 
multi--hop communication), and the dynamic
control systems are not able to resolve these conflicts efficiently.
By using an off--line connection scheduling algorithm, compiled 
communication handles this type of communications efficiently.
The performance study confirms the conclusion in 
\cite{Hinrichs94} that static management of the dense
communication patterns results in large performance gains. 

%Compiled communication achieves high performance for 
%all types of static patterns in this set of experiments.
%Four factors contribute to the performance gain. First, 
%compiled communication eliminates dynamic control overhead. This is
%most significant for communication with small message sizes, where
%the overhead in the dynamic single--hop communication  is large 
%compared to the communication time.
%Second, compiled communication takes the whole 
%communication pattern into consideration, while dynamic communication, which
%considers the connection requests one by one, suffers from the 
%head--of--line effect \cite{Sivalingam93}.
%Third, the off--line message scheduling algorithm further
%optimizes the communication efficiency for compiled communication.
%Fourth, compiled communication allows the system to adapt to the
%communication requirement in a program. In dynamic single--hop system, 
%communication, control mechanism with variable multiplexing
%degree is very difficult to implement and
%results in large overhead, while in dynamic multi--hop systems, the system
%cannot choose an efficient logical topology without knowing the communication
%requirement of a program. Thus, an optical network with 
%dynamic control may provide good performance for some applications whose
%communication requirement matches the network capacity, but it will 
%not have good performance for other applications. 
%The compiler has the exact knowledge
%of the communication pattern in the three programs and thus, the compiled
%communication offers high performance. In the next set of experiments, I will
%consider some HPF benchmark programs where the compiler may not have the
%exact knowledge of the communication performance and study how much the
%communication performance for the compiled communication 
%is affected by the approximation in the compiler analysis. 

\section{HPF parallel benchmarks}

%\begin{table}
%\small
%\footnotesize
%\begin{center}
%\begin{tabular} {|c|c|}
%\hline
%benchmarks & description \\
%\hline
%0001 & Solution of 2-D Poisson Equation by ADI\\
%\hline
%0003 & 2-D Fast Fourier Transform \\
%\hline
%0004 & NAS EP Benchmark - Tabulation of Random Numbers \\
%\hline
%0008 & 2-D Convolution\\
%\hline
%0009 & Accept/Reject for Gaussian Random Number Generation \\
%\hline
%0011 & Spanning Percolation Cluster Generation in 2-D \\
%\hline
%0013 & 2-D Potts Model Simulation using Metropolis Heatbath\\
%\hline
%0014 & 2-D Binary Phase Quenching of Cahn Hilliard Cook Equation \\
%\hline
%0022 & Gaussian Elimination - NPAC Benchmark \\
%\hline
%0025 & N-Body Force Calculation - NPAC Benchmark  \\
%\hline
%0039 & Segmented Bitonic Sort \\
%\hline
%0041 & Wavelet Image Processing \\
%\hline
%0053 & Hopfield Neural Network \\
%\hline
%\end{tabular}
%\end{center}
%\caption{HPF Benchmarks and their descriptions}
%\label{desc1}
%\end{table}

This set of programs is from the Syracuse University HPF benchmark suite.
The benchmarks and their descriptions are 
listed in table~\ref{desc} in Section~\ref{evalphase} .
% The data 
%distributions are obtained from the original benchmark programs and are
%thus, optimized for the programs. 
The benchmarks include many different 
types of applications, however, all of the programs contain only 
regular computations.

The major difference between this experiment and the first
experiment is that, in this experiment,  compiled
communication is applied to the whole program instead of each individual
communication pattern. Assuming a multiplexing degree of 10,
the compiler tries to aggregate as many communications as possible into a 
phase  as opposed to the first experiment where 
compiled communication is assumed to have an infinite number of 
virtual channels  to handle each individual communication pattern 
in the programs. In addition, this set of programs contains 
communication patterns about which
the compiler cannot obtain precise information.
Two factors may degrade the performance of compiled communication.
First, compiler approximations may result in the waste of bandwidth for
establishing connections that are not used. Second, aggregating more 
communications in a phase reduces the number of network 
reconfigurations, but may result in larger communication time since
larger multiplexing degree is needed for more communications. This experiment
aims at studying the performance of compiled communication under these
limitations.

Table~\ref{HPFperformance} shows the communication time of the
programs using different communication schemes. 
Table~\ref{HPFperformanceN} shows the normalized time.
Even with the limitations discussed earlier, compiled communication in general 
out--performs dynamic communications to a large degree. 
The benefits of managing channels at compile time and 
eliminating the runtime path reservation overhead over--weights
the bandwidth losses through the imprecision of compiler analysis.
The average normalized time for compiled communication is 1.1 which
indicates that compiled communication almost delivers the best communication
performance for this set of programs. However, performance degradation
in compiled communication due to the conservative
approximation in compiler analysis is observed in some of the programs.
For example, compiler over--estimating the communication requirement 
is found in benchmarks 0009 and 0022. 
Note that the overall communication time for
the programs in Table~\ref{HPFperformance} may not show this, because each
program contains many communication patterns and the pattern that
is approximated may not dominate the overall communication time.
The performance loss due to aggregating communications, which results 
in larger multiplexing degree, is observed in benchmark 0025.
%It is desirable to develop more advanced communication phase analysis 
%techniques that can use different multiplexing degrees for different parts 
%of a program to achieve  best performance. 
Nonetheless, the 
overall trend of this experiment is very similar to that in 
the first experiment.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0001  & 0003 & 0004& 0008 & 0009 & 0011\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 45,624 &752  &1,368 &2,256 &2,394 & 105,252 \\
\hline
     & torus &197,760  &3,296  &1,776  &9,888  &3,108 & 158,594\\
%\cline{2-8}
Multihop & hypercube &159,840  &2,664  &1,032 &7,992  &1,806 & 147,496\\
%\cline{2-8} 
comm. & allXY &125,280&2,088  &1,704  & 6,439  &2,982 & 265,636\\
%\cline{2-8}
      & alltoall &87,960  &1,466  &4,944  & 4,398 &8,652 & 1,027,818\\
\hline
      & $d=1$  &888,240  &14,804  &1,920 &44,412 &3,360 & 141,052\\
Single--hop & $d=4$&357,600 &5,960  &2,208 &17,880 & 3,864&181,506 \\
comm &        $d=14$ &267,360  &4,456  &3,504 &13,368 &6,132 &372,678 \\
  &          $d=20$ &273,360 &4,556  &4,224 &13,668 &7,392 &484,374 \\
\hline
\end{tabular}

\vspace{0.5in}

\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0013  & 0014 & 0022& 0025 & 0039 & 0041\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &166,280 &63,400 &3,244,819 &29,854 &
68,704 &1,504  \\
\hline
     & torus &257,980  &129,800 &6,382,683  &25,470  & 106,525 &6,592 \\
%\cline{2-8}
Multihop & hypercube &363,340  &200,600  &9,509,070 &58,661  & 132,348 &5,328 \\
%\cline{2-8} 
comm. & allXY &748,220 &379,200  &5,922,920  & 63,264  &135,353 &4,176 \\
%\cline{2-8}
      & alltoall &3,368,600  &1,679,200  &6,379,275  &214,343  &393,166 &2,932 \\
\hline
      & $d=1$  &154,080  &71,200  &6,844,054 &23,440 &115,488 &29,608 \\
Single--hop & $d=4$&256,240 &125,200  &6,402,631 &31,221 &136,390 &11,920 \\
comm &        $d=14$ &779,920  &391,200  &6,516,485 &61,712 &214,042 &8,912 \\
  &          $d=20$ &1,086,160 &550,800  &6,925,278 &81,958 &261,832 &9,112 \\
\hline
\end{tabular}

\end{center}
\normalsize
\caption{Communication time for the HPF benchmarks.}
\label{HPFperformance}
\end{table} 


\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0001  & 0003 & 0004& 0008 & 0009 & 0011\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 1.0 &1.0  &1.3 &1.0 &1.3 & 1.0 \\
\hline
     & torus &4.3  &4.4  &1.7  &4.4  &1.7 & 1.5\\
%\cline{2-8}
Multihop & hypercube &3.5  &3.5  &1.0 &3.5  &1.0 & 1.4\\
%\cline{2-8} 
comm. & allXY &2.7&2.3  &1.7  & 2.9  &1.7 & 2.5\\
%\cline{2-8}
      & alltoall &1.9  &1.9  &4.8  & 2.0 &4.8 & 9.8\\
\hline
      & $d=1$  &19.3  &19.7  &1.9 &19.7 &1.9 & 1.3\\
Single--hop & $d=4$&7.8 &7.9  &2.1 &7.9 & 2.1&1.7 \\
comm &        $d=14$ &5.8  &5.9 &3.4 &5.9 &3.4 &3.5 \\
  &          $d=20$ &5.9 &6.0  &4.1 &6.0 &4.1 &4.6 \\
\hline
\end{tabular}

\vspace{0.5in}

\begin{tabular}{|c|c|r|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0013  & 0014 & 0022& 0025 & 0039 & 0041 &average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1.1 &1.0 &1.0 &1.3 &1.0 &1.0&1.1  \\
\hline
     & torus &1.7  &2.1 &2.0  &1.1  & 1.6 & 4.4 & 2.6 \\
%\cline{2-8}
Multihop & hypercube &2.4  &3.2  &2.9 &2.5  & 1.9 &3.5 & 2.5 \\
%\cline{2-8} 
comm. & allXY &4.9 &6.0  &1.8  & 2.7  &2.0 &2.8 & 2.8 \\
%\cline{2-}
      & alltoall &21.9  &26.7  &2.0  &9.1  &5.7 &1.9 & 7.6 \\
\hline
      & $d=1$  &1.0  &1.1  &2.1 &1.0 &1.7 &19.7 & 7.5 \\
Single--hop & $d=4$&1.7 &1.9  &2.0 &1.3 &2.0 &7.9 & 3.9 \\
comm &        $d=14$ &5.1  &6.2  &2.0 &2.7 &3.1 &5.9 & 4.4 \\
  &          $d=20$ &7.1 &8.7  &2.1 &3.6 &3.8 &6.1 & 5.2 \\
\hline
\end{tabular}

\end{center}
\normalsize
\caption{Normalized time for the HPF benchmarks.}
\label{HPFperformanceN}
\end{table} 

\section{Programs from SPEC95}

Four programs, ARTDIF (from HYDRO2D), TOMCATV, SWIM and 
ERHS (from APPLU) are used in this experiment.
These programs are also used in Section~\ref{evalanalyzer}, where
the descriptions of these programs can be found,  to
evaluate performance of the communication analyzer 
in the E--SUIF compiler.

%This set of programs includes 4 programs. 
%The first benchmark, ARTDIF, 
%is a kernel routine obtained from the HYDRO2D program, 
%which is an astrophysical program for the computation of galactical jets
%using hydrodynamical Navier Stokes equations. 
%The second benchmark, TOMCATV, does the mesh generation with 
%Thompson's solver. 
%The third program, SWIM, 
%is the SHALLOW weather prediction program.
%The fourth program, ERHS, is part of the
%APPLU program, which is the solver for five coupled 
%parabolic/elliptic partial differential equations. The programs, HYDRO2D,
%TOMCATV, SWIM and APPLU, are sequential programs 
%originally from the SPEC95 benchmark suite.
%The purpose of this experiment is to study the communication performance of
%the three communication mechanisms for 
%automatically parallelized programs. 



\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & ARTDIF  & TOMCATV & SWIM & ERHS\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1,224 &15,480 &2,708 &6,689 \\
\hline
     & torus &2,724  & 34,260 & 1,378  &4,380 \\
%\cline{2-8}
Multihop & hypercube &4,338 &57,240  &2,309 &6,482\\
%\cline{2-8} 
comm. & allXY &8,583  &108,900 &4,409 & 15,117\\
%\cline{2-8}
      & alltoall &38,772 &491,460  &19,169   &68,800\\
\hline
      & $d=1$  &666 &8,280  & 669 &  1,148\\
Single--hop & $d=4$&2,478  &31,260  &1,574  &4,382\\
comm &        $d=14$ &8,538 &108,060 &4,511  & 15,166\\
  &          $d=20$ &12,168 &154,140  &6,343  &21,614\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Communication time for SPEC95 benchmark programs.}
\label{SPEC95performance}
\end{table} 



\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & ARTDIF  & TOMCATV & SWIM & ERHS & average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1.8 &1.9 &4.0 &5.8 & 3.3 \\
\hline
     & torus &4.1  & 4.1 & 2.1  &3.8 & 3.5 \\
%\cline{2-9}
Multihop & hypercube &6.5 &6.9  &3.5 &5.6 & 5.6\\
%\cline{2-9} 
comm. & allXY &12.9  &13.1 &6.6 & 13.1& 11.4\\
%\cline{2-9}
      & alltoall &58.2 &59.2  &28.7   &59.9 & 51.5\\
\hline
      & $d=1$  &1.0 &1.0  & 1.0 &  1.0 & 1.0\\
Single--hop & $d=4$&3.7  &3.8  &2.3  &3.8 & 3.4\\
comm &        $d=14$ &12.8 &13.0 &6.7  & 13.2 & 11.4\\
  &          $d=20$ &18.2 &18.6  &9.4  &18.8 & 16.3\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Normalized communication time for SPEC95 benchmark programs.}
\label{SPEC95performanceN}
\end{table} 


Table~\ref{SPEC95performance} shows the communication performance of the
programs. Table~\ref{SPEC95performanceN} shows the normalized time.
The test inputs are used as the inputs to these program, which
determine the problem size. To reduce the simulation time, the main
iteration numbers in programs ARTDIF, SWIM and ERHS are reduced to one.
All programs ARTDIF, SWIM, TOMCATV and ERHS  only contains simple 
nearest neighbor communication patterns. Compiled communication performs
worse than dynamic single--hop communication with a multiplexing degree
of one because it  aggregates communications and uses larger
multiplexing degree than needed. 
Hence, it is desirable to develop more advanced communication phase analysis 
techniques that can use different multiplexing degrees for different parts 
of a program to achieve  best performance. 
However, considering all the programs evaluated,
compiled communication out--performs other schemes to a large degree
as shown in Table~\ref{overallN}.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Comm.    & Comp.  & \multicolumn{4}{c|}{Multi--hop} & 
\multicolumn{4}{c|}{Single--hop}\\
\cline{3-10}
schemes &  comm. &   torus & hype. & allXY & alltoall & $1$ &
$4$ & $14$ & $20$ \\
\hline
Norm. time & 1.5 & 3.0 & 3.3 & 4.6 & 16.1 & 6.6 & 4.0 & 5.9 & 7.4 \\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Average normalized communication time for each scheme.}
\label{overallN}
\end{table} 

\section{Chapter summary}

This chapter studied the communication performance for the three
communication mechanisms, dynamic single--hop communication,
dynamic multi--hop communication and compiled communication using
three sets of programs. The following conclusions were drawn from the
study.

\begin{itemize}

\item %Even after taking into considerations the limitations of 
%compiler analysis, 
Compiled communication 
out--performs dynamic  communications
to a large degree for applications with regular computations.

\item The performance of compiled communication can be further 
improved by incorporating 
more advanced communication phase analysis techniques that allow 
different multiplexing degrees in different parts of a program.

\item The major disadvantage of dynamic communications is that they cannot
adapt to different communication requirements. Thus, they support
some communication patterns efficiently while they are inefficient for
other communication patterns. 
Compiled communication efficiently 
supports all types of communication patterns that 
can be determined at compile time.
 
\item Comparing dynamic multi--hop communication 
and dynamic single--hop communication,
dynamic multi--hop communication achieves better performance when the
message size is small and when the communication is dense, while  dynamic
single--hop communication is better when the message size is large.
This result matches the results in Section~\ref{multisingle} where
dynamic single--hop communication is compared with dynamic
multi--hop communication using randomly generated communication patterns.  

\end{itemize}
  
\chapter{Conclusion}

While optical interconnection networks have the potential to provide very large
bandwidth, network control, which is performed in the electronic domain 
due to the lack of suitable photonic logic devices,
has become the communication bottleneck in such networks. In order
to design efficient optical networks where end users can utilize the 
large bandwidth, efficient network control mechanisms must be developed
to reduce the control overheads. This thesis addresses the 
network control bottleneck problem in optical networks by considering three
communication schemes, {\em dynamic single--hop communication}, {\em dynamic
multi--hop communication} and {\em compiled communication}. In addition
to developing techniques to improve communication performance in each scheme, 
this thesis also compares the communication performance
of the three schemes and identifies the advantages and the limitations of each
scheme.  In the following sections, 
the thesis contributions are summarized and directions for future research
are identified.


\section{Thesis contributions}

This thesis makes contributions in the {\em design of control mechanisms}
for time--multiplexed 
optical interconnection networks. The contributions are in two areas:
optical interconnection networks and compiler analysis techniques. In the 
optical interconnection
networks area, this thesis introduces efficient control schemes 
for dynamic single--hop communication and dynamic multi--hop communication. 
This thesis also 
proposes and validates the idea of applying the 
compiled communication technique 
to optical TDM  networks.  In the compiler area, 
this thesis addresses all the issues needed
to apply the compiled communication paradigm 
to optical interconnection networks,
including communication optimization, 
communication analysis, connection scheduling and 
communication phase analysis. 
The main contributions of the thesis are detailed as follows.

\begin{itemize}
\item {\bf Dynamic single--hop communication}. Two sets of efficient 
path reservation algorithms, 
{\em forward path reservation protocols} and  {\em backward 
path reservation protocols}, are designed. Variants of the protocols, 
including holding/dropping and aggressive/conservative schemes, 
are considered. The performance of the protocols and the impact
of system parameters on these protocols are evaluated. 
Forward path reservation protocols extend traditional path reservation
schemes for electronic networks and are simpler compared to  backward
path reservation protocols. However, these  
schemes suffer from either the over--locking problem for 
aggressive schemes or the low successful
reservation rate for  conservative schemes.  
Backward path reservation protocols overcome 
these problems by probing the network state before reserving channels. 
Performance study has established that in optical time--division 
multiplexing networks, backward path reservation protocols, 
though more complex than forward path reservation protocols, 
result in better communication performance when the corresponding 
system and protocol parameters are the same. It is also found that 
while some system or protocol parameters, such as the holding time for 
holding schemes, do not have a significant impact on the performance
of the protocols, other parameters, such as the aggressiveness of a protocol
and the speed of the control network, affect the performance drastically.
Similar techniques can be extended for the path reservation in 
WDM wide area networks \cite{Yuan96b,Yuan98b}.

\item {\bf Dynamic multi--hop communication}. 
Schemes for realizing 
four logical topologies, torus, hypercube, allXY and all--to--all, 
on top of the physical torus topologies are considered. 
Optimal and near optimal routing and channel assignment (RCA) schemes 
for realizing hypercube on array, ring, mesh and torus 
topologies are developed.
An analytical model for analyzing the maximum throughput and the 
average packet delay is developed and verified via simulation.
This model is used to study the performance of dynamic multi--hop 
communication using the four logical topologies. It is found that
in terms of the maximum throughput, the logical all--to--all topology
is the best while the logical torus topology has the lowest performance.
In terms of the average packet delay, the logical torus topology achieves best 
results only when the router is fast and the network is under light load,
while the logical all--to--all topology is best only when the router
is slow and the network is almost saturated. In all other cases, logical
hypercube and allXY topologies out--perform logical torus and 
all--to--all topologies. In addition, 
the impact of system parameters, such as the packet switching time, 
on these topologies are studied. In general,  the performance of the logical
topologies with low connectivity, such as the torus and 
hypercube topologies, are more sensitive to the network load 
and the router speed while the logical topologies with more connectivity, 
such as the all--to--all and allXY topologies, are more sensitive to 
network size. Some of the techniques developed for multi--hop 
communication in optical TDM networks can be applied to other areas. 
The optimal scheme to realize hypercube on mesh--like
topologies can be used to 
efficiently perform communications in algorithms that contain
hypercube communication patterns \cite{Leighton92}. 
The modeling technique can be extended to the modeling of WDM networks or
electronic networks that perform multi--hop communication.

\item {\bf Compiled communication}. 
This thesis considers all the issues necessary
to apply compiled communication
to optical TDM networks, including communication optimization,
communication analysis, connection scheduling and communication phase
analysis.

\begin{itemize}
\item {\it Communication optimization and communication analysis}. 
A communication descriptor called 
{\em Section Communication Descriptor} (SCD) that describes 
communications on virtual processor grids is developed. A 
communication analyzer which performs a number of communication 
optimizations, including message vectorization, redundant communication 
elimination and message scheduling, is presented. All the optimizations
use a demand driven global array data flow analysis framework. This framework
improves previous data flow analysis algorithms for 
communication optimizations by reducing analysis cost and increasing 
analysis precision. 
Algorithms are developed to derive communications on physical processors
from SCDs. 
These algorithms address the problem of effective approximations
in the cases when the information in a SCD is insufficient for deriving
precise communication on physical processors. 
The communication optimization technique is general
and can be implemented in a compiler that compiles HPF--like 
programs for distributed memory machines.   
The communication analysis technique can be used by a compiler 
that requires the knowledge of the communication requirement of a program
on physical processors. 

\item {\it Connection scheduling}. A number of heuristic connection scheduling
algorithms are developed to schedule connections on torus topologies. Some
of the algorithms can also be applied to other topologies. 

\item {\it Communication phase analysis}. 
A communication phase analysis algorithm
is designed to partition a program into phases such that each phase contains
communications that can be supported by the underlying network, while 
capturing the communication locality in the program 
to reduce the reconfiguration overheads. 
This algorithm can also be applied to compiled communication 
on electronic networks. 

\end{itemize}

\item{\bf Communication performance comparison}.
A number of benchmarks and real application programs, including 
hand--coded parallel programs, HPF kernel benchmarks and 
programs from SPEC95, are used to  compare the communication 
performance of the three communication schemes. The relative strengths and
weaknesses of the three schemes are evaluated. 
%Following conclusions are drawn. 
The study establishes that even
with the limitations of compiler analysis, compiler communication
generally out--performs dynamic communications.
%The major advantage of compiled communication is its ability to
%adapt to the communication requirement during different phases of a 
%program. 
It delivers high communication performance for all types 
of communication patterns that are known at compile time.
The dynamic single--hop communication and dynamic multi--hop 
communication both suffers from the 
inability to adapt to the communication requirement. Given
a fixed system setting, they provide good performance for some
communication patterns while fail to achieve high performance for other
communication patterns. Comparing these two communication schemes, 
multi--hop communication has the advantage when the message size is small
and when the communication requires dense connections, 
while single--hop communication
has the advantage when the message size is large. 
\end{itemize}

\section{Future research}

The research of this thesis can be extended in various ways. Some of the
algorithms  can be improved. Additional 
work may either extend the applicability of the techniques or improve
the techniques. Following are a number of future research directions that
are related to this thesis.

\begin{itemize}

\item {\bf Improving backward path reservation algorithms}.   
In the backward reservation, once a channel is reserved,
the reservation fails only when the network state changes. 
Due to the distributed manner of collecting channel states and 
reserving channels in backward path reservation algorithms,
the information for channels on links close to the source node is not as
accurate as the information for channels on links close to the 
destination node.
This problem can be severe when the network size is large. Two 
possible solutions to this problem are as follows. 
First,  a more efficient control network can be used 
to route control messages.
For example,  a Multistage Interconnection Network (MIN) with multi--cast
capability can be used to route control messages so that 
control messages can reach all nodes along the path at the same time.
This allows a protocol to collect the channel usage information 
more efficiently and increases the chance of successful reservation.
Second, assuming that the control network has the same topology as the 
data network,
the backward path reservation protocols can selectively collect the 
channel usage information. The idea behind this improvement
is that wrong information may be worse than no information. 
    
\item {\bf Path reservation with adaptive routing}. In the thesis, path
reservation algorithms assume a deterministic routing.
Preliminary research on extending the path reservation protocols 
with adaptive routing was carried out. The preliminary 
results show that using current
path reservation protocols (both forward and backward reservations), 
the adaptive routing yields lower maximum throughput on the physical torus 
topology for uniform communication traffics. Further research is 
needed to explain this phenomenon and to design path reservation
protocols that take advantage of adaptive routing.

\item {\bf Topologies for multi--hop communication}. In this thesis,
four logical topologies, torus, hypercube, allXY and alltoall,
on top of the physical torus topologies are considered. 
There are two ways to extend this work. 
First, a different physical topology can be considered. For instance,  
it would be interesting to consider efficiently 
realizing regular topologies, such as mesh, torus, on top of an irregular 
topology. Second, there are logical topologies other than the four 
logical topologies considered that can achieve good communication 
performance. Examples include the tree and the shuffle--exchange topologies.

\item {\bf Interprocedural communication optimization}. 
The communication analyzer in the thesis performs
a number of communication optimizations, including message vectorization,
global redundant communication elimination and global message scheduling, 
using intraprocedural array data flow analysis. By incorporating the
interprocedural array data flow analysis, more optimization opportunities
can be uncovered. The intraprocedural array data flow analysis framework
uses interval analysis. It can naturally be extended to 
interprocedual analysis
by treating a procedure as an interval. However, many details,
such as array reshaping at subroutine boundaries and its impact on
communications, must be considered in order for the interprocedural 
analysis to work. 

\item {\bf Improving communication phase analysis}.
The communication phase analysis algorithm in the thesis follows simple 
heuristics, it considers the control structures in a program using
post--order traversal. This enables the algorithm to consider
communications in innermost loops first, 
aggregate the communications out of loops to reduce
the reconfiguration overhead and capture the communication locality. 
However, while the algorithm is simple to implement, the phases it generates
are not optimal in the sense that there may exist other program partitioning
schemes that result in less phases in a program.
More advanced communication phase analysis algorithms based on 
better communication model \cite{Salisbury97}
may be developed by using a general control flow graph for program 
representation and by considering the communication requirement
of the whole procedure when generating phases. 

\item {\bf Compact communication descriptor}.
The communication descriptor in the compiler 
that describes communication patterns
on physical processors is a flat structure. It contains all pairs
of source and destination nodes. This descriptor is both large and hard
to manipulate. More compact communication descriptor is desirable for
the compiler. The challenge however, is that 
the descriptor must both be compact and easy to use by the 
analysis algorithms. 

\item {\bf Irregular communication patterns}

Many scientific codes contain irregular communication patterns that
can only be determined at runtime. This thesis has restricted the
compiled communication technique to be applied to the programs
that contain only regular computations. This restriction can be
relaxed by using a strategy similar to the Chaos runtime 
library\cite{Sussman92}.
This library performs an inspector phase that calculates the runtime
schedule once for many executions of the communication pattern. Similarly
the connection scheduling algorithms can gather communication 
information at runtime and assign channels to all connections 
within the next looping structure to be used for subsequent iterations.
  
\end{itemize}

\section{Impact of this research}

This thesis establishes that the compiled communication technique
is more efficient than both dynamic single--hop communication and 
dynamic multi--hop communication. The compiler algorithms that enable
the application of compiled communication on optical TDM networks, though
can be further improved, are available in this thesis. Although
the compiled communication technique can only apply to the communication 
patterns that are known at compile time, mechanisms that allow the compiler
to manage network resources so that compiled communication can be
supported must be incorporated in future optical TDM networks for 
multiprocessor systems to achieve high performance. Dynamic communication
schemes must be used to handle general communication in an optical TDM network.
Dynamic single--hop communication incurs large startup overhead and is thus
inefficient for small messages which occur frequently in parallel
applications. Dynamic multi--hop communication is efficient 
for small messages, however, it places electronic processing in the 
critical path of data transmission and cannot fully utilize the large
bandwidth in optical links when the optical data
transmission speed is significantly faster than the electronic 
processing speed. Hence, both schemes have their own advantages and the better
choice between these two schemes depends on the application programs and the
advances in optical networking technology.

This thesis develops techniques for efficient communication in optical TDM
networks. Many techniques developed  can be applied to other areas. The 
path reservation algorithms for dynamic single--hop communication can be
extended for WDM wide area networks. The efficient routing and channel
assignment algorithms for hypercube communication pattern 
can be used to efficiently perform communications in algorithms that contain
hypercube communication patterns. The modeling technique for multi--hop 
communication in optical TDM networks can be extended for WDM networks
and electronic networks with multi--hop communication. The communication 
optimization technique based on a demand driven data flow analysis technique
can be incorporated in a compiler that compiles a HPF--like language 
for distributed memory machines. The communication analysis technique can
be used by compilers that perform architectural dependent communication 
optimizations, or compiled communication on electronic networks.









\section{Preliminary work}
\label{prework}

This section describes the preliminary work that has been done toward 
achieving the goals described in Chapter 3. A number of 
distributed path reservation protocols for 
point--to--point optical TDM networks have been designed and 
 a network simulator that
simulates the performance of these protocols has been developed. I also
designed and implemented a demand driven communication optimization
data flow analyzer which performs message vectorization and redundant
communication elimination. The analyzer represents the  logical communication
patterns in programs in communication descriptors
 and can be used to generate physical communication patterns
for both optimized and non--optimized programs. In addition, connection
scheduling algorithms that can be used to schedule communication patterns
that are known at compile time are developed for the torus topology.

\subsection{Distributed path reservation}

\subsubsection{The protocols}
\label{protocol}

In order to support a distributed control mechanism for connection
establishment, it is  assumed that in addition to the optical data network,
there is a logical {\em shadow network} through which
all the control messages are communicated. 
The shadow network has the same physical topology as the data network.
The traffic on the shadow network consists of small control packets
and thus is much lighter than the traffic on the data network. 
The shadow network  operates in packet switching mode; routers at 
intermediate nodes examine the control packets and update local bookkeeping
information and switch states accordingly. 
The shadow network can be implemented as an 
electronic network or alternatively a virtual channel on the data network
can be reserved exclusively for exchanging control messages.
It is  also assumed that a node can send or receive messages through
different virtual channels simultaneously. 
%This is always true for a 
%TDM system. For WDM system, this implies that each nodes must
%have one sender and receiver for each wavelength it operates on. 
%%The protocols can be modified to 
%apply to the WDM system with single tunable
%sender and receiver in each node.

A path reservation protocol ensures that the path from a source node
to a destination node is reserved before the connection is used. A path 
includes the virtual channels on the links that form the connection, the
transmitter at the source node and the receiver at the destination node.
Reserving the transmitter and receiver is the same as reserving a
virtual channel on the link from a node to the switch attached to that
node.
Hence, only reservation of virtual channels on links forming a
connection with path multiplexing will be considered.
There are many options available with respect to different aspects of the 
 path reservation mechanisms, which are discussed next.

\begin{itemize}

%\noindent
%$\bullet$
\item {\em Forward reservation} versus {\em backward reservation}.
Locking mechanisms are needed by the distributed path reservation
protocols  to ensure the exclusive usage of a virtual channel for a connection.
This variation characterizes the timing at which
the protocols  perform the locking.
Under forward reservation, the virtual channels are locked 
by a control message that travels from
the source node to the destination node.
Under backward reservation, a control message travels to the
destination to probe the path, then virtual channels that are found to be
available are locked by another
control message which travels from the destination node to the source node.

%\noindent 
%$\bullet$
\item {\em Dropping} versus {\em holding}. This variation characterizes
the behavior of the protocol when it 
determines that a connection establishment does not progress.
Under the  dropping approach, once the protocol
determines that  the establishment of a connection is not progressing,
it releases the virtual channels  locked on the partially established
path and informs the source node that the reservation has failed.
Under the holding approach, when the protocol determines
that  the establishment of a connection is not progressing,
it keeps the virtual channels  on the partially established path locked for
some period of time, hoping that during this period, the reservation
will progress. If, after this timeout period, the reservation still does not
progress, the partial path is then released and the
source node is informed of the failure.
Dropping can be viewed as holding with holding time equal to 0.

%\noindent 
%$\bullet$
\item {\em Aggressive} reservation versus {\em conservative} reservation. This
variation characterizes the protocol's treatment of each reservation. Under
the aggressive reservation, the protocol tries to establish a connection
by locking as many virtual channels as possible during the reservation process.
Only one of the locked channels is then used for the connection, while the
others are released.
Under the  conservative reservation approach, the protocol
locks only one virtual channel during the reservation process.

\end{itemize}

\subsubsection*{Deadlock}

Deadlock in the control network can arise from two sources.
First, with limited number of buffers, a request loop can be formed within the
control network.
Second, deadlock can occur when a request is holding (locking)
virtual channels on some links while requesting other channels on other
links.
This second source of deadlock can be avoided by the dropping or holding mechanisms
described above.
Specifically, a request will give up all the locked channels if 
it does not progress within a certain timeout period.

Many deadlock avoidance or deadlock prevention techniques for 
packet switching networks proposed in the literature \cite{Dally87} 
can be used to deal with deadlock within the control network (the
first source of deadlock).
Moreover, the control network is under light traffic, and
each control message consists of only a single packet of small size 
(4 bytes). Hence, it is feasible to provide a large number of buffers in each 
router to reduce or eliminate the chance of deadlock.

\subsubsection*{States of virtual channels}

The control network router at each node maintains a state for each
virtual channel on links connected to the router. For forward reservation,
the control router maintains the states for the outgoing links, while
in backward reservation, the control router maintains the states
for the incoming links. As discussed later, 
this setting enables the router to have the information
needed for reserving virtual channels and updating the switch states.
A virtual channel, $V$, on link $L$, can be in one of the following states:

\begin{itemize}
\item $AVAIL$: indicates that the virtual channel $V$ on link $L$
is available and can be used to establish a new connection,
\item $LOCK$: 
indicates that $V$ is locked by some request in the process of establishing
a connection.
\item $BUSY$: indicates that $V$
is being used by some established connection to transmit data.
\end{itemize}

For a link, $L$, the set of virtual channels that are in the $AVAIL$ state is
denoted as $Avail(L)$. When a virtual channel, $V$, is not in $Avail(L)$,
an additional field, $CID$, is maintained to identify the connection request
locking  $V$, if $V$ is in the $LOCK$ state, or the connection using $V$, if $V$
is in the $BUSY$ state.

\subsubsection*{Forward reservation schemes}

In the connection establishment protocols,
each connection request is assigned a unique identifier, $id$, which
consists of the identifier of the source node and a serial number
issued by that node. 
Each control message related to the establishment of a connection carries its
$id$, which becomes the identifier of the connection when it is successfully
established. It is this $id$ that is maintained in the $CID$ field of
locked or busy virtual channels on links.
Four types of packets are used in the forward reservation
 protocols to establish a connection.

\begin{itemize}

%\noindent 
%$\bullet$
\item {\em Reservation packets} ($RES$), used to reserve virtual channels.
In addition to the connection $id$, a $RES$ packet contains a bit vector,
$cset$, of size equal to the number of virtual channels in each link.
The bit vector $cset$ is used to keep track of the set of virtual channels 
that can be used to satisfy the connection request carried by $RES$.
These virtual channels are locked
at intermediate nodes while the $RES$ message
progresses towards the destination node. The switch
states are also set to connect the locked channels on the input and output links.

%\noindent 
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$), used to inform source nodes of the
success of connection requests.
An $ACK$ packet contains a $channel$ field which indicates the virtual
channel selected for the connection.
As an $ACK$ packet travels from the destination to the source, it changes
the state of the virtual channel 
selected for the connection to $BUSY$, and unlocks
(changes from $LOCK$ to $AVAIL$)
all other virtual channels that were locked by the corresponding $RES$ packet.

%\noindent 
%$\bullet$
\item {\em Fail or Negative ack packets} ($FAIL/NACK$), used to inform source
nodes of the failure of connection requests. While traveling back to the source
node, a $FAIL/NACK$ packet unlocks all virtual channels that were locked by the
corresponding $RES$ packet.
 
%\noindent 
%$\bullet$
\item {\em Release packets} ($REL$),  used to release connections.
A $REL$ packet traveling from a source to a destination changes the
state of the virtual channel
reserved for that connection from $BUSY$ to $AVAIL$.

\end{itemize}

The protocols require that control packets from a destination, $d$, to a source, $s$,
follow the same paths (in opposite directions) as packets from $s$
to $d$.
The fields of a packet will be denoted by $packet.field$.
For example, $RES.id$ denotes the $id$ field of the $RES$ packet.

The forward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $RES$ packet with $RES.cset$ set to the
virtual channels that the node may use. This message is then routed to the
destination. When an intermediate node receives the $RES$ packet, 
it determines the next outgoing link, $L$, on the path to the destination, and
updates $RES.cset $ to $ RES.cset \cap Avail(L)$.
If the resulting $RES.cset$ is empty,
the connection cannot be established, and a $FAIL/NACK$ message is sent back to the
source node. The source node will retransmit the request after some
period of time.
This process of failed reservation is shown in Figure~\ref{FORWARD}(a). 
Note that if $Avail(L)$ is represented by a bit-vector, then
$RES.cset \cap Avail(L)$ is a bit-wise "$AND$" operation.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/forward.pstex,height=2.2in}}
\caption{Control messages in forward reservation}
\label{FORWARD}
\end{figure}

If the resulting $RES.cset$ is not empty, the router reserves all the 
virtual channels in $RES.cset$ on link $L$ by changing their states to $LOCK$
and updating $Avail(L)$.
The router will then set the switch state to connect the virtual channels in the
resulting $RES.cset$ of the corresponding incoming and outgoing links.
Maintaining the states of outgoing links is sufficient for these two
tasks.
The $RES$ message is then forwarded to the next node on the path to the destination.
This way,
as $RES$ approaches the destination, the 
path is reserved incrementally. Once $RES$ reaches the
destination with a non-empty $RES.cset$, the destination selects from 
$RES.cset$ a virtual channel to be used for the connection and informs
the source node that the channel is selected by sending an $ACK$ message 
with $ACK.channel$ set to the selected virtual channel.
The source can start sending data once it 
receives the $ACK$ packet. After all data is sent, the source
node sends a $REL$ packet to tear down the connection. This successful
reservation process is shown in Figure~\ref{FORWARD} (b). Note that although
in the algorithm described above, the switches are set during the processing
of the $RES$ packet, they can instead be set during the processing of
the $ACK$ packet.

\noindent
{\bf Holding}: The protocol described above can be modified to 
use the holding policy instead of the dropping policy.
Specifically, when an intermediate node
determines that the connection for a reservation cannot be established, 
that is when $RES.cset \cap Avail(L) = \phi$, the node buffers the $RES$ packet
for a limited period of time. If within
this  period, some virtual channels in the original $RES.cset$ become
available, the $RES$ packet can then continue its journey. Otherwise, 
the $FAIL/NACK$ packet is  sent back to the source.
Implementing the holding policy 
requires each node to maintain a holding queue and
to periodically check that queue to determine if any of the virtual channels 
has become available. In addition, some timing 
mechanism must be incorporated in the routers to timeout 
held control packets. This increases the hardware
and software complexities of the routers.

\noindent
{\bf Aggressiveness}: 
The aggressiveness
of the reservation is reflected in the size of the 
virtual channel set, $RES.cset$, initially chosen by the source node.
In the most aggressive scheme, the source node sets
$RES.cset$ to $\{0, ..., N-1\}$, where $N$ is the number of 
virtual channels in the system. This ensures that the reservation
will be successful if there exists an available virtual channel on the path.
On the other hand, 
the most conservative reservation assigns
$RES.cset$ to include only a single virtual channel. In this case, the
reservation can be successful only when the virtual channel chosen by the
source node is available in all the links on the path. Although 
the aggressive scheme seems to have advantage over the conservative scheme,
it results in excessive locking of the virtual channels in the system. Thus, in
heavily loaded networks, this is expected to decrease the overall throughput.
To obtain optimal performance, the aggressiveness of the protocol should be
chosen appropriately between the most aggressive and the most conservative extremes.

The retransmit time  is another protocol parameter.
In traditional non--multiplexed networks, the retransmit time
is typically chosen randomly from a range [0,MRT], where MRT
denotes some maximum retransmit time.
In such systems, MRT must be set to a reasonably
large value to avoid live-lock. However, this may increase the average
message latency time and decrease the throughput.
In a multiplexed network, the problem of live-lock only 
occurs in the most aggressive scheme (non--multiplexed circuit switching
networks can be considered as  having a multiplexing degree of 1 and 
using aggressive reservation). 
For less aggressive schemes, the
live-lock problem can be avoided by changing the virtual channels selected in
$RES.cset$ when $RES$ is retransmitted.
Hence, for these schemes, a small retransmit time can be used.

\subsubsection*{Backward reservation schemes}

In  the forward locking protocol, the initial decision concerning the 
virtual channels to be locked for a connection request is made in the 
source node without any information about network usage. The backward
reservation scheme tries to overcome this handicap by probing the network
before making the decision. In the backward reservation schemes,
a forward message is used to probe the availability of virtual channels.
After that,
the locking of virtual channels is performed by a backward message. 
The backward reservation scheme uses six types of control
packets, all of which carry the connection $id$, in addition to other
fields as discussed next:

\begin{itemize}
%\noindent
%$\bullet$
\item {\em Probe packets} ($PROB$) travel from sources to destinations 
gathering
information about virtual channel usage without locking any virtual channel.
A $PROB$ packet carries a bit vector, $init$,
to represent the set of virtual channels that are
available to establish the connection.

%\noindent
%$\bullet$
\item {\em Reservation packets} ($RES$) are similar to the $RES$ packets in the forward
scheme, except that they travel from destinations to sources, lock
virtual channels as they go through intermediate nodes, and set the
states of the switches accordingly.
A $RES$ packet contains a $cset$ field.

%\noindent
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$) are similar to $ACK$ packets in the forward
scheme except that they travel from sources to destinations.
An $ACK$ packet contains a $channel$ field.


%\noindent
%$\bullet$
\item {\em Fail packets} ($FAIL$) unlock the virtual channels locked by the
$RES$ packets in cases of failures to establish connections.

%\noindent
%$\bullet$
\item {\em Negative acknowledgment packets} ($NACK$) are
used to inform the source nodes of reservation failures.

%\noindent
%$\bullet$
\item {\em Release packets} ($REL$) are
used to release connections after the communication is completed.

\end{itemize}

Note that a $FAIL/NACK$ message in the forward scheme performs the functions
of both a $FAIL$ message and a $NACK$ message in the backward scheme. 

The backward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $PROB$ message with $PROB.init$ set to contain all
virtual channels in the system.
This message is then routed to the destination.
When an intermediate node receives the $PROB$ packet, 
it determines the next outgoing link, $L_f$, on the forward path to the
destination,  and updates $PROB.init $ to $PROB.init \cap Avail(L_f)$.
If the resulting $PROB.init$ is empty,
the connection cannot be established and a $NACK$ packet is sent back to the
source node.  The source node will try the reservation again after a certain 
retransmit time.
Figure~\ref{BACKWARD}(a) shows this failed reservation case.

If the resulting $PROB.init$ is not empty, the node 
forwards $PROB$ on $L_f$ to the next node. 
This way,
as $PROB$ approaches the destination, the virtual channels available
on the path are recorded in the $init$ set.
Once $PROB$ reaches the
destination,  the destination forms a $RES$ message with $RES.cset$
equal to a selected subset of $PROB.init$ and sends this message back
to the source node.
When an intermediate node receives the $RES$ packet, it determines the
next link, $L_b$, on the backward path to the source, and updates
$RES.cset $ to $RES.cset \cap Avail(L_b)$. 
If the resulting $RES.cset$ is empty, 
the connection cannot be established. In this case the node sends
a $NACK$ message to the source node to inform it of the failure,
and sends a $FAIL$ message to the 
destination to free the virtual channels locked
by $RES$. This process is shown in Figure~\ref{BACKWARD}(b).

\begin{figure}[htp]
\centerline{\psfig{figure=fig/back.pstex,height=2.2in}}
\caption{Control messages in backward reservation}
\label{BACKWARD}
\end{figure}

If the resulting $RES.cset$ is not empty,
the virtual channels in $RES.cset$ are locked, the switch is set accordingly
and $RES$ is forwarded on $L_b$
to the next node.  When $RES$ reaches the source with a non-empty
$RES.cset$,
the source  selects a
virtual channel from the $RES.cset$ for the connection and sends
an $ACK$ message to the destination with $ACK.channel$ set to the
selected virtual channel. This $ACK$ message unlocks all the virtual channels 
locked by $RES$, except the one in $channel$.
The source node can start sending data as soon as it sends the $ACK$ message.
After all data is sent, the source
node sends a $REL$ packet to tear down the connection.
The process of successful reservation is shown in Figure~\ref{BACKWARD}(c).

\noindent
{\bf Holding}: Holding can be incorporated in the backward reservation scheme
as follows.
In the protocol, there are two cases that cause the reservation to fail. 
The protocol may determine that the reservation fails when processing
the $PROB$ packet. In this case, no holding is necessary since 
no resources have yet been locked.
When the protocol determines that the 
reservation fails during the  processing of a
$RES$ packet, a holding mechanism
similar to the one used in the forward reservation scheme may be applied.

\noindent
{\bf Aggressiveness}:
The aggressiveness of the backward reservation protocols is reflected in the 
initial size of $cset$ chosen by the destination node.
The aggressive approach sets
$RES.cset$ equal to $PROB.init$, while the conservative
approach sets $RES.cset$ to contain a single virtual channel from $PROB.init$.
Note that if a protocol supports only the conservative scheme,
the $ACK$ messages may be omitted, and thus only five types of messages 
are needed. 
As in the forward reservation schemes, the 
retransmit time is a parameter in the backward schemes.

\subsubsection{A preliminary network simulator \& Experimental evaluation}
\label{simulator}

A preliminary network simulator has been developed to simulate the behavious
of multiplexed torus networks. The simulator models the network with 
various choices of system parameters and protocols. Specifically, 
the simulator provides the following options for protocol parameters.

\begin{itemize}
\item {\em forward and backward} reservations, this determines which
protocol to be simulated.

\item {\em initial $cset$ size}: This parameter determines the
initial size of $cset$ in the reservation packet. 
It restricts the set of virtual channels under
consideration for a reservation. For FD and FH, 
the initial $cset$ is chosen when the source node composes the RES packet.
Assuming that $N$ is the multiplexing degree in the system,
an $RES.cset$ of size $s$ is chosen by generating a random number,
$m$, in the range $[0,$N$ - 1]$, 
and assigning $RES.cset$ = $\{m\ mod\ N, m+1\ mod\ N..., N+s-1\ mod N\}$.
In the backward schemes, the initial $cset$ is set when
the destination node composes the $ACK$ packet. An $ACK.cset$ of size $s$ 
is generated in the following manner.
If the available set, $RES.INIT$,
has less available channels than $s$, the $RES.INIT$ is copied to $ACK.cset$.
Otherwise,  the available channels are represented in a linear
array and the method used in generating the $cset$  in the forward schemes
is used.

\item {\em timeout value}: This  value 
determines how long a reservation packet can be put in a waiting queue.
The dropping scheme can be considered as a holding scheme with timeout time
equal to 0.

\item {\em maximum retransmit time} (MTR): 
This specifies the period after which a node will retry a
failed reservation. As discussed earlier,
this value is crucial for avoiding live-lock
in the most aggressive schemes. The actual retransmit time
is chosen randomly between 0 and  $MRT -1$.
\end{itemize}

Besides the protocol parameters, the simulator also allows the 
choices of various system parameters.

\begin{itemize}

\item {\em system size}: This specifies the size of the network. All our
simulations are done on torus topology.

\item {\em multiplexing degree}. 
This specifies the number of virtual
channels supported by each link. In our simulation, the multiplexing degree
ranges from 1 to 32.

\item {\em message size}: The message size directly affects  the time that
 a connection is kept before it is released.
In our simulations,  fixed size messages are assumed.

\item {\em request generation rate at each node (r)}: This specifies the traffic on
the network. The connection requests at each node is assumed to have a Poisson
inter-arrival distribution. When a request is
generated at a node, the destination of the request is generated randomly
among the other nodes in the system. When a generated request is blocked,
it is put into a queue, waiting to be re-transmitted.

\item {\em control packet processing and propagation time}: This specifies the speed of the
control networks. The control packet processing time is the time for an
intermediate node to process a control packet. The control packet
propagation time is the time for a control packet to be transferred from one node
to the next. It is assumed
 that all the control packets have the same
processing and propagation time.
\end{itemize}

In the following discussion, $F$ is used to denote forward reservation,
$B$ denotes the backward reservation, $H$ denotes 
holding and $D$ denotes dropping
schemes. For example, $FH$ means the forward holding scheme.
I have implemented a network simulator with various  control mechanisms 
including FH, FD, BH and BD.
Although the simulator can simulate both WDM and TDM torus networks, 
only the results for TDM networks will be presented in this paper.
The results for WDM networks follow similar patterns.
In addition to the options of backward/forward reservation and holding/dropping
policy, the simulation uses the following parameters.

The average latency and throughput are used to evaluate the protocols.
The latency is the period between the time when a message is ready and the time
when the first packet of the message is sent.
The  throughput is the number of messages received per time unit.
Under light traffic, 
the performance of the protocols is measured by the average message latency,
while under heavy traffic, the throughput 
is used as  the performance metric.  
The simulation time is measured in time slots, where a time slot is the
time to transmit an optical data packet between any two nodes in the network.
Note that in multiprocessing applications, nodes are physically close
to each other, and thus signal propagation time is very small (1 foot per
nsec) compared to the length of a message.
Finally, deterministic XY--routing is assumed in the torus topology.


\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Throughput]
  {\psfig{figure=eps/CMP1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CMP2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Comparison of the reservation schemes with dropping}
\label{DFMUL}
\end{figure}


Figure~\ref{DFMUL} depicts the throughput and average latency as a function of
the request generation rate for six
protocols that use the dropping policy in a $16\times 16$ torus.
The multiplexing degree is taken to be 32, the
message size is assumed to be 8 packets and the control packets
processing and propagation time is assumed to be 2 time units. 
For each of the forward and backward schemes, three variations are considered 
with varying aggressiveness.
The conservative variation in which the
initial $cset$ size is 1, the most aggressive variation in which
the initial set size is equal to the multiplexing degree and an optimal variation 
in which the initial set size is chosen (by repeated trials) to maximize the
throughput.
The letters  $C$, $A$ and $O$ are used to
denote these three variations, respectively.
For example, $FDO$ means the forward dropping scheme with optimal $cset$ size.
Note that the use of the optimal $cset$ size reduces the delay in addition to
increasing the throughput. Note also that the network saturates when
the generation rate is between 0.006 and 0.018, depending on the protocol
used. The maximum saturation rate that the $16\times 16$ torus can achieve in the
absence of contention and control overhead can be calculated from
\[
\frac{number\ of\ links}{no.\ of\ PEs \times av. \ no.\ of\ links\ per\ msg
\times msg\ size} = \frac{1024}{256\times 8 \times 8} = 0.0625
\]
Hence, the optimal backward protocol can achieve
almost 30\% of the theoretical full utilization rate. 

Figure~\ref{DFMUL}(b) also reveals that,
when the request generation rate, $r$, is small, for example $r = 0.003$, 
the network is under light traffic and 
all the protocols achieve the same throughput, which is equal to $r$ times
the number of processors.
In this case, the performance of the network should be measured by the
average latency.
In the rest of the performance study,
the maximum throughput (at saturation) and the average latency
(at $r = 0.003$) will be used to measure the performance of the protocols.
Two sets of experiments are performed. The first set 
evaluates the effect of the protocol parameters on the network throughput and
delay, and the second set
evaluates the impact of system parameters on performance.

\subsubsection*{Effect of protocol parameters}

In this set of experiments,
the effect of the initial $cset$ size, the holding time and the retransmit time on the 
performance of the protocols are studied. 
the system parameters for this set of experiment are chosen as follows:
System size = $16\times 16$,
message size = 8 packets, control packet processing and propagation time = 2 time
units.

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET3.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET4.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on forward schemes}
\label{CSETF}
\end{figure}
\begin{figure}[hbtp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on backward schemes}
\label{CSETB}
\end{figure}

Figure~\ref{CSETF} shows the effect of the initial $cset$ size on the forward
holding scheme with different multiplexing degrees, namely
1, 2,  4, 8, 16 and 32.
The holding time is taken to be 10 time units and the MTR is 5 time units
for all the protocols with initial $cset$ size less than the multiplexing degree
and 60 time units for the most aggressive forward scheme.
Large MTR is used in the most aggressive forward scheme because it is 
observed that small MTR often leads to live-lock in that scheme.
only the protocols with the holding policy will be shown since using the
dropping policy leads to similar patterns. The effect of holding/dropping will
be considered in a later figure.
Figure~\ref{CSETB} shows the results for the backward
schemes with the dropping policy.

From Figure~\ref{CSETF} (a), it can be seen that when the multiplexing 
degree is larger than 8, both the most
conservative protocol and the most aggressive protocol
do not achieve the best throughput. Figure~\ref{CSETF}(b) shows that these
two extreme protocols do not achieve the smallest latency either.
The same observation applies to the backward schemes in Figure~\ref{CSETB}.
The effect of choosing the optimal initial $cset$ is significant on both
throughput and delay. That effect, however, is more significant in the
forward scheme than in the backward scheme. For example, with multiplexing
degree = 32,
choosing a non-optimal $cset$ size may reduce the throughput by 50\%
in the forward scheme and only by 25\% in the backward scheme. 
In general, the optimal initial $cset$ size is hard to find.
Table~\ref{OPTCSET} lists the optimal initial $cset$ size for each multiplexing
degree.
A rule of thumb to approximate the optimal $cset$ size is to use 1/3 and 1/10 of the
multiplexing degree for forward schemes and backward schemes, respectively.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Multiplexing & \multicolumn{2}{|c|}{Optimal $cset$ size}\\
\cline{2-3}
Degree & Forward & Backward\\
\hline
4 & 1 & 1\\
\hline
8 & 2 & 1\\
\hline
16 & 5 & 2\\
\hline
32 & 10 & 3\\
\hline
\end{tabular}
\end{center}
\caption{Optimal $cset$ size}
\label{OPTCSET}
\end{table}

Figure~\ref{HOLDSET} shows the effect of the holding time on the performance of
the protocols for a multiplexing degree of 32. 
As shown in Figure~\ref{HOLDSET}(a), the holding time has little
effect on the maximum throughput. It slightly increases the performance for the
forward aggressive and the backward aggressive schemes. As for the 
average latency at light working load, the holding time also has little effect
except for the forward aggressive scheme, where the
latency time decreases by about 20\% when
the holding time at each intermediate node increases from 0 to 30 time units.
Since holding requires extra hardware support compared to
dropping, it is  concluded that holding is not 
cost--effective for the reservation protocols. In the rest of the paper,
only protocols with dropping policies will be considered.


\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/HOLDSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/HOLDSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of holding time}
\label{HOLDSET}
\end{figure}

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/RETRYSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/RETRYSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of maximum retransmit time}
\label{RETRYSET}
\end{figure}



Figure~\ref{RETRYSET} shows the effect of the maximum
retransmit time (MRT)
on the performance. Note that the retransmit time is uniformly distributed
in the range $0..MRT-1$. As shown in Figure~\ref{RETRYSET} (a),
increasing MRT results in performance degradation in
all the schemes except FDA, in which the performance improves
with the MRT. This confirms that the MRT value is important to 
avoid live-lock in the network when aggressive reservation is used.
In other schemes this parameter is not important, because when 
retransmitting a failed request, virtual channels different than the ones
that have been tried may be included in $cset$.
This result indicates another drawback of the 
forward aggressive schemes: in order to avoid live-lock, the MRT
must be a reasonably large value, which decreases the overall performance.

The results of the above set of experiments may be summarized as follows: 

\begin{itemize}

\item With proper protocols, multiplexing results in higher
maximum throughput. Multiplexed networks are significantly more efficient than
non--multiplexed networks.

\item Both the most aggressive and the most
conservative reservations cannot achieve optimal performance. 
However, the performance of the forward schemes is more sensitive to the
initial $cset$ size than the performance of the backward schemes.

\item The value
of the holding time in the holding schemes does not have significant
impact on the performance. In general, however, dropping is more efficient
than holding.

\item  The retransmit time 
has little impact on all the schemes except the FDA scheme.

\end{itemize} 

In the next section,
only  dropping schemes with
MRT equal to 5 time units for all schemes except FDA will be considered.
The MRT for FDA schemes is set to 60.

\subsubsection*{Effect of other system parameters}

This set of  experiments focuses on studying the performance of the 
protocols under different multiplexing degrees, system sizes, message sizes and 
control network speeds.
Only one parameter is changed in each experiment, with the other
parameters set to the following default values (unless stated otherwise):
network size = $16\times 16$ torus, multiplexing degree = 
16, message size = 8 packets, 
control packet processing and propagation time = 2 time units.

\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
  {\psfig{figure=eps/MUL1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/MUL2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{The performance of the protocols for different multiplexing degree}
\label{DBMUL}
\end{figure}


Figure~\ref{DBMUL}
shows the performance of  the protocols for different multiplexing degrees. 
When the multiplexing degree is small,  BDO and FDO
have the same maximum bandwidth as BDC and FDC, respectively. When 
the multiplexing degree is large, BDO and FDO offers better throughput.
In addition, for all multiplexing degrees, BDO is the best among
all the schemes. As for the average latency, both FDA and BDA have significantly
larger latency than all other schemes. Also, FDO and BDO have the smallest latencies.
It can be seen from this experiment that the backward
schemes always provide the same or better performance (both maximum
throughput and latency) than their forward reservation counterparts for all
multiplexing degrees considered.

Figure~\ref{SIZE} shows the effect of the network size on the performance of
the protocols.
It can be seen from the figure that all the protocols, except the aggressive
ones, scale nicely with the network size.
This indicates that the aggressive protocols cannot 
take advantage of the spatial diversity of the communication. This is 
a result of excessive reservation of channels. When the network size
is small, there is little big difference in the performance of the protocols.
When the network size is larger, the backward schemes show their superiority.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/SIZE1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
 {\psfig{figure=eps/SIZE2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the network size}
\label{SIZE}
\end{figure}


\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/MSIZE1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
{\psfig{figure=eps/MSIZE2.eps,height=2.2in}}
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the message size}
\label{MSIZE}
\end{figure}



Figure~\ref{MSIZE} shows the effect of the message size on the protocols.
The multiplexing degree  in this experiment is 16.
The throughput in this figure is normalized to reflect the
number of packets that pass through the network, rather than the number
of messages.  That is,\\
\centerline{$normalized\ throughput\ =\ msg\ size \times \ throughput$}
Both the forward and backward locking schemes achieve higher
throughput for larger messages. When messages are sufficiently large,
the signaling overhead in the protocols is small and
all protocols have almost the same performance. 
However, when the message size is small, the BDO scheme achieves 
higher throughput
than the other schemes. This indicates that BDO incurs less 
overhead in the path reservation than the other schemes. 

The effect of message size on the latency of the protocols is interesting.
Forward schemes incur larger latency when the message size is large. 
By blindly chosing initial cset, forward schemes
do not avoid chosing virtual channels used in communications, which
increases the latency when the message size is large (so that connections are
hold longer for communications).
Backward schemes probe
the network before chosing the initial csets. Hence, the latency in backward
shemes does not degrade as
much as in forward schemes when message size increases. 
Another observation is that in both forward and backward protocols, 
aggressive schemes sustain the increment of message size better
than the conservative schemes. This is also because of the longer
communication time with larger message size. Aggressive schemes are more 
efficient in finding a path in case of large message size. Note that 
this merit of aggressive schems is offsetted by the 
over reservations.
Another interesting point is that
the latency for messages of  size 1 results in higher latency than 
messages of size 8 in BDA scheme. This can be explained by the overly crowded
control messages in the network
in the case when data message contains a single packet (and
thus can be transmitted fast). The conflicts of control messages result in
larger latency.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
{\psfig{figure=eps/CNS1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CNS2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the speed of the control network}
\label{CNS}
\end{figure}

Figure~\ref{CNS} shows the effect of the control network speed on performance. 
The multiplexing degree  in this experiment is 32.
The speed of the
control network is determined by the
time for a control packet to be transferred from one node to the next node,
and the time for the control router to process the control packet. From the 
figure, it can be seen that, when the control speed is slower, the maximum
throughput and the average latency degrade.
The most aggressive schemes in both 
forward and backward reservations, however, are more sensitive to the
control network speed. Hence, it is important to have a reasonably fast 
control network when these reservation protocols are used. 

The results of the above set of experiments may be summarized as follows:

\begin{itemize}
\item The performance of FDA is significantly worse than other protocols. 
Moreover, this protocol cannot take advantage of both larger multiplexing 
degree and larger network size.
\item The backward reservation schemes provide better performance than
the forward reservation schemes for all multiplexing degrees. 
\item The difference of the protocols does not affect the communication efficiency
when the network size is small. However, for large networks, the
backward schemes provide better performance.
\item The backward schemes provide better performance when the message size
is small. When the message size is large, all the protocols have similar 
performance.
\item The speed of the control network affects the performance of the 
protocols greatly.
\end{itemize}

In this section, I have discussed distributed path reservation algorithms 
to establish connections with path multiplexing for communication
requests that arrive at the network dynamically.  In the next section, 
preliminary work on the communication analysis for compiled communication
will be presented.

\subsection{Communication analysis for compiled communication}
\label{commlab}

In order for the compiler to perform compiled communication, data structures 
must be designed for the compiler to represent the communication requirement
in the program. The data structure must both be powerful enough to represent 
the communication reqirement and easy to be obtained from the source program.
A communication descriptor which is an extension of the {\em array
section descriptor} \cite{callahan88} is designed. 
The descriptor describes the communication 
pattern in logical processor space. It can be used for both communication 
optimization and to derived the communication pattern in physical processor
space. In the following, the communication descriptor and 
how to calculate the communication descriptor from the source program will be 
described.


\subsubsection{Section communication descriptor (SCD)}

In this section,  
{\em Section Communication Descriptor} 
(SCD), which can be used 
to represent logical communication patterns in a program, will be introduced. 
The calculation of initial SCDs in programs will also be described.
Further information about SCD and its operations can be found in \cite{Yuan96}.

\subsubsection*{The descriptor}

The processor space is considered as an unbounded grid of virtual processors.
The abstract processor space is similar to a {\em template} in High
Performance Fortran (HPF) \cite{HPF}, which is a grid over which 
different arrays are aligned. In the rest of this section,  
{\em communication} means communication on the virtual processor grid.

The Section Communication Descriptor(SCD) is composed of three parts, 
(1) an array region which describes the parts of arrays that are involved 
in the communication, (2) a communication mapping which describes the
source and destination (in virtual processor grid) 
relationship in the communication and (3) a
communication qualifier which describes the iterations in which
the communication should 
be performed. More specifically, a SCD is defined as 
$<N, D, M, Q>$, where $N$ is an array name, $D$ is the source array region
of the communication, and $M$ is the descriptor of source-destination mapping,
and $Q$ is a descriptor that indicates in which iterations (in the interval)
the communication should be performed. 

The {\em bounded regular section descriptor} (BRSD)\cite{callahan88} is used
to describe the source region of communications. As discussed in 
\cite{callahan88}, set operations can be efficiently performed over BRSDs.
The source region
D is a vector of subscript values such that each of its elements is either
(1) an expression of the form $\alpha*i + \beta$, where i is a loop 
index variable and $\alpha$ and $\beta$ are invariants, (2) a triple 
$l:u:s$, where $l$, $u$ and $s$ are invariants, or (3) $\perp$, indicating
no information about the subscript value.   

The source-destination mapping $M$ is denoted as
$<source, destination, qual>$. The $source$
is a vector whose elements are of the form  
$\alpha*i + \beta$, where $i$ is a loop 
index variable and $\alpha$ and $\beta$ are invariants. The $destination$ 
is a vector whose elements are of the form 
$\sum_{j=1}^{n} \alpha_{j}*i_j + \beta_j$, where $i_j$'s are 
loop index variables  and $\alpha_j$'s and $\beta_j$'s are
invariants. The {\em mapping qualifier} list, 
$qual$, is a list of elements whose
format is $i = l:u:s$, where
$i$ is a variable, $l$, $u$ and $s$ are invariants. $l:u:s$  
denotes the ranges of the variable $i$. 
The mapping qualifier is used to describe the broadcast
effect, which may be needed during message vectorization.

The qualifier $Q$ is of the form $i = l:u:s$, where $i$ is the
induction variable of the interval. The notation
$Q=\perp$ is used to indicate that 
the communication is to be performed 
in every iteration.
$Q$ will be referred to as the SCD's {\em communication 
qualifier} in the rest of the paper. Including $Q$ in the descriptor
enables SCD to describe the communications that can be partially
vectorized. 

Initially, communications are required
before assignment statements with remote references.
It is assumed that owner computes rule is enforced. The owner
computes rule requires each item referenced on the {\em right hand side} (rhs)
 of an 
assignment statement to be sent to the processor that owns the {\em left hand
side} (lhs).
Before  the calculation of SCD is presented, I will first 
describe the calculation of  the ownership of array elements.


\begin{figure}[tbph]
%\begin{singlespace}
\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{11.5cm}  ALIGN (i, j) with VPROCS(i, j) :: x, y, 
                                      z\\
\hspace{11.5cm}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
\hspace{11.5cm}  ALIGN (i) with VPROCS(i, 1) :: a, b\\
\hspace{11.5cm}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
\hspace{11.5cm}(s2)\hspace{0.1in}\>b(i-1) = a(i)...\\
\hspace{11.5cm}(s3)\hspace{0.1in}\>a(i+2) = ...\\
\hspace{11.5cm}(s4)\hspace{0.1in}\>b(i) = a(i+1) ...\\
\hspace{11.5cm}(s5)\hspace{0.1in}\>do\=\ j = 1, 100\\
\hspace{11.5cm}(s6)\hspace{0.1in}\>\>x(i, j) = w (i, j)\\
\hspace{11.5cm}(s7)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s8)\hspace{0.1in}end do\\
\hspace{11.5cm}(s9)\hspace{0.1in}if (...) then\\
\hspace{11.5cm}(s10)\hspace{0.1in}\>do i = 1, 100\\
\hspace{11.5cm}(s11)\hspace{0.1in}\>\>do\=\ j = 50, 100\\
\hspace{11.5cm}(s12)\hspace{0.1in}\>\>\>x(i+j-1, 2*i+2*j-3) = w (i, j)\\
\hspace{11.5cm}(s13)\hspace{0.1in}\>\>\>y(i, j) = w(i, j)\\
\hspace{11.5cm}(s14)\hspace{0.1in}\>\>end do\\
\hspace{11.5cm}(s15)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s16)\hspace{0.1in}\>do i = 1, 100\\
\hspace{11.5cm}(s17)\hspace{0.1in}\>\>y(i, 150) = x(i+1, 150)\\
\hspace{11.5cm}(s18)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s19)\hspace{0.1in}end if\\
\hspace{11.5cm}(s20)\hspace{0.1in}do i = 1, 100\\
\hspace{11.5cm}(s21)\hspace{0.1in}\>b(i) = a(i+1)\\
\hspace{11.5cm}(s22)\hspace{0.1in}\>do j = 1, 200\\
\hspace{11.5cm}(s23)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
\hspace{11.5cm}(s24)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s25)\hspace{0.1in}\>w(i+1, 200) = ...\\
\hspace{11.5cm}(s26)\hspace{0.1in}end do\\

\end{tabbing}
\end{minipage}

\begin{minipage}{20cm}
\begin{picture}(0,0)%
\special{psfile=fig/2.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0038in}%
%\begin{picture}(1080,1070)(385,-240)
%\end{picture}

\end{minipage}
\normalsize
\caption{An example program and its interval flow graph}
\label{EXAMPLE}
\end{figure}


\subsubsection*{Ownership}

It is assumed that the arrays are all aligned to a single virtual space by 
simple affine functions. 
The alignments allowed are scaling, axis alignment and 
offset alignment.  The mapping from a point $\vec{d}$ in the data space to the 
corresponding point $\vec{v}$ in the virtual processor grid can be specified by
an alignment matrix $M$ and an alignment offset vector $\vec{\alpha}$. 
Thus, the ownership information of an element $\vec{d}$ can be calculated
using the formula $\vec{v} = M \vec{d} + \vec{\alpha}$. 
For example, consider the 
alignments of array $w$ and $a$ in the example program in 
Fig.~\ref{EXAMPLE}, the alignment matrices and offset vectors for 
array $w$ and $a$ are the following:
\begin{center}
\small
\footnotesize
\[ M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{\alpha}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   M_a  = \left(
       \begin{array}{c} 1 \\ 0 \end{array} \right),\
   \vec{\alpha}_a = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right).
\]
\end{center}

\subsubsection*{Initial SCD calculation}

Once the ownership information is provided, initial SCDs can be calculated
from the program structure. Let  $<N, D, M, Q>$ be an initial $SCD$, where
$N$ is the array to be communicated.
The array region 
$D$ contains a single element which is determined by the index expressions.
Since initially communication is always performed in 
every iteration, $Q = \perp$.  Let $M = <src, dst, qual>$. Since
initially communication does not perform broadcast, 
$qual = \perp$. Next I will 
describe how to calculate the source processor, $src$,
 and destination processor,
$dst$, for the mapping $M$ from the program structure.

Let $\vec{i}$ be the vector of loop indices. When the subscript
expressions are affine functions of the loop indices, the array 
references can be
expressed as $N(G\vec{i} + \vec{g})$, where $N$ is the array name, $G$ is a matrix and $\vec{g}$ is  a vector. 
I call G the {\em data access matrix}
and $\vec{g}$ the {\em access offset vector}.  The 
matrix, $G$, and the vector, $\vec{g}$, describe
a mapping from each
point in the iteration space to the corresponding point in the data space.
Let $G_l$, $\vec{g}_l$, $M_l$, $\vec{\alpha}_l$ be the data access matrix,
access offset vector, alignment matrix and alignment vector 
for the left
hand side array, and  
$G_r$, $\vec{g}_r$, $M_r$, $\vec{\alpha}_r$ be the corresponding quantities
for the right hand side array. The source processor $src$, which represents
the processor of the array element in $rhs$, and destination
processor $dst$, which represents the processor of the element in $lhs$
can be obtained from the  following equations.\\
\centerline{$src = M_r(G_r\vec{i} + \vec{g}_r) + \vec{\alpha}_r,$ \hspace{1in}
            $dst = M_l(G_l\vec{i} + \vec{g}_l) + \vec{\alpha}_l$}

Consider the communication in statement $s11$ for Fig.~\ref{EXAMPLE}.
The compiler can obtain from the program the following  data 
access matrices,
access offset vectors, alignment matrices and alignment vectors. 
\begin{center}
\small
\footnotesize
\[ M_x  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{\alpha}_x = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{\alpha}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
%\end{center}
%\begin{center}
\[ G_l  = \left(
       \begin{array}{c} 1 \\ 2 \end{array}
       \begin{array}{c} 1 \\ 2 \end{array} \right),\
   \vec{g}_l = \left(
       \begin{array}{c} -1 \\ -3 \end{array} \right),\
   G_r  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_r = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right)
\]
\end{center}
Thus, the initial SCD for statement $s12$ is \\
\centerline{$<N=w, D=<(i, j)>, M=<(2*j, i+1), (i+j-1, 2*i+2*j-3), 
            \perp>, Q= \perp>$}
As an indication of the complexity of a SCD, the structure for this 
communication required 712 bytes to store.


\subsubsection*{Operations on SCD}

Operations, such as intersection, difference and union, on the SCD descriptors
are needed in our analysis. Since the analysis can not guarantee exact
results of the operations, {\em subset} and {\em superset} versions of these
operations are implemented. 
Based on the purpose of the operation, the compiler uses the proper
version to obtain a conservative approximation. These operations,
the union ($\cup$), the intersection ($\cap$) and the difference ($-$), 
are straight-forward extension of the operations on BRSD \cite{callahan88}.
Details can be found in \cite{Yuan96}. 

%Here, we will describe an important
%operation: testing whether a communication $SCD_1=<N_1, R_1, M_1, Q_1>$ is 
%a sub--communication of another communication $SCD_2=<N_2, R_2, M_2, Q_2>$.
%
%\noindent
%{\bf Range testing operations} Ranges are denoted as $l:u:s$, where
%$l$ is the lower bound, $u$ is the upper bound and $s$ is the step.
%Range testing operations check the relation between two ranges. 
%Most commonly used range
%testing operations include the subrange testing, which test whether one
%range is a subrange of another range. This testing is reduced to the
%testing of the relation of the $l$, $s$ and $u$ in the two ranges.
%
%\noindent
%{\bf Subset Mapping testing}. 
%Testing that a relation $M_1$ ($= <s_1, d_1, q_1>$) is a subset of
%another relation $M_2$ ($= <s_2, d_2, q_2>$) is
%done by checking if the equations $s_1 = s_2$ and $d_1 = d_2$ and
%subrange testing $q_1 \subseteq q_2$. The equations 
%$s_1 = s_2$ and $d_1 = d_2$ can easily be solved by 
%treating variables in $M_1$ as constants and 
%variables in $M_2$ as variables. 
%Note that since the elements
%in $s_1$ and $s_2$ are of the form $\alpha*i+\beta$, the equations can
%generally be solved efficiently.
%
%\noindent
%{\bf SCD subset testing}\hspace{0.2in}
%$SCD_1 \subseteq SCD_2 \Longleftrightarrow N_1 = N_2 \wedge
%R_1 \subseteq R2 \wedge M_1 \subseteq M_2 \wedge Q_1 \subseteq Q_2$

\subsubsection{Array data flow analysis for communication optimization}
\label{arraydflow}


Many communication optimization opportunities can be uncovered by
propagating the SCD for a statement globally. For example, if a SCD
can be propagated from a loop body to the loop header without being killed
in the process of propagation, the communication represented by the SCD
can be hoisted out of the loop body, i.e. the communication can be vectorized.
Another example is the redundant communication elimination.
While propagating $SCD_1$,  if $SCD_2$ is encountered such that $SCD_2$ is 
a  subset of the $SCD_1$, then the communication represented by $SCD_2$ can
be subsumed by the communication represented by $SCD_1$ and can be eliminated.
Propagating  SCDs backward can find the latest point to place the 
communication, while propagating SCDs forward can find the last point where
the effect of the communication is destroyed. Both these
two propagations are useful in communication optimization. 
Since forward and backward propagation
are similar, I will only focus on 
backward propagation of SCDs.

I present generic demand driven algorithms to propagate
SCDs through the Interval flow graph representations of
programs. The analysis techniques are
the reverse of the interval-analysis \cite{gupta93}.
Specially, by reversing the information flow associated with program points,
I derive a system of request propagation rules.
The SCD descriptors are propagated 
until they cannot be propagated any further, 
i.e. all the elements in the SCD are
killed. However, in practice, the compiler may choose to 
terminate the propagation prematurely to
save analysis time while there are still elements in SCDs. 
In this case, since the analysis starts from the 
points that contribute to the  optimizations,
the points that are textually close to the starting points, where
most of the optimization oppurtunities present, have been considered.
This gives the demand driven algorithm the ability to trade precision for
time.
In the propagation, at a certain time, only a single interval
is under consideration. Hence, the propagations are logically done in
an acyclic flow graph. During the propagation, a 
SCD may expand when it is propagated out of a loop. When a set of
elements of SCD is killed inside a loop, the set is propagated into the loop
to determine the exact point where the elements are killed. There are 
two types of propagations,
{\em upward} propagation, in which SCDs may need to be 
expanded, and {\em downward} propagation, in which SCDs may need to be 
shrunk. 


The format of a data flow {\em propagation request}
is  $<S, n, [UP|DOWN], level, cnum>$, where S is a SCD, n is a node
in the flow graph, constants $UP$ and $DOWN$ indicate the request is  
upward propagation  or downward propagation, $level$ indicates
at which level is the request and the value $cnum$ 
indicates which child node of 
$n$ has triggered the request. A special value $-1$ for $cnum$ is used as
the indication of the beginning of downward propagation.
The propagation request triggers 
some local actions and causes the propagation of a SCD from the node n. 
The propagation of SCD follows the following rules. It is  assumed that node 
$n$ has $k$ children.

\subsubsection*{Propagation rules}

{\bf RULE 1: upward propagation: regular node}. 
The request on a regular node takes an action based
 on SCD set $S$ and the local
information. It also propagates the information upward.
The request stops when S become empty. The rule is shown in the following 
pseudo code. In the code, functions $action$ and $local$
are depended on the type of optimization to be performed.
The $pred$ function finds all the nodes that are  predecessors in the 
interval flow graph and the 
set $kill_n$ includes all the elements defined in node
$n$. Note that $kill_n$ can be represented as an SCD.

\begin{tabbing}
\hspace{1in}re\=quest($<S_1, n, UP, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, UP, level, k>$) : \\
\hspace{1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{1in}\>action(S, local(n))\\
\hspace{1in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - kill_n, m, UP, level, j>$)\\
\end{tabbing}
A response to requests in a node $n$ 
occurs only when all its successors have been processed. This 
guarantees that in a acyclic flow graph
each node will only be processed once. The side effect is that 
the propagation will not pass beyond a branch point.
A more aggressive scheme can 
propagate a request through a node without checking whether
all its successors are processed. In that scheme, however, a nodes may need 
to be processed multiple times to obtain the final solution.    

{\bf RULE 2: upward propagation: same level loop header node}.
The loop is contained in the current level. The request needs to obtain the
summary information, $K_n$, for the interval, perform the action
based on $S$ and the summary information, propagate the information passed 
the loop and trigger a downward propagation to propagate the information
into the loop nest. Here, 
the summary function $K_n$, summarizes all the elements defined
in the interval. 
It can  be calculated either before hand or in a 
demand driven manner. I describe how to 
calculate the summary in a demand driven manner later. 
Note that a loop header can only have one successor besides the 
entry edge into the loop body. The $cnum$ of the downward request
is set to -1 to indicate that it is the start of the downward propagation.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{1in}\>action(S, $K_n$) \\
\hspace{1in}\>if\=\ ($S - K_n \ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - K_n, m, UP, level, j>$)\\
\hspace{1in}\>if ($S\cap K_n \ne \phi$) then\\
\hspace{1in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$)\\
\end{tabbing}



{\bf RULE 3: upward propagation: lower level loop header node}.

The relative level between the propagation request
and the node can be determined by 
comparing the level in the request and the level of the node. Once a request
reaches the loop header. The request will need to be expanded to be 
propagated in the upper level. At the mean time, this request triggers 
a downward propagation for the set that must stay in the loops. 
Assume that the loop index variables is $i$ with bounds $low$ and $high$.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{1in}\>calculate the summary of loop $n$\\
\hspace{1in}\>outside = $expand(S, i, low:high) - 
                     \cup_{def}expand(def, i, low:high)$\\ 
\hspace{1in}\>inside = $expand(S, i, low:high) \cap 
                     \cup_{def}expand(def, i, low:high)$\\
\hspace{1in}\>if\=\ (outside $\ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<outside, m, UP, level -1, j>$)\\
\hspace{1in}\>if (inside $\ne \phi$) then\\
\hspace{1in}\>\>request($<inside,  n, DOWN, level, -1>$)\\
\end{tabbing}

The variable $outside$ stores the elements that can be propagated out of
the loop, while the variable $inside$ store the elements that are killed
within the loop. The expansion function has the same definition as in 
\cite{gupta93}. For a SCD descriptor S, expand(S, k, low:high) is a 
function which replaces
all single data item references $\alpha*k+\beta$ used in any
array section descriptor D in S by the triple ($\alpha*low+\beta:
\alpha*high+\beta:\alpha$). 
The sets $def$ includes all the definition that are the source of a
flow-dependence.

{\bf RULE 4: downward propagation: lower level loop header node}.

This is the initial downward propagation. The loops index variable, $i$, 
is treated as a constant in the downward propagation. 
Hence, SCDs that are propagated into the loop body 
must be changed to be the initial
available set for iteration $i$, that is, subtract all the variables 
killed in the iteration i+1 to high and propagate the information from the 
tail node  to the head node. This propagation prepares the downward 
propagation into the loop body by shrinking the SCD for each iteration.

\begin{tabbing}
\hspace{1in}qu\=ery($<S, n, UP, level,cnum>$): \\
\hspace{1in}\>if\=\ $(cnum = -1)$ then\\
\hspace{1in}\>\>calculate the summary of loop $n$;\\
\hspace{1in}\>\>request($<S - \cup_{def}expand(def, k, i+1:high), 
                    l, DOWN, level-1, 1>$);\\
\hspace{1in}\>else\\
\hspace{1in}\>\> STOP /* interval processed */\\
\end{tabbing}

{\bf RULE 5: downward propagation: regular node}.
For regular node, the downward propagation is the same as the upward
propagation.

\begin{tabbing}
\hspace{1in}re\=quest($<S_1, n, DOWN, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, DOWN, level, k>$) : \\
\hspace{1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{1in}\>action(S, local(n))\\
\hspace{1in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - kill_n, m, DOWN, level, j>$)\\
\end{tabbing}


{\bf RULE 6: downward propagation: same level loop header node}.
When downward propagation reaches a loop header(not the loop header
whose body is being processing), it must generate further downward
propagation request to go deeper into the body.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, DOWN, level, 1>$): \\
\hspace{1in}\>action(S, summary(n)); \\
\hspace{1in}\>if\=\ ($S-K_n \ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - K_n, m, DOWN, level, j>$);\\
\hspace{1in}\>if\=\ ($S\cap K_n \ne \phi$) then\\
\hspace{1in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$);\\
\end{tabbing}

\subsubsection*{Summary calculation}

\begin{figure}[htbp]
\begin{tabbing}
\hspace{1in}(1)\hspace{0.5in}Su\=mmary\_kill(n)\\
\hspace{1in}(2)\hspace{0.5in}\>$K_{out}(tail)$ = $\phi$\\
\hspace{1in}(3)\hspace{0.5in}\>fo\=r all $m\in T(n)$ and 
                               level(m) = level(n)-1 in backward order\\
\hspace{1in}(4)\hspace{0.5in}\>\>if\=\ m is a loop header then\\
\hspace{1in}(5)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                    K_{in}(s)$\\
\hspace{1in}(6)\hspace{0.5in}\>\>\>$K_{in}(m)$ = summary\_kill(m) $\cup
                                   K_{out}(m)$\\
\hspace{1in}(7)\hspace{0.5in}\>\>else\\ 
\hspace{1in}(8)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                   K_{in}(s)$\\
\hspace{1in}(9)\hspace{0.5in}\>\>\>$K_{in}(m)$ = $kill(m) \cup K_{out}(m)$\\
\hspace{1in}(10)\hspace{0.5in}\>return (expand($K_{in}(header)$, i, low:high))\\
\end{tabbing}
\caption{Demand driven summary calculation}
\label{KILL}
\end{figure}

During the request propagation, the summary information of an interval is
needed when a loop header is encountered. In this section, I describe
an algorithm to obtain the summary information in a demand driven manner.
I use the calculation of kill set of the interval as an example. Let 
$kill(i)$ 
be the variables killed in node $i$, $K_{in}$  and $K_{out}$ 
be the variables killed before and after node respectively.
Fig.~\ref{KILL} depicts the demand driven algorithm. The 
algorithm propagates the data flow information from the tail node to the 
header node in the interval using the following data flow equation:\\

\centerline{$K_{out}(n) = \cup_{s\in succ(n)}K_{in}(s)$}
\centerline{$K_{in}(n) = kill(n)\cup K_{out}(n)$}

When inner loop header is
encountered, a recursive call is issued to get the summary information
for the inner interval. Once loop header is reached, the kill set need
to be expanded to be used by the outer loop.

\subsubsection{Implementation}

The analyzer is implemented on top of the Stanford SUIF compiler.
Since I do not have a backend code generator, I am not able to  
evaluate the optimization results in terms of execution time. 
Instead, I developed a communication emulation system,
which takes SCD descriptors as input and emulates the 
communications described by the SCDs in a multiprocessor system. 
The virtual to physical processor mapping is provided
to the emulation system to emulate the communication in physical
processors.
The emulation system provides an interface with C program as  library 
calls whose arguments include all information in a SCD.
A compiler backend automatically generates a library call for each 
SCD remaining in the program. In this way, the communication performance
can be evaluated in the emulation system
by running the program generated by the compiler backend.

The generation of a program used for evaluation is carried out in
the following steps. 
(1) A sequential program is compiled using SUIF frontend, $scc$, 
to generate the SUIF intermediate representation. 
(2) SUIF transformer, $porky$, is used to perform scalar optimizations,
forward propagation, dead code elimination and induction variable
detection. (3) A communication preprocessing phase annotates the
global arrays with data alignment information. (4) The
analyzer is invoked to analyze and optimize the communications required
in the program. After communication optimization, the backend of the
analyzer inserts a library call into the SUIF intermediate representation
for each SCD remaining in the program.  (5) The $s2c$ tool
is used to convert the SUIF intermediate representation into C program, which
is the one used for evaluation.

Six programs are used in the experiments. The first benchmark, 
L18, is the explicit hydrodynamics kernel in livermore loops (loop 18).
The second benchmark, ARTDIF, is a kernel routine obtained from 
HYDRO2D program, 
which is an astrophysical program for the computation of galactical jets
using hydrodynamical Navier Stokes equations. 
The third benchmark, TOMCATV, does the mesh generation with 
Thompson's solver. 
The fourth program, SWIM, 
is the SHALLOW weather prediction program.
The Fifth program, MGRID, is the simple multigrid solver for
computing a three
dimensional potential field. This sixth program, ERHS, is part of the
APPLU program, which is the solver for five coupled 
parabolic/elliptic partial differential equations. The programs, HYDRO2D,
TOMCATV, SWIM, MGRID and APPLU, originally come from SPEC95 benchmark suite.

Table~\ref{analysis} shows  the analysis cost of the
analyzer.  The analyzer, which applies the algorithms on all SCDs in the
programs,  is run on SPARC 5 with 32MB memory. 
Row 2 of the table shows the size of the programs and Row 3 shows the 
number of initial SCDs in the programs.
Row 4 shows the memory space requirement of the analyzer in SCD units. 
The value in bracket
is the maximum number of SCDs in a single node during the analysis. 
In the analyzer the size of the SCDs ranged from 0.6 to about 2 kbytes.
This memory does not include the memory to store the initial and final data 
flow information. By repeatedly using 
the same memory for propagating different
SCDs,  the analyzer requires very small amount of extra memory. Row 5 shows the
cumulative memory requirement, which is the sum of the number of SCDs passing 
through each node. This number is equivalent to the memory requirement of
traditional data flow analysis.
The value in bracket
is the maximum cumulative SCDs in a node.
Row 6 are the ratio between numbers in
row 5 and row 4, which is the ratio of the memory
memory requirements of the traditional method and this method. 
On an average this method reduces the memory requirement
by a factor of 26.
Row 7 gives the raw analysis times and row 8 shows the rate at which
the analyzer operates in units of source $lines/sec$. On an average
the analyzer compiles 166 lines per second for the six programs. 
Row 9 shows the total time,
which includes analysis time and the time to load and store the
SUIF structure, for reference.
This experiment shows that the analyzer is efficient in space and time.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Program &            L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
size(lines) &         83  &    101 &     190 &  429 &   486 & 1104\\
%\hline
%\# of tree nodes   &  85  &    110 &     337 &  543 &   660 &  958\\
\hline
\# of initial SCDs &  35  &     12 &     108 &   76 &   125 &  403\\
\hline
 memory requirement &  15(1)  &  16(1) & 74(3) &   60(1) &    71(1) &  232 (5)\\
\hline 
accu. memory req.  & 348(39) &  175(24) & 5078(156) &  767(27) &  1166(60) & 
                                                                    6029(173)\\
\hline
memory size ratio  & 23.2 &    10.9&    68.6 & 12.8 &  16.4 & 26.0\\
\hline
analysis time(sec) & 0.60 &   0.27 &    3.57 & 1.82 &  4.40 & 23.25\\
\hline 
lines / sec        & 138  &   374  &      94 & 235  &  110  & 47\\
\hline
total time(sec)    & 1.83 &   1.50 &    7.26 & 6.58 & 12.87 & 37.95\\
\hline
\end{tabular}
\end{center}
\caption{Analysis time}
\label{analysis}
\end{table}

Fig.~\ref{size} shows the performance of redundant communication optimization
of the analyzer. The performance metric is the number of elements in the 
communication. The results are obtained under the assumption that
the programs are executed on a 16 PE system and all arrays use 
the CYCLIC distribution.
% and the PEs are distributed as 16 for 1 dimension arrays, 
%$4\times 4$ for two dimensional arrays, $4\times 2\times 2 $ for three 
%dimensional arrays and $2\times 2\times 2 \times 2 \times 2$ for 4 dimensional
%arrays. 
This experiment is conducted using the test input size provided by
the SPEC95 benchmark for program TOMCATV, SWIM, MGRID, ERHS. Problem 
sizes of $6\times 100$ for L18 and $402\times 160$ for ARTDIF are used.
Some programs, such as MGRID, do not have any
optimization opportunities for redundant communication optimization. 
Others, such as the TOMCATV program, present many opportunities 
for optimizations. On an average, for the six programs, 30.5\% of
the total communication elements are reduced by the analyzer. This indicates 
that global communication optimization results in large performance gain and
the analyzer is effective in finding optimization opportunities.
%The performance evaluation for message vectorization optimization
%will be included in the full paper.

\begin{figure}
\centerline{\psfig{figure=fig/size.eps,width=5in}}
\caption{The reduction of the number of array elements in communications}
\label{size}
\end{figure}

%% TO BE WRITE LATER

This section introduced a communication descriptor that can represent
logical communication patterns in a program, describes the propagation rules
used in an array data flow analyzer for communication optimization and reports
the experimence with the analyzer. This tool is able to generate logical 
communication patterns in both optimized and non--optimized codes for a
program. Algorithms to derive physical communiation patterns from the
logical communication descriptors remain to be developed.
In the next section,  connection
scheduling algorithms for handling static patterns in compiled communication
are presented.


\subsection{Connection scheduling algorithms for compiled communication}

Compiled communication uses off--line algorithms to perform
connection scheduling. This section presents the connection scheduling 
algorithms and their performance evaluation. These algorithms assume
a torus topology. However, the foundamental principles of the algorithms
can be extended to other topologies. 

For a given network, a set of connections that do not share any link is called
a configuration. In an optical TDM network with path multiplexing, 
multiple configurations can be supported simultaneously. Specifically, for 
a network with multiplexing degree $K$, $K$ configurations can be 
established concurrently. Thus, for a given communication pattern,
realizing the communication 
pattern with a minimum multiplexing degree is equivalent to  determining the
minimum number of configurations that contain all the connections in 
the pattern. Next,
 some definitions will be presented to formally state 
the problem of
connection scheduling. A connection request from a source $s$ 
to a destination $d$ is denoted as $(s, d)$.

\begin{description}
\item A pair of connection requests $(s_1, d_1)$ and $(s_2, d_2)$ are said
to {\bf conflict}, if they cannot be simultaneously established because
they are using the same link.

\item A {\bf configuration} is a set of connection requests
$\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ such that
no requests in the set conflict.

\item Given a set of communication requests  
$R = \{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$,
the set $MC =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} is a
{\bf minimal configuration set} for $R$ iff: \\
$\bullet$ 
each $C_i \in MC$ is a configuration and each request 
$(s_{i}, d_{i}) \in R$ 
is contained in exactly one configuration in $MC$; and \\
$\bullet$
each pair of configurations $C_i,C_j \in MC$ contain requests 
$(s_i, d_i) \in C_i$ and $(s_j, d_j) \in C_j$ such that
$(s_i, d_i)$ conflicts with $(s_j, d_j)$.
\end{description}

Thus, the goal of connection scheduling heuristics is to compute a
minimal configuration set for a given request set $R$.
Next I present three such heuristics.

%
\subsubsection{Greedy algorithm}
%
In the greedy algorithm, a configuration is created by repeatedly 
putting connections into the configuration until no additional 
connection can be established in that configuration.
If additional requests remain, another configuration is created
and this process is repeated till all requests have been processed.
This algorithm is an modification of an algorithm proposed in \cite{Qiao94}.
The algorithm is shown in Fig.~\ref{SIMPLE}. The time complexity of
the algorithm is $O(|R|\times max_i(|C_{i}|)\times K)$, where $|R|$ 
is the number of the requests, $|C_{i}|$ is the number of connections
in configuration $C_{i}$ and $K$ is the number of configurations
generated.

\begin{figure}[htmb]
%\small
\begin{tabbing}
\hspace{1.0in}(1)\hspace{0.6in}MC = $\phi$, k = 1\\
\hspace{1.0in}(2)\hspace{0.6in}{\bf re}\={\bf peat}\\
\hspace{1.0in}(3)\hspace{0.6in}\> $C_{k} = \phi$\\
\hspace{1.0in}(4)\hspace{0.6in}\>{\bf for}\= {\bf each} $(s_{i}, d_{i}) \in R$\\
\hspace{1.0in}(5)\hspace{0.6in}\>\>{\bf if}\=\ $(s_{i}, d_{i})$ does 
                                   not conflict with any 
                                   connection in $C_{k}$ {\bf then}\\
\hspace{1.0in}(6)\hspace{0.6in}\>\>\>$C_{k} = C_{k} \bigcup$ { $(s_{i}, d_{i})$ }\\
\hspace{1.0in}(7)\hspace{0.6in}\>\>\>R = R - { $(s_{i}, d_{i})$ }\\
\hspace{1.0in}(8)\hspace{0.6in}\>\>{\bf end if}\\
\hspace{1.0in}(9)\hspace{0.6in}\>{\bf end for}\\
\hspace{1.0in}(10)\hspace{0.6in}\>MC = MC $\bigcup$ { $C_{k}$ }\\
\hspace{1.0in}(11)\hspace{0.6in}{\bf until} $R = \phi$\\
\end{tabbing}
\normalsize
\caption{The greedy algorithm.}
\label{SIMPLE}
\end{figure}

For example consider the linearly connected nodes shown in Fig.~\ref{EXAM}. 
The result for applying the greedy algorithm to schedule connection requests 
set \{(0, 2), (1, 3),(3, 4), (2, 4)\} is shown in Fig.~\ref{EXAM}(a). 
In this case, (0, 2) will be in time slot 1, (1, 3) in time slot 2, (3, 4) 
in time slot 1 and (2, 4) in time slot 3. 
Therefore, multiplexing degree 3 is needed to establish the paths for the 
four connections.  However,  as shown in Fig.~\ref{EXAM} (b), 
the optimal scheduling for the four connections, which can be obtained
by considering the connection in different order, is to schedule (0, 2) in 
slot 1, (1, 3) in slot 2, (3, 4) in slot 2 and (2, 4) in slot 1. 
The second assignment only use 2 time slots to establish all the connections. 

\begin{figure}[htbp]
\begin{center}
\begin{picture}(0,0)%
\special{psfile=/afs/cs.pitt.edu/usr0/xyuan/research/paper/write/962SC96/fig/961.3.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0050in}%
\begin{picture}(920,120)(75,640)
\end{picture}

\end{center}
\caption{Scheduling requests (0, 2), (1, 3),(3, 4), (2, 4)}
\label{EXAM}
\end{figure}


\subsubsection{Coloring algorithm}

The greedy algorithm  processes the requests in an arbitrary order.
In this section, I will describe an algorithm that applies a heuristic 
to determine the order in which the process the connection requests.
The heuristic assigns higher priorities to connection requests with fewer
conflicts. By giving the requests with less conflicts higher priorities, 
each configuration is likely to accommodate more requests and thus the
multiplexing degree needed for the patterns is likely to decrease. 

The problem of computing the minimal configuration set is formalized
as a graph coloring problem. A coloring of a graph is an assignment of 
a color to each node of the graph in such a manner that no two nodes 
connected by an edge have the same color. A conflict graph for a set of
requests is built in the following manner, (1) 
each node in the graph 
corresponds to a connection request and (2) an edge
is introduced between two nodes if the requestes represented by the 
two nodes are conflicted.
As stated by the theorem given below,
the number of colors used to color the graph is the number of 
configurations needed to handle the connection requests. 

\begin{description}
\item
{\bf Theorem:} Let $R=\{(s_{1}, d_{1}),(s_{2}, d_{2}),...,(s_{m}, d_{m})\}$
be the set of requests and $G = (V, E)$ be the conflict graph for $R$. 
There exists a configuration set $M = \{C_{1}, C_{2}, ..., C_{t}\}$
for $R$ if and only if $G$ can be colored with $t$ colors.
\end{description}

%Prove: ($\Rightarrow$) Assuming R has 
%configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. Let 
%$(s_{i}$, $d_{i}) \in C_{k}$, node $n_{i}$ can be colored by color $k$. 
%Therefore, there are totally $t$ colors in the graph. Now, we need to prove
%that for any two node $n_{i}$, $n_{j}$ such that $(i, j) \in E$, the two nodes
%are colored by different color. By the construction of conflict graph,
%if $(i, j) \in E$, node $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ share same links,
%hence, by the construction of configuration, $(s_{i}, d_{i})$ and
%$(s_{j}, d_{j})$  is in different configuration, thus $n_{i}$ and $n_{j}$ is
%colored by different colors. Hence, G can be colored by $t$ colors.
%
%($\Leftarrow$) Assuming G can be colored by $t$ colors. Let 
%$C_{i}$ = {$( s_{j}, d_{j})$ : $n_{j}$ is colored by color j}, 
%$M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. To prove 
%M is a configuration for R, we need to prove 1) for any $(s_{i}, d_{i}) \in R$,
%there exists a $C_{k}$ such that $(s_{i}, d_{i}) \in C_{k}$, and 2) $C_{k}$
% must
%be a configuration. The first condition is trivial. Now, let us consider
%the second condition. Let $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ belong to 
%$C_{k}$, by the construction the G, $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$
%do not share any links. Therefore, $C_{k}$ is a configuration. Hence, there
%exist configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} for R. $\Box$
%
%\begin{description}
%\item
%{\bf Corollary:} The optimal multiplexing degree for establishing 
%connections in $R$ is equivalent to the minimum number colors to 
%color graph $G$.
%\end{description}


Thus, our 
coloring algorithm attempts to minimize the number of colors used in 
coloring the graph. Since the coloring problem is known to be NP-complete, 
a heuristic is used for graph coloring. Our heuristic determines the order 
in which nodes are colored using the node priorities.
The algorithm is summarized in Fig~\ref{COLOR}. It should be noted that
after a node is colored, our algorithm updates the priorities of uncolored 
nodes. This is because in computing the degree of an uncolored node, 
only  the edges that connect the node to other uncolored nodes are 
considered. 
The algorithm finds a solution in linear time (with respect to the 
size of the conflict graph). The time complexity of the algorithm is 
$O(|R|^2\times max_i(|C_{i}|)\times K)$, where $|R|$ is the number of the 
requests, $|C_{i}|$ is the number of requests in configuration $C_{i}$ and 
$K$ is the total number of configurations generated.


\begin{figure}[htbp]
%\small
\begin{tabbing}
\hspace{1.0in}(1)\hspace{0.6in} Construct conflict graph G = (V, E)\\
\hspace{1.0in}(2)\hspace{0.6in} Calculate the priority for each node\\
\hspace{1.0in}(3)\hspace{0.6in} MC = $\phi$, k = 1\\
\hspace{1.0in}(4)\hspace{0.6in} NCSET = V\\
\hspace{1.0in}(5)\hspace{0.6in} {\bf re}\={\bf peat}\\
\hspace{1.0in}(6)\hspace{0.6in} \>Sort NCSET by priority\\
\hspace{1.0in}(7)\hspace{0.6in} \> WORK = NCSET\\
\hspace{1.0in}(8)\hspace{0.6in} \> $C_{k} = \phi$\\
\hspace{1.0in}(9)\hspace{0.6in} \>{\bf wh}\={\bf ile} (WORK $\ne \phi$)\\
\hspace{1.0in}(10)\hspace{0.6in} \>\> Let $n_{f}$ be the first 
                                      element in WORK\\
\hspace{1.0in}(11)\hspace{0.6in} \>\>$C_{k} = C_{k} \bigcup \{<s_{f}, d_{f}>\}$\\
\hspace{1.0in}(12)\hspace{0.6in} \>\>NCSET = NCSET $- \{n_{f}\}$\\
\hspace{1.0in}(13)\hspace{0.6in} \>\>{\bf fo}\={\bf r} {\bf each}  $n_{i} \in NCSET$ 
                                      and $(f, i) \in E$ {\bf do} \\
\hspace{1.0in}(14)\hspace{0.6in} \>\>\> update the priority of $n_{i}$\\
\hspace{1.0in}(15)\hspace{0.6in} \>\>\> WORK = WORK - $\{n_{i}\}$\\
\hspace{1.0in}(16)\hspace{0.6in} \>\>{\bf end for}\\
\hspace{1.0in}(17)\hspace{0.6in} \>{\bf end while}\\
\hspace{1.0in}(18)\hspace{0.6in} \>MC = MC + $\{C_{k}\}$\\
\hspace{1.0in}(19)\hspace{0.6in} {\bf until} NCSET = $\phi$
\end{tabbing}
\normalsize
\caption{The graph coloring heuristic.}
\label{COLOR}
\end{figure}

For torus and mesh networks, a suitable choice for priority for a
connection request is the ratio of the number of links in the path 
from the source to the destination and the degree of the node 
corresponding to the request in $G$. 
Applying the coloring algorithm to the example in Fig.~\ref{EXAM},
in the first iteration, the request is reordered as 
$\{(0, 2), (1, 3), (2, 4), (3, 4)\}$ and connections (0, 2), (2, 4) will be
put in time slot 1. In the second iteration, connections (1, 3), (3, 4) are
put in time slot 2. Hence, applying the 
coloring algorithm will use 2 time slots
to accommodate the requests.


\subsubsection{Ordered AAPC algorithm}

The graph coloring algorithm has better performance than the greedy heuristic.
However, for dense communication patterns the heuristics cannot guarantee that
the multiplexing degree found would be bounded by the minimum multiplexing 
degree needed to realize the all-to-all pattern. The algorithm described in 
this section targets dense communication patterns. By grouping the connection
requests
in a more organized manner, better performance can be achieved for dense 
communication.

The worst case of arbitrary communication is the {\em all-to-all personalized 
communication} (AAPC)  where each node sends a message to every 
other node in the system. Any communication pattern can be embedded in AAPC. 
Many algorithms \cite{Hinrichs94,Horie91} have been designed to 
perform AAPC efficiently for different topologies.
Among these algorithms, the ones that are of 
interests to us are the phased AAPC algorithms, in which the AAPC connections 
are partitioned into contention--free phases. A phase in this kind of AAPC 
corresponds to a configuration. Some phased AAPC algorithms are optimal in
that every link is used in each phase and every connection follows the
shortest path. Since all the connections in each AAPC phase are contention--free,
they form a configuration that uses all the links in the system. 
Each phase in the phased AAPC communication forms an {\em AAPC configuration}.
The set of {\em AAPC configurations} for AAPC communication pattern is 
called {\em AAPC configurations set}. 
The following theorem states the property 
of connection scheduling using AAPC phases.

\begin{description}
\item {\bf Theorem: } Let $R =
\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$be the 
set of requests, if $R$ can be partitioned into $K$ phases 
$P_1 = \{(s_{1}, d_{1}), ... , (s_{i_{1}}, d_{i_{1}})\}$, 
$P_2 = \{(s_{i_{1} + 1}, d_{i_{1} + 1}), ... , (s_{i_{2}}, d_{i_{2}})\}$,
... ,\\
 $P_K = \{(s_{i_{K-1} + 1}, d_{i_{K-1} + 1}), ... , 
(s_{i_{K}}, d_{i_{K}})\}$, such that $P_i$, $ 1 \le i \le K$, is a subset
of an AAPC configuration. Using the greedy algorithm to schedule the
connections results in multiplexing degree less than or equal to K.
\end{description}

The theorem states that if the connection requests
are reordered by the AAPC phases,
at most all AAPC 
phases are needed to realize arbitrary pattern using the
greedy scheduling algorithm. For example, following the algorithms in 
\cite{Hinrichs94}, $N^3/8$ phases are needed for a $N\times N$ torus. 
Therefore, in a $N\times N$ torus, $N^3/8$ degree is enough to satisfy
any communication pattern.

To obtain better performance on dense communication patterns, it is 
better to keep the connections in their AAPC format as much as possible. 
It is therefore better to schedule the phases with higher link 
utilization first. This heuristic is used in the ordered AAPC algorithm.
In ordered AAPC algorithm, the rank of the AAPC phases is calculated so 
that the phase that has higher utilization has higher rank. The phases 
are then scheduled according to their ranks. The algorithm is depicted in 
Figure~\ref{ORDAAPC}. The time complexity of this algorithm is
$O(|R|(lg(|R|) + max_i(|C_{i}|)\times K))$, where $|R|$ is the number of 
the requests, $|C_{i}|$ is the number of requests in configuration
$C_{i}$ and $K$ is the number of configurations needed. The advantage 
of this algorithm is that for this algorithm the multiplexing degree 
is bounded by $N^3/8$. Thus, in situations where the greedy or coloring
heuristics fail to meet this bound, AAPC can be used.  

\begin{figure}[ht]
%\small
\begin{tabbing}
\hspace{1in}(1)\hspace{0.6in}PhaseRank[*] = 0\\
\hspace{1in}(2)\hspace{0.6in}{\bf for}\= $(s_{i}, d_{i}) \in R$ {\bf do}\\
\hspace{1in}(3)\hspace{0.6in}\>let $(s_{i}, d_{i}) \in A_{k}$\\
\hspace{1in}(4)\hspace{0.6in}\>PhaseRank[k] = PhaseRank[k] + length($(s_{i}, d_{i})$)\\
\hspace{1in}(5)\hspace{0.6in}{\bf end for}\\
\hspace{1in}(6)\hspace{0.6in}sort phase according to PhaseRank\\
\hspace{1in}(7)\hspace{0.6in}Reorder R according the sorted phases.\\
\hspace{1in}(8)\hspace{0.6in}call greedy algorithm\\
\end{tabbing}
\caption{Ordered AAPC scheduling algorithm}
\label{ORDAAPC}
\normalsize
\end{figure}

\subsubsection{Performance for the scheduling algorithms}

In this section,  the performance of the connection scheduling
algorithms on $8\times 8$ torus topology is studied. 
The performances of the algorithms 
are evaluated using randomly generated communication patterns, patterns
encountered during data redistribution, and some frequently used 
communication patterns. The metric used to compare the algorithms is the 
multiplexing degree needed to establish the connections.
It should be noted that a dynamic scheduling algorithm will not perform
better than the greedy algorithm since it must establish the connections 
by considering the requests in the order that they arrive. 

A {\em random communication pattern} consists of a certain number of 
random connection  requests. A random connection request is obtained
by randomly generating a source and a destination. Uniform probability
distribution is used to generate the randomly sources and destinations.
The {\em data redistribution communication patterns} are obtained by
considering the communication results from array redistribution. In this
study,   data redistributions of a 3D array are considered. The array
has block--cyclic distribution in each dimension. The distribution of a
dimension can be specified by the block size and the number of processors
in the dimension.  A distribution is denoted  as {\em p:block(s)}, where
$p$ is the number of processors in the distribution and $s$ is the block size.
When the distribution of an
array is changed (which may results from the changing of the value $p$ or 
$s$), communication may be needed. 
Many programming
languages for supercomputers, such as CRAFT FORTRAN, allow an array to be
redistributed within the program. 

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
number of  & greedy  & coloring & AAPC & combined &improvement\\
connections. & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
100  & 7.0  & 6.7 & 6.9 & 6.6 & 6.3\%\\
\hline
400 & 16.5  & 16.1 & 16.5 & 15.9 & 3.8\%\\
\hline
800 & 27.2  & 25.9 & 26.5 & 25.6 & 6.3\%\\
\hline 
1200 & 36.3 & 34.5 & 35.3 & 34.2 & 6.1\%\\
\hline
1600 & 45.0  & 43.5 & 43.4 & 42.8 & 5.1\%\\
\hline
2000 & 53.4  & 50.4 & 50.4 & 49.7 & 7.4\%\\
\hline
2400 & 60.8  & 57.5 & 57.4 & 56.7 & 7.2\%\\
\hline
2800 & 68.8  & 64.4 & 62.4 & 62.4 & 10.2\%\\
\hline
3200 & 76.3  & 70.8 & 64 & 64 & 19.2\%\\
\hline
3600 & 83.9  & 76.8 & 64 & 64 & 31.1\%\\
\hline
4000 & 91.6  & 83 & 64 & 64 & 43.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Performance for random patterns}
\label{RANDOM}
\end{table}



Table~\ref{RANDOM} shows the multiplexing degree required to establish
connections for random communication patterns using the algorithms
presented.
The results in each row are the averages obtained from scheduling 100 different
randomly generated patterns with the specific number of connections.
The results in the column labeled {\em combined algorithm} are obtained by using 
the minimum of  the coloring algorithm and
the AAPC algorithm results.
Note that in compiled communication, more time can be spent
to obtain better runtime network utilization. Hence,  the
combined algorithm can be used to obtain better result by the compiler. The 
percentage  improvement shown in the sixth column
is achieved by the combined algorithm over the
dynamic scheduling. 
It is  observed that the coloring algorithm is always
better than the greedy  algorithm
and the AAPC algorithm is better than 
the other algorithms when the communication is dense. 
It can be seen that for sparse random patterns (100 - 2400
connections), the 
improvement range varies from 3.8\% to 7.2\%. Larger improvement
results for dense communication.  For example, the combined algorithm
uses 43.1\% less multiplexing degree than that of the greedy algorithm
for all--to--all pattern. 
This result confirms the result in \cite{Hinrichs94} that
it is desirable to use compiled communication for dense communication.


\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
No. of  & No. of  & greedy   & coloring & AAPC & combined &improvement\\
connections & patterns & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
0 - 100 & 34  & 1.2 &  1.2 & 1.2 & 1.2 & 0.0\%\\
\hline
101 - 200 & 50 & 5.9 & 4.9 & 4.8 & 4.6 & 28.3\%\\
\hline
200 - 400 & 54 & 10.6 &  9.7 & 10.0 &  9.5 & 11.6\%\\
\hline 
401 - 800 & 105 & 17.7 & 15.9 & 16.0& 15.5 & 14.2\%\\
\hline
801 - 1200 & 122 & 31.7 & 28.7 & 28.6 & 27.6 & 14.9\%\\
\hline
1201 - 1600 & 0  & 0      & 0    & 0    &0     &    0\%\\
\hline
1601 - 2000 & 15 & 46.3 &  42.8 & 35.1 & 35.1 & 31.9\%\\
\hline
2001 - 2400 & 77 & 55.5 &  51.5 & 51.9 & 50.4 & 10.1\%\\
\hline
2401 - 4031 & 0  & 0       & 0    & 0     &   0   &  0\% \\
\hline
4032     & 43 & 92  & 83 & 64 &  64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for data distribution patterns}
\label{REDIST}
\end{table}



To obtain more realistic results,  the performance is also evaluated using
the communication patterns for data redistribution and some
frequently used communication patterns.
Table~\ref{REDIST} shows the performance of the algorithms for data 
redistribution patterns. The communication patterns
 are obtained by extracting from the communication resulted from 
the random data redistribution of a 3D array of size
 $64 \times64 \times 64$. 
The  random data redistribution is created by randomly generating
the source data distribution and
the destination data distribution with regard to
the number of processors allocated to each dimension and the block size
in each dimension. Precautions are taken to make sure that the 
total processor number is 64 and the block size is not too large so that
some processors do not contain any part of the array.
The table lists the results for 500 random data redistributions. The first
column lists the range of the number of connection requests in each pattern.
The second column list the number of data redistrictions whose number of 
connection request fell into the range. For example, the second column in the
last row indicates that among the 500 random data redistributions, 43
results in 4032 connection requests. The third, fourth, fifth and sixth column
list the multiplexing degree required by the greedy algorithm, the coloring
algorithm, the AAPC algorithm and the 
combined algorithm respectively. The seventh 
column list the improve ratio by the combined algorithm over the greedy
algorithm.
The result shows that the 
 multiplexing degree required to establish connections resulting
from data redistribution is less than the random communication patterns. 
For the data redistribution pattern, the improvement ratio obtained by using
the combined algorithm  ranges from
10.1\% to 31.9\%, which is larger than the improvement ratio for the random 
communication patterns.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Pattern & No. of conn. & greedy  & coloring & AAPC & comb & ratio\\
\hline
ring    & 128 &  3  &  2 & 2  &  2   &   50\%\\
\hline
nearest neighbor & 256 & 6  & 4 & 4  &  4 & 50\%   \\
\hline
hypercube & 384 &  9 & 7 & 8 &  7 & 28.6\%\\
\hline
shuffle--exchange & 126 &  6& 4 & 5  & 4 & 50\% \\
\hline 
all--to--all & 4032 &  92 & 83 & 64 & 64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for frequently used patterns}
\label{FUSED}
\end{table}

Table~\ref{FUSED} shows the performance for some
frequently used communication patterns. In the ring and the
nearest neighbor patterns, no conflicts arise in the links. 
However, there are conflicts in the communication switches.
The performance gain is higher for these 
specific patterns when the combined algorithm is used.



\section{Conclusion and remaining work}

The advances of architectures that exploit massive parallelism has
created new requirements for interconnection networks. While optical networks
potentially offer large bandwidths, the slow network control has hindered the
full exploitation of the large bandwidth available in fiber optics. This
research 
proposes two methods to address this problem: (1)  efficient distributed
path reservation protocols that reduce control overhead via protocol design
and (2) compiled communication that reduces control overhead by shifting
the control overhead into compile time processing.

A summary of  what has been done and what remains to be
done in this research is listed below.

\begin{itemize}
\item Dynamic communication, work done.
  \begin{itemize}
    \item Various distributed path reservation algorithms have been
          developed. The algorithms assume deterministic routing. A 
          network simulator has been
          developed to evaluate the algorithms. The performance
          of the protocols and the impact of system parameters on
          the protocols have been studied. \cite{yuan97}
  \end{itemize}

\item Dynamic communication, work remaining.
  \begin{itemize}
    \item   Distributed path reservation schemes with adaptive routing and 
      their performance study remain to be done.
  \end{itemize}

\item Compiled communication, work done.
  \begin{itemize}
    \item A communication descriptor which can describe
      logical communication patterns
      in a program has been
      developed and a data flow analyzer for communication
      optimizations using that descriptor has been designed and implemented.
      \cite{Yuan96a}
    \item Connection scheduling algorithms for torus topologies have been
      developed and evaluated. \cite{Yuan96}
    \item A simulator which simulates compiled communication for the 
      communication
      patterns that are known at compile time has been developed.
  \end{itemize}
\item Compiled communication, work remaining.
  \begin{itemize}
    \item Algorithms to derive physical communication patterns from the
      communication descriptor are needed.  
    \item Interprocedural data flow analysis for communication optimization
      needs to be done. 
      Our previous work performs message vectorization and redundant 
      communication elimination within each procedure. This framework
      can be extended to incorporate more optimizations and to the
      interprocedural analysis.
    \item Efficient logical topologies for the communication patterns that 
          are unknown at compile time needs to be designed and evaluated.
    \item A network simulator which simulates multi--hop communication is to be
      designed to study the performance of the logical topologies. 
    \item Experiments to study the performance of compiled communication is 
     to be carried out.
  \end{itemize}
\item Dynamic communication versus compiled communication.
  A comparison between these two control mechanisms is to be carried out.
\end{itemize}



The study of distributed path reservation protocols will provide  general
guidelines for designing optical interconnection networks
 with dynamic network control.
The study of compiled communication will address issues involved when applying
compiled communication and give insights into the advantages and 
the limitations of compiled communication. Some results of this research
can also be applied
to electronic networks, since statically managing network resources
can also improve communication performance in those networks. In addition,
other factors  that affect the communication performance will
also be studied and understood in this research.















\chapter{Dynamic single--hop communication}
\label{single}

This chapter discusses the path reservation protocols for dynamic 
single--hop communication. Two types of 
distributed path reservation protocols, the {\em forward path reservation
protocols} and the {\em backward path reservation protocols},  
 have been designed for 
point--to--point optical TDM networks. A 
network simulator that simulates all the protocols has been developed 
and has been used to 
study the performance of the two types of protocols and to evaluate
the impact of system parameters such as the control packet processing
time and the message size on the protocols.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/singleexam.eps,width=4.5in}}
\caption{An optical network with distributed control.}
\label{singleexam}
\end{figure}

In order to support a distributed control mechanism for connection
establishment, it is assumed that in addition to the optical data network,
there is a logical {\em shadow network} through which
control messages are communicated. 
The shadow network has the same physical topology as the data network.
The traffic on the shadow network consists of small control packets
and thus is much lighter than the traffic on the data network. 
The shadow network  operates in packet switching mode; routers at 
intermediate nodes examine control packets and update local bookkeeping
information and switch states accordingly. 
The shadow network can be implemented as an 
electronic network or alternatively a virtual channel on the data network
can be reserved exclusively for exchanging control messages.
Figure~\ref{singleexam} shows the network architecture.
A virtual channel in the optical data network corresponds to a time slot. 
It is  also assumed that a node can send or receive messages through
different virtual channels simultaneously. 


A path reservation protocol ensures that the path from a source node
to a destination node is reserved before the connection is used. A path 
includes the virtual channels on the links that form the connection, the
transmitter at the source node and the receiver at the destination node.
Reserving the transmitter and the receiver is the same as reserving a
virtual channel on the link from a node to the switch attached to that
node. Hence, only the reservation of virtual channels on links forming a
connection with path multiplexing will be considered.
There are many options available with respect to different aspects of the 
path reservation mechanisms. These are discussed next.

\begin{itemize}

%\noindent
%$\bullet$
\item {\em Forward reservation} versus {\em backward reservation}.
Locking mechanisms are needed by the distributed path reservation
protocols  to ensure the exclusive usage of a virtual channel 
for a connection. This variation characterizes the timing at which
the protocols  perform the locking.
Under forward reservation, virtual channels are locked 
by a control message that travels from
the source node to the destination node.
Under backward reservation, a control message travels to the
destination to probe the path, then virtual channels that are found to be
available are locked by another
control message which travels from the destination node to the source node.

%\noindent 
%$\bullet$
\item {\em Dropping} versus {\em holding}. This variation characterizes
the behavior of the protocol when it 
determines that a connection establishment does not progress.
Under the  dropping approach, once the protocol
determines that  the establishment of a connection is not progressing,
it releases the virtual channels  locked on the partially established
path and informs the source node that the reservation has failed.
Under the holding approach, when the protocol determines
that  the establishment of a connection is not progressing,
it keeps the virtual channels  on the partially established path locked for
some period of time, hoping that during this period, the reservation
will progress. If, after this timeout period, the reservation still does not
progress, the partial path is then released and the
source node is informed of the failure.
Dropping can be viewed as holding with holding time equal to zero.

%\noindent 
%$\bullet$
\item {\em Aggressive} reservation versus {\em conservative} reservation. This
variation characterizes the protocol's treatment of each reservation. Under
the aggressive reservation, the protocol tries to establish a connection
by locking as many virtual channels as possible during the reservation process.
Only one of the locked channels is then used for the connection, while the
others are released.
Under the  conservative reservation approach, the protocol
locks only one virtual channel during the reservation process.

\end{itemize}

\subsection*{Deadlock}

Deadlock in the control network can arise from two sources.
First, with limited number of buffers, a request loop can be formed within the
control network.
Second, deadlock can occur when a request is holding (locking)
virtual channels on some links while requesting other channels on other
links.
This second source of deadlock can be avoided by the dropping or holding mechanisms
described above.
Specifically, a request will give up all the locked channels if 
it does not progress within a certain timeout period.

Many deadlock avoidance or deadlock prevention techniques for 
packet switching networks proposed in the literature \cite{Dally87} 
can be used to deal with deadlock within the control network (the
first source of deadlock).
Moreover, the control network is under light traffic, and
each control message consists of only a single packet of small size 
(4 bytes). Hence, it is feasible to provide a large number of buffers in each 
router to reduce or eliminate the chances of deadlocks.

\subsection*{States of virtual channels}

The control network router at each node maintains a state for each
virtual channel on links connected to the router. For forward reservation,
the control router maintains the states for the outgoing links.
% while
% in backward reservation, the control router maintains the states
% for the incoming links. 
As discussed later, this enables the router to have the information
needed for reserving virtual channels and updating the switch states.
A virtual channel, $V$, on link $L$, can be in one of the following states:

\begin{itemize}
\item $AVAIL$: indicates that the virtual channel $V$ on link $L$
is available and can be used to establish a new connection,
\item $LOCK$: 
indicates that $V$ is locked by some request in the process of establishing
a connection.
\item $BUSY$: indicates that $V$
is being used by some established connection to transmit data.
\end{itemize}

For a link, $L$, the set of virtual channels that are in the $AVAIL$ state is
denoted as $Avail(L)$. When a virtual channel, $V$, is not in $Avail(L)$,
an additional field, $CID$, is maintained to identify the connection request
locking  $V$, if $V$ is in the $LOCK$ state, or the connection using $V$, if $V$
is in the $BUSY$ state.

\section{Forward reservation schemes}

In the connection establishment protocols,
each connection request is assigned a unique identifier, $id$, which
consists of the identifier of the source node and a serial number
issued by that node. 
Each control message related to the establishment of a connection carries its
$id$, which becomes the identifier of the connection when it is successfully
established. It is this $id$ that is maintained in the $CID$ field of
locked or busy virtual channels on links.
Four types of packets are used in the forward reservation
 protocols to establish a connection.

\begin{itemize}

%\noindent 
%$\bullet$
\item {\em Reservation packets} ($RES$), used to reserve virtual channels.
In addition to the connection $id$, a $RES$ packet contains a bit vector,
$cset$, of size equal to the number of virtual channels in each link.
The bit vector $cset$ is used to keep track of the set of virtual channels 
that can be used to satisfy the connection request carried by $RES$.
These virtual channels are locked
at intermediate nodes while the $RES$ message
progresses towards the destination node. The switch
states are also set to connect the locked channels on the input and output links.

%\noindent 
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$), used to inform source nodes of the
success of connection requests.
An $ACK$ packet contains a $channel$ field which indicates the virtual
channel selected for the connection.
As an $ACK$ packet travels from the destination to the source, it changes
the state of the virtual channel 
selected for the connection to $BUSY$, and unlocks
(changes from $LOCK$ to $AVAIL$)
all other virtual channels that were locked by the corresponding $RES$ packet.

%\noindent 
%$\bullet$
\item {\em Fail or Negative ack packets} ($FAIL/NACK$), used to inform source
nodes of the failure of connection requests. While traveling back to the source
node, a $FAIL/NACK$ packet unlocks all virtual channels that were locked by the
corresponding $RES$ packet.
 
%\noindent 
%$\bullet$
\item {\em Release packets} ($REL$),  used to release connections.
A $REL$ packet traveling from a source to a destination changes the
state of the virtual channel
reserved for that connection from $BUSY$ to $AVAIL$.

\end{itemize}

The protocols require that control packets from a destination, $d$, to a source, $s$,
follow the same paths (in opposite directions) as packets from $s$
to $d$.
The fields of a packet will be denoted by $packet.field$.
For example, $RES.id$ denotes the $id$ field of the $RES$ packet.

The forward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $RES$ packet with $RES.cset$ set to the
virtual channels that the node may use. This message is then routed to the
destination. When an intermediate node receives the $RES$ packet, 
it determines the next outgoing link, $L$, on the path to the destination, and
updates $RES.cset $ to $ RES.cset \cap Avail(L)$.
If the resulting $RES.cset$ is empty,
the connection cannot be established
 and a $FAIL/NACK$ message is sent back to the
source node. The source node will retransmit the request after some
period of time.
This process of failed reservation is shown in Figure~\ref{FORWARD}(a). 
Note that if $Avail(L)$ is represented by a bit-vector, then
$RES.cset \cap Avail(L)$ is a bit-wise "$AND$" operation.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/forward.pstex,height=2.2in}}
\caption{Control messages in forward reservation}
\label{FORWARD}
\end{figure}

If the resulting $RES.cset$ is not empty, the router reserves all the 
virtual channels in $RES.cset$ on link $L$ by changing their states to $LOCK$
and updating $Avail(L)$.
The router will then set the switch state to connect the virtual channels in the
resulting $RES.cset$ of the corresponding incoming and outgoing links.
Maintaining the states of outgoing links is sufficient for these two
tasks.
The $RES$ message is then forwarded to the next node on the path to the destination.
This way,
as $RES$ approaches the destination, the 
path is reserved incrementally. Once $RES$ reaches the
destination with a non-empty $RES.cset$, the destination selects from 
$RES.cset$ a virtual channel to be used for the connection and informs
the source node that the channel is selected by sending an $ACK$ message 
with $ACK.channel$ set to the selected virtual channel.
The source can start sending data once it 
receives the $ACK$ packet. After all data is sent, the source
node sends a $REL$ packet to tear down the connection. This successful
reservation process is shown in Figure~\ref{FORWARD}~(b). Note that although
in the algorithm described above, the switches are set during the processing
of the $RES$ packet, they can instead be set during the processing of
the $ACK$ packet.

\noindent
{\bf Holding}: The protocol described above can be modified to 
use the holding policy instead of the dropping policy.
Specifically, when an intermediate node
determines that the connection for a reservation cannot be established, 
that is when $RES.cset \cap Avail(L) = \phi$, the node buffers the $RES$ packet
for a limited period of time. If within
this  period, some virtual channels in the original $RES.cset$ become
available, the $RES$ packet can then continue its journey. Otherwise, 
the $FAIL/NACK$ packet is  sent back to the source.
Implementing the holding policy 
requires each node to maintain a holding queue and
to periodically check that queue to determine if any of the virtual channels 
has become available. In addition, some timing 
mechanism must be incorporated in the routers to timeout 
held control packets. This increases the hardware
and software complexities of the routers.

\noindent
{\bf Aggressiveness}: 
The aggressiveness
of the reservation is reflected in the size of the 
virtual channel set, $RES.cset$, initially chosen by the source node.
In the most aggressive scheme, the source node sets
$RES.cset$ to $\{0, ..., N-1\}$, where $N$ is the number of 
virtual channels in the system. This ensures that the reservation
will be successful if there exists an available virtual channel on the path.
On the other hand, 
the most conservative reservation assigns
$RES.cset$ to include only a single virtual channel. In this case, the
reservation can be successful only when the virtual channel chosen by the
source node is available in all the links on the path. Although 
the aggressive scheme seems to have advantage over the conservative scheme,
it results in excessive locking of the virtual channels in the system. Thus, in
heavily loaded networks, this is expected to decrease the overall throughput.
To obtain optimal performance, the aggressiveness of the protocol should be
chosen appropriately between the most aggressive and the most conservative extremes.

The retransmit time  is another protocol parameter.
In traditional non--multiplexed networks, the retransmit time
is typically chosen randomly from a range [0,MRT], where MRT
denotes some maximum retransmit time.
In such systems, MRT must be set to a reasonably
large value to avoid live-lock. However, this may increase the average
message latency time and decrease the throughput.
In a multiplexed network, the problem of live-lock only 
occurs in the most aggressive scheme (non--multiplexed circuit switching
networks can be considered as  having a multiplexing degree of 1 and 
using aggressive reservation). 
For less aggressive schemes, the
live-lock problem can be avoided by changing the virtual channels selected in
$RES.cset$ when $RES$ is retransmitted.
Hence, for these schemes, a small retransmit time can be used.

\section{Backward reservation schemes}

In  the forward locking protocol, the initial decision concerning the 
virtual channels to be locked for a connection request is made in the 
source node without any information about network usage. The backward
reservation scheme tries to overcome this handicap by probing the network
before making the decision. In the backward reservation schemes,
a forward message is used to probe the availability of virtual channels.
After that,
the locking of virtual channels is performed by a backward message. 
The backward reservation scheme uses six types of control
packets, all of which carry the connection $id$, in addition to other
fields as discussed next:

\begin{itemize}
%\noindent
%$\bullet$
\item {\em Probe packets} ($PROB$) travel from sources to destinations 
gathering
information about virtual channel usage without locking any virtual channel.
A $PROB$ packet carries a bit vector, $init$,
to represent the set of virtual channels that are
available to establish the connection.

%\noindent
%$\bullet$
\item {\em Reservation packets} ($RES$) are similar to the $RES$ packets in the forward
scheme, except that they travel from destinations to sources, lock
virtual channels as they go through intermediate nodes, and set the
states of the switches accordingly.
A $RES$ packet contains a $cset$ field.

%\noindent
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$) are similar to $ACK$ packets in the forward
scheme except that they travel from sources to destinations.
An $ACK$ packet contains a $channel$ field.


%\noindent
%$\bullet$
\item {\em Fail packets} ($FAIL$) unlock the virtual channels locked by the
$RES$ packets in cases of failures to establish connections.

%\noindent
%$\bullet$
\item {\em Negative acknowledgment packets} ($NACK$) are
used to inform the source nodes of reservation failures.

%\noindent
%$\bullet$
\item {\em Release packets} ($REL$) are
used to release connections after the communication is completed.

\end{itemize}

Note that a $FAIL/NACK$ message in the forward scheme performs the functions
of both a $FAIL$ message and a $NACK$ message in the backward scheme. 

The backward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $PROB$ message with $PROB.init$ set to contain all
virtual channels in the system.
This message is then routed to the destination.
When an intermediate node receives the $PROB$ packet, 
it determines the next outgoing link, $L_f$, on the forward path to the
destination,  and updates $PROB.init $ to $PROB.init \cap Avail(L_f)$.
If the resulting $PROB.init$ is empty,
the connection cannot be established and a $NACK$ packet is sent back to the
source node.  The source node will try the reservation again after a certain 
retransmit time.
Figure~\ref{BACKWARD}(a) shows this failed reservation case.

If the resulting $PROB.init$ is not empty, the node 
forwards $PROB$ on $L_f$ to the next node. 
This way,
as $PROB$ approaches the destination, the virtual channels available
on the path are recorded in the $init$ set.
Once $PROB$ reaches the
destination,  the destination forms a $RES$ message with $RES.cset$
equal to a selected subset of $PROB.init$ and sends this message back
to the source node.
When an intermediate node receives the $RES$ packet, it determines the
next link, $L_b$, on the backward path to the source, and updates
$RES.cset $ to $RES.cset \cap Avail(L_b)$. 
If the resulting $RES.cset$ is empty, 
the connection cannot be established. In this case the node sends
a $NACK$ message to the source node to inform it of the failure,
and sends a $FAIL$ message to the 
destination to free the virtual channels locked
by $RES$. This process is shown in Figure~\ref{BACKWARD}~(b).

\begin{figure}[htp]
\centerline{\psfig{figure=fig/back.pstex,height=2.2in}}
\caption{Control messages in backward reservation}
\label{BACKWARD}
\end{figure}

If the resulting $RES.cset$ is not empty,
the virtual channels in $RES.cset$ are locked, the switch is set accordingly
and $RES$ is forwarded on $L_b$
to the next node.  When $RES$ reaches the source with a non-empty
$RES.cset$,
the source  selects a
virtual channel from the $RES.cset$ for the connection and sends
an $ACK$ message to the destination with $ACK.channel$ set to the
selected virtual channel. This $ACK$ message unlocks all the virtual channels 
locked by $RES$, except the one in $channel$.
The source node can start sending data as soon as it sends the $ACK$ message.
After all data is sent, the source
node sends a $REL$ packet to tear down the connection.
The process of successful reservation is shown in Figure~\ref{BACKWARD}(c).

\noindent
{\bf Holding}: Holding can be incorporated in the backward reservation scheme
as follows.
In the protocol, there are two cases that cause the reservation to fail. 
The protocol may determine that the reservation fails when processing
the $PROB$ packet. In this case, holding is not desirable because the PROB
packet is used to collect the channel usage information and holding could
reduce the precision of the information collected (the status of channels
on other links may change during the holding period).
%In this case, no holding is necessary since 
%no resources have yet been locked.
When the protocol determines that the 
reservation fails during the  processing of a
$RES$ packet, a holding mechanism
similar to the one used in the forward reservation scheme may be applied.

\noindent
{\bf Aggressiveness}:
The aggressiveness of the backward reservation protocols is reflected in the 
initial size of $cset$ chosen by the destination node.
The aggressive approach sets
$RES.cset$ equal to $PROB.init$, while the conservative
approach sets $RES.cset$ to contain a single virtual channel from $PROB.init$.
Note that if a protocol supports only the conservative scheme,
the $ACK$ messages may be omitted, and thus only five types of messages 
are needed. 
As in the forward reservation schemes, the 
retransmit time is a parameter in the backward schemes.

\section{Network simulator and experimental results}
\label{simulator}

A network simulator has been developed to simulate the behavior
of multiplexed torus networks. The simulator models the network with 
various choices of system parameters and protocols. Specifically, 
the simulator provides the following options for protocol parameters.

\begin{itemize}
\item {\em forward and backward} reservations, this determines which
protocol to be simulated.

\item {\em initial $cset$ size}: This parameter determines the
initial size of $cset$ in the reservation packet. 
It restricts the set of virtual channels under
consideration for a reservation. In forward schemes,
the initial $cset$ is chosen when the source node composes the RES packet.
Assuming that $N$ is the multiplexing degree in the system,
an $RES.cset$ of size $s$ is chosen by generating a random number,
$m$, in the range $[0,$N$ - 1]$, 
and assigning $RES.cset$ = $\{m\ mod\ N, m+1\ mod\ N..., N+s-1\ mod\ N\}$.
In the backward schemes, the initial $cset$ is set when
the destination node composes the $ACK$ packet. An $ACK.cset$ of size $s$ 
is generated in the following manner.
If the available set, $RES.INIT$,
has less available channels than $s$, the $RES.INIT$ is copied to $ACK.cset$.
Otherwise,  the available channels are represented in a linear
array and the method used in generating the $cset$  in the forward schemes
is used.

\item {\em timeout value}: This  value 
determines how long a reservation packet can be put in a waiting queue.
The dropping scheme can be viewed as a holding scheme with timeout time
equal to zero.

\item {\em maximum retransmit time} (MRT): 
This specifies the period after which a node will retry a
failed reservation. As discussed earlier,
this value is crucial for avoiding live-lock
in the most aggressive schemes. The actual retransmit time
is chosen randomly between 0 and  $MRT -1$.
\end{itemize}

Besides the protocol parameters, the simulator also allows the 
choices of various system parameters.

\begin{itemize}

\item {\em system size}: This specifies the size of the network. All our
simulations are done on torus topology.

\item {\em multiplexing degree}. 
This specifies the number of virtual
channels supported by each link. In our simulation, the multiplexing degree
ranges from 1 to 32.

\item {\em message size}: The message size directly affects  the time that
 a connection is kept before it is released.
In our simulations,  fixed size messages are assumed.

\item {\em request generation rate at each node (r)}: This specifies the traffic on
the network. The connection requests at each node are assumed to have a Poisson
inter-arrival distribution. When a request is
generated at a node, the destination of the request is generated randomly
among the other nodes in the system. When a generated request is blocked,
it is put into a queue, waiting to be re-transmitted.

\item {\em control packet processing and propagation time}: This specifies the speed of the
control networks. The control packet processing time is the time for an
intermediate node to process a control packet. The control packet
propagation time is the time for a control packet to be transferred from one node
to the next. It is assumed
 that all the control packets have the same
processing and propagation time.
\end{itemize}

In the following discussion, $F$ is used to denote forward reservation,
$B$ denotes the backward reservation, $H$ denotes 
holding and $D$ denotes dropping
schemes. For example, $FH$ means the forward holding scheme.
%A network simulator with various  control mechanisms 
%including FH, FD, BH and BD was implemented.
%Although the simulator can simulate both WDM and TDM torus networks, 
%only the results for TDM networks will be presented in this paper.
%The results for WDM networks follow similar patterns.
In addition to the options of backward/forward reservation and holding/dropping
policy, the simulation uses the following parameters.
The average latency and throughput are used to evaluate the protocols.
The latency is the period between the time when a message is ready and the time
when the first packet of the message is sent.
The  throughput is the number of messages received per time unit.
Under light traffic, 
the performance of the protocols is measured by the average message latency,
while under heavy traffic, the throughput 
is used as  the performance metric.  
The simulation time is measured in time slots, where a time slot is the
time to transmit an optical data packet between any two nodes in the network.
Note that in multiprocessor applications, nodes are physically close
to each other, and thus signal propagation time is very small (1 foot per
nsec) compared to the length of a message.
Finally, deterministic XY--routing is assumed in the torus topology.


\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Throughput]
  {\psfig{figure=eps/CMP1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CMP2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Comparison of the reservation schemes with dropping}
\label{DFMUL}
\end{figure}


Figure~\ref{DFMUL} depicts the throughput and average latency as a function of
the request generation rate for six
protocols that use the dropping policy in a $16\times 16$ torus.
The multiplexing degree is taken to be 32, the
message size is assumed to be 8 packets and the control packets
processing and propagation time is assumed to be 2 time units. 
For each of the forward and backward schemes, three variations are considered 
with varying aggressiveness.
The conservative variation in which the
initial $cset$ size is 1, the most aggressive variation in which
the initial set size is equal to the multiplexing degree and an optimal variation 
in which the initial set size is chosen (by repeated trials) to maximize the
throughput.
The letters  $C$, $A$ and $O$ are used to
denote these three variations, respectively.
For example, $FDO$ means the forward dropping scheme with optimal $cset$ size.
Note that the use of the optimal $cset$ size reduces the delay in addition to
increasing the throughput. Note also that the network saturates when
the generation rate is between 0.006 and 0.018, depending on the protocol
used. The maximum saturation rate that the $16\times 16$ torus can achieve in the
absence of contention and control overhead is given by
\[
\frac{number\ of\ links}{no.\ of\ PEs \times av. \ no.\ of\ links\ per\ msg
\times msg\ size} = \frac{1024}{256\times 8 \times 8} = 0.0625.
\]
Hence, the optimal backward protocol can achieve
almost 30\% of the theoretical full utilization rate. 

Figure~\ref{DFMUL}(b) also reveals that,
when the request generation rate, $r$, is small, for example $r = 0.003$, 
the network is under light traffic and 
all the protocols achieve the same throughput, which is equal to $r$ times
the number of processors.
In this case, the performance of the network should be measured by the
average latency.
In the rest of the performance study,
the maximum throughput (at saturation) and the average latency
(at $r = 0.003$) were used to measure the performance of the protocols.
Two sets of experiments are performed. The first set 
evaluates the effect of the protocol parameters on the network throughput and
delay and the second set
evaluates the impact of system parameters on performance.

\subsubsection*{Effect of protocol parameters}

In this set of experiments,
the effect of the initial $cset$ size, the holding time and the retransmit time on the 
performance of the protocols are studied. 
The system parameters for this set of experiments are chosen as follows:
system size = $16\times 16$,
message size = 8 packets, control packet processing and propagation time = 2 time
units.

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET3.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET4.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on forward schemes}
\label{CSETF}
\end{figure}
\begin{figure}[hbtp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on backward schemes}
\label{CSETB}
\end{figure}

Figure~\ref{CSETF} shows the effect of the initial $cset$ size on the forward
holding scheme with different multiplexing degrees, namely
1, 2,  4, 8, 16 and 32.
The holding time is taken to be 10 time units and the MRT is 5 time units
for all the protocols with initial $cset$ size less than the multiplexing degree
and 60 time units for the most aggressive forward scheme.
Large MRT is used in the most aggressive forward scheme because it is 
observed that small MRT often leads to live-lock in this scheme.
Only the protocols with the holding policy will be shown since using the
dropping policy leads to similar patterns. The effect of holding/dropping will
be considered in a later figure.
Figure~\ref{CSETB} shows the results for the backward
schemes with the dropping policy.

From Figure~\ref{CSETF} (a), it can be seen that when the multiplexing 
degree is larger than 8, both the most
conservative protocol and the most aggressive protocol
do not achieve the best throughput. Figure~\ref{CSETF}(b) shows that these
two extreme protocols do not achieve the smallest latency either.
The same observation applies to the backward schemes in Figure~\ref{CSETB}.
The effect of choosing the optimal initial $cset$ is significant on both
throughput and delay. That effect, however, is more significant in the
forward scheme than in the backward scheme. For example, with multiplexing
degree = 32,
choosing a non-optimal $cset$ size may reduce the throughput by 50\%
in the forward scheme and only by 25\% in the backward scheme. 
In general, the optimal initial $cset$ size is hard to find.
Table~\ref{OPTCSET} lists the optimal initial $cset$ size for each multiplexing
degree.
A rule of thumb to approximate the optimal $cset$ size is to use 1/3 and 1/10 of the
multiplexing degree for forward schemes and backward schemes, respectively.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Multiplexing & \multicolumn{2}{|c|}{Optimal $cset$ size}\\
\cline{2-3}
Degree & Forward & Backward\\
\hline
4 & 1 & 1\\
\hline
8 & 2 & 1\\
\hline
16 & 5 & 2\\
\hline
32 & 10 & 3\\
\hline
\end{tabular}
\end{center}
\caption{Optimal $cset$ size}
\label{OPTCSET}
\end{table}

Figure~\ref{HOLDSET} shows the effect of the holding time on the performance of
the protocols for a multiplexing degree of 32. 
As shown in Figure~\ref{HOLDSET}(a), the holding time has little
effect on the maximum throughput. It slightly increases the performance for the
forward aggressive and the backward aggressive schemes. As for the 
average latency under light work load, the holding time also has little effect
except for the forward aggressive scheme, where the
latency time decreases by about 20\% when
the holding time at each intermediate node increases from 0 to 30 time units.
Since holding requires extra hardware support compared to
dropping, it is  concluded that holding is not 
cost--effective for the reservation protocols. In the rest of the paper,
only protocols with dropping policies will be considered.


\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/HOLDSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/HOLDSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of holding time}
\label{HOLDSET}
\end{figure}

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/RETRYSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/RETRYSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of maximum retransmit time}
\label{RETRYSET}
\end{figure}



Figure~\ref{RETRYSET} shows the effect of the maximum
retransmit time
on the performance. Note that the retransmit time is uniformly distributed
in the range $0..MRT-1$. As shown in Figure~\ref{RETRYSET} (a),
increasing MRT results in performance degradation in
all the schemes except FDA, in which the performance improves
with MRT. This confirms that the MRT value is important to 
avoid live-lock in the network when aggressive reservation is used.
In other schemes this parameter is not important, because when 
retransmitting a failed request, virtual channels different than the ones
that have been tried may be included in $cset$.
This result indicates another drawback of the 
forward aggressive schemes: in order to avoid live-lock, the MRT
must be a reasonably large value, which decreases the overall performance.

\subsubsection*{Effect of other system parameters}

In this section,
only  dropping schemes with
MRT equal to 5 time units for all schemes except FDA will be considered.
The MRT for FDA schemes is set to 60.
This set of  experiments focuses on studying the performance of the 
protocols under different multiplexing degrees, 
system sizes, message sizes and 
control network speeds.
One parameter is changed in each experiment, with the other
parameters set to the following default values (unless stated otherwise):
network size = $16\times 16$ torus, multiplexing degree = 
16, message size = 8 packets, 
control packet processing and propagation time = 2 time units.




\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
  {\psfig{figure=eps/MUL1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/MUL2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{The performance of the protocols for different multiplexing degree}
\label{DBMUL}
\end{figure}


Figure~\ref{DBMUL}
shows the performance of  the protocols for different multiplexing degrees. 
When the multiplexing degree is small,  BDO and FDO
have the same maximum bandwidth as BDC and FDC, respectively. When 
the multiplexing degree is large, BDO and FDO offer better throughput.
In addition, for all multiplexing degrees, BDO is the best among
all the schemes. As for the average latency, both FDA and BDA have significantly
larger latency than all other schemes. Also, FDO and BDO have the smallest latencies.
It can be seen from this experiment that the backward
schemes always provide the same or better performance (both maximum
throughput and latency) than their forward reservation counterparts for all
multiplexing degrees considered.

Figure~\ref{SIZE} shows the effect of the network size on the performance of
the protocols.
It can be seen from the figure that all the protocols, except the aggressive
ones, scale nicely with the network size.
This indicates that the aggressive protocols cannot 
take advantage of the spatial diversity of the communication. This is 
a result of excessive reservation of channels. When the network size
is small, there is little difference in the performance of the protocols.
When the network size is larger, the backward schemes show their superiority.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/SIZE1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
 {\psfig{figure=eps/SIZE2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the network size}
\label{SIZE}
\end{figure}


\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/MSIZE1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
{\psfig{figure=eps/MSIZE2.eps,width=2.95in}}
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the message size}
\label{MSIZE}
\end{figure}



Figure~\ref{MSIZE} shows the effect of the message size on the protocols.
The multiplexing degree  in this experiment is 16.
The throughput in this figure is normalized to reflect the
number of packets that pass through the network, rather than the number
of messages,  that is,\\
\centerline{$normalized\ throughput\ =\ msg\ size \times \ throughput.$}
Both the forward and backward locking schemes achieve higher
throughput for larger messages. When messages are sufficiently large,
the signaling overhead in the protocols is small and
all protocols have almost the same performance. 
However, when the message size is small, the BDO scheme achieves 
higher throughput
than the other schemes. This indicates that BDO incurs less 
overhead in the path reservation than the other schemes. 

The effect of message size on the latency of the protocols is interesting.
Forward schemes incur larger latency when the message size is large. 
By blindly choosing initial cset, forward schemes
do not avoid choosing virtual channels used in communications, which
increases the latency when the message size is large (so that connections are
held longer for communications).
Backward schemes probe
the network before choosing the initial csets. Hence, the latency in backward
schemes does not increase as
much as in forward schemes when message size increases. 
Another observation is that in both forward and backward protocols, 
aggressive schemes sustain the increment of message size better
than the conservative schemes. This is also because of the longer
communication time with larger message sizes. Aggressive schemes are more 
efficient in finding a path in case of large message size. Note that 
this merit of aggressive schemes is offset by 
over reservation.
Another interesting point is that
the latency for messages of  size 1 results in higher latency than 
messages of size 8 in BDA scheme. This can be attributed to too many
control messages in the network
in the case when data message contains a single packet (and
thus can be transmitted fast). The conflicts of control messages result in
larger latency.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
{\psfig{figure=eps/CNS1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CNS2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the speed of the control network}
\label{CNS}
\end{figure}

Figure~\ref{CNS} shows the effect of the control network speed on performance. 
The multiplexing degree  in this experiment is 32.
The speed of the
control network is determined by the
time for a control packet to be transferred from one node to the next node
and the time for the control router to process the control packet. From the 
figure it can be seen that when the control speed is slower, the maximum
throughput and the average latency degrade.
The most aggressive schemes in both 
forward and backward reservations, however, are more sensitive to the
control network speed. Hence, it is important to have a reasonably fast 
control network when these reservation protocols are used. 


\section{Chapter summary}

This chapter presented the forward path reservation algorithms
and the backward path reservation algorithms to establish connections 
with path multiplexing for connection requests that arrive at the 
network dynamically. Holding and dropping variants of these protocols 
were described. A holding scheme holds the reservation packet for a period 
of time when it determines that there is no available channel on the next link
for the connection. A dropping scheme drops the reservation
packet and starts a new reservation once it finds that there is no 
available channel on the next link for the connection.  Protocols
with various aggressiveness were discussed. The most aggressive schemes
reserve as many channels as possible for each reservation to increase
the probability of a successful reservation, while the most
conservative schemes reserve one channel for each reservation to reduce
the over--locking problem. The performance of the protocols and 
the impact of system parameters on the protocols were studied. 
The major results obtained in the experiments are summarized as follows.

\begin{itemize}

\item With proper protocols, multiplexing results in higher
maximum throughput. Multiplexed networks are significantly more efficient than
non--multiplexed networks.

\item Both the most aggressive and the most
conservative reservations cannot achieve optimal performance. 
The performance of the forward schemes is more sensitive to the
aggressiveness than the performance of the backward schemes.

\item The value of the holding time in the holding schemes does 
not have significant impact on the performance. 
In general, however, dropping is more efficient than holding.

\item  The retransmit time 
has little impact on all the schemes except the forward aggressive
dropping scheme.

\item The performance of the forward aggressive dropping scheme
is significantly worse than other protocols. 
Moreover, this protocol cannot take advantage of both larger multiplexing 
degree and larger network size.

\item The backward reservation schemes provide better performance than
the forward reservation schemes for all multiplexing degrees. 

\item The difference of the protocols does not affect the communication 
efficiency when the network size is small. However, for large networks, the
backward schemes provide better performance.

\item The backward schemes provide better performance when the message size
is small. When the message size is large, all the protocols have similar 
performance.

\item The speed of the control network significantly 
affects the performance of the protocols.

\end{itemize}

These protocols achieve all--optical communication
in data transmission. However, they require extra hardware support to 
exchange control messages and incur large startup overhead. 
An alternative to the single--hop communication is 
the multi--hop communication. Multi--hop networks
do not require extra hardware support. They use intermediate hops
to route messages toward their destinations. In the next chapter, 
multi--hop networks are considered.









\section{Goals and Approaches}
\label{goal}

This research will consider network control for optical TDM
point--to--point networks in multiprocessor environments. There are 
three primary goals of this work: (1) to develop and evaluate
dynamic network control mechanisms which establish connections
(virtual paths)
for the connection requests that arrive at the network dynamically, 
(2) to investigate the feasibility
of applying compiled communication technique in optical multiplexed
interconnection networks, and to address  relevant issues when applying 
compiled communication, and (3) to compare the communication performance
of these two schemes.  Shared memory
programs compiled for  distributed memory machines and 
programs using explicit message passing will be used for the
study of compiled communication. The torus topology will be assumed to be
the underlying physical topology in this research for performance
evaluations. This topology  
has been demonstrated to be an efficient
topology for point--to--point networks
 and is used in many commercial supercomputers. 
Note, however, that many  techniques developed in
this research can also be applied to other topologies.

Optical interconnection networks offer large bandwidth and
low error rates. To fully exploit the large bandwidth in an optical network,
the control overhead must be reduced.
Since dynamic network control must be performed in the electronic domain, 
it is crucial to 
develop efficient network control mechanisms that 
 establish all--optical paths
for connection requests without incurring too much control overhead. 
The study of dynamic network control schemes is 
the first goal of this research. The results of the study will give
general guidelines for the design of dynamic network control schemes
in optical multiplexed interconnection networks. The study will also
lead to the 
understanding of  the impact of system parameters on the control schemes.

One way to reduce the control overhead is to use compiled communication.
Compiled communication reduces runtime control overhead by
managing communications at compile time.  The study of compiled communication
 is the second focus of this
research. A number of issues must be addressed in order to apply 
compiled communication. How to efficiently 
obtain  communication patterns in 
a program?  How to manage network resources when communication patterns
are known? How to handle communication patterns that are 
unknown at compile time?
These are some questions to be answered among others.
The study of compiled communication will try to give solutions to
the problems encountered when applying compiled communication.
The results of the study will give insights into compiled 
communication in optical TDM point--to--point networks as well as the
potential performance gain by compiled communication.

In order to achieve the above goals, the steps outlined below are proposed.

\begin{enumerate}
\item 
  Study  efficient dynamic network control mechanisms for  optical TDM
  interconnection networks.
  \begin{enumerate}
    \item Develop distributed path reservation protocols for optical TDM
     point--to--point networks.
    \item Develop a network simulator that simulates the network behavior
          under all designed protocols.
    \item Study the performance of the protocols and 
          the impact of various
          system parameters, such as system size, message size, etc, on 
          the protocols. Latency and maximum throughput will be used 
          as performance measures.
  \end{enumerate}
\item Design and implement a tool that synthesizes  communication
          patterns from application programs.
   \begin{enumerate}
     \item Design a communication descriptor that can describe
           the communication requirement (logical communication patterns)
           in a program.
     \item Design and implement data flow analysis algorithms for
           communication optimization to obtain communication
           patterns in the optimized code of a program.
     \item Design and implement schemes that derive phyiscal 
                communication patterns from logical communication patterns.
   \end{enumerate}
\item 
  Study   compiled communication and evaluate its 
  performance in optical TDM networks.
  \begin{enumerate}
    \item Design communication scheduling algorithms that can be used by 
          the compiler to schedule the communication patterns that
          are known at compile time.
    \item Design efficient logical topologies that can be used to 
          route messages in the communication patterns that are unknown at
          compile time.
    \item Develop a network simulator that simulates  network behavior
          for networks
          with compiled communication. The simulator should include  
          off--line
          connection scheduling algorithms to establish connections
          for the communication patterns that are known at compile time. The 
          simulator should also be able to simulate  multi--hop
          communication for handling the  communication
          patterns that are unknown at compile time.
    \item Evaluate the performance of compiled communication. 
          This includes the performance  for 
          static patterns,  dynamic patterns, and the overall
          performance for all patterns in programs.
  \end{enumerate}
\item
  Carry out a comparison of dynamic communication and compiled communication
  using actual application programs.
\end{enumerate}
  
\subsection{Dynamic network control}
 
In order to fully explore the potential of optical communication,
optical signals should be transmitted in a pure circuit--switching 
fashion in the optical domain (all--optical communication). No buffering and 
optical-to-electronic or electronic-to-optical conversions should be needed
at intermediate nodes. Dynamic network control processes the connection
requests arriving at the network dynamically and negotiates with the
nodes in the network to establish virtual paths for 
connection requests before the communications can start. In this work, 
distributed path reservation algorithms for 
the establishment of VPs are to be designed and evaluated.
The major issues to be considered are the control overhead, 
the efficiency of the  virtual channel utilization  and the 
hardware overhead.  Some trade--offs among these issues may be  needed to
obtain an efficient algorithm. 

A cycle level network
simulator which simulates the network behavior for networks with
different  
protocols is needed. 
In order to  study all  variants of the protocols, the 
simulator must provide the following capabilities.
\begin{itemize}
\item It must be able to simulate all different protocols designed.
\item It must be able to simulate networks with different parameters, such as
      network size and multiplexing degree.
\item It must be able to simulate protocols with different protocol
      parameters.
\end{itemize}
Using the network simulator, the performance of the 
protocols will be studied, 
the impact of system parameters and protocol parameters
on the protocols will be determined. 
Based upon the results, the protocol parameters can be tuned to obtain 
good performance.


\subsection{A tool to extract communication patterns in a program}

To apply compiled communication, the compiler must be able to obtain the
communication requirements in a program. Traditional methods
analyze communication requirements on the logical processor 
array (logical communication patterns) 
\cite{Hinrichs95,Gupta92} and represent the communications in 
mathematical notations, such as matrix notation in \cite{Hinrichs95}.
Based on the form of the mathematical notation,  communication
patterns on  physical
processors (physical communication patterns)  may be obtained. 
Obtaining physical communication patterns
in this manner may not be sufficient for compiled communication, since
in compiled communication, we would like to know all  communication
patterns that can be synthesized by the compiler. Note that 
communication patterns that are unknown at 
compile time 
result in much larger overhead when 
using compiled communication. 

In this research, a descriptor, which can describe  logical communication 
patterns, is developed. The 
descriptor can be easily calculated from program structures. Traditional
communication optimization algorithms will  be implemented using
this descriptor to represent communications in programs. Note that,
since communication optimization are commonly implemented in compilers,
implementing traditional communication optimizations
will give  more realistic communication patterns in programs.
Once the communication optimizer is implemented,
 logical communication patterns in both optimized and 
non-optimized programs can be obtained and represented in the descriptor.
An efficient scheme will then be designed to derive  physical
communication patterns from the logical communication patterns. 
 Note that brute force methods can be used to derive physical communication
patterns. However, this may result in large
analysis overhead and must be avoided if possible.

\subsection{Compiled communication}

Under compiled communication, the compiler will manage the communications in
a program. For a static pattern, the compile can  insert codes in the 
program to set the network state such that, at runtime, a path will
exist before a communication starts without  dynamic path reservation.
For a dynamic pattern, the compile will route messages in the pattern
through a logical topology.
The following steps will be carried
out to study compiled communication.

\subsubsection{Connection scheduling for static patterns}

Once the compiler knows the communication patterns in a  program, it 
employs  off--line connection scheduling algorithms to manage the 
communications.  Since the communication time of a communication pattern
in an optical TDM system is proportional
 to the multiplexing degree required for 
the communication pattern,
the primary goal of these 
algorithms is to achieve the minimum multiplexing degree that contains all
the connections in the pattern. Different scheduling schemes may be needed for
different patterns. For example, connection scheduling for dense communication
patterns may be different from that for sparse communication patterns.

\subsubsection{Efficient logical topology design for dynamic patterns}

When a logical topology is to be used to route
messages in the communication patterns
that are unknown at compile time, 
 the communication performance depends on three 
factors, the multiplexing degree needed to realize the logical topology, the 
average number of intermediate hops and the processing time in each 
intermediate node. When the logical topology is the same as the physical
topology, no multiplexing is needed (multiplexing degree $= 1$). However,
messages may need to be routed through a large number of intermediate nodes.
When the logical topology is the complete graph, no intermediate hops are
needed for a connection. However, realizing a complete graph may require
a large multiplexing degree. Other choices between these two extremes
may result in better communication performance. 
All possible candidates for the logical topologies (on top of a physical
torus topology) will be examined and compared. 

\subsubsection{Network simulator for compiled communication}

A network simulator will be designed and implemented to study
the communication performance of compiled communication.
The simulator will take the trace and/or communication patterns generated
from compiler analysis of a program as inputs
and simulate compiled communication.
When communication patterns are known at compile time,  off--line 
algorithms will be used to perform the connection scheduling. When 
communication patterns are unknown at compile time, a pre--determined
communication pattern will be used to route the messages. Hence, the
simulator must be able to simulate  single--hop communication for
static patterns and multi--hop communication for dynamic patterns.
Again, it should provide sufficient flexibility similar to the flexibility
of  the simulator for
dynamic communications.

\subsubsection{Performance of compiled communication}

With the simulator, the performance of various off--line
connection scheduling algorithms can be studied and compared.
The performance of various logical topologies can be evaluated. The impact
of various system parameters on compiled communication can also be examined.
Synthesis communication patterns, as well as communication
patterns that come from real application programs, will be used in the 
evaluations.

\subsection{Performance comparison of dynamic communication and compiled 
communication}

Once the above steps are completed, I will be able to compare the 
communication performance of dynamic communication and compiled 
communication using benchmark programs including the floating point benchmark
in SPEC95 and other application programs. This study will quantify
the performance gained by compiling communication for patterns that are known
at compiled time and the performance lost by  routing messages over logical
topologies for  patterns that are unknown 
at compiled time.

\newpage






%% background seciton
\chapter{Background and related work}
\label{backg}

\section{Optical TDM networks}
\label{opnet}

An optical point--to--point network 
consists of switches with a fixed number of input and output ports.
One input port and one output port are used to connect 
the switch to a local processing element and all remaining 
input and output ports of a switch are
used for connections to other switches.   
An example of such networks is the $4 \times 4$ torus shown in 
Figure~\ref{TORI}.
In these networks, each link in the network is time--multiplexed to support 
multiple virtual channels.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/tori.eps,width=3.5in}}
\caption{A torus connected network}
\label{TORI}
\end{figure}

Two approaches can be used to establish connections in multiplexed networks,
namely {\em link multiplexing} (LM) and {\em path multiplexing} (PM) 
\cite{Qiao95}. PM uses the same channel on all
the links along the path to form a connection. On the other hand,
LM may use different 
channels on different links along the path, 
thus requiring time-slot interchange in TDM networks at each 
intermediate node. Fig.~\ref{pmlm} shows the PM and LM connections at a 
$2\times 2$ switch where each link supports two channels.
LM is similar to the multiplexing technique in electronic networks where
a data packet can change channels when it passes a switch. Using LM for 
communication has many advantages over using PM. For example, the path
reservation for a LM connection is simpler than that for a PM connection, and
LM results in better channel utilization. However,
optical devices for LM are still in the research stage and 
are very expensive using current technology. Hence,
this thesis is concerned only with  path multiplexing because
the enabling technology is more mature. 

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/pmlm.eps,height=2.2in}}
\caption{Path multiplexing and link multiplexing}
\label{pmlm}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/pm.eps,width=4.5in}}
\caption{Path multiplexing in a linear array}
\label{PM}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/ts.eps,width=3.5in}}
\caption{Changing the state of a switch in TDM}
\label{TS}
\end{figure}

In order to time--multiplex a network with path multiplexing, 
a {\em time slot} is defined to be a fixed period of time and the time
domain is divided into a repeated sequence of $d$ time slots, where $d$
is  the {\em multiplexing degree}. Different virtual channels on each link
occupy different time slots.
Figures \ref{PM} and \ref{TS} illustrate path multiplexing on a linear
array. In these two figures,  two virtual channels are supported
on each link by dividing the time domain into two time slots, 
and using alternating time
slots for the two channels $c0$ and $c1$.
Let us use $(u,v)$ to denote a connection from node $u$ to node $v$.
Figure \ref{PM} shows four established connections over the two channels,
namely connections
$(0, 2)$ and $(2, 1)$ that are established using channel $c0$, and
connections $(2, 4)$ and $(3, 2)$ that are established using channel $c1$.
The switches, called {\em Time--Multiplexed Switches} (TMS),
are globally synchronized at time slot boundaries, and each switch
is set to alternate between the two states that are needed to realize the
established connections.
For example, Figure~\ref{TS} shows the two states
that the $3 \times 3$ switch attached to PE 2 must realize for the 
establishment of the connections shown in Figure~\ref{PM}.
Note that each switch can be an electro-optical switch
(Ti:LiNbO$_3$ switch, for example
\cite{hinton}) which connects optical inputs to optical outputs without
E/O and O/E conversions. The state of a switch is controlled by 
setting electronic bits in a {\em switch state register}.

The duration of a time slot may be equal to the duration over which 
several hundred bits may be transmitted.
For synchronization purposes, a guard band at each end of a time slot
must be used to allow for changing the state of switches (shifting
a shift register) and to accommodate possible drifting or jitter. For example, 
if the duration of a time slot is $276ns$, which includes a guard band of
$10ns$ at each end, then $256ns$ can be used to transmit data. If the
transmission rate is $1Gb/s$, then a packet of 256 bits can be transmitted
during each time slot. 
Note that
the optical transmission rate is not affected by the relatively
slow speed of changing the state of  switches ($10 ns$)
since that change is performed only every $276 ns$.

Communications in TDM networks can either be {\em single--hop} or 
{\em multi--hop}. In single--hop communication, circuit--switching
style communications  are carried out. A path for a 
communication must be established before the communication starts. 
In general, any $N \times N$ network, other than a completely 
connected network, has a limited connectivity in the sense
that only  subsets, $C = \{ (x, y) | 0\le x, y < N\}$, of the possible
$N^2$ connections can be established simultaneously without conflict.
For single--hop communication the network
must be able to establish any possible connection in one hop, without
intermediate relaying or routing.
Hence,  the network
must be able to change the connections it supports at different times.
This thesis considers switching networks in which the set of connections 
that may be established simultaneously (that is, the state of the network) 
is selected by changing the contents of hardware registers. The single--hop
communication can be achieved in two ways. First, a path reservation
algorithm can be used to dynamically establish and tear down all--optical
connections for arbitrary communications. Second, compiled communication 
uses the compiler to analyze the communication requirement of a program
and insert code to establish all--optical 
connections (at phase boundaries) before communications start.  
Unlike the case in a single--hop system where connections 
are dynamically established and torn down, connections in 
a multi--hop system are fixed and a message may travel through 
a number of lightpaths to reach its destination.
Dynamic single--hop communication,
dynamic multi--hop communication and compiled communication will be discussed
in some details next.

\section{Dynamic single--hop communication with PM}

To establish a connection in an optical TDM  network,
a physical path, $PP$, from the source to the destination is first chosen. 
Then, a virtual path, $VP$, consisting of a virtual channel in each link 
in $PP$ is selected and the connection is established.  The selection 
of $PP$ has been studied extensively and is well understood \cite{Leighton92}.
It can be classified into {\em deterministic} routing, where
$PP$ can be determined from the source node and the destination node
(e.g., X--Y routing on a mesh), 
or {\em adaptive} routing, where  $PP$ is selected from
a set of possible paths. 
Once $PP$ is selected, a time slot is used
in all the links along  $PP$. 

The control in optical TDM networks is responsible for the establishment of a
virtual path for each connection request. 
Due to the similarity of TDM and WDM networks, many techniques for virtual channel assignment in one of these two types of networks can also apply to
the other type.
Network control for multiplexed optical networks can be classified into
two categories, centralized control and  distributed control. 
Centralized control assumes a central controller which maintains the 
state of the whole network and schedules all communication requests. 
Many time slot assignment  and wavelength assignment
algorithms have been proposed for  
centralized control. In \cite{Qiao94} a number
of time slot assignment algorithms are proposed for TDM multi-stage 
interconnection networks. In \cite{Chlamtac92} wavelength assignment for
wide area networks is studied. A time wavelength assignment algorithm for 
WDM star networks is proposed in \cite{ganz92}. Theoretical study
for optimal routing and wavelength assignment for arbitrary networks is
presented in \cite{Ramaswami94}. 

Distributed control does not assume a central controller and thus is more
practical for large  networks. 
Little work has been done on distributed control for 
optical multiplexed networks.
In \cite{Qiao94} a distributed path reservation scheme
for optical Multistage Interconnection  Networks (MIN) is
proposed. Distributed path reservation methods for both path multiplexing and 
link multiplexing are  presented in \cite{Qiao96}. 
This thesis proposes distributed path reservation algorithms that are more
efficient than the previous algorithms, investigates variations in 
channel reservation methods 
and studies the impact of the system parameters on the 
protocols.

\section{Dynamic multi--hop communication with PM}

By using path multiplexing, efficient logical topologies can be established
on top of the physical topology. The connections in the logical topologies
are lightpaths that may span a number of links. In such systems, the 
switching architecture consists of an optical component and an electronic
component. The optical component is an all--optical switch, which can 
switch the optical signal from some input channels to output channels in
the optical domain (i.e., without E/O and O/E conversions), and which can
locally terminate some other lightpaths by directing them to the node's 
electronic component. The electronic component is an electronic packet
router which serves as a store--and--forward electronics overlaid
on top of the optical virtual topology. 
Figure~\ref{ROUTER1} provides a schematic diagram of the architecture of the 
nodal switch in a physical torus topology.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/multiswitch.eps,width=1.5in}}
\caption{A nodal switching architecture}
\label{ROUTER1}
\end{figure}

Since the electronic processing is slow compared to the optical
data transmission, it is desirable to reduce the number of intermediate
hops in a multi--hop network. This can be achieved by having a logical topology
whose connectivity is high. However, realizing a logical topology with a large 
number of connections requires a large multiplexing degree. In a TDM system,
large multiplexing degree results in a large time to transmit a 
packet through a lightpath because every light path is established only
for a fraction of the time. Hence, there exists a performance trade--off
in the logical topology design between a logical topology with large 
multiplexing degree and high connectivity  and  a logical topology with
small multiplexing degree and low connectivity. As will be shown in 
this dissertation, both topologies have advantages for certain types
of communication patterns and system settings.

Multi--hop networks have been extensively studied in the area  of
WDM wide area networks. The works in 
\cite{Bannister90,chlamtac93,Labour91,Mukherjee94,Venkat96} 
consider the realization of logical topologies on optical multiplexed 
networks. These works consider wide area networks and focus on 
designing efficient logical topologies on top of irregular networks. 
Since finding an optimal logical topology on irregular networks
is an NP--hard problem, heuristics and 
simulated annealing algorithms are used to find suboptimal schemes.
This dissertation considers regular networks in multiprocessor 
environments and derives optimal connection scheduling schemes for 
realizing hypercube communications. Besides logical topology design,
connection scheduling algorithms can also be used to realize 
logical topologies. In \cite{Qiao96} message scheduling for permutation
communication patterns in mesh--like networks is considered. 
In \cite{Qiao94} optimal schemes for realizing
all--to--all patterns in multi-stage networks are presented. 
In \cite{Hinrichs94} message scheduling for all--to--all communication 
in mesh--like topologies is described. 

The performance of multi--hop
networks has also been previously studied. However,  
most previous performance studies for optical multi--hop networks assume 
a broadcast based underlying WDM network, such as an optical star network 
\cite{kovacevic95,Sivarajan91}, 
where the major concerns are the number of transceivers in each
node and  the tuning speed of the
transceivers. This thesis studies the logical topologies
on top of a physical torus topology in a TDM network, 
where the major focus is the 
trade--off between the multiplexing degree and the connectivity of a topology.

\section{Compiled communication}

Compiled communication has recently drawn the attention of several 
researchers \cite{Cappello95,Hinrichs95}. Compiled communication
has been used in combination with message passing in the iWarp system 
\cite{Gross89,Gross94,Hinrichs95a}, where
it is used for  specific subsets 
of static patterns. All other communications are handled using  
message passing. The prototype system described in \cite{Cappello95} 
eliminates the cost of supporting multiple communication models. It relies
exclusively upon compiled communication. However, the performance of this
system is severely limited due to frequent dynamic reconfigurations of 
the network. 
%This frequency can be reduced in  optical networks through
%the use of TDM. 
Compiled communication is more beneficial in optical multiplexed 
networks. Specifically, it reduces the control overhead, which is 
one of the major factors that limit the communication performance 
in optical networks. Moreover, multiplexing, which is natural in optical
interconnection networks, enables a network to support simultaneously more
connections  than  a non--multiplexed  network, which
reduces the reconfiguration overhead in compiled communication.

The communication patterns in an application program can be broadly
classified into two categories: {\em static patterns} that can be
recognized by the compiler and {\em dynamic patterns} that are only 
known at run-time. For a static pattern, compiled
communication computes a minimal set of network configurations that 
satisfies the connection requirement of the pattern and thus,  
handles  static patterns with high efficiency.
Recent studies \cite{Lahaut94} have shown that about 95\% of the 
communication patterns in scientific programs are static patterns.
Thus, using the compiled communication technique to improve the 
communication performance for the static patterns is likely to 
improve the overall communication performance. 
Some advantages of using compiled communication for
handling static patterns are as follows.

\begin{itemize}
\item Compiled communication totally eliminates the
path reservation and the large startup overhead associated with
the path reservation. 

\item The connection scheduling algorithm is executed 
off-line by the compiler. Therefore,  complex strategies 
can be employed to improve network utilization. 

\item No routing decisions are made at runtime which means that 
the packet header can be shortened causing the network 
bandwidth to be utilized more effectively.

\item Optical networks efficiently support multiplexing
which reduces the chance of network reconfigurations due to the lack of
network capacity. 

\item Compiled communication adapts to the 
communication requirement in each phase. For example, it can use different
multiplexing degrees for different phases in a program. In contrast,
dynamic communications always use the same configuration to handle 
all communications in a program which may not be optimal.

\end{itemize}

In order to apply compiled communication to a large scale multiprocessor 
system, three main problems must be addressed: 

\begin{description}
\item
{\bf Communication Pattern Recognition:}
This problem has been considered by 
many researchers since information on communication patterns has been
previously used to perform communication optimizations
\cite{Bromley91,Gupta92,Hinrichs95,Li91}. The stencil compiler 
\cite{Bromley91} for CM-2 recognizes {\em stencil} communication
patterns. Chen and Li \cite{Li91} incorporated a pattern extraction 
mechanism in a compiler to support the use of collective communication 
primitives. Techniques for recognizing a broad set of communication 
patterns were also proposed in \cite{Gupta92}. 
However, most of these methods determine a specific subset of static
communication
patterns, such as the broadcast pattern and the nearest neighbor pattern, 
which is  not sufficient for compiled communication. Since the 
communication performance of compiled communication relies heavily
on the precision of the communication analysis, it is desirable to 
perform more precise analysis that can recognize arbitrary communication
patterns. Furthermore, compiled communication
requires the partitioning of a program into phases, such that each 
phase contains communications that can be supported by the underlying 
network, and the scheduling of connections within each phase. These are
new problems that must be addressed.

\item
{\bf Compiling Static Patterns:} Once the compiler determines a 
communication pattern within each phase, which is called a 
{\em static pattern}, the compiler must be able to schedule the
communication pattern on the multiplexed network. 
In TDM networks, communication performance
is proportional to the multiplexing degree. Given a 
communication pattern, the smaller the multiplexing degree, the
less time the communication lasts. Thus, connection scheduling
algorithms that schedule all connection requests in a phase with
a minimal multiplexing degree must be designed to handle the static
patterns. It has been shown that optimal message scheduling 
for arbitrary topologies is NP-complete \cite{Chlamtac92}.
Hence, heuristic  algorithms that provide good performance need to be 
developed.

\item
{\bf Handling Dynamic Patterns:} A number of techniques can be used to
handle dynamic communication patterns. 
One approach is to setup all-to-all connections among 
all nodes in the system. This way each node has a time slot to 
communicate with every other node. However, establishing paths for 
the all-to-all communication can be prohibitively expensive for large systems.
An alternative is to perform dynamic single--hop or multi-hop communications. 
The dynamic communications are not as efficient as compiled communication. 
However, since this method is not used frequently, its
effect on the overall performance is limited. 

\end{description}

\section{Programming and machine model}

Compiled communication requires the compiler to extract  communication
patterns from application programs. 
The method to extract communication patterns in a program
depends on both programming model and  machine model.
The programming model includes the ones using
explicit communication primitives and the ones that require implicit
communication through remote memory references. There are two 
different machine architectures, the shared memory machine and the
distributed memory machine. Communication requirements for these two
machine models are different for a program.  In 
shared memory machines with hardware cache coherence, communications
result from cache coherence traffic, while 
in distributed memory machines,
communications  result from data movements  between
processors. 

{\bf Explicit communication}:
Most of the current commercial distributed memory
supercomputers support the explicit communication programming
model. In such programs, programmers explicitly use  communication 
primitives to perform the communication required in a  program.
The communication primitives can be high level library routines, such 
as PVM \cite{PVM94} or MPI \cite{MPI93}, or low level communication primitives
such as the shared memory operations in the CRAY T3D \cite{Numrich94} and
the CRAY T3E. Communication
patterns in a  program with explicit communication
primitives can be obtained from the
analysis of the communication primitives in the program.

{\bf Implicit communication}: Managing explicit communication is tedious
and error-prone. This has motivated considerable research towards 
developing compilers that relieve programmers from  the burden of 
generating communication
\cite{amarasinghe93,banerjee95,gupta95,hiran92,rogers89,zima88}.
Such compilers take sequential or shared memory 
parallel programs and generate Single Program Multiple Data (SPMD)
programs with explicit message
passing. This type of programs will be referred to as 
{\em shared memory programs}.
Shared memory programs can be compiled for execution on both
distributed memory machines and shared memory
machines. In the case when a program is to be run on a distributed memory
machine,
the communication requirements of the program can be obtained from 
memory references. 
If a program is to be run on a shared memory machine,
the communication requirements
depend on the cache behavior. However,
a superset of the communication patterns may be obtained by examining the
memory references in the program. 
This work will consider data parallel
shared memory programs compiled for execution on
distributed memory machines. 
Compilers that exploit task parallelism \cite{Gross94a,Subhlok93}
are not considered. However, similar techniques may also apply to 
task parallel programs.

\section{Compilation for distributed memory machines}
\label{comp}

While communication requirements of a shared memory
program can be obtained by analyzing
the remote memory references in the program, the actual communication patterns
in the program depend on the compilation techniques used. 
To obtain realistic communication patterns,
 compilation techniques for compiling 
shared memory programs for  distributed memory machines
 must be considered. 
The most important issues to be addressed
when compiling for distributed memory machines
are data partitioning,  code generation and 
communication optimization. This section surveys previous work
 on these issues.

{\em Data partitioning} decides the distribution of array elements
to processors. There are
two approaches for handling the data partitioning problem. 
The first approach is to add user directives to programming languages
and let the users  
specify the data distribution. This approach is used in Fortran D 
\cite{hiran92}, Vienna Fortran \cite{Chapman92} and High Performance
Fortran (HPF) \cite{HPF} among others. 
It uses  human knowledge of  
application programs and simplifies the compiler design. However, using this
approach requires programmers to work at a low level abstraction 
(understanding the detail of memory layout). Since the best placement 
decision will vary between different architectures, with explicit user
placement, the programmer must reconsider the data placement for each
new architecture. Hence, many algorithms have been developed to
perform automatic data distribution. An algorithm has been designed 
for the CM Fortran compiler that attempts to minimize and identify 
alignment communications in data parallel Fortran programs \cite{Knobe90}.
Similar algorithms have been proposed in 
\cite{Chatterjee93,Gupta92a,Hinrichs95,Li91a}.
Data partitioning directly affects the communication
requirements in a program running on a distributed memory machine. Once 
data partitioning is decided, the minimum requirement of data movements in
a program, which results in communications, is fixed.

{\em Code generation} generates the communication code to ensure the
correctness of a program. The {\em Owner computes} rule is generally used 
for distributing the computation onto processors. 
Under owner computes rule, the owner of the array
element on the
left hand side of an assignment statement executes the statement. Thus, the
owner of an array element on the right hand side of the assignment statement
must send the element to the owner of the left hand side, which results
in communication. Without considering efficiency, a simple scheme
can be used to generate the correct SPMD
 code by inserting guarded communication
primitives \cite{rogers89}. 
However, the communication and synchronization overhead of this scheme can 
be so
large that there may be 
no benefit for running the program on a multiprocessor system.
Several researchers have 
proposed techniques for generating efficient code for array statements, 
given {\em block}, {\em cyclic} and {\em block--cyclic} distributions. 
In \cite{Koelbel90,Koelbel91} compile time analysis of array 
statement with block and cyclic distribution is presented.
In \cite{Chatterjee93a} Chatterjee et al. present a framework for compiling
array assignment statements in terms of constructing a finite state machine. 
This method handles block, cyclic and block--cyclic distributions. Method
in \cite{Stichnoth93} improves Chatterjee's method in terms of buffer space and
communication code generation overheads. Other compilers
\cite{amarasinghe93,banerjee95,gupta95,hiran92,rogers89,zima88} use
communication optimization to generate efficient code for programs
on distributed memory machines. Different ways of code 
generation result in different communication patterns at runtime.
 For example, the compiler may 
decide to send/receive all elements in an array to speed up the 
communication. It may also decide to send/receive one element
 at a time to save buffer space. 

{\em Communication optimizations} reduce the cost of  communication 
in a program.
Communication performance not only
affects the performance of a  parallel application but also limits
its scalability. Therefore, communication optimization is 
crucial for the performance of programs compiled for a
distributed memory machine. Many communication optimizations are applied
within a single loop using data dependence information. Examples of such
optimizations include message vectorization \cite{hiran92,zima88}, 
collective communication \cite{Gupta92,Li91}, message coalescing
\cite{hiran92} and 
message pipelining
\cite{gupta95,hiran92}. Earlier methods are based on
 {\em location based} data dependence, which is not precise since it 
only determines whether two references refer to the same memory 
location. Later schemes refine the information and use 
{\em value based} data dependence \cite{amarasinghe93}.  
In value based data dependence, a read
reference depends on a write reference only if the write provides 
the value
for the read reference.

Communication optimizations based only on  data dependence information
usually result in redundant communications \cite{Chakrabarti96}. 
The more recently developed
optimizations use data flow information to reduce redundant communication
and perform other optimizations. In \cite{Gong93} 
a data flow framework which can integrate
a number of communication optimizations is presented. However, the method
 can only apply to a very small subset of programs 
which are constrained
in the forms of loop nests and  array indices. In \cite{Gupta96} a unified 
framework which uses global array data flow analysis for communication 
optimizations is 
described. Since only a very simplified version of the analysis algorithm
is implemented, it is not clear whether this approach is practical for large
programs. In \cite{Chakrabarti96,Kennedy95} methods 
that combine traditional data flow analysis 
techniques with data dependence analysis for performing global
communication optimizations are described. 
These schemes are very efficient in terms of their 
analysis cost since bit vectors are used to represent  data flow 
information. However, they cannot obtain the array data flow information 
that is as 
precise as the information computed using
array data flow analysis approaches.
Communication optimization changes the communication behavior of a program.
Since many communication optimizations are commonly used in production
compilers, these optimizations
must be considered to obtain realistic communication patterns in a program.

%Notice that most of the communication optimizations
%described above  assume the traditional communication model, 
%which might not be true in compiled communication. 
%Therefore, it is interesting to investigate
%useful communication optimizations under the assumption of
%compiled communication.
%\newpage

%\section{Chapter summary}

%In this chapter, I describe the multiplexing techniques used in optical
%interconnection networks, discuss the issues in dynamic single--hop
%communication, dynamic multi--hop communication and compiled 
%communication and survey previous results in these areas. 
%I have also summarized the related research in the 
%compilation for distribution memory machines. This chapter gives 
%the background and surveys previous research related to the 
%thesis. In the next chapter, I will discuss the techniques for 
%dynamic single--hop communication. 



\chapter{Compiled communication}
\label{compiled}

%Recently many researchers have shown that communication performance
%of dense matrix  applications can be greatly improved 
%by allowing network resources to be managed by compiler and using 
%the {\em compiled communication} technique\cite{Cappello95,Hinrichs95,Yuan96}.
In compiled communication, the compiler analyzes a program to determine
its communication requirement. The compiler can then
use the knowledge of the underlying
architecture, together with the knowledge of the communication requirement,
to manage network resources statically. As a result, 
runtime communication overheads, such as the path reservation overhead 
and the buffer allocation overhead, can be reduced or eliminated, 
and the communication performance can be improved.
Due to the limited resources, the underlying network  
cannot support arbitrary communication patterns. 
Thus, compiled communication
requires the compiler to analyze a program and partition
the program into phases such that each phase has a fixed, pre-determined 
communication pattern that the underlying network can support.
The compiler inserts code to reconfigure the network at  
phase boundaries, uses the knowledge of the 
communication requirement within each
phase to manage network resources directly, 
and optimizes the communication 
performance. 

A number of compiler issues must be addressed 
in order to apply the compiled communication technique 
to optical TDM networks.  Specifically, 
given a multiplexing degree, the compiler must
partition a program into phases such that each phase contains connections that
can be realized by the underlying network with the given multiplexing degree.
To obtain good performance, each phase must contain as much communication
locality as possible so that less reconfiguration overhead will be incurred
at runtime. A compiler, called the E-SUIF (extended SUIF) compiler, 
is implemented to support compiled communication.
The structure of the compiler is shown in Figure~\ref{overall}. 
There are four major components in the 
system. The first component is the {\em communication analyzer} that
analyzes a program and obtains
its communication requirement on virtual processor
grids. The second component is the {\em virtual to physical processor 
mapping} subsystem that computes the 
communication requirement of a program on physical processors.
The third component is the {\em communication phase analysis} subsystem that
partitions the program into phases such that each phase contains 
communications that the underlying network
can support. The communication phase analysis utilizes a fourth component
of the system, the {\em connection scheduling algorithms}, to realize 
a given communication pattern with a minimal number of channels. 


\begin{figure}
\centerline{\psfig{figure=fig/overall.eps,height=3.5in}}
\caption{The major components in the E--SUIF compiler}
\label{overall}
\end{figure}
 
Next, the programming model of the compiler will be discussed, followed
by the four components needed to support compiled communication.

\section{Programming model}

The E--SUIF compiler considers structured HPF--like programs 
that contain conditionals 
and nested loops, but no arbitrary goto statements. The programmer
explicitly specifies the data alignments and distributions. 
For simplicity, this chapter assumes that all arrays are aligned to a 
single virtual processor grid template, and the data distribution is 
specified through the distribution of the template.
However, the implementation of the communication analyzer
handles multiple virtual processor grids. 
Arrays are aligned to the virtual processor grid by 
simple affine functions. The alignments allowed are scaling, 
axis alignment and  offset alignment.  The mapping from a point 
$\vec{d}$ in data space to the
corresponding point $\vec{e}$ on the virtual processor grid is specified by
an alignment matrix $M$ and an alignment offset vector $\vec{v}$. 
$\vec{e} = M \vec{d} + \vec{v}$. 
The alignment matrix $M$ specifies the scaling and the axis alignment, thus
it is a permutation of a diagonal matrix. 
The distribution of the virtual 
processor grid can be cyclic, block or block--cyclic. Assuming that there
are $p$ processors in a dimension, and the block size of that 
dimension is $b$, the virtual processor $e$ is in physical processor
$e\ mod\ (p*b) / b$. For cyclic distribution, $b=1$. For block distribution,
$b=n/p$, where $n$ is the size of the virtual processes along the dimension. 


The communication analyzer performs communication optimizations
on each subroutine. 
A subroutine is represented by an {\em
interval flow graph} $G = (N, E)$, with nodes N and edges E.
The communication optimizations are 
based upon a variant of Tarjan's intervals \cite{Tarjan74}.
The optimizations require that there are no {\em critical edges}
which are edges that 
connect a node with multiple outgoing edges to a node with multiple 
incoming edges. The critical edges can be eliminated by edge splitting 
transformation\cite{Gupta96}. Figure~\ref{EXAMPLE} shows
an example code and its corresponding interval flow graph.
%Notice that intervals can be easily identified
%in SUIF's hierarchical intermediate representation.


\begin{figure}[tbph]
%\begin{subfigRow*}
\begin{minipage}{10cm}
%\small
%\footnotesize
\begin{tabbing}
\hspace{0.5in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
\hspace{0.5in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
\hspace{0.5in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
\hspace{0.5in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
\hspace{0.5in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
\hspace{0.5in}(s4)\hspace{0.1in}\>enddo\\
\hspace{0.5in}(s5)\hspace{0.1in}enddo\\
\hspace{0.5in}(s6)\hspace{0.1in}do i = 1, 100\\
\hspace{0.5in}(s7)\hspace{0.1in}\>do j = 1, 100\\
\hspace{0.5in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
\hspace{0.5in}(s9)\hspace{0.1in}\>enddo\\
\hspace{0.5in}(s10)\hspace{0.1in}enddo\\
\hspace{0.5in}(s11)\hspace{0.1in}do i = 1, 100\\
\hspace{0.5in}(s12)\hspace{0.1in}\>do j = 1, 100\\
\hspace{0.5in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
\hspace{0.5in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
\hspace{0.5in}(s15)\hspace{0.1in}\>end do\\
\hspace{0.5in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
\hspace{0.5in}(s17)\hspace{0.1in}end do\\
\end{tabbing}

\end{minipage}

\begin{minipage}{10cm}
\centerline{\psfig{figure=fig/3.eps,width=4in}}
\end{minipage}
%\end{subfigRow*}
\normalsize
\caption{An example program and its interval flow graph}
\label{EXAMPLE}
\end{figure}

\section{The communication analyzer}
\label{commlab}

The communication analyzer analyzes the communication
requirement on virtual processor grids and performs a number of common
communication optimizations. This section
presents the data flow descriptor used in the analyzer 
to describe communication, the general data flow algorithms to propagate
the data flow descriptor, and the communication optimizations
performed by the analyzer. 

\subsection{Section communication descriptor (SCD)}

In order for the compiler to analyze the communication requirement of a 
program, data structures must be designed for the compiler to 
represent the communications in the program. 
The data structures must both be powerful enough to represent 
the communication requirement and simple enough to be  manipulated easily.
%A communication descriptor, call {\em Section Communication Descriptor} (SCD),
%is designed for the analyzer.
%It can be used for both communication 
%optimization and to derived the communication pattern in physical processor
%space. 

\subsubsection*{The descriptor}

The communication analyzer represents communication using
{\em Section Communication Descriptor} (SCD).
% This subsection
%describes the format of the descriptor.
A $SCD = <A, D, CM, Q>$ consists of
 three components.
The first component is the array region that is involved in the
communication. This includes the array name $A$ and the array region
descriptor $D$. The second component is the communication mapping descriptor
$CM$, which describes the source--destination relationship of 
the communication.
The third component is a qualifier descriptor $Q$, which specifies the time
when the communication is performed.

The {\em bounded regular section descriptor} (BRSD)\cite{callahan88} is used
as the region descriptor. The region $D$ is a vector of subscript values.
Each element in the vector is either
(1) an expression of the form $\alpha*i + \beta$, where
$\alpha$ and $\beta$ are invariants and i is a loop
index variable, or (2) a triple
$l:u:s$, where $l$, $u$ and $s$ are invariants. The triple,
 $l:u:s$, defines a set of values, $\{l$, $l+s$, $l+2s$, ..., $u\}$, as used
in the array statement in HPF.

The source--destination mapping $CM$ is denoted as
$<src, dst, qual>$. The source, $src$, is a vector whose elements are
of the form $\alpha*i + \beta$, where $\alpha$ and $\beta$ are
invariants and $i$ is a loop index variable. The destination,
$dst$, is a vector whose elements are of the form
$\gamma*j + \delta$, where
 $\gamma$ and $\delta$ are
invariants and $j$ is a
loop index variable. The {\em mapping qualifier} list,
$qual$, is a list of range descriptors. Each range descriptor
is  of the form $i = l:u:s$, where $l$, $u$ and $s$ are invariants and
$i$ is a loop index variable.
The notation $qual=NULL$ and $qual = \perp$ denote that
no mapping qualifier is needed.
The mapping qualifier specifies the range of a variable in $dst$ that
does not occur in $src$ to express the broadcast effect.

The qualifier $Q$ is a range descriptor of the form $i = l:u:s$,
where $i$ is the loop index variable of the loop that directly
encloses the SCD. This qualifier is used to indicate the
iterations of the loop in which the SCD should be performed.
If the SCD is to be performed in every iteration in the loop,
$Q=NULL$ or $Q = \perp$. $Q$ will be referred to as the
{\em communication qualifier}.
Notice that the qualifiers in most SCDs are NULL.

\subsubsection*{Operations on SCD}

Operations, such as intersection, difference and union, on SCD descriptors
are defined next. Since in many cases, operations do not have sufficient
information to yield  exact results, {\em subset} and {\em superset} versions 
of these operations are implemented. The analyzer uses a proper
version to obtain conservative approximations. These operations
are extensions of the operations on BRSD. 
%Since SCD 
%descriptors are composed of three parts, their operations usually reduce to
%operations on their components. 

\noindent
{\bf Subset Mapping testing}. Testing whether a mapping is a subset of another
mapping is one of the most commonly used operations in the analyzer.
Testing that a mapping relation $CM_1$ ($= <s_1, d_1, q_1>$) is a subset of
another mapping relation $CM_2$ ($= <s_2, d_2, q_2>$) is
done by checking for a solution of equations $s_1 = s_2$ and $d_1 = d_2$, 
where variables in $CM_1$ are treated as constants and variables in $CM_2$ 
as variables,  and
subrange testing $q_1 \subseteq q_2$. 
%The equations 
%$s_1 = s_2$ and $d_1 = d_2$ can easily be solved by 
%treating variables in $M_1$ as constants and 
%variables in $M_2$ as variables. 
Note that since the elements
in $s_1$ and $s_2$ are of the form $\alpha*i+\beta$, the equations can
generally be solved efficiently. Two mappings, $CM_1$ and $CM_2$ are 
{\em related}
if $CM_1 \subseteq CM_2$ or $CM_2 \subseteq CM_1$. Otherwise, they are
unrelated.

\noindent
{\bf Subset SCD  testing}. Let
$S_1=<A_1, D_1,CM_1, Q_1>$, $S_2=<A_2, D_2,CM_2, Q_2>$,
$SCD_1 \subseteq SCD_2 \Longleftrightarrow A_1 = A_2 \wedge
D_1 \subseteq D_2\wedge CM_1 \subseteq CM_2 \wedge Q_1 \subseteq Q_2$.

\noindent
{\bf Intersection Operation}. The intersection of two SCDs represents the 
elements constituting the common part of their array sections that have the 
same mapping relation. The following algorithm describes the subset version of
the intersection operation. 
%The superset version can just return any of the two SCDs.
Note that the operation requires the qualifier $Q_1$ to be equal to $Q_2$ to 
obtain a non empty result. $\phi$ denotes an empty set.
This approximation will not hurt the 
performance significantly since most 
SCDs have $Q=\perp$.

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> \cap <A_2, D_2, CM_2, Q_2>$\\ 
\hspace{0.2in}= $\phi$,  if $A_1 \ne A_2$ or $CM_1$ and $CM_2$ are 
                             unrelated or 
                            $Q_1 \ne Q_2$\\
\hspace{0.2in}= $<A_1, D_1\cap D_2, CM_1, Q_1>$,  if $A_1 = A_2$ and $CM_1 
                                            \subseteq CM_2$ and $Q_1 = Q_2$\\
\hspace{0.2in}= $<A_1, D_1\cap D_2, CM_2, Q_1>$,  if $A_1 = A_2$ and $CM_1 
                                            \supseteq CM_2$ and $Q_1 = Q_2$
\end{tabbing}

\noindent
{\bf Difference Operation}. The difference
 operation causes a part of the array 
region associated with the first operand to be invalidated at all the 
processors where it was available. In the analysis, the difference operation
is only used to subtract  elements killed (by a statement, or by a 
region), which means that the SCD to be subtracted always has 
$CM=\top$ and  $Q=\perp$. 

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> - <A_2, D_2, \top, \perp>$\\
\hspace{0.2in}= $<A_1, D_1, CM_1, Q_1>$,  if $A_1 \ne A_2$\\
\hspace{0.2in}= $<A_1, D_1 - D_2, CM_1, Q_1>$, if $A_1 = A_2$.
\end{tabbing}


\noindent
{\bf Union operation.} The union of two SCDs represents the 
elements that can be in either part of their array section.
This operation is given by:

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> \cup <A_2, D_2, CM_2, Q_2>$\\
\hspace{0.2in}= $<A_1, D_1\cup D2, CM_1, Q_1>$, if $A_1 = A_2$ and 
                                            $CM_1 = CM_2$ and $Q_1=Q_2$\\
\hspace{0.2in}= list($<A_1, D_1, CM_1, Q_1>$, $<A_2, D_2, CM_2, Q_2>$), 
                otherwise.
\end{tabbing}

\subsection{A demand driven array data flow analysis framework}
\label{arraydflow}

Many communication optimization opportunities can be uncovered by
propagating SCDs globally. For example, if a SCD
can be propagated from a loop body to the loop header without being killed
in the process of propagation, the communication represented by the SCD
can be hoisted out of the loop body, that is,
 the communication can be vectorized.
Another example is the redundant communication elimination.
While propagating $SCD_1$,  if $SCD_2$ is encountered such that $SCD_2$ is 
a  subset of the $SCD_1$, then the communication represented by $SCD_2$ can
be subsumed by the communication represented by $SCD_1$ and can be eliminated.
Propagating  SCDs backward can find the earliest point to place the 
communication, while propagating SCDs forward can find the latest point where
the effect of the communication is destroyed. Both these
two propagations are useful in communication optimizations. 
Since forward and backward propagation
are quite similar, only backward propagation will be presented next.

Generic demand driven algorithms are developed to propagate
SCDs through interval flow graph. The analysis technique is
the reverse of the interval-analysis \cite{gupta93}.
Specially, by reversing the information flow associated with program points,
a system of request propagation rules is designed.
SCDs are propagated 
until they cannot be propagated any further, 
that is, all the elements in the SCDs are
killed. However, in practice, the compiler may choose to 
terminate the propagation prematurely to
save analysis time while there are still elements in SCDs. 
In this case, since the analysis starts from the 
points that contribute to the  optimizations,
the points that are textually close to the starting points, where
most of the optimization opportunities are likely to be 
present, are considered.
This gives the demand driven algorithm the ability to trade precision for
time. In the propagation, at a given time, only a single interval
is under consideration. Hence, the propagations are logically done in
an acyclic flow graph. During the propagation, a 
SCD may expand when it is propagated out of a loop. When a set of
elements of SCD is killed inside a loop, the set is propagated into the loop
to determine the exact point where the elements are killed. There are 
two types of propagations,
{\em upward} propagation, in which SCDs may need to be 
expanded, and {\em downward} propagation, in which SCDs may need to be 
shrunk. 

The format of a data flow {\em propagation request}
is  $<S, n, [UP|DOWN], level, cnum>$, where S is a SCD, n is a node
in the flow graph, constants $UP$ and $DOWN$ indicate whether the request is  
upward propagation  or downward propagation, $level$ indicates
at which level is the request and the value $cnum$ 
indicates which child node of 
$n$ has triggered the request. A special value $-1$ for $cnum$ is used as
the indication of the beginning of downward propagation.
The propagation request triggers 
some local actions and causes the propagation of a SCD from the node n. 
The propagation of SCDs follows the following rules. It is  assumed that node 
$n$ has $k$ children.

\subsubsection*{Propagation rules}

\noindent
{\bf RULE 1: upward propagation: regular node}. 
The request on a regular node takes an action based
 on SCD set $S$ and the local
information. It also propagates the information upward.
The request stops when S become empty. The rule is shown in the following 
pseudo code. In the code, functions $action$ and $local$
are depended on the type of optimization to be performed.
The $pred$ function finds all the nodes that are  predecessors in the 
interval flow graph and the 
set $kill_n$ includes all the elements defined in node
$n$. Note that $kill_n$ can be represented as an SCD.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S_1, n, UP, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, UP, level, k>$) : \\
\hspace{0.2in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.2in}\>action(S, local(n))\\
\hspace{0.2in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - kill_n, m, UP, level, j>$)\\
\end{tabbing}

A response to requests in a node $n$ 
occurs only when all its successors have been processed. This 
guarantees that in an acyclic flow graph
each node will only be processed once. The side effect is that 
the propagation will not pass beyond a branch point.
A more aggressive scheme can 
propagate a request through a node without checking whether
all its successors are processed. In that scheme, however, a nodes may need 
to be processed multiple times to obtain the final solution.    

\noindent
{\bf RULE 2: upward propagation: same level loop header node}.
The loop is contained in the current level. The request needs to obtain the
summary information, $K_n$, for the interval, perform the action
based on $S$ and the summary information, propagate the information past 
the loop and trigger a downward propagation to propagate the information
into the loop nest. Here, 
the summary function $K_n$, summarizes all the elements defined
in the interval. 
It can  be calculated either before hand or in a 
demand driven manner. The method to 
calculate the summary in a demand driven manner will be described later. 
Note that a loop header can only have one successor besides the 
entry edge into the loop body. The $cnum$ of the downward request
is set to -1 to indicate that it is the start of the downward propagation.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{0.2in}\>action(S, $K_n$) \\
\hspace{0.2in}\>if\=\ ($S - K_n \ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - K_n, m, UP, level, j>$)\\
\hspace{0.2in}\>if ($S\cap K_n \ne \phi$) then\\
\hspace{0.2in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$)\\
\end{tabbing}

\noindent
{\bf RULE 3: upward propagation: lower level loop header node}.
The relative level between the propagation request
and the node can be determined by 
comparing the level in the request and the level of the node. Once a request
reaches the loop header. The request will need to be expanded to be 
propagated in the upper level. At the same time, this request triggers 
a downward propagation for the set of elements that are killed in the loop. 
Assume that the loop index variable is $i$ with bounds $low$ and $high$.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{0.2in}\>calculate the summary of loop $n$\\
\hspace{0.2in}\>outside = $expand(S, i, low:high) - 
                     \cup_{def}expand(def, i, low:high)$\\ 
\hspace{0.2in}\>inside = $expand(S, i, low:high) \cap 
                     \cup_{def}expand(def, i, low:high)$\\
\hspace{0.2in}\>if\=\ (outside $\ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<outside, m, UP, level -1, j>$)\\
\hspace{0.2in}\>if (inside $\ne \phi$) then\\
\hspace{0.2in}\>\>request($<inside,  n, DOWN, level, -1>$)\\
\end{tabbing}

The variable $outside$ contains the elements that can be propagated out of
the loop, while the variable $inside$ contains the elements that are killed
within the loop. The expansion function has the same definition as in 
\cite{gupta93}. For a SCD descriptor S, expand(S, k, low:high) is a 
function which replaces
all single data item references $\alpha*k+\beta$ used in any
array section descriptor D in S by the triple ($\alpha*low+\beta:
\alpha*high+\beta:\alpha$). 
The set $def$ includes all the definitions that are the source of a
flow-dependence.

\noindent
{\bf RULE 4: downward propagation: lower level loop header node}.
This is the initial downward propagation. The loops index variable, $i$, 
is treated as a constant in the downward propagation. 
Hence, SCDs that are propagated into the loop body 
must be changed to be the initial
available set for iteration $i$, that is, subtract all the variables 
killed in the iteration i+1 to high and propagate the information from the 
tail node  to the head node. This propagation prepares the downward 
propagation into the loop body by shrinking the SCD for each iteration.

\begin{tabbing}
\hspace{0.2in}qu\=ery($<S, n, UP, level,cnum>$): \\
\hspace{0.2in}\>if\=\ $(cnum = -1)$ then\\
\hspace{0.2in}\>\>calculate the summary of loop $n$;\\
\hspace{0.2in}\>\>request($<S - \cup_{def}expand(def, k, i+1:high), 
                    l, DOWN, level-1, 1>$);\\
\hspace{0.2in}\>else\\
\hspace{0.2in}\>\> STOP /* interval processed */\\
\end{tabbing}

\noindent
{\bf RULE 5: downward propagation: regular node}.
For regular node, the downward propagation is similar to the upward
propagation.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S_1, n, DOWN, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, DOWN, level, k>$) : \\
\hspace{0.2in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.2in}\>action(S, local(n))\\
\hspace{0.2in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - kill_n, m, DOWN, level, j>$)\\
\end{tabbing}

\noindent
{\bf RULE 6: downward propagation: same level loop header node}.
When downward propagation reaches a loop header (not the loop header
whose body is being processing), it must generate further downward
propagation request to go deeper into the body.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, DOWN, level, 1>$): \\
\hspace{0.2in}\>action(S, summary(n)); \\
\hspace{0.2in}\>if\=\ ($S-K_n \ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - K_n, m, DOWN, level, j>$);\\
\hspace{0.2in}\>if\=\ ($S\cap K_n \ne \phi$) then\\
\hspace{0.2in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$);\\
\end{tabbing}

\subsubsection*{Summary calculation}

During the request propagation, the summary information of an interval is
needed when a loop header is encountered. 
An algorithm is described 
to obtain the summary information in a demand driven manner.
The calculation of kill set of the interval is used as an example. Let 
$kill(i)$ 
be the variables killed in node $i$, $K_{in}$  and $K_{out}$ 
be the variables killed before and after the node respectively.
Figure.~\ref{KILL} depicts the demand driven algorithm. The 
algorithm propagates the data flow information from the tail node to the 
header node in the interval using the following data flow equation:\\

\centerline{$K_{out}(n) = \cup_{s\in succ(n)}K_{in}(s)$}
\centerline{$K_{in}(n) = kill(n)\cup K_{out}(n)$}

When an inner loop header is
encountered, a recursive call is issued to get the summary information
for the inner interval. Once a loop header is reached, the kill set needs
to be expanded to be used by the outer loop.


\begin{figure}[htbp]
\begin{tabbing}
\hspace{0.2in}(1)\hspace{0.5in}Su\=mmary\_kill(n)\\
\hspace{0.2in}(2)\hspace{0.5in}\>$K_{out}(tail)$ = $\phi$\\
\hspace{0.2in}(3)\hspace{0.5in}\>fo\=r all $m\in T(n)$ and 
                               level(m) = level(n)-1 in backward order\\
\hspace{0.2in}(4)\hspace{0.5in}\>\>if\=\ m is a loop header then\\
\hspace{0.2in}(5)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                    K_{in}(s)$\\
\hspace{0.2in}(6)\hspace{0.5in}\>\>\>$K_{in}(m)$ = summary\_kill(m) $\cup
                                   K_{out}(m)$\\
\hspace{0.2in}(7)\hspace{0.5in}\>\>else\\ 
\hspace{0.2in}(8)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                   K_{in}(s)$\\
\hspace{0.2in}(9)\hspace{0.5in}\>\>\>$K_{in}(m)$ = $kill(m) \cup K_{out}(m)$\\
\hspace{0.2in}(10)\hspace{0.5in}\>return (expand($K_{in}(header)$, i, low:high))\\
\end{tabbing}
\caption{Demand driven summary calculation}
\label{KILL}
\end{figure}


\subsection{The analyzer}

The analyzer performs message vectorization, redundant communication 
elimination and communication scheduling using algorithms based upon
the demand driven algorithms described in the previous section. 
The analyzer performs the following steps:

\begin{enumerate}
\item {\em Initial SCD calculation}. 
Here the analyzer calculates the 
communication requirement for each statement that contains remote
memory references. Communications required
by each statement
are called {\em initial SCDs} for the statement and 
are placed preceding the statement.

\item {\em Message vectorization and available communication summary 
calculation}. The analyzer propagates
initial SCDs to the outermost loops in which they
can be placed. 
In addition to message vectorization optimization, 
this step also calculates
the summary of communications that are available after each loop. This 
information is used in the next step
for redundant communication elimination.

\item {\em Redundant communication elimination}. 
The analyzer performs redundant communication elimination using a demand 
driven version
of availability communication analysis \cite{gupta93}, which computes 
communications that are available before each statement. A communication
in a statement 
is redundant if it can be subsumed by available communications at
the statement. The analyzer also eliminates partially redundant 
communications. 

\item {\em Message scheduling}. 
The analyzer schedules messages
within each interval by placing messages with the same communication patterns
together and combining the messages to reduce the number
of messages. 
\end{enumerate}

\subsubsection*{Initial SCD Calculation}

The {\em owner computes} rule is assumed which 
requires each remote item referenced on the right handside of an 
assignment statement to be sent to the processor that owns the left
handside variable.
Initial SCDs for each statement represent this data movement.
Since the ownership of array elements determines  communication patterns,
the ownership of array elements will be described before the initial
SCD calculation step is presented.

\subsubsection*{Ownership.}

All arrays are aligned to a single virtual processor grid by 
affine functions. 
The alignments allowed are scaling, axis alignment and 
offset alignment.  The mapping from a point $\vec{d}$ in data space to a 
corresponding point $\vec{e}$ on the virtual processor grid (the owner of 
$\vec{d}$) can be specified by
an alignment matrix $M$ and an alignment offset vector $\vec{v}$
such that
$\vec{e} = M \vec{d} + \vec{v}$.  Using the alignment matrix and the 
offset vector, the owner of a data element
can be determined. Consider the 
 array $w$ in the example program in 
Figure~\ref{EXAMPLE}, the alignment matrix and the offset vector
 are given below.
\begin{center}
\small
\[ M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{v}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
\end{center}

\subsubsection*{Initial SCD Calculation.}

Using the ownership information, the initial SCDs are calculated
as follows. Let us consider each component in an 
initial $SCD = <A, D, CM, Q>$. $A$ is the array to be communicated.
The  region 
$D$ contains a single index given by the array subscript expression. 
The qualifier $Q = \perp$ since initial communications
must be performed in every iteration. Let $CM = <src, dst, qual>$. Since
initially communication does not perform broadcast, 
$qual = \perp$. Hence, the calculation of  $src$ and $dst$, 
which will be discussed in the following text, 
 is the only non-trivial computation in the calculation of initial
SCDs. 

Let $\vec{i}$ be the vector of loop induction variables. When  subscript
expressions are affine functions, an array reference can be
expressed as $A(G\vec{i} + \vec{g})$, where $A$ is the array name, 
$G$ is a matrix and $\vec{g}$ is  a vector. 
$G$ is called the {\em data access matrix}
and $\vec{g}$ the {\em access offset vector}.  The data access
matrix, $G$, and the access offset vector, $\vec{g}$, describe
a mapping from a point in the iteration space to a point in the data space.
Let $G_l$, $\vec{g}_l$, $M_l$, $\vec{v}_l$ be the data access matrix,
the access offset vector, the alignment matrix and the alignment vector 
for the {\em lhs} array reference, and  
$G_r$, $\vec{g}_r$, $M_r$, $\vec{v}_r$ be the corresponding quantities
for the {\em rhs} array reference. 
The source processor $src$ and destination processor $dst$
are given by:

\centerline{$src = M_r(G_r\vec{i} + \vec{g}_r) + \vec{v}_r,$ \hspace{1in}
            $dst = M_l(G_l\vec{i} + \vec{g}_l) + \vec{v}_l$}

Consider the communication of $w(i,j)$ in statement $s13$ 
in Figure~\ref{EXAMPLE}.
The analyzer can obtain from the program the data 
access matrices, access offset vectors, alignment matrices 
and alignment vectors and from them the  SCD for the communication 
given below.
As an indication of the complexity of a SCD, the structure for this 
communication required 524 bytes to store.
\begin{center}
\small
\[ M_z  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{v}_z = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{v}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
\[ G_l  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_l = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   G_r  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_r = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right)
\]
\end{center}
\centerline{$<A=w, D=(i, j), CM=<(2*j, i+1), (i, j), 
            \perp>, Q= \perp>$}

\subsubsection*{Message Vectorization and Available Communication Summary}

In this phase, the analyzer computes 
{\em backward exposed} communications, which 
are SCDs that can be hoisted out of a loop,
and  {\em forward exposed} communications, which are SCDs that are
available after the loop.
Backward exposed communications represent 
actual communications vectorized from inside the loop. When a SCD is 
vectorized, the initial SCD at the assignment statement
are replaced by SCDs
for  backward exposed communications at loop headers. 
Forward exposed communications
represent the  communications that are performed inside a loop and 
are still alive after the loop. 
Hence they can be used to subsume communications appearing
after the loop. By using data dependence information,  backward and
forward exposed communications are calculated by
propagating  SCDs from inner loop bodies to loop headers using a 
simplified version of the rules discussed in previous section.
% The rule is simplified in that the DOWN propagate of SCDs back into
%the loop body is no longer performed. 

%The calculations of forward exposed communications do not effect the
%SCDs in the original statement, since  the available communication
%only represents the effect
%of communications. 
Algorithms for the forward and backward exposed communication calculation
are  described in Figure~\ref{phase1}~(a) and (b). 
%Notation 
%$Request(S, n, UP|DOWN)$ denotes placing an UP or DOWN propagation of S after
%node n.  
Since only UP propagation
is needed,  $Request(S, n)$ is used to denote placing a
propagation of $S$ after node $n$.
In the algorithms,  $S$ is a
SCD occurring inside the interval whose header is node $n$ and whose induction
variable is $i$ with lower bound $1$ and upper bound $h$, 
$anti\_def$ is the set of definitions in the interval that have 
anti--dependence relation with the original array reference that causes  the
communication $S$, 
$flow\_def$ is the set of definitions in the interval that have 
flow--dependence relation with the original array reference that causes the
communication $S$. 
%The expansion function has the similar definition as in 
%\cite{gupta93}. 
For a SCD, S, $expand(S, i, 1:h)$ first 
determines which portion of the $S=<A, D, CM, Q>$ to be expanded.
If $D$ is to be expanded, that is, $i$ occurs in D, 
the function will replace
all single data item references $\alpha*i+\beta$ used in D
by the triple $\alpha+\beta:\alpha*h+\beta:\alpha$. 
If $D$ cannot be expanded, that is, after expansion $D$ is not in the 
allowed form, then the communications will stay inside the loop.
If $CM=<src, dst, qual>$ 
is to be expanded, that is,  $i$ occurs in $dst$ but not in $src$ and $D$,
the function will add $i=1:h:1$ 
into the mapping qualifier list $qual$. 

The algorithms determine the part of communications $Outside$, that 
can be hoisted out of a loop, and $Inside$, that cannot be hoisted
out of the loop. In forward exposed communication calculation, 
the analyzer makes $Outside$ as the forward exposed communication and ignores
the $Inside$ part. 
%Note again that forward exposed 
%communication calculation only calculates the communications that are alive
%after a loop and does not actually change the communications in a program.
In backward exposed communication calculation, the analyzer
makes $Outside$ as backward exposed communication. In addition, the analyzer
must also change the original SCD according to contents of $Inside$. In
the case when the SCD can be fully vectorized, 
%$Inside$ will be empty and 
%all communications in the SCD inside the loop are summarized by $Outside$ and 
%are hoisted out of the loop. 
the SCD in the original statement is 
removed. In the case when the SCD cannot be fully vectorized, part of the
communication represented by 
$Outside$ is hoisted  out of the loop, while other part represented by
$Inside$ stays at the original statement. Thus, the SCD in the original
statement must be modified by a  communication qualifier to indicate that
the SCD only remains in iterations that generate communications 
in $Inside$.
%Note that in forward exposed communication calculation,
%communications are killed by anti--dependence while in backward exposed 
%communication calculation,  communications are killed by flow--dependence.

\begin{figure}[htbp]
\small
\footnotesize
\begin{subfigRow*}
\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}re\=qu\=es\=t$(S, n)$ : \\
\>$Outside = expand(S, i, 1:h) -$\\  
\>\>$\cup_{anti\_{def}}expand(anti\_def, i, 
             1:h)$\\
\>if $(Outside \ne \phi)$ then\\
\>\>record $Outside$ as\\ 
\>\>\>forward exposed in node n \\
\>\>Let m be the header of the\\
\>\>\>interval including node $n$\\
\>\>$request(Outside, m)$;\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
(a) Forward exposed communication
\end{tabbing}
\end{minipage}

\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}re\=qu\=es\=t$(S, n)$ : \\
\>$Outside = expand(S, i, 1:h) -$\\
\>\>$\cup_{flow\_def}expand(flow\_def, i, 
             1:h)$\\
\>$Inside = expand(S, i, 1:h) \cap$\\
\>\>$\cup_{flow\_def}expand(flow\_def, i, 1:h)$\\
\>if $(Outside \ne \phi)$ then\\
\>\>convert $Inside$ in terms of $S$\\
\>\>\>with qualifier, denoted as $D$\\
\>\>if (conversion not successful) then\\
\>\>\>stop /* fail */\\
\>\>else\\
\>\>\>ch\=ange the S into D\\
\>\>\>record $Outside$ as backward\\
\>\>\>\>exposed comm. at node $n$.\\
\>\>\>Let m be the header of the\\
\>\>\>\>interval including node $n$\\
\>\>\>$request(Outside, m)$;\\
\\
(b) Backward exposed communication
\end{tabbing}
\end{minipage}
\end{subfigRow*}
\caption{Algorithms for the forward and backward exposed communication}
\label{phase1}
\end{figure}


%\begin{figure}
%\begin{subfigRow*}
%\small
%\footnotesize
%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} DO\=\ i = 1, 100\\
%              \>COMM: $C_1= <A, (1), <(1), (i), \perp>,\perp>$\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2= <B, (i-1), <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (a)
%\end{tabbing}
%\end{minipage}
%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} FORWARD COMM: $C_1^f=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} FORWARD COMM: $C_2^f=<B, [0:99], <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in} DO\=\ i = 1, 100\\
%              \>COMM: $C_1= <A, (1), <(1), (i), \perp>,\perp>$\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2= <B, (i-1), <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (b)
%\end{tabbing}
%\end{minipage}

%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} FORWARD COMM: $C_1^f=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} FORWARD COMM: $C_2^f=<B, [0:99], <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in} BACKWARD COMM: $C_1^b=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} BACKWARD COMM: $C_2^b=<B, (0), <(i-1), (i), \perp>, 
%                               \perp>$\\
%\hspace{0.2in} DO\=\ i = 1, 100\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2^s= <B, (i-1), <(i-1,1), (i,1), \perp>, i=2:100:1>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (c)
%\end{tabbing}
%\end{minipage}
%\end{subfigRow*}
%\caption{Message vectorization}
%\label{VEC}
%\end{figure}

\begin{figure}[tbph]
\centerline{\psfig{figure=fig/vexam.eps,width=3.5in}}
\caption{Calculating backward exposed communications}
\label{VEC}
\end{figure}

Consider communications in the loop in 
Figure~\ref{VEC}. Assume that arrays $a$, $b$ and $d$ are identically aligned
to the virtual processor grid, initial SCDs, $C1$ and $C2$,  are shown in 
Figure~\ref{VEC}.
$C3$, $C4$ and $C5$ are the communications after the backward exposed 
communication calculation. 
Calculating the backward exposed communication for $C1$
results in  communication $C3$ 
in the loop header and the removal of  the communication $C1$ from its
original statement.
Calculating  the backward exposed communication for $C2$
puts $C4$ in the loop header and changes $C2$ into $C5$. 
Note that, there is a  flow--dependence relation from b(i) to 
 b(i-1). In calculating the 
backward exposed communication for SCD $C2$, 
$Inside = <b, (1:99:1), <(i-1,1), (i,1), \perp>, \perp>$. Converting $Inside$
back in terms of $C2$ results in $C5$.


%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{1.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{1.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{1.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{1.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{1.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{1.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_2=<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_3=<w, (2:101:1, 1:99:1), <(2*j, i+1), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_4=<y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 100)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{1.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{1.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{1.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{1.2in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
%\hspace{1.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after vectorization}
%\label{EXAMVEC}
%\end{figure}


%\begin{figure}[htbp]
%\small
%\footnotesize
%\begin{tabbing}
%\hspace{1in}\hspace{0.5in}Do\=\ i=1, 100\\
%\hspace{1in}s1: \>D[i] = A[1]\\
%\hspace{1in}s2:\>B[i] = B[i-1] + 1\\
%\hspace{1in}\hspace{0.5in}End Do\\
%\hspace{1in}\hspace{0.5in}Do i=1, 100\\
%\hspace{1in}s3:\>C[i] = B[i-1] + 1\\
%\hspace{1in}\hspace{0.5in}End Do
%\end{tabbing}
%\caption{backward and forward exposed communication}
%\label{BACKWARD}
%\end{figure}

\subsubsection*{Redundant Communication Elimination}

This phase calculates  available communications before each statement,
and eliminates a communication at the statement if the communication
is available. This optimization is done by propagating SCDs forward until
all elements are killed. During the propagation, if another SCD that can be
subsumed is encountered, that SCD is redundant and can be eliminated.

\begin{figure}[htbp]
\small
\footnotesize
\begin{subfigRow*}
\begin{minipage}{10cm}
\begin{tabbing}
\hspace{0.1in}re\=quest($S_1, n, UP$) $\wedge$ ... \\
\hspace{0.1in}$\wedge$request($S_k, n, UP$) : \\
\hspace{0.1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.1in}\>if\=\ (SCDs in n is a subset of S) then\\
\hspace{0.1in}\>\>remove the SCDs\\
\hspace{0.1in}\>if $(S-kill_n \ne \phi)$ then\\
\hspace{0.1in}\>\>fo\=r all $m\in succ(n)$\\
\hspace{0.1in}\>\>\>request($S - kill_n, m, UP$)\\
\\
\hspace{0.1in}{(a) Actions on nodes within an interval}\\
\end{tabbing}
\end{minipage}

\begin{minipage}{10cm}
\begin{tabbing}
\hspace{0.1in}re\=quest($S, n, UP$): \\
\hspace{0.1in}\>calculate the summary of loop $n$\\
\hspace{0.1in}\>In\=side= $expand(S, i, 1:i-1) \cap$\\
\hspace{0.1in}\>\> $(\cup_{def}expand(def, i, 1:i-1))$\\
\hspace{0.1in}\>if\=\ (inside $\ne \phi$) then\\
\hspace{0.1in}\>\>Let $l$ be the first node.\\
\hspace{0.1in}\>\>request($Inside,  l, DOWN$)\\
\\
\\
\hspace{0.1in}{(b) Actions on a loop header}\\
\end{tabbing}
\end{minipage}
\end{subfigRow*}
\vspace{-0.15in}
\caption{Actions in forward propagation}
\label{FORWARD1}
\vspace{-0.15in}
\end{figure}

Using the interval analysis
technique \cite{gupta93}, two passes are needed to obtain
the data flow solutions in an interval. 
Initially, UP propagations are performed.
Once the UP propagations reach interval headers, summaries of the SCDs
are calculated and  DOWN propagations of the summaries
are triggered. Note that since 
the data flow effect of propagating SCDs between
intervals is captured in the message vectorization
 phase of the analyzer, both the UP
and DOWN propagations are
performed within an interval in this phase.

Assuming that node $n$ has $k$ predecessors. 
When propagating SCDs within an interval in forward propagation, 
actions in a node
will be triggered only when all its predecessors place requests. The nodes
calculate the SCD available by performing intersection on all
SCDs that reach it, check whether communications within the node can 
be subsumed, and propagate the live communications  forward. 
 Figure~\ref{FORWARD1}~(a) describes actions 
on the nodes inside the interval in an UP forward propagation.
When the UP propagation reaches an interval boundary,
the summary information is calculated by obtaining all the elements that
are available in iteration $i$, and a DOWN propagation is triggered.
Note that in forward propagation,   communications
can be safely assumed to be performed in every iteration ($Q=\perp$), 
since the effect of the communication
must guarantee that the valid values are at the proper processors for the 
computation. Figure~\ref{FORWARD1}~(b)
shows actions at interval boundaries. The
propagation of a DOWN request is similar to that of an UP request except that
a DOWN propagation stops at interval boundaries.
% The results of redundant 
%communication elimination in the program in Figure~\ref{EXAMVEC} is shown in 
%Figure~\ref{EXAMRED}. As can be seen from the figure, the communication
%$C_3$ in Figure~\ref{EXAMVEC} is subsumed by communication $C_1$.

%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{1.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{1.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{1.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{1.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{1.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{1.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_2=<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_4=<y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 100)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{1.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{1.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{1.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{1.2in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
%\hspace{1.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after redundant communication elimination}
%\label{EXAMRED}
%\end{figure}


\subsubsection*{Global Message Scheduling}

After the redundant communication elimination phase, the analyzer further
reduces the number of messages using a global message scheduling
algorithm  proposed by Chakrabarti et al. in \cite{Chakrabarti96}.
The idea of this optimization 
is to combine messages that are of the same communication
pattern into a single message to reduce the number of messages in 
a program.  In order to perform message scheduling, the analyzer first
determines the earliest and latest points for each communication.
Placing the communication in any point between the earliest and 
the latest points that dominates the latest point 
always yields correct programs.
Thus, the analyzer can 
schedule the placement of messages  such that messages of 
same communication
patterns are placed together and are combined to reduce the number of
messages.

The latest point for a communication is the place of the SCD 
after redundant communication elimination.
Note that after message vectorization, SCDs are  placed
in the outermost loops that can perform the communications.
The earliest point for a SCD can be found by propagating
the SCD backward. 
As in \cite{Chakrabarti96}, it is  assumed 
that communication
for a SCD is performed at a single point. Hence, the backward
propagation will stop after an assignment statement, a loop header or a
branch statement where part of the SCD is killed. Since the propagation of
SCDs stops at a loop header node, only the UP propagation is needed.
Once the earliest and latest points for each communication are known, 
the greedy heuristic in \cite{Chakrabarti96} is used to perform the 
communication scheduling.
% The greedy algorithm
%propagates SCDs from their earliest points to the latest points, 
%along the path,
%if some communications are of the same pattern, the communications will
%be combined. The combined communication will be further propagated to the 
%latest point common to all communications combined.
%Consider communications $C_2$ and $C_4$ in  Figure~\ref{EXAMRED}.
%The earliest and the latest points for $C_2$
%are before statement $s6$ and $s11$,
%respectively. The earliest and the latest points for $C_4$ are before 
%statement $s11$. Since these two communications are of the same communication
%pattern, they can be placed before statement $s11$ and combined.


%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{0.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{0.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{0.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{0.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{0.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{0.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{0.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{0.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{0.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{0.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{0.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{0.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{0.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{0.4in}COMM: $C_{24} =\{<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>,  $\\
%\hspace{0.7in}  $ <y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>\}$\\
%\hspace{0.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 200)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{0.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{0.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{0.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{0.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{0.2in}(s16)\hspace{0.1in}\>w(i+1, 200) = ...\\
%\hspace{0.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after message scheduling}
%\label{EXAMCOM}
%\end{figure}

\subsection{Evaluation of the analyzer}
\label{evalanalyzer}

The analyzer is implemented as part of the E--SUIF compiler
which is developed to support compiled communication on optical TDM networks.
The E--SUIF compiler is based on the Stanford SUIF compiler \cite{SUIF}.
The generation of a program used for evaluations is carried out in
the following steps. First,
a sequential program is compiled using SUIF frontend, $scc$, 
to generate the SUIF intermediate representation. Next, 
the SUIF transformer, $porky$, is used to perform 
a number of scalar optimizations including
copy propagation, dead code elimination and induction variable
elimination. The communication preprocessing phase is used to annotate
global arrays with data alignment information. 
The analyzer is then invoked to analyze and optimize  communications 
in the program. After communication optimizations, the backend of the
compiler inserts a library call into the SUIF intermediate representation
for each SCD remaining in the program. Finally, the $s2c$ tool
is used to convert the SUIF intermediate representation into C program, which
is the one that is executed for evaluation.

To evaluate performance of the analyzer, 
a communication emulation system is developed. 
The system takes SCDs as input, emulates the 
communications described by the SCDs and collects statistics about 
the required communications, such as
the total number of elements communicated and the 
total number of messages communicated.
The emulation system provides an interface to C program in the form of 
a library call whose arguments include all information in a SCD.
The compiler backend in E--SUIF 
automatically generates the library call for each 
SCD remaining in the program. 
In this way, the communication performance of a program
can be evaluated in the emulation system
by running  programs generated by the E--SUIF compiler.

%Once this program is generated, it can be
%compiled with the emulation library and run to collect the communication 
%performance statistics. 
%In our experiments, we used the number of elements
%to be communicated as the performance measure.

Six programs, L18, ARTDIF, TOMCATV, SWIM, MGRID and ERHS
are used in the experiment. Programs ARTDIF, TOMCATV, SWIM, MGRID and ERHS
are from the SPEC95 benchmark suite. 
The descriptions of the programs are as follows.
\begin{enumerate}
\item L18 is the explicit hydrodynamics kernel in livermore loops (loop 18).
\item ARTDIF is a kernel routine obtained from  HYDRO2D program, 
which is an astrophysical program for the computation of galactical jets
using hydrodynamical Navier Stokes equations. 
\item TOMCATV does the mesh generation with Thompson's solver. 
\item SWIM is the SHALLOW weather prediction program.
\item MGRID is the simple multigrid solver for computing a three
dimensional potential field. 
\item ERHS is part of the APPLU program, which is the solver for five 
coupled \\
parabolic/elliptic partial differential equations. 
\end{enumerate}

Table~\ref{analysis} shows  the analysis cost of the analyzer. 
The analyzer, which implements all the optimization
algorithms on all SCDs in the programs,  was run on a 
SPARC 5 machine with 32MB memory. 
Row 2 and Row 3 shows the program sizes.
Row 4 shows the
cumulative memory requirement, which is the sum of number of SCDs passing 
through each node. This number is approximately equal 
to the memory requirement of
traditional data flow analysis.
The value in parenthesis
is the maximum number of cumulative SCDs in a node, which is the extra memory
needed by the analyzer.
In the analyzer, the size of a SCD ranges from 0.6 to about 3 kbytes.
The results show that traditional analysis method will require large
amount of memory when a program is large, while the analyzer uses
little extra memory. 
Row 5 gives the raw analysis times and row 6 shows the rate at which
the analyzer operates in units of  $lines/sec$. On an average,
the analyzer compiles 172 lines per second for the six programs. 
Row 9 shows the total time,
which includes analysis time and the time to load and store the
SUIF structure, for reference. In most cases, the analysis time is only
a fraction of the load and store time.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Program &            L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
size(lines) &         83  &    101 &     190 &  429 &   486 & 1104\\
%\hline
%\# of tree nodes   &  85  &    110 &     337 &  543 &   660 &  958\\
\hline
\# of initial SCDs &  35  &     12 &     108 &   76 &   125 &  403\\
\hline
accu. memory req.  & 348(1) &  175(1) & 5078(3) &  767(1) &  1166(1) & 
                                                                    6029(5)\\
\hline
analysis time(sec) & 0.62 &   0.32 &    3.47 & 1.87 &  1.92 & 20.92\\
\hline 
lines / sec        & 133  &   316  &      54 & 229  &  253  & 52 \\
\hline
total time(sec)    & 2.00 &   1.75 &    6.95 & 6.65 & 12.52 & 35.42\\
\hline
\end{tabular}
\end{center}
\caption{Analysis time}
\label{analysis}
\end{table}

Table~\ref{elements} and Table~\ref{messages} show the effectiveness
of the optimizations in the analyzer. 
Table~\ref{elements} shows the reduction of
the 
total number of elements to be communicated and Table~\ref{messages} shows
the reduction of the total number of messages. 
Both cyclic and block distributions on 16 PE systems are considered.
This experiment is conducted using the test input  provided by
the SPEC95 benchmark for programs TOMCATV, SWIM, MGRID ERHS. The outermost
iteration number in MGRID is reduced to 1 (from 40). Problem 
sizes of $6\times 100$ for L18 and $402\times 160$ for ARTDIF are used.
The number of elements and number of messages
communicated after all optimizations is compared
to those after message vectorization optimization.
Table~\ref{elements} shows that for cyclic distribution, an
average reduction of 31.5\% of the total communication elements is achieved.
The block distribution greatly reduces the number of elements to be
communicated and affects the optimization performance of the analyzer.
For block distribution, the average reduction is 23.1\%. 
Table~\ref{messages} shows that the analyzer reduces the 
total number of messages
by 36.7\% for cyclic distribution and by 35.1\% for block
distribution. These results indicate
that global communication optimization opportunities are quite common and
the analyzer developed is effective in finding these opportunities.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dist. & Opt. &        L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
      &      &       $\times10^4$& $\times10^5$ & $\times10^8$  & 
$\times10^7$ & $\times10^7$ & $\times10^6$\\
\hline
      & Vector. &    1.38  & 7.01   & 1.38    & 6.38   & 5.69 & 3.62\\
\cline{2-8}
cyclic & Final  &   0.96  & 5.73   & 0.34 &  4.58   & 5.69 & 2.29 \\
\cline{3-8}
       &        &   69.6\% & 81.7\% & 24.6\% & 71.8\% & 100\%& 63.3\%\\
\hline
\hline
       &        &   $\times10^3$ & $\times10^4$ & $\times10^6$ & 
$\times10^6$ & $\times10^6$ & $\times10^6$\\
\hline
       & Vector. &  3.26    & 7.17 & 5.74 & 3.38 & 8.49 & 3.11\\
\cline{2-8}
block  & Final &  2.57 &  6.97 & 5.12 & 1.08 & 8.49 & 1.65\\
\cline{3-8}
       &       &  78.8\% & 97.2\% & 89.1\% & 32.0\% & 100\% & 53.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Total number of elements to be communicated}
\label{elements}
\end{table}

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dist. & Opt. &        L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
   & Vector. &    368  & 400   & 68555  & 3892   & 17662 & $1.14\times10^6$\\
\cline{2-8}
cyclic & Final  & 96  &  336   & 41075  & 1807   & 17662 & $0.72\times10^6$\\
\cline{3-8}
       &        &   26.1\% & 84.0\% & 59.9\% & 46.4\% & 100\%& 63.1\%\\
\hline
\hline
       & Vector. &  330  & 185 & 16750 & 3894 & 14650 & $9.20\times10^5$\\
\cline{2-8}
block  & Final &  90 &  161 & 10915 & 2209 & 14650 & $4.89\times10^5$ \\
\cline{3-8}
       &       &  27.3\% & 87\% & 65.2\% & 56.7\% & 100\% & 53.2\%\\
\hline
\end{tabular}
\end{center}
\caption{Total number of messages}
\label{messages}
\end{table}

%\begin{figure}
%\centerline{\psfig{figure=fig/size.eps,width=6in}}
%\caption{The reduction of the number of array elements in communications}
%\label{size}
%\end{figure}
%\vspace{-0.2in}

\section{Virtual to physical processor mapping}

%The communication analyzer analyzes the communication requirement on
%virtual processor grids. 
In order to support compiled communication,
communication patterns on physical processors must be computed.
This section assumes that the physical processor grid has the same
number of dimensions as the logical processor grid. Notice that this is
not a restriction because a dimension in the physical processor grid
can always be collapsed by assigning a single processor to that dimension.
This section presents algorithms to compute  communications
on physical processors from SCDs. The computation
may not always be precise due to  symbolic 
constants in the SCD that are unknown at compile time.
The algorithms employ multi--level approximation schemes to 
obtain best information. 
%In this case, approximations are needed.

Given a $SCD = <A, D, CM = <src, dst, qual> , Q>$, let us first consider
the case where $A$ is an one-dimensional array and the virtual processor
grid is also one-dimensional. Let $src = \alpha*i+\beta$ and
$dst = \gamma*i + \delta$, $\alpha \ne 0$, $\gamma \ne 0$, and $qual = NULL$. 
$qual \ne NULL$ will be considered later
when multi-dimensional arrays and multi-dimensional virtual processor
grids are discussed. 
Let the alignment matrix and the offset vector be $M_A$ and $v_A$, that 
is, element A[n] is owned by virtual processor $M_A*n + v_A$. 
Let us assume that the number
of physical processors is $p$ and the block size of the distribution of
virtual processor grid is $b$. For an element A[n], the 
physical source processor of the communication can be computed as follows.
 
\centerline{$(M_A*n+ v_A)\ mod\ (p*b) / b$}

\noindent
The virtual destination processor can be computed by
first solving the equation \\
\centerline{$(M_A*n + v_A) = \alpha*i+\beta$ 
to obtain $i = (M_A*n+v_A-\beta) / \alpha$}

\noindent
and then replacing the value of $i$ in  $dst$ to obtain the 
virtual destination processor $\gamma*(M_A*n+v_A-\beta)/\alpha + \delta$. 
Thus, the
physical destination processor is given by\\
\centerline{$ (\gamma*(M_A*n+v_A-\beta)/\alpha + \delta)\ mod\ (p*b) / b$.} 

\noindent
The physical communication pattern for the SCD can be obtained
by considering all elements in $D$. 
However, there are situations that the exact
region $D$ cannot be determined at compile time. It is desirable to 
have a good approximation scheme that computes
the communication patterns when $D$ cannot be determined at compile time.

Before the approximation scheme is presented, let us first examine
the relation between communications on physical processors and 
that on virtual processors. Let us use notation $src\rightarrow dst$ 
to represent
a communication from $src$ to $dst$. Given a data region 
$D=l:u:s$, the communications on virtual processors can be derived as follows.
By mapping $D$ to the virtual processor grid, the source processors
of the communications can be obtained. Since the mapping from data space to 
the virtual processor grid is linear, the set of source processors can 
be represented as a triple $vs_l:vs_u:vs_s$, that is, the 
source processors on the virtual processor grid are $vs_l$, $vs_l+vs_s$, 
$vs_l+2*vs_s$, ..., $vs_u$. Due to the way in which 
$CM.src = \alpha*i + \beta$ 
is computed, 
equations $vs_l+i*vs_s = CM.src$, $i = 0, 1, 2, ...$,  
always have integer solutions.
Since $CM.dst$ is of the form $\gamma*i+\delta$, 
where $\gamma$ and $\delta$ are constants, 
the destination processors on the virtual processor grid can 
also be represented
as a triple $vd_l:vd_u:vd_s$, where 
$vd_l = \gamma*((vs_l-\beta)/\alpha) + \delta$, 
$vd_u = \gamma*((vs_u-\beta)/\alpha) + \delta$ and $vd_s = \gamma*vs_s/\alpha$. Notice that because of the way in which
$CM$ is computed, all the division operations in the formula result in 
integers. Thus, communications on the virtual processor grid can be
represented as $vs_l\rightarrow vd_l:vs_u\rightarrow vd_u:vs_s\rightarrow vd_s$, meaning the set\\
\centerline{$\{vs_l\rightarrow vd_l, vs_l+vs_s\rightarrow vd_l+vd_s, ..., vs_u\rightarrow vd_u\}$.}

\noindent
Communications on physical processors are obtained by mapping virtual 
processors onto  physical processors.  Given a block--cyclic distribution 
with block size $b$ and processor number $p$, a sequence of processors
on the virtual processor grid $l, l+s, l+2*s, ...$ will be mapped
to a sequence of physical processors repeatedly. For example, assuming that 
$p=2$ and $b=2$, the sequence of virtual processors 
$2, 2+3=5, 2+2*3=8, 2+3*3=11, ....$ will be mapped to physical processors
$1,0,0,1$ repeatedly as shown in Figure~\ref{virtualspace}. As will be seen
later, this characteristic can be utilized to develop an approximation 
algorithm for the cases when $D$ is unknown at compile time.  
A point $e$ in the virtual processor grid can be represented by 
two components $(pp, o)$, where
$pp = e\ mod\ (p*b) / b $ is the physical processor that contains 
$e$ and $o = e\ mod\ b$ is the offset of $e$ within the processor.
Let $(pp_k, e_k)$ correspond to $l+k*s$, $k = 0, 1, ...$.
It can be easily shown that \\
\centerline{$pp_i = pp_j \wedge e_i=e_j$ implies 
            $pp_{i+1} = pp_{j+1} \wedge e_{i+1}=e_{j+1}$}

\noindent
Since in the $(pp, o)$ space, there are $p$ choices for $pp$ and $b$ choices
for $o$, Thus, there exists a $k$, $k \le p*b$, such that $pp_k=p_0$ and 
$e_k=e_0$, which determines a repetition point. In the previous 
example, consider the 
sequence\\
\centerline{$2 =(1,0), 5=(0,1), 8=(0,0), 11=(1,1), 14=(1,0) ...$.}

\noindent
Thus, the physical processors repeat the sequence $1,0,0,1$.

Communications on physical processor contains two processors, the source
processor and the destination processor. Thus, in order for the 
communications to repeat, both source and destination processors must
repeat. Following the above discussion, the communication on the virtual 
processor grid, $src\rightarrow dst$, can be represented by four components 
$(spp, so, dpp, do)$, where $spp$ is the physical processor that
contains $src$, $so$ is the offset of $src$ within the processor, 
$dpp$ is the physical processor that contains $dst$, $do$ is the 
offset of $dst$ within the processor. Assuming that the source array and the 
destination array are mapped to the same virtual processor grid, 
there are $p$ choices for $spp$ and $dpp$, and  $b$ choices for $so$ and $do$.
Thus, there exists $k$, $k\le p^2b^2$, such that both source and destination 
processors, and thus the communication pattern, will repeat themselves.
The following lemma summarizes these results. Using this lemma,
communication patterns can be obtained by considering the elements in $D$ 
until the repetition point or the end of $D$, whichever occurs first.


\begin{figure}
\centerline{\psfig{figure=fig/virtualspace.eps,width=3.5in}}
\caption{Virtual processor space}
\label{virtualspace}
\end{figure}

\noindent
{\bf Lemma: } Assume that the virtual processor grid is distributed over $p$
processors with block size $b$.
Let $SCD = <A, D=l:u:s, CM = <src, dst, qual> , Q>$, 
assuming u is infinite, there exist a value
$k$, $k\le p^2b^2$, such that the communication for all 
$m \ge k$, $A[l+m*s]$ has the same
source and destination as the communication for $A[l+(m-k)*s]$.

\noindent
{\bf Proof}: Follows from above discussions. $\Box$

The implication of the lemma is that the algorithm to determine the
communication pattern for the SCD can stop when the
repetition point occurs. In other words, when the upper bound of 
$D$ is unknown, the communication pattern can be approximated by using
the repetition point. Figure~\ref{algo1} shows the algorithm to compute
the physical communication pattern for a 1--dimensional array and 
a 1--dimensional virtual processor grid. 
The algorithm first checks the SCD.
Let $D=l:u:s$ and $CM=<\alpha+\beta*i, \gamma+\delta*i, \perp>$.
If $l$ contains variables or the mapping is not clean ($\alpha$, $\beta$, 
$\gamma$ or $\delta$ are symbolic constants), the communication is 
approximated  with all--to--all connections.
Note that
by the semantics of array sections, when $l$ is unknown, the compiler
cannot determine the actual sequence of elements in an array section.
%The algorithm then
%deals with the situation when array region $D$ contains
%variables.  
When $s$ contains variables, it will be 
approximated by 1, that is, $D$ is approximated by a superset $l:u:1$.  
When  $u$ contains variables, 
the physical communication is approximated by
considering all elements until the repetition point. Note that when  
$u$ contains a variable, the sequence in $D$ is 
$l$, $l+s$, $l+2*s$, .... Although
the upper bound of the sequence is unknown to the compiler, 
the repetition point can be used
to approximate the communication pattern. 

\begin{figure}
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Co\=mpute\_1--dimensional\_pattern($D$, $CM.src$, 
                                                       $CM.dst$)\\
\\
              \>Let $D=l:u:s$, $CM.src=\alpha*i+\beta$, $CM.dst=\gamma*i+\delta$\\
              \>{\bf if}\=\ ($l$ contains variables) {\bf then}\\ 
              \>\>{\bf return} all--to--all connections\\
              \>{\bf end if}\\
              \>{\bf if}\=\ ($\alpha$, $\beta$, $\gamma$ or $\delta$ 
                  are variables) {\bf then}\\ 
              \>\>{\bf return} all--to--all connections\\
              \>{\bf end if}\\
              \>$pattern = \phi$\\
              \>{\bf for each} element $i$ in $D$ {\bf do} \\
              \>\>$pattern = pattern + communication\ of\ i$\\
              \>\>{\bf if}\=\ (communication repeated) {\bf then}\\
              \>\>\>{\bf return} $pattern$\\
              \>\>{\bf end if}\\
              \>{\bf end for}
\end{tabbing}
\end{center}
\caption{Algorithm for 1-dimensional arrays and 1-dimensioanl virtual processor grid}
\label{algo1}
\end{figure}

Now let us consider multi-dimensional arrays and multi--dimensional
virtual processor grids. In an 
n--dimensional virtual processor grid,  a
processor is represented by a $n$--dimensional coordinate
$(p_1, p_2, ..., p_n)$. The algorithm to compute the communication pattern
finds all pairs of source and destination processors that require
communication. This is done by considering the dimensions in virtual 
processor grid one at a time.
A set of $src=(sp_1,sp_2,...,sp_n)\rightarrow dst=(dp_1,dp_2,...,dp_n)$ 
pairs is used to represent the communications. 
A wild--card, $*$, is used to represent the dimension within a tuple
that has not been considered. Initially the communication set 
contains a single element where 
all dimensions are wild--cards. When one dimension is considered, 
it generates a 1-dimensional communication pattern for a specific dimension
in the source and the destination, denoted as $src\_dim$ and $dst\_dim$
respectively. This 1-dimensional pattern may degenerate to contain only source
processors or destination processors.
A cross product operation is defined to merge the 1-dimensional 
communication patterns into the $n$-dimensional communication. 
This operation is similar to the cross product of  sets
except that  specific dimensions are involved in the operation.
For the degenerate form of the 1-dimensional pattern,
the operation only involves source processors or destination processors.

For example, consider the communication for\\ 
\centerline{$SCD = <y, (1:4:1,1:4:1), <src = (i,j), dst = (j,i), qual =NULL>, NULL>$.}

\noindent
Further assume
that the virtual processor grid is distributed on 2 processors
with block size of 2 in each dimension 
and array $y$ is identically mapped to the virtual 
processor grid. Initially, the communication set contains a single element
$(*,*) \rightarrow  (*,*)$, indicating that all dimensions in the source and destination
processor have not been considered. Considering the first dimension 
in the data space, which is identically mapped to the first dimension
of the virtual grid. Hence, $src\_dim = 1$. From the mapping relation
$CM.src$ and $CM.dst$, it is can found that dimension 2 in the destination
processor correspond to dimension 1 in the source processor. 
Hence, $dst\_dim = 2$. Applying the algorithm for the 1--dimensional 
communication pattern obtains the communication to be $\{0\rightarrow 0, 1\rightarrow 1\}$ with
$src\_dim=1, dst\_dim=2$.
Taking the cross product of this pattern 
with the 2-dimensional communication set 
$\{(*,*) \rightarrow  (*,*)\}$ yields $\{(0,*)\rightarrow (*,0), (1,*)\rightarrow (*,1)\}$. Considering the
second dimension of the data space, the 1--dimensional communication set
is $\{0\rightarrow 0, 1\rightarrow 1\}$ with $src\_dim=2, dst\_dim=1$. 
Taking the cross product of this pattern set
to the 2--dimensional communication set gives 
$\{(0,0)\rightarrow (0,0), (0, 1)\rightarrow (1, 0), (1, 0)\rightarrow (0, 1), (1, 1)\rightarrow (1, 1)\}$, which is
the physical communication for the $SCD$. 

The above example does not take constant mappings and non--NULL 
qualifiers into consideration.
The algorithm to compute communication patterns for multi-dimensional
arrays that is  shown in Figure~\ref{algo2} considers all these situations.
The algorithm first checks whether the mapping relation can be processed.
If one loop induction variable occurs in two or more dimensions 
in $CM.src$ or $CM.dst$, the algorithm cannot
find the correlation between dimensions in source and destination processors,
and the communication pattern for the SCD is 
approximated by all--to--all connections. 
If the SCD passes the mapping relation test, the algorithm determines for each
dimension in the data space the corresponding dimension $sd$ in the source 
processor grid.
If it does not exist, the data dimension is not distributed
and need not be considered. 
If there exists such a dimension, the algorithm
then tries to find the corresponding dimension $dd$ in the destination
processor grid by checking whether there is a dimension $dd$ such that
$CM.dst[dd]$ contains the same looping index variable as the source 
dimension $CM.src[sd]$.
If such dimension exists, 
the algorithm computes 1-dimensional communication pattern
between dimension $sd$ in the source processor and dimension $dd$ in 
the destination processor, then cross--products the 1-dimensional 
communication pattern into the $n$-dimensional communication pattern.
When $dd$ does not exist, the algorithm determines a degenerate 1-dimensional
pattern, where only source processors are considered, and cross-products
the degenerate 1-dimensional pattern into the communication pattern.
After all dimensions in the data space are considered, there may still
exist dimensions in the source processor (in the virtual processor grid)
that have not been considered. These dimensions should be constants and
are specified by the alignment matrix and the alignment offset vector.
The algorithm fills in the constants in the source processors.
Dimensions in destination processor may not be fully considered, either.
When $CM.qual \ne NULL$, the algorithm  finds for each
item in $CM.qual$ the corresponding dimension, computes all possible 
processors in that dimension and cross--products the list into the 
communication list. Finally, the algorithm fills in all constant
dimensions in the destination.


%\begin{figure}
%\begin{center}
%\begin{tabbing}
%\hspace{0.1in}Co\=mpute communication pattern\\
%              \>{\bf if}\=\ (the mapping relation is not good) {\bf then}\\
%              \>\> approximate with all-to-all connections and stop.\\
%              \>{\bf for each} dimension in the array {\bf do}\\
%              \>\>determine the corresponding dimension in source and
%                  destination processor grids.\\ 
%              \>\>compute 1-dimensional communication pattern.\\
%              \>\>cross product this one-dimensional mapping to previous
%                    mappings.\\
%              \>fills in all other dimensions in the source processor if
%                necessary.\\
%              \>{\bf for each} elements in the mapping qualifier {\bf do}\\
%              \>\>determine the corresponding destination dimension.\\
%              \>\>compute all possible processors in dimension.\\
%              \>\>cross product the destination processors to 
%                  the previous mappings.\\
%              \>fills in all other dimensions in the destination processor if
%                necessary.\\
%\end{tabbing}
%\end{center}
%\caption{Algorithm for multi--dimensional array}
%\label{algo2}
%\end{figure}

\begin{figure}
\begin{center}
\begin{tabbing}
\hspace{0.1in}Co\=mpute communication pattern(SCD)\\
              \>Let $SCD=<A,D,CM,Q>$\\
              \>{\bf if}\=\ (the format of $CM$ is not good) {\bf then}\\
              \>\> {\bf return}  all-to-all connections\\
              \>{\bf end if}\\
              \>$pattern = \{(*,*,..., *)\}$\\
              \>{\bf for each} dimension $i$ in the array {\bf do}\\
              \>\>Let $sd$ be the corresponding dimension in source processor
                  grids.\\
              \>\>Let $dd$ be the corresponding dimension in destination
                  processor grids.\\ 
              \>\>1dpattern = compute\_1-dimensional\_pattern($D[i]$,
                  $CM.src[sd]$, $CM.dst[dd]$)\\
              \>\>pattern = cross\_product(pattern, 1dpattern)\\
              \>{\bf end for}\\
              \>pattern = source\_processor\_constants(pattern)\\
              \>{\bf for each} element $i$ in the mapping qualifier {\bf do}\\
              \>\>Let $dd$ be the corresponding destination processor 
                  dimension.\\
              \>\>1dpattern = compute\_1-dimensional\_pattern($CM.qual[i]$,
                              $\perp$, $CM.dst[dd]$)\\
              \>\>pattern = cross\_product(pattern, 1dpattern)\\
              \>{\bf end for}\\
              \>pattern = destination\_processor\_constants(pattern)\\
              \>{\bf return} pattern
\end{tabbing}
\end{center}
\caption{Algorithm for multi--dimensional array}
\label{algo2}
\end{figure}

\begin{figure}
\small
\footnotesize
\begin{center}
\begin{tabbing}
\hspace{1.5in}\=ALIGN (i, j) with VPROCS(2*j, i+2, 1) :: x\\
\>ALIGN (i) with VPROCS(1, i+1, 2) :: y\\
\>DO i = 1, 5\\
\>DO\=\ j = 1, 5\\
\>\>x(i, j) = y(i) + 1\\
\>END DO\\
\>END DO\\
\end{tabbing}
\end{center}
\caption{An example}
\label{ANEXAM}
\end{figure}


An example in Figure~\ref{ANEXAM} illustrates how 
communications on physical processors are derived.
In the program, the virtual processor grid is $3$-dimensional and 
the alignment array and 
the alignment offset vector
for arrays $x$ and $y$ are as follows:

\begin{center}
\[ M_x  = \left(
       \begin{array}{c} 0 \\ 1 \\ 0 \end{array}
       \begin{array}{c} 2 \\ 0 \\ 0\end{array} \right),\
   \vec{v}_x = \left(
       \begin{array}{c} 0 \\ 2 \\ 1 \end{array} \right)
%\]
%\[
   M_y  = \left(
       \begin{array}{c} 0 \\ 1 \\ 0 \end{array}
        \right),\
   \vec{v}_y = \left(
       \begin{array}{c} 1 \\ 1 \\ 2\end{array} \right)
\].
\end{center}

Let us assume that the virtual processor grid, $VPROCS$, is distributed as 
$p= (2,2,1)$,
which means 2 processors in dimension 0, 2 processors in dimension 1 and 
1 processor in dimension 2, and $b= (2,2,1)$, which means 
the block size 2 in dimension 0,  2 in dimension 1 and 1 in 
dimension 2. After communication analysis, the SCD to represent
the communication is as follows:\\
\centerline{\small $SCD = <y, (1:5:1), <src = (1, i+1, 2), 
dst = (2*j, i+2, 1), qual = \{j=1:5:1\}>, NULL>$.}

The communication on physical processors is computed as follows. 
First consider
the dimension 0 in the array $y$. From the alignment, the algorithm 
knows that dimension 1
in the virtual processor grid corresponds to this dimension in the data space. 
Checking $dst$ in $M$, the algorithm can find that dimension 1 in destination 
corresponds to dimension 1 in source processors. 
Applying the 1-Dimensional
mapping algorithm, an 1--dimensional communication pattern
$\{0\rightarrow 1, 1\rightarrow 0\}$ with $src\_dim=1$
and $dst\_dim=1$ is obtained. 
Thus the communication list becomes $\{(*, 1, *)\rightarrow (*, 0, *),
(*, 0, *)\rightarrow (*, 1, *)\}$ after taking the cross product with the 
1--dimensional pattern. 
Next, the other dimensions in source
processors, including dimension 0 that is always mapped to processor 0 and
dimension 2 that is always mapped to processor 1 are considered.
After filling in the physical processor in these dimensions in 
source processors, the communication pattern 
becomes $\{(0, 1, 1)\rightarrow (*, 0, *), (0, 0, 1)\rightarrow 
(*, 1, *)\}$. Considering the $qual$ in $M$,  the dimension 0 of
the destination processor can be either 0 or 1. Applying the cross product
operation, the new communication list 
$\{(0, 1,1)\rightarrow (0, 0, *),\ (0, 1, 1,)\rightarrow (1, 0, *),\ (0, 0, 1)\rightarrow (0, 1, *),\
   (0, 0, 1)\rightarrow (1, 1, *)\}$ is obtained. Finally, 
the dimension 2 in the destination processor is always mapped to processor 0,
Thus, the final mapping is  
$\{(0, 1,1)\rightarrow (0, 0, 0), (0, 1, 1s)\rightarrow (1, 0, 0), (0, 0, 1)\rightarrow (0, 1, 0),
(0, 0, 1)\rightarrow (1, 1, 0)\}$.


There are several levels of approximations in the algorithm. 
First, 
when the algorithm cannot correlate the source and destination processor
dimensions from the mapping relation, the algorithm
uses an  approximation
of all--to--all connections. If the mapping relation contains
sufficient information to distinguish the relation of 
the source and destination processor dimension, 
computing the communication pattern for 
a multi-dimensional array reduces to computing 1-dimensional 
communication patterns, thus the approximations within each 
dimension are isolated to that dimension and will not affect the 
patterns in other dimensions. Using this multi-level approximation
scheme, some information is obtained when the compiler does not have
sufficient information for a communication. 

\section{Connection scheduling algorithms}
\label{cscheduling}

Once the communication requirement on physical processors is obtained,
the compiler uses off--line algorithms to perform
connection scheduling and determines the communication phases in a program. 
This section presents the connection scheduling 
algorithms and their performance evaluation. These algorithms assume
a torus topology.
%However, the fundamental principles of the algorithms
%can be extended to other topologies. 

For a given network, a set of connections that do not share any link is called
a configuration. In an optical TDM network with path multiplexing, 
multiple configurations can be supported simultaneously. Specifically, for 
a network with multiplexing degree $d$, $d$ configurations can be 
established concurrently. Thus, for a given communication pattern,
realizing the communication 
pattern with a minimum multiplexing degree is equivalent to  determining the
minimum number of configurations that contain all the connections in 
the pattern. Next,
 some definitions will be presented to formally state 
the problem of
connection scheduling. A connection  from a source $src$ 
to a destination $dst$ is denoted as $(src, dst)$.

\begin{description}
\item A pair of connections  $(s_1, d_1)$ and $(s_2, d_2)$ are said
to {\bf conflict}, if they cannot be simultaneously established because
they use the same link.

\item A {\bf configuration} is a set of connections
$\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ such that
no connections in the set conflict.

\item Given a set of connections  
$Comm = \{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$,
the set $MC =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} is a
{\bf minimal configuration set} for $Comm$ iff: \\
$\bullet$ 
each $C_i \in MC$ is a configuration and each connection 
$(s_{i}, d_{i}) \in R$ 
is contained in exactly one configuration in $MC$; and \\
$\bullet$
each pair of configurations $C_i,C_j \in MC$ contain connections 
$(s_i, d_i) \in C_i$ and $(s_j, d_j) \in C_j$ such that
$(s_i, d_i)$ conflicts with $(s_j, d_j)$.
\end{description}

It has been shown that optimal message scheduling 
for arbitrary topologies is NP-complete \cite{Chlamtac92}.
Therefore these algorithms are heuristics that are demonstrated to
provide good performance. Three 
connection scheduling heuristic algorithms that  compute a
minimal configuration set for a given connection set $Comm$ are described next.

%
\subsection{Greedy algorithm}
%
In the greedy algorithm, a configuration is created by repeatedly 
putting connections into the configuration until no additional 
connection can be established in that configuration.
If additional connections remain, another configuration is created
and this process is repeated till all connections have been processed.
This algorithm is a modification of an algorithm proposed in \cite{Qiao94}.
The algorithm is shown in Figure~\ref{SIMPLE}. The time complexity of
the algorithm is $O(|Comm|\times max_i(|C_{i}|)\times d)$, where $|Comm|$ 
is the number of the connections, $|C_{i}|$ is the number of connections
in configuration $C_{i}$ and $d$ is the number of configurations
generated.

\begin{figure}[htmb]
%\small
\begin{tabbing}
\hspace{0.5in}(1)\hspace{0.3in}MC = $\phi$, k = 1\\
\hspace{0.5in}(2)\hspace{0.3in}{\bf re}\={\bf peat}\\
\hspace{0.5in}(3)\hspace{0.3in}\> $C_{k} = \phi$\\
\hspace{0.5in}(4)\hspace{0.3in}\>{\bf for}\= {\bf each} $(s_{i}, d_{i}) 
                                              \in Comm$\\
\hspace{0.5in}(5)\hspace{0.3in}\>\>{\bf if}\=\ $(s_{i}, d_{i})$ does 
                                   not conflict with any 
                                   connection in $C_{k}$ {\bf then}\\
\hspace{0.5in}(6)\hspace{0.3in}\>\>\>$C_{k} = C_{k} \bigcup$ { $(s_{i}, d_{i})$ }\\
\hspace{0.5in}(7)\hspace{0.3in}\>\>\>Comm = Comm $-$ { $(s_{i}, d_{i})$ }\\
\hspace{0.5in}(8)\hspace{0.3in}\>\>{\bf end if}\\
\hspace{0.5in}(9)\hspace{0.3in}\>{\bf end for}\\
\hspace{0.5in}(10)\hspace{0.3in}\>MC = MC $\bigcup$ { $C_{k}$ }\\
\hspace{0.5in}(11)\hspace{0.3in}{\bf until} $Comm = \phi$\\
\end{tabbing}
\normalsize
\caption{The greedy algorithm.}
\label{SIMPLE}
\end{figure}

For example consider the linearly connected nodes shown in Figure~\ref{EXAM}. 
The result for applying the greedy algorithm to schedule connections 
set \{(0, 2), (1, 3),(3, 4), (2, 4)\} is shown in Figure~\ref{EXAM}(a). 
In this case, (0, 2) will be in time slot 1, (1, 3) in time slot 2, (3, 4) 
in time slot 1 and (2, 4) in time slot 3. 
Therefore, multiplexing degree 3 is needed to establish the paths for the 
four connections.  However,  as shown in Figure~\ref{EXAM} (b), 
the optimal scheduling for the four connections, which can be obtained
by considering the connection in different order, is to schedule (0, 2) in 
slot 1, (1, 3) in slot 2, (3, 4) in slot 2 and (2, 4) in slot 1. 
The second assignment only use 2 time slots to establish all the connections. 

\begin{figure}[htbp]
\begin{center}
\begin{picture}(0,0)%
\special{psfile=../962SC96/fig/961.3.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0050in}%
\begin{picture}(920,120)(75,640)
\end{picture}

\end{center}
\caption{Scheduling connections (0, 2), (1, 3),(3, 4), (2, 4)}
\label{EXAM}
\end{figure}


\subsection{Coloring algorithm}

The greedy algorithm  processes the connections in an arbitrary order.
This subsection describes an algorithm that applies a heuristic 
to determine the order to process the connections.
The heuristic assigns higher priorities to connections with fewer
conflicts. By giving the connections with less conflicts higher priorities, 
each configuration is likely to accommodate more connections and thus the
multiplexing degree needed for the patterns is likely to decrease. 

The problem of computing the minimal configuration set is formalized
as a graph coloring problem. A coloring of a graph is an assignment of 
a color to each node of the graph in such a manner that no two nodes 
connected by an edge have the same color. A conflict graph for a set of
connections is built in the following manner, (1) 
each node in the graph 
corresponds to a connection and (2) an edge
is introduced between two nodes if the connections represented by the 
two nodes are conflicted.
As stated by the theorem given below,
the number of colors used to color the graph is equal to the number of 
configurations needed to handle the connections. 

%\begin{description}
%\item
\noindent
{\bf Theorem:} Let $Comm=\{(s_{1}, d_{1}),(s_{2}, d_{2}),...,(s_{m}, d_{m})\}$
be the set of connections and $G = (V, E)$ be the conflict graph for $Comm$. 
There exists a configuration set $M = \{C_{1}, C_{2}, ..., C_{t}\}$
for $R$ if and only if $G$ can be colored with $t$ colors.
%\end{description}

\noindent
{\bf Proof}: Since connections that correspond to the nodes with the same 
color do not conflict with each other, they can be placed in 
one configuration. $\Box$

%Prove: ($\Rightarrow$) Assuming R has 
%configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. Let 
%$(s_{i}$, $d_{i}) \in C_{k}$, node $n_{i}$ can be colored by color $k$. 
%Therefore, there are totally $t$ colors in the graph. Now, we need to prove
%that for any two node $n_{i}$, $n_{j}$ such that $(i, j) \in E$, the two nodes
%are colored by different color. By the construction of conflict graph,
%if $(i, j) \in E$, node $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ share same links,
%hence, by the construction of configuration, $(s_{i}, d_{i})$ and
%$(s_{j}, d_{j})$  is in different configuration, thus $n_{i}$ and $n_{j}$ is
%colored by different colors. Hence, G can be colored by $t$ colors.
%
%($\Leftarrow$) Assuming G can be colored by $t$ colors. Let 
%$C_{i}$ = {$( s_{j}, d_{j})$ : $n_{j}$ is colored by color j}, 
%$M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. To prove 
%M is a configuration for R, we need to 
%prove 1) for any $(s_{i}, d_{i}) \in R$,
%there exists a $C_{k}$ such that $(s_{i}, d_{i}) \in C_{k}$, and 2) $C_{k}$
% must
%be a configuration. The first condition is trivial. Now, let us consider
%the second condition. Let $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ belong to 
%$C_{k}$, by the construction the G, $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$
%do not share any links. Therefore, $C_{k}$ is a configuration. Hence, there
%exist configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} for R. $\Box$
%
%\begin{description}
%\item
%{\bf Corollary:} The optimal multiplexing degree for establishing 
%connections in $R$ is equivalent to the minimum number colors to 
%color graph $G$.
%\end{description}


Thus, the
coloring algorithm attempts to minimize the number of colors used in 
coloring the graph. Since the coloring problem is known to be NP-complete, 
a heuristic is used for graph coloring. The heuristic determines the order 
in which nodes are colored using the node priorities.
The algorithm is summarized in Fig~\ref{COLOR}. It should be noted that
after a node is colored, the algorithm updates the priorities of uncolored 
nodes. This is because in computing the degree of an uncolored node, 
only  the edges that connect the node to other uncolored nodes are 
considered. 
The algorithm finds a solution in linear time (with respect to the 
size of the conflict graph). The time complexity of the algorithm is 
$O(|Comm|^2\times max_i(|C_{i}|)\times d)$, where $|Comm|$ is the 
number of the 
connections, $|C_{i}|$ is the number of connections
 in configuration $C_{i}$ and 
$d$ is the total number of configurations generated.


\begin{figure}[htbp]
%\small
\begin{tabbing}
\hspace{0.5in}(1)\hspace{0.3in} Construct conflict graph G = (V, E)\\
\hspace{0.5in}(2)\hspace{0.3in} Calculate the priority for each node\\
\hspace{0.5in}(3)\hspace{0.3in} MC = $\phi$, k = 1\\
\hspace{0.5in}(4)\hspace{0.3in} NCSET = V\\
\hspace{0.5in}(5)\hspace{0.3in} {\bf re}\={\bf peat}\\
\hspace{0.5in}(6)\hspace{0.3in} \>Sort NCSET by priority\\
\hspace{0.5in}(7)\hspace{0.3in} \> WORK = NCSET\\
\hspace{0.5in}(8)\hspace{0.3in} \> $C_{k} = \phi$\\
\hspace{0.5in}(9)\hspace{0.3in} \>{\bf wh}\={\bf ile} (WORK $\ne \phi$)\\
\hspace{0.5in}(10)\hspace{0.3in} \>\> Let $n_{f}$ be the first 
                                      element in WORK\\
\hspace{0.5in}(11)\hspace{0.3in} \>\>$C_{k} = C_{k} \bigcup \{<s_{f}, d_{f}>\}$\\
\hspace{0.5in}(12)\hspace{0.3in} \>\>NCSET = NCSET $- \{n_{f}\}$\\
\hspace{0.5in}(13)\hspace{0.3in} \>\>{\bf fo}\={\bf r} {\bf each}  $n_{i} \in NCSET$ 
                                      and $(f, i) \in E$ {\bf do} \\
\hspace{0.5in}(14)\hspace{0.3in} \>\>\> update the priority of $n_{i}$\\
\hspace{0.5in}(15)\hspace{0.3in} \>\>\> WORK = WORK - $\{n_{i}\}$\\
\hspace{0.5in}(16)\hspace{0.3in} \>\>{\bf end for}\\
\hspace{0.5in}(17)\hspace{0.3in} \>{\bf end while}\\
\hspace{0.5in}(18)\hspace{0.3in} \>MC = MC + $\{C_{k}\}$\\
\hspace{0.5in}(19)\hspace{0.3in} {\bf until} NCSET = $\phi$
\end{tabbing}
\normalsize
\caption{The graph coloring heuristic.}
\label{COLOR}
\end{figure}

For torus and mesh networks, a suitable choice for priority for a
connection is the ratio of the number of links in the path 
from the source to the destination and the degree of the node 
corresponding to the connection in $G$. 
Applying the coloring algorithm to the example in Figure~\ref{EXAM},
in the first iteration, the connections are reordered as 
$\{(0, 2), (1, 3), (2, 4), (3, 4)\}$ and connections (0, 2), (2, 4) will be
put in time slot 1. In the second iteration, connections (1, 3), (3, 4) are
put in time slot 2. Hence, applying the 
coloring algorithm will use 2 time slots
to accommodate the connections.

\subsection{Ordered AAPC algorithm}

The graph coloring algorithm has better performance than the greedy heuristic.
However, for dense communication patterns the heuristics cannot guarantee that
the multiplexing degree found would be bounded by the minimum multiplexing 
degree needed to realize the all-to-all pattern. The algorithm described in 
this section targets dense communication patterns. By grouping the connections
in a more organized manner, better performance can be achieved for dense 
communication.

The worst case of arbitrary communication is the {\em all-to-all personalized 
communication} (AAPC)  where each node sends a message to every 
other node in the system. Any communication pattern can be embedded in AAPC. 
Many algorithms \cite{Hinrichs94,Horie91} have been designed to 
perform AAPC efficiently for different topologies.
Among these algorithms, the ones that are of 
interests to us are the phased AAPC algorithms, in which the AAPC connections 
are partitioned into contention--free phases. A phase in this kind of AAPC 
corresponds to a configuration. Some phased AAPC algorithms are optimal in
that every link is used in each phase and every connection follows the
shortest path. Since all the connections in each AAPC phase are 
contention--free,
they form a configuration that uses all the links in the system. 
Each phase in the phased AAPC communication forms an {\em AAPC configuration}.
The set of {\em AAPC configurations} for AAPC communication pattern is 
called {\em AAPC configurations set}. 
The following theorem states the property 
of connection scheduling using AAPC phases.

%\begin{description}
%\item 

\noindent{\bf Theorem: } Let $Comm =
\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ be the 
set of connections, if $Comm$ can be partitioned into $K$ phases 
$P_1 = \{(s_{1}, d_{1}), ... , (s_{i_{1}}, d_{i_{1}})\}$,\\ 
$P_2 = \{(s_{i_{1} + 1}, d_{i_{1} + 1}), ... , (s_{i_{2}}, d_{i_{2}})\}$,
... ,
 $P_K = \{(s_{i_{K-1} + 1}, d_{i_{K-1} + 1}), ... , 
(s_{i_{K}}, d_{i_{K}})\}$, such that $P_i$, $ 1 \le i \le K$, is a subset
of an AAPC configuration. Using the greedy algorithm to schedule the
connections $(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})$
results in a multiplexing degree less than or equal to K.

\noindent
{\bf Proof}: The theorem is proven  by contradiction 
that for any $\alpha$, 
$1\le \alpha \le m$, let $(s_\alpha, d_\alpha) \in P_\beta$, 
$1\le \beta \le K$, connections $(s_1, d_1)$, ..., $(s_\alpha, d_\alpha)$
can be scheduled by the greedy algorithm using a multiplexing degree
less than or equal to $\beta$.

Let $(s_\alpha, d_\alpha) \in P_\beta$ be the first connection that does
not satisfy the above proposition. That is,  
$(s_1, d_1)$, ..., $(s_{\alpha-1}, d_{\alpha-1})$ are scheduled using 
a multiplexing degree of $\beta$ and $(s_\alpha, d_\alpha)$ cannot be
accommodated in configuration $\beta$. Since the connections 
in $P_\beta$ do not conflict with each other, 
another connection that belongs to  $P_\gamma$, $\gamma < \beta$ must be
scheduled in configuration $\beta$. Hence, $(s_\alpha, d_\alpha)$ is not the
first connection that does not satisfy the proposition, which contradicts
the assumption. $\Box$

The theorem states that if the connections
are reordered by the AAPC phases,
at most all AAPC 
phases are needed to realize arbitrary pattern using the
greedy scheduling algorithm. For example, following the algorithms in 
\cite{Hinrichs94}, $N^3/8$ phases are needed for a $N\times N$ torus. 
Therefore, in a $N\times N$ torus, $N^3/8$ degree is enough to satisfy
any communication pattern.

To obtain better performance on dense communication patterns, it is 
better to keep the connections in their AAPC format as much as possible. 
It is therefore better to schedule the phases with higher link 
utilization first. This heuristic is used in the ordered AAPC algorithm.
In ordered AAPC algorithm, the rank of the AAPC phases is calculated so 
that the phase that has higher utilization has higher rank. The phases 
are then scheduled according to their ranks. The algorithm is depicted in 
Figure~\ref{ORDAAPC}. The time complexity of this algorithm is
$O(|Comm|(lg(|Comm|) + max_i(|C_{i}|)\times K))$, where $|Comm|$ 
is the number of 
the connections, $|C_{i}|$ is the number of connections in configuration
$C_{i}$ and $K$ is the number of configurations needed. The advantage 
of this algorithm is that for this algorithm the multiplexing degree 
is bounded by $N^3/8$. Thus, in situations where the greedy or coloring
heuristics fail to meet this bound, AAPC can be used.  

\begin{figure}[ht]
%\small
\begin{tabbing}
\hspace{1in}(1)\hspace{0.3in}PhaseRank[*] = 0\\
\hspace{1in}(2)\hspace{0.3in}{\bf for}\= $(s_{i}, d_{i}) \in Comm$ {\bf do}\\
\hspace{1in}(3)\hspace{0.3in}\>let $(s_{i}, d_{i}) \in A_{k}$\\
\hspace{1in}(4)\hspace{0.3in}\>PhaseRank[k] = PhaseRank[k] + length($(s_{i}, d_{i})$)\\
\hspace{1in}(5)\hspace{0.3in}{\bf end for}\\
\hspace{1in}(6)\hspace{0.3in}sort phase according to PhaseRank\\
\hspace{1in}(7)\hspace{0.3in}Reorder $Comm$ according the sorted phases.\\
\hspace{1in}(8)\hspace{0.3in}call greedy algorithm\\
\end{tabbing}
\caption{Ordered AAPC scheduling algorithm}
\label{ORDAAPC}
\normalsize
\end{figure}

\subsection{Performance of the scheduling algorithms}

In this section,  the performance of the connection scheduling
algorithms on $8\times 8$ torus topology is studied. 
The performances of the algorithms 
are evaluated using randomly generated communication patterns, patterns
encountered during data redistribution, and some frequently used 
communication patterns. The metric used to compare the algorithms is the 
multiplexing degree needed to establish the connections.
It should be noted that a dynamic scheduling algorithm will not perform
better than the greedy algorithm since it must establish the connections 
by considering the connections in the order that they arrive. 

A {\em random communication pattern} consists of a certain number of 
random connections. A random connection is obtained
by randomly generating a source and a destination. Uniform probability
distribution is used to generate the sources and destinations.
The {\em data redistribution communication patterns} are obtained by
considering the communication results from array redistribution. In this
study,   data redistributions of a 3D array are considered. The array
has block--cyclic distribution in each dimension. The distribution of a
dimension can be specified by the block size and the number of processors
in the dimension.  A distribution is denoted  as {\em p:block(s)}, where
$p$ is the number of processors in the distribution and $s$ is the block size.
When the distribution of an
array is changed (which may result from the changing of the value $p$ or 
$s$), communication may be needed. 
Many programming
languages for supercomputers, such as CRAFT FORTRAN, allow an array to be
redistributed within a program. 

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
number of  & greedy  & coloring & AAPC & combined &improvement\\
connections. & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
100  & 7.0  & 6.7 & 6.9 & 6.6 & 6.3\%\\
\hline
400 & 16.5  & 16.1 & 16.5 & 15.9 & 3.8\%\\
\hline
800 & 27.2  & 25.9 & 26.5 & 25.6 & 6.3\%\\
\hline 
1200 & 36.3 & 34.5 & 35.3 & 34.2 & 6.1\%\\
\hline
1600 & 45.0  & 43.5 & 43.4 & 42.8 & 5.1\%\\
\hline
2000 & 53.4  & 50.4 & 50.4 & 49.7 & 7.4\%\\
\hline
2400 & 60.8  & 57.5 & 57.4 & 56.7 & 7.2\%\\
\hline
2800 & 68.8  & 64.4 & 62.4 & 62.4 & 10.2\%\\
\hline
3200 & 76.3  & 70.8 & 64 & 64 & 19.2\%\\
\hline
3600 & 83.9  & 76.8 & 64 & 64 & 31.1\%\\
\hline
4000 & 91.6  & 83 & 64 & 64 & 43.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Performance for random patterns}
\label{RANDOM}
\end{table}

Table~\ref{RANDOM} shows the multiplexing degree required to establish
connections for random communication patterns using the algorithms
presented.
The results in each row are the averages obtained from scheduling 100 different
randomly generated patterns with the specific number of connections.
The results in the column labeled {\em combined algorithm} are obtained 
by using 
the minimum of  the coloring algorithm and
the AAPC algorithm results.
Note that in compiled communication, more time can be spent
to obtain better runtime network utilization. Hence,  the
combined algorithm can be used to obtain better result by the compiler. The 
percentage  improvement shown in the sixth column
is achieved by the combined algorithm over the
dynamic scheduling. 
It is  observed that the coloring algorithm is always
better than the greedy  algorithm
and the AAPC algorithm is better than 
the other algorithms when the communication is dense. 
It can be seen that for sparse random patterns (100 - 2400
connections), the 
improvement range varies from 3.8\% to 7.2\%. Larger improvement
results for dense communication.  For example, the combined algorithm
uses 43.1\% less multiplexing degree than that of the greedy algorithm
for all--to--all pattern. 
This result confirms the result in \cite{Hinrichs94} that
it is desirable to use compiled communication for dense communication.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
No. of  & No. of  & greedy   & coloring & AAPC & combined &improvement\\
connections & patterns & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
0 - 100 & 34  & 1.2 &  1.2 & 1.2 & 1.2 & 0.0\%\\
\hline
101 - 200 & 50 & 5.9 & 4.9 & 4.8 & 4.6 & 28.3\%\\
\hline
200 - 400 & 54 & 10.6 &  9.7 & 10.0 &  9.5 & 11.6\%\\
\hline 
401 - 800 & 105 & 17.7 & 15.9 & 16.0& 15.5 & 14.2\%\\
\hline
801 - 1200 & 122 & 31.7 & 28.7 & 28.6 & 27.6 & 14.9\%\\
\hline
1201 - 1600 & 0  & 0      & 0    & 0    &0     &    0\%\\
\hline
1601 - 2000 & 15 & 46.3 &  42.8 & 35.1 & 35.1 & 31.9\%\\
\hline
2001 - 2400 & 77 & 55.5 &  51.5 & 51.9 & 50.4 & 10.1\%\\
\hline
2401 - 4031 & 0  & 0       & 0    & 0     &   0   &  0\% \\
\hline
4032     & 43 & 92  & 83 & 64 &  64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for data distribution patterns}
\label{REDIST}
\end{table}

To obtain more realistic results,  the performance is also evaluated using
the communication patterns for data redistribution and some
frequently used communication patterns which occurs in the programs 
analyzed by the E--SUIF compiler.
Table~\ref{REDIST} shows the performance of the algorithms for data 
redistribution patterns. The communication patterns
are  extracted from the communication resulting from 
the random data redistribution of a 3D array of size
$64 \times64 \times 64$. 
The  random data redistribution is created by randomly generating
the source data distribution and
the destination data distribution with regard to
the number of processors allocated to each dimension and the block size
in each dimension. Precautions are taken to make sure that the 
total processor number is 64 and the block size is not too large so that
some processors do not contain any part of the array.
The table lists the results for 500 random data redistributions. The first
column lists the range of the number of connections in each pattern.
The second column lists the number of data redistrictions whose number of 
connections fell into the range. For example, the second column in the
last row indicates that among the 500 random data redistributions, 43
results in 4032 connections. Columns three to six
list the multiplexing degree required by the greedy algorithm, the coloring
algorithm, the AAPC algorithm and the 
combined algorithm respectively. The seventh 
column lists the percentage improvement
 by the combined algorithm over the greedy
algorithm.
The result shows that the 
 multiplexing degree required to establish connections resulting
from data redistribution is less than that resulting 
from the random communication patterns. 
For the data redistribution pattern, the percentage improvement 
obtained by using
the combined algorithm  ranges from
10.1\% to 31.9\%, which is larger than the improvement for the random 
communication patterns.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Pattern & No. of conn. & greedy  & coloring & AAPC & comb & percentage\\
\hline
ring    & 128 &  3  &  2 & 2  &  2   &   50\%\\
\hline
nearest neighbor & 256 & 6  & 4 & 4  &  4 & 50\%   \\
\hline
hypercube & 384 &  9 & 7 & 8 &  7 & 28.6\%\\
\hline
shuffle--exchange & 126 &  6& 4 & 5  & 4 & 50\% \\
\hline 
all--to--all & 4032 &  92 & 83 & 64 & 64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for frequently used patterns}
\label{FUSED}
\end{table}

Table~\ref{FUSED} shows the performance for some
frequently used communication patterns. In the ring and the
nearest neighbor patterns, no conflicts arise in the links. 
However, there are conflicts in the communication switches.
The performance gain is higher for these 
specific patterns when the combined algorithm is used.

\section{Communication Phase analysis} 

Armed with the connection scheduling algorithms, the compiler can
determine when two communication patterns can be combined so that
the underlying network can support both patterns simultaneously and
thus, can partition a program into phases such that each phase contains
connections that can be supported by the underlying network. 
This section considers the compiler algorithm to partition a program.
%into phases so that each phase will contains communications
%that can be supported by the underlying network. 
%Besides using  the 
%algorithms described in the previous section to obtains communication 
%patterns (on physical processors), we also use the algorithms in 
%\cite{Yuan96} to obtain the number of channels needed to 
%support a communication pattern on torus networks.

The communication phase analysis is carried out in a recursive manner on 
the high level SUIF representation of a program, 
which is similar to an abstract syntax tree. 
%Here, the high level SUIF
%will be briefly introduced. 
%Before I present the communication phase analysis algorithm, I 
%will first briefly describe the high level SUIF representation. 
%Details about 
%the SUIF representation can be found in \cite{SUIF}.
SUIF represents a program in a hierarchical manner. 
A procedure contains a list of SUIF nodes, where each
node can be of different types and can contain sub--lists. Some important
SUIF node types include TREE\_FOR, TREE\_LOOP,
TREE\_IF, TREE\_BLOCK and TREE\_INSTR. 
A TREE\_FOR node represents a for--loop structure. 
It contains four sub--lists, $lb\_list$ which contains the SUIF
to compute the 
lower bound, $ub\_list$ which contains the nodes to compute the upper bound,
$step\_list$ which contains the nodes to compute the step, and 
$body$ which contains the 
loop body. A TREE\_LOOP node represents a 
while--loop structure. It contains two sub--lists, $test$ and $body$.
A TREE\_IF node represents an if--then--else structure. It contains
three sub--lists, $header$ which is the test part, $then\_part$ which contains
the nodes in the then part, and the $else\_part$. A TREE\_BLOCK node
represents a block of statements, it contains a sub--list $body$.
A TREE\_INSTR nodes represents a statement.

%Each structure has two variables associated with it, the 
%communication pattern, $pattern$, which is the communication patterns 
%expose in the header before any, and the phase indicator, $phase$ to indicate
%whether there are phases (network reconfiguration) within the structure. 

%\subsection{Phase analysis algorithms}

Given a SUIF representation of a program, which contains a list of nodes,
the communication phase analysis algorithm determines the communication
phases for each sub--lists in the list and then determines the 
communication phases of the list. 
In addition to the annotations for communications, a {\em composite node}, 
which contains sub--lists, is associated with
two variables, $pattern$, which is the communication pattern that is exposed
from the sub--lists, and the $kill\_phase$, which has a boolean value, 
indicating whether its sub--lists contain phases.

\begin{figure}[tbph]
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Communication\_Phase\_Analysis(list)\\
\hspace{0.5in}Input: $list$: a list of SUIF nodes\\
\hspace{0.5in}Output:\=\ $pattern$: communication pattern exposed out of the 
                      list\\ 
              \>$kill\_phase$: whether there are phases within the list\\
\\
\hspace{0.5in}\=An\=al\=yze communication phases for each node in the list.\\
       \>$c\_pattern = NULL, kill\_phase = 0$\\
       \>{\bf For each} node $n$ in list in backward order {\bf do}\\
       \>\> {\bf if} ($n$ is annotated with $kill\_phase$) {\bf then}\\
       \>\>\>Generate a new phase for $c\_pattern$ after $n$.\\
       \>\>\>$c\_pattern = NULL, kill\_phase = 1$\\
       \>\>{\bf end if}\\
       \>\> {\bf if} ($n$ is annotated with communication pattern $a$) 
           {\bf then}\\
       \>\>\>$new\_pattern = c\_pattern + a$\\
       \>\>\>{\bf if}\=\ ($multiplexing\_degree(new\_pattern) \le d$) 
             {\bf then}\\
       \>\>\>\>$c\_pattern$ = new\_pattern\\
       \>\>\>{\bf else}\\
       \>\>\>\>Generate a new phase for $c\_pattern$ after $n$.\\
       \>\>\>\>$c\_pattern = a, kill\_phase = 1$\\
       \>\>\>{\bf end if}\\
       \>\>{\bf end if}\\
       \>{\bf end for}\\
       \>{\bf return} $c\_pattern$ and $kill\_phase$
\end{tabbing}
\end{center}
\caption{Communication phase analysis algorithm}
\label{PHASE}
\end{figure}





\begin{figure}[tbph]
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Communication\_Phase\_Analysis for TREE\_IF\\
\\
\hspace{0.5in}\=Analyze the $header$ list.\\
             \>Analyze the $then\_part$ list.\\
             \>Analyze the $else\_part$ list.\\
             \>Let $comb =$ the combined communications from the three 
               sub--lists.\\
             \>{\bf If}\=\ (there are phase changes in the sub--lists) 
               {\bf then}\\
             \>\>Generate a phase in each sub--list for the 
                 communication exposed.\\
             \>\>$pattern = NULL, kill\_phase = 1$\\
             \>{\bf if} ($multiplexing\_degree(comb) > d$) {\bf then}\\
             \>\>Generate a phase in each sub--list for the 
                 communication exposed.\\
             \>\>$pattern = NULL, kill\_phase = 1$\\
             \> {\bf else}\\
             \>\> $pattern = comb, kill\_phase = 0$\\
             \>{\bf end if}\\
             \>Annotate the TREE\_IF node with $pattern$ and $kill\_phase$.
\end{tabbing}
\end{center}
\caption{Communication phase analysis for TREE\_IF nodes}
\label{treeif}
\end{figure}

The algorithm to analyze communication phases in a program
for a node list is shown in Figure~\ref{PHASE}. 
The algorithm assumes that the multiplexing degree for the 
system is $d$. It also uses one of the algorithms discussed in 
section~\ref{cscheduling}, denoted as\\
$multiplexing\_degree(Comm)$,
to compute the multiplexing degree required to 
realize communication pattern $Comm$. Given a node list, the algorithm 
first recursively examines the sub--lists of all nodes 
and annotates the nodes with $pattern$ and $kill\_phase$. 
This post--order traversal of the
SUIF program accumulates the communications in the innermost loops first,
and thus can capture the communication locality when it exists and is 
supported by the underlying
network. Figure~\ref{treeif} describes the operations for
TREE\_IF nodes. The algorithms for TREE\_IF node computes
the phases for the three sub--lists. In the cases when there are phases 
within the sub--lists and  when the network does not have enough capacity
to support the combined communication, a phase is created in 
each of the sub--list to accommodate the corresponding communication
from that sub--list. Otherwise, the TREE\_IF node is annotated with 
the combined communication indicating the communication requirement of
the IF statement. Algorithms for processing other node types are 
similar. After all sub--lists in all nodes in the list are analyzed, 
the node list contains a straight line program, whose nodes are annotated with
communication, $pattern$ and $kill\_phase$.
The algorithm examines all these annotations in each node
from back to front. A variable $c\_pattern$ is
used to maintain all communications currently accumulated. 
There are two cases when a phase is generated. First, 
once a $kill\_phase$ annotation is encountered, which indicates there are 
phases in the sub--lists,  thus, it does not make sense to maintain 
a phase passing the node since there are phase changes during the
execution of the sub--lists, a new
phase is created to accommodate the connection requirement after the node. 
Second, in the cases when adding a new communication pattern 
into the current (accumulated) pattern exceeds the network capacity, 
a new communication phase is needed.
% Notice that a SCD can represent a communication pattern that is
%as complex as all to all connections. It might require 
% several communication phases for a single communication.


\begin{figure}
\centerline{\psfig{figure=fig/phaseexam.eps,width=5.9in}}
\caption{An example for communication phase analysis}
\label{phaseexam}
\end{figure}

Figure~\ref{phaseexam} shows an example for the communication phase analysis.
The program in the example contains six communications, $C0$, $C1$,
$C2$, $C3$, $C4$, $C5$ and $C6$, an IF structure and a DO structure.
The communication phase analysis algorithm first analyzes
the sub--lists in the IF and DO structures. Assuming the combination
of  $C1$ and $C2$ can be supported by the underlying network, while 
combining communications $C1$, $C2$ and $C3$ exceeds the network 
capacity, which results in the two phases in the IF branches and the 
$Kill\_phase$ is set for the IF header node. Assuming that all communications
of $C5$ within the DO loop can be supported by the underlying network,
Figure~\ref{phaseexam}~(a) shows the results after the sub--lists are analyzed.
The algorithm then analyzes the list by considering each node from back
to forth, it combines communications $C4$ and $C5$. Since the IF header
node is annotated with $kill\_phase$. A new phase is generated for 
communications $C4$ and $C5$ after the IF structure. The algorithm then
proceeds to create a phase for communication $C0$. Figure~\ref{phaseexam}~(c)
shows the final result of the communication phase analysis for this example.

%Here let me construct a communication phase analysis example.
%Let me think of a good example.

\subsection{Evalutation of the communication phase analysis algorithm}
\label{evalphase}

This section  presents the performance evaluation of the E--SUIF compiler
for compiled communication.
The compiler is evaluated with respect to the analysis time and 
the runtime performance.  The E--SUIF compiler 
analyzes the communication requirement of a program
and partitions the program into phases 
such that each phase contains a communication pattern that can be realized 
by a multiplexing degree of $d$, where $d$ is a parameter. In addition, the
compiler also gives channel assignments for connections in each phase.
It is  assumed that the underlying network is a $8\times 8$ torus.

\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|}
\hline
Prog. & Description & Distrib.\\
\hline
0001 & Solution of 2-D Poisson Equation by ADI & (*, block)\\
\hline
0003 & 2-D Fast Fourier Transform & (*, block)\\
\hline
0004 & NAS EP Benchmark - Tabulation of Random Numbers & (*, block)\\
\hline
0008 & 2-D Convolution & (*, block)\\
\hline
0009 & Accept/Reject for Gaussian Random Number Generation & (block) \\
\hline
0011 & Spanning Percolation Cluster Generation in 2-D & (*, block)\\
\hline
0013 & 2-D Potts Model Simulation using Metropolis Heatbath & (*, block)\\
\hline
0014 & 2-D Binary Phase Quenching of Cahn Hilliard Cook Equation & (*, block) \\
\hline
0022 & Gaussian Elimination - NPAC Benchmark & (*, cyclic) \\
\hline
0025 & N-Body Force Calculation - NPAC Benchmark & (block, *) \\
\hline
0039 & Segmented Bitonic Sort & (block) \\
\hline
0041 & Wavelet Image Processing & (*, block)\\
\hline
0053 & Hopfield Neural Network & (*, block) \\
\hline
\end{tabular}
\end{center}
\caption{Benchmarks and their descriptions}
\label{desc}
\end{table}

Programs from the HPF benchmark suite
at Syracuse University are used to evaluate the algorithms. 
The benchmarks and their descriptions are listed in Table~\ref{desc}. 
The table also shows the data
distribution of the major arrays in the programs. These 
distributions are obtained from the original benchmark programs.
Table~\ref{analysistime} breaks down the analysis
time. The table shows the time for overall analysis, the logical
communication analysis and the communication phase analysis.
The overall analysis includes
the time to load and store the program, 
the time to analyze communication requirement on the
virtual processor grid, 
the time to derive communication requirement on the
physical processor grid and the time for communication phase analysis.
The communication phase analysis time accounts for a significant portion
of the overall analysis time for all the programs. 
This is because the communication phase operates on large sets of data
(communication pattern). %Hence, to apply this technique for large programs,
%the analysis cost must be improved by carefully designing the analysis 
%algorithms and data structures. 
However, for medium size programs, such as the benchmarks used,
the analysis time is not significant. 

\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|c|c|}
\hline
benchmarks & size (lines) & overall & logical communication & phase analysis\\
\hline
0001 & 545 & 11.33 & 0.45 & 8.03 \\
\hline
0003 & 372 & 24.83 & 0.50 & 11.80\\
\hline
0004 & 599 & 19.08 & 0.42 & 15.02\\
\hline
0008 & 404 & 27.08 & 0.68 & 13.28\\
\hline
0009 & 491 & 46.72 & 4.45 & 19.65\\
\hline
0011 & 439 & 14.78 & 0.57 & 11.37\\
\hline
0013 & 688 & 23.08 & 1.07 & 17.30\\
\hline
0014 & 428 & 15.58 & 1.03 & 11.38 \\
\hline
0022 & 496 & 22.57 & 0.77 & 18.35 \\
\hline
0025 & 295 & 5.77 & 0.78 & 3.35 \\
\hline
0039 & 465 & 16.08 & 0.38 & 13.13 \\
\hline
0041 & 579 & 9.93 & 0.28 & 6.62\\
\hline
0053 & 474 & 7.39 & 0.35 & 4.33\\
\hline
\end{tabular}
\end{center}
\caption{Communication phase analysis time}
\label{analysistime}
\end{table}


\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|c|c|c|c|}
\hline
benchmark & \multicolumn{3}{c|}{connections per phase} &  \multicolumn{3}{c|}{
channels per phase}\\
\cline{2-7}
programs  & actual & compiled & percentage & actual & compiled & percentage\\
\hline
0001 & 564.4 & 564.4 & 100\% & 9.1 & 9.1 &100\% \\
\hline
0003 & 537.6 & 537.6 & 100\% & 8.6 & 8.6 & 100\% \\
\hline
0004 & 116.3 & 116.3 & 100\% & 5.5 & 5.5 & 100\% \\
\hline
0008 & 562.6 & 562.6 & 100\% & 8.9 & 8.9 & 100\% \\
\hline
0009 & 91.2 & 230.7 & 39.6\% & 4.3 & 6.6 & 65.1\% \\
\hline
0011 & 126.3 & 126.3 & 100\% & 5.2 & 5.2 & 100\% \\
\hline
0013 & 67.3 & 67.3 & 100\% & 3.1 & 3.1 & 100\% \\
\hline
0014 & 126.4 & 126.4 & 100\% & 4.0 & 4.0 & 100\% \\
\hline
0022 & 13.1 & 413.2 & 3\% & 4.6 & 8.9 & 52.7\% \\
\hline
0025 & 80.0  &80.0 & 100\% & 3.0 & 3.0 & 100\% \\
\hline
0039 &125.7 & 125.8 & 99.9\% & 8.8 & 8.8 & 99.9\%\\
\hline
0041 & 556.1 & 556.1 & 100\% & 8.8 & 8.8& 100\% \\
\hline
0053 & 149.2& 575.2 & 25.9\% & 9.0 & 9.1 & 98.9\\
\hline
\end{tabular}
\end{center}
\caption{Analysis precision}
\label{precision}
\end{table}


Table~\ref{precision} shows the precision of the analysis. It compares
the average number of channels and connections per phase obtained from
our algorithms with  those in  actual executions. The number of channels and 
connections per phase in actual executions is obtained by accumulating the
connections within each phase, which is determined by the compiler. When 
a phase change occurs, the statistics about the number of connections within
each phase is collected and the connection scheduling algorithm is invoked
to compute the number of channels needed for the connections in that phase.
For most programs, the analysis results match the actual program executions,
which indicates that approximations are seldom used. 
For the programs where
 approximations occur, the channel approximation is better
than the connection approximation as shown in benchmark 0022. This is mainly
due to the approximation of the communications that are not vectorized.
For such communications, if the underlying network can support all connections
 in a loop, the phase will contain the loop and use the channels
for all communications in the loop. However, for the connections, the 
compiler approximates each individual communication inside the loop with
all communications of the loop. Since the number of channels for a
communication pattern determines the communication performance, this 
type of approximation does not hurt the communication performance.

\section{Chapter summary}

This chapter addressed the compiler issues for applying compiled 
communication. 
In particular, algorithms for communication analysis were presented
which take into consideration common communication optimizations
including message vectorization, redundant communication elimination and 
message scheduling. A demand
driven array data flow framework, which improves over previous communication
optimization algorithms by reducing the analysis cost and improving the 
analysis precision,  was developed 
for the communication optimizations. 
Three off-line connection
scheduling algorithms were described that realize 
a given communication pattern with a minimal multiplexing degree. 
A communication phase analysis algorithm, which partitions a program 
into phases such that each phase contains communications that can be
supported by the underlying network, was developed. 
The algorithm also exploits 
communication locality to reduce the amount of reconfiguration overhead
during program execution.

A compiler, called the E--SUIF compiler, implements all the above
algorithms and thus, supports  compiled communication.  
The E--SUIF compiler 
compiles a HPF--like program, analyzes its communication requirement,
partitions the program into phases such that each phase contains 
connections that can be supported by the underlying network,
assigns channels for connections in each phase, and outputs a C program
with the communication and phase annotations such that when 
the program is executed, the communications (and phases) 
in the program can be simulated.
All the algorithms were evaluated in the compiler. It was found that
the communication optimization algorithms are efficient in terms of 
the analysis cost and are  effective in finding the optimization 
opportunities. The communication phases analysis algorithm 
generally captures the program runtime behavior accurately. 

In the last three chapters, techniques for the 
three communication schemes are discussed.
Next chapter evaluates the three 
communication schemes and compares their performance using real application
programs. 










\documentstyle[11pt,psfig,setspace,subfigureh,psubfigure,fancyheadings]{thesis}
\newcounter{DefCount} 

\begin{document}

\maketitlepage

\makesignaturepage

\makecopyrightpage

\abstract
{
%With the increasing computation power of parallel computers, 
%interprocessor communication has become an important factor that
%limits the performance of supercomputing systems.
Optical interconnection networks are promising networks for future 
supercomputers due to their large bandwidths. However, the speed mismatch
between the fast optical data transmission and the relatively
slow electronic control components poses challenges for 
designing an optical network whose large bandwidth can be utilized 
by end users. The Time--Division--Multiplexing (TDM) technique 
alleviates this mismatch problem by sacrificing part of the large
optical bandwidth for efficient network control. 
This thesis studies efficient control mechanisms for optical
TDM point--to--point networks. Specifically, 
three communication schemes are considered, 
dynamic single--hop communication, dynamic 
multi--hop communication and compiled communication. 

%\thispagestyle{fancy}
%\setlength{\headrulewidth}{0pt}
%\rhead{Dr. Rami Melhem, Dr. Rajiv Gupta}
%\markright{\hspace{3.5in}Dr. Rami Melhem, Dr. Rajiv Gupta}

Dynamic single--hop communication uses a path reservation protocol to 
establish all--optical paths for connection requests that arrive at
the network dynamically. 
%In this 
%scheme, once a message is converted into the optical domain, 
%it remains there until it reaches the destination.
%No electronic/optical (E/O) and optical/electronic (O/E) conversions, 
%or electronic processing and buffering are performed 
%at intermediate nodes during data transmission. 
%In this communication scheme, 
%the electronic processing
%is isolated in the path reservation process. Hence, 
An efficient path 
reservation protocol is essential for this scheme to achieve high 
performance. In this thesis, a number of efficient distributed path
reservation protocols are designed and the impact of 
system parameters on these protocols is studied. 

Dynamic multi--hop communication allows intermediate hops to 
route messages toward their destinations.
% and thus, requires
%E/O and O/E conversions and electronic processing at intermediate
%hops. 
%
% Since the routing at 
%intermediate hops requires E/O and O/E conversions, it is
%important to reduce the number of intermediate hops in this 
%scheme. 
In optical TDM networks,
efficient dynamic multi--hop communication can be achieved
by routing messages through a logical topology that is more efficient than 
the physical topology. 
This thesis studies efficient schemes to realize logical topologies 
on top of physical torus topologies, presents an analytical model
for analyzing the maximum throughput and the average packet delay  
for multi--hop networks, and evaluates the performance of the optical 
communication on the logical 
topologies.

%In compiled communication, the compiler analyzes the communication
%requirement of a program and  partitions the program into phases 
%such that each phase contains communications 
%that can be supported by the underlying network. Using the 
%knowledge of the underlying network and the communication requirement, 
%the compile manages network resources, such as virtual
%channels, statically. By using the compiled communication technique, 
%runtime communication overheads, such as the path reservation overhead, 
%can be reduced or eliminated, and the communication 
%performance is improved. This technique is particularly attractive to 
%optical interconnection networks since optical networks have large control 
%overheads compared to electronic networks. 

Compiled communication eliminates the runtime communication overheads
of the dynamic communications by managing network resources 
at compile time. 
%Many compiler issues must be addressed in order to apply
%this technique. 
This thesis considers
issues for   applying  the compiled communication technique to
optical TDM networks, including communication analysis, connection scheduling
and communication phase analysis. 
%Specifically, it describes algorithms to 
%analyze the communication requirement of a program, proposes off--line 
%connection scheduling schemes that schedule connections using 
%a minimal multiplexing degree, and describes a 
%communication phase analysis algorithm which 
%partitions the program into phases such that each phase 
%contains communications that can be supported by the underlying network. 
A compiler, called the E--SUIF compiler, is implemented to support
compiled communication on optical TDM networks.
%All the algorithms are implemented and evaluated in a compiler based on the 
%Stanford SUIF compiler. 

%Communication in optical interconnection networks can be carried out 
%using any of the three communication schemes. 
Each communication scheme has its 
advantages and limitations
and is more efficient for some types of
communication patterns. 
%Dynamic single--hop communication 
%achieves all--optical communication during data transmission, 
%however, it requires extra hardware support
%to exchange control messages and results in large startup overhead.
%Dynamic multi--hop communication does not have large startup overhead,
%yet it requires electronic processing during data transmission. Compiled
%communication accomplishes all--optical communication without extra
%hardware support and large startup overhead. However, the performance
%of compiled communication relies heavily on the precision of compiler 
%analysis. It cannot be applied to applications whose communication patterns
%are known only at runtime time. 
This thesis compares the performance 
of the three communication schemes using a number of benchmarks and real
application programs and identifies the situations where each communication 
scheme out--performs the other schemes. 
}

\tableofcontents
\listoffigures
\listoftables

\bibliographystyle{plain}

\input{intro}

\input{background}

\input{singlehop}
\input{multihop}
\input{compiled}
\input{perform}

%\newpage

\begin{thebibliography}{9}

\bibitem{Acampora89}
A.S. Acampora and M.J. Karol, ``An Overview of Lightwave Packet Network.''
{\em IEEE Network Mag.}
3(1), pages 29-41, 1989.

\bibitem{amarasinghe93}
S. P. Amarasinghe and M. S. Lam ``Communication Optimization and Code
Generation for Distributed Memory Machine.'' In {\em Proceedings ACM SIGPLAN'93
Conference on Programming Languages Design and Implementation}, June 1993.

\bibitem{Amar95}
S. P. Amarasinghe, J. M. Anderson, M. S. Lam and C. W. Tseng,
``The SUIF Compiler for Scalable Parallel Machines.'' 
{\em Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing}, February, 1995.

\bibitem{As94}
H.R. As, ``Media Access Techniques: the Evolution towards Terabit/s LANs
and MANs.'' {\em Computer Networks and ISDN Systems},
26(1994) 603--656.

\bibitem{banerjee95}
P. Banerjee, J. A. Chandy, M. Gupta, E. W. Hodges IV, J. G. Holm, A. Lain, D. J. Palermo, S. Ramaswamy, and E. Su.
``The PARADIGM Compiler for Distributed-Memory Multicomputers.''
     in {\em IEEE Computer}, Vol. 28, No. 10, pages 37-47, October 1995.

\bibitem{Bannister90}
J.A. Bannister, L. Fratta and M. Gerla ``Topological Design of the
Wavelength--Division Optical Network.'' {\em IEEE INFOCOM'90}, 
pages 1005---1013, 1990.


\bibitem{Barry95}
R.A. Barry and P.A. Humblet. ``Models of Blocking Probability in All--optical
Networks with and without Wavelength Changers.'' In {\em Proceeding of
IEEE Infocom}, pages 402-412, April 1995.

\bibitem{beauquier97} B. Beauquier, J. Bermond, L. Gargano, P. Hell,
S. Perennes and U. Vaccaro ``Graph Problems Arraying from Wavelength--Routing
in All--Optical Networks.'' {\em Workshop on Optics and 
Computer Science}, 1997. 

\bibitem{Brackett90}
C. A. Brackett, ``Dense wavelength division multiplexing networks:
Principles and applications,'' {\em IEEE Journal on Selected Areas of 
Communications}, Vol. 8, pp. 948-964, Aug. 1990.

\bibitem{Brassil94} 
J. Brassil, A. K. Choudhury and N.F. Maxemchuk, ``The Manhattan Street 
Network: A High Performance, Highly Reliable Metropolitan Area Network,''
{\em Computer Networks and ISDN Systems},
26(6-8), pages 841-858, 1994.


\bibitem{Bromley91} 
M. Bromley, S. Heller, T. McNerney and G. L. Steele, Jr. ``Fortran at
Ten Gigaflops: the Connection Machine Convolution Compiler'.'' In
{\em Proc. of SIGPLAN'91 Conf. on Programming Language Design and 
Implementation}. June, 1991.

\bibitem{callahan88}
D. Callahan and K. Kennedy ``Analysis of Interprocedural Side Effects in a 
Parallel Programming Environment.'' {\em Journal of Parallel and Distributed
Computing}, 5:517-550, 1988.

\bibitem{Cappello95}
F. Cappelllo and C. Germain. ``Toward high 
communication performance through compiled communications on a 
circuit switched interconnection network.'' In {\em Proceedings of the Int'l
Symp. on High Performance Computer Architecture}, pages 44-53, Jan. 1995.

\bibitem{Chakrabarti96}
S. Chakrabarti, M. Gupta and J. Choi ``Global Communication 
Analysis and Optimization.'' {\em Proceedings of the ACM SIGPLAN'96
Conference on Programming Language Design and Implementation} (PLDI),
Pages 68 --- 78, Philadelphia, PA, May, 1996.

\bibitem{Chapman92}
B. Chapman, P. Mehrotra and H. Zima ``Programming in Vienna Fortran.''
{\em Scientific Programming}, 1:31--51, Fall 1992.

\bibitem{Chatterjee93}
S. Chatterjee, J. R. Gilbert, R. Schreiber and S. Teng ``Automatic Array 
Alignment in Data--Parlllel Programs.'' {\em Proceedings of the 20th Annual
ACM Symposium on Principles of Programming Languages}, Charleston, SC, Jan.
 1993.

\bibitem{Chatterjee93a}
S. Chatterjee, J. Gilbert, F. J. E. Long, R. Schreiber and S. Teng 
``Generating local addresses and communication sets for data--parallel
programs.'' In {\em Proc. of PPoPP}, pages 149--158, San Diego, CA, May 1993.


\bibitem{chen96}
C. Chen and s. Banerjee, ``A New Model for Optimal Routing and Wavelength
Assignment in Wavelength Division Multiplexed Optical Networks,'' {\em 
Proc. IEEE Infocom'96}, pages 164--171, 1996.

\bibitem{Chlamtac92}
I. Chlamtac, A. Ganz and G. Karmi. ``Lightpath 
Communications: An Approach to High Bandwidth Optical WAN's'' {\em 
IEEE Trans. on
 Communications}, Vol. 40, No. 7, July 1992.

\bibitem{chlamtac93}
I.Chlamtac, A. Ganz and G. Karmi ``Lightnets: Topologies for High--Speed
Optical Networks.'' {\em Journal of Lightwave Technology}, Vol. 11,
No. 5/6, pages 951---961, May/June 1993.


\bibitem{Dally87}
W. Dally and C. Seitz, ``Deadlock--Free Message Routing in Multiprocessor
Interconnection Networks.'' {\em IEEE trans. on Computers}, Vol. C--36,
No. 5, May 1987.

\bibitem{Dowd93}
P. Dowd, K. Bogineni and K. Ali,
``Hierarchical Scalable Photonic Architectures for High-Performance Processor Interconnection'',
{\em IEEE Trans. on Computers},
vol. 42, no. 9, pp. 1105-1120, 1993.

\bibitem{ganz92}
A. Ganz and Y. Gao,
``A Time-Wavelength assignment algorithm for WDM Start Networks'',
{\em Proc. of IEEE INFOCOM},
1992.

\bibitem{Gong93}
C. Gong, R. Gupta and R. Melhem. ``Compilation Techniques for
Optimizing Communication on Distributed-Memory System''. {\em International
conference on Parallel Processing}. Vol. II, pages 39-46, 
August 1993.

\bibitem{Gross89}
T. Gross. ``Communication in iWarp Systems.'' In {\em Proceedings 
Supercomputing}'89, pages 436--445, ACM/IEEE, Nov. 1989.

\bibitem{Gross94} 
T. Gross, A. Hasegawa, S. Hinrichs, D. O'Hallaron, and T. Stricker
``Communication Styles for Parallel Systems.'' {\em IEEE Computer}, 
vol.27, no. 12, December, 1994, pp. 34-44.

\bibitem{Gross94a}
T. Gross, D. O'Hallaron, and J. Subhlok ``Task parallelism in a High 
Performance Fortran framework.'' {\em IEEE Parallel \& Distributed Technology},
 vol 2, no 2, 1994, pp 16-26.

\bibitem{Gupta92}
M. Gupta and P. Banerjee. ``A Methodology for High--Level Synthesis of 
Communication on Multicomputers.'' In {\em International Conference on
Supercomputing}, Pages 357--367, 1992.

\bibitem{Gupta92a}
M. Gupta and P. Banerjee. ``Demonstration of Automatic Data Partitioning
Techniques for Parallelizing Compilers on Multicomputers.'' {\em IEEE
Trans. on Parallel and Distributed Systems}, 3(2)179-193, 1992.

\bibitem{gupta93}
M. Gupta and E. Schonberg ``A Framework for Exploiting Data Availability to
Optimize Communication.'' In {\em 6th International Workshop on Languages
and Compilers for Parallel Computing}, LNCS 768, pp 216-233, August 1993.

\bibitem{gupta95}
M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K.Y. Wang, D. Shields,
W.M. Ching and T. Ngo. ``An HPF compiler for the IBM SP2.'' In 
{\em proc. Supercomputing'95}, San Diego, CA, Dec. 1995.

\bibitem{Gupta96}
M. Gupta, E. Schonberg and H. Srinivasan ``A Unified Framework for
Optimizing Communication in Data-parallel Programs.'' In {\em IEEE trans.
on Parallel and Distributed Systems}, Vol. 7, No. 7, pages 689-704,
July 1996.

\bibitem{Hinrichs94}
S. Hinrichs, C. Kosak, D.R. O'Hallaron, T. Stricker and 
R. Take. ``An Architecture for Optimal All--to--All Personalized
Communication.'' In {\em 6th Annual ACM Symposium on Parallel Algorithms and
Architectures}, pages 310-319, June 1994.

\bibitem{Hinrichs95}
S. Hinrichs. ``Compiler Directed Architecture--Dependent Communication
Optimization.'' Ph.D dissertation, School of Computer Science, 
Carnegie Mellon University, 1995.

\bibitem{Hinrichs95a}
S. Hinrichs ``Simplifying Connection--Based Communication.'' {\em IEEE Parallel
and Distributed Technology}, 3(1)25--36, Spring 1995.

\bibitem{hinton}
H. Scott Hinton,
``Photonic Switching Using Directional Couplers'',
{\em IEEE Communication Magazine},
Vol 25, no 5, pp 16-26, 1987.

\bibitem{hiran92}
S. Hiranandani, K. Kennedy and C. Tseng ``Compiling Fortran D for MIMD
Distributed--memory Machines.'' {\em Communications of the ACM}, 35(8):66-80,
August 1992.

\bibitem{Horie91}
T. Horie and K. Hayashi. ``All--to--All Personalized 
Communication on a wrap--around Mesh.'' In {\em Proceedings of CAP Workshop},
November, 1991.

\bibitem{HPF}
High Performance Fortran Forum. {\em High Performance Fortran Language 
Specification Version 1.0.}, May 1993.

\bibitem{Ikegami97}
T. Ikegami ``WDM Devices, State of the Art.'' {\em Photonic Networks},
Springer, pages 79--90, 1997.

\bibitem{Kennedy95}
K. Kennedy and N. Nedeljkovic ``Combining dependence and data-flow analyses
to optimize communication.'' In {\em Proceedings of the 9th International
Parallel Processing Symposium}, Santa Barbara, CA, April 1995.

\bibitem{Knobe90}
K. Knobe, J.D. Lukas and G.L. Steele, Jr. ``Data Optimization:
Allocation of Arrays to Reduce Communication on SIMD Machines.''
{\em Journal of Parallel and Distributed Computing}, 8:102-118, 1990.

\bibitem{Koelbel90}
C. Koelbel ``Compiling Programs for Nonshared Memory
Machines.'' Ph.D thesis, Purdue University, August 1990.

\bibitem{Koelbel91}
C. Koelbel and P. Mehrotra ``Compiling global name--space parallel loops
for distributed execution.'' {\em IEEE Trans. on Parallel and Distributed 
Systems}, 2(4):440-451, Oct. 1991.

\bibitem{kovacevic95}
M. Kovacevic, M. Gerla and J.A. Bannister, ``On the performance of 
shared--channel multihop lightwave networks,'' {\em
Proceedings IEEE INFOCOM'95}, Boston, MA, pages 544--551, April 1995.

\bibitem{Kumar92}
M. Kumar. ``Unique Design Concepts in GF11 and Their Impact on
Performance''. {\em IBM Journal of Research and Development}. Vol. 36
No. 6, November 1992.

\bibitem{Labour91}
J. P. Labourdette and A. S. Acampora ``Logically Rearrangeable Multihop
Lightwave Networks.'' {\em IEEE Trans. on Communications}, Vol. 39, No. 8,
pages 1223---1230, August 1991.


\bibitem{Lahaut94}
D. Lahaut and C. Germain, ``Static Communications in 
Parallel Scientific Programs.'' In {\em 
Parallel Architecture \& Languages}, Europe,
pages 262--274, Athen, Greece, July 1994.

\bibitem{lee95}
S. Lee, A. D. Oh and H.A. Choi ``Hypercube Interconnection in TWDM
Optical Passive Star Networks'', {\em Proc. of the 2nd  International 
Conference on Massively Parallel Processing Using Optical Interconnections.}
San Antonio, Oct. 1995.

\bibitem{Leighton92}
F. Leighton, {\em Introduction to parallel algorithms and architecture: arrays,
trees, hypercubes.} Morgan Kaufmann, 1992.

\bibitem{Li91}
J. Li and M. Chen. ``Compiling  Communication --efficient Programs for 
Massive Parallel Machines.'' {\em IEEE Trans. on Parallel and Distributed 
Systems}, 2(3):361-376, July 1991.

\bibitem{Li91a}
J. Li and M. Chen ``The Data Alignment Phase in Compiing  Programs for
Distributed Memory Machines.'' {\em Journal of Parallel and Distributed 
Computing}, 13(2):213--221, October 1991.


\bibitem{Melhem95}
R. Melhem, ``Time--Multiplexing Optical Interconnection
Network; Why Does it Pay Off?'' In {\em Proceedings of the 1995 ICPP 
workshop on Challenges for Parallel Processing}, pages 30--35, August 1995.

\bibitem{MPI93} 
``The Message Passing Interface Forum''. {\em Draft Document for
a Standard Message Passing Interface}, November 1993.

\bibitem{Mukherjee92a}
 B. Mukherjee, ``WDM--based local lightwave networks --- Part I: Single--hop 
systems,'' {\em IEEE Network Magazine}, vol. 6, no. 3, pp. 12--27, May 1992.

\bibitem{Mukherjee92b} 
 B. Mukherjee, ``WDM--based local lightwave networks --- Part II: Multihop
systems,'' {\em IEEE Network Magazine}, vol. 6, no. 4, pp. 20--32, July 1992.

\bibitem{Mukherjee94}
B. Mukherjee, S. Ramamurthy, D. Banerjee and A. Mukherjee ``Some Principles
for Designing a Wide--Area Optical Network.'' {\em IEEE INFOCOM'94}, Vol. 1,
pages 1d1.1---1d1.10, 1994.



\bibitem{Nugent88} 
S. Nugent, ``The iPSC/2 direct--connect communications technology.'' In
{\em Proceedings of the 3rd conference on Hypercube Concurrent Computers and 
Application}, Volume 1, Jan. 1988.

\bibitem{Numrich94} R. W. Numrich, P.L. Springer and J.C. Peterson, 
``Measuerment of Communication Rates on the CRAY-T3d Interprocessor
Network''. In {\em Proceedings of High Performance Computing and Networking},
LNCS 797.

\bibitem{PVM94}
R. Manchek, ``Design and Implementation of PVM version 3.0'',
Technique report, University of Tennessee, Knoxville, 1994.


\bibitem{Qiao94}
C. Qiao and R. Melhem, ``Reconfiguration with Time Division Multiplexed 
MIN's for Multiprocessor Communications.'' {\em IEEE Trans. on Parallel and
Distributed Systems}, Vol. 5, No. 4, April 1994.

\bibitem{Qiao95}
C. Qiao and R. Melhem. ``Reducing Communication Latency with Path Multiplexing
in Optically Interconnected Multiprocessor Systems.'' In {\em Proceedings
of the International Symposium on High Performance Computer Architecture},
pages 34-43, January 1995.

\bibitem{Qiao96}
C. Qiao and Y. Mei, ``On the Multiplexing Degree Required to Embed 
Permutation in a Class of Networks with Direct Interconnects.'' 
In {\em IEEE Symp. on High Performance Computer Architecture}, Feb. 1996.

\bibitem{Ramaswami94}
R. Ramaswami and K. Sivarajan, ``Optimal Routing and Wavelength Assignment
in All--Optical Networks.'' {\em IEEE INFOCOM'94}, vol. 2, pages 970--979,
June 1994.

\bibitem{rogers89}
A. Rogers and K. Pingali ``Process decomposition through locality of
reference.'' In {\em Proc. SIGPLAN'89 conference on Programming Language
Design and Implementation}, pages 69-80, June 1989.

\bibitem{Salisbury97}
C. Salisbury and R. Melhem ``Modeling Communication Costs in Multiplexed 
Optical Switching Networks'', The {\em 
International Parallel Processing Symposium}, Geneva, 1997.

\bibitem{Sivarajan91}
K.Sivarajan and R. Ramaswami, ``Multihop networks based on de bruiji graphs,''
{\em Proceedings IEEE INFOCOM'91}, Bal Harbour, FL, pages 1001--1011, 
April 1991.

\bibitem{Sivalingam93}
K.M. Sivalingam and P.W. Dowd, ``Latency hiding strategies of pre--allocation
based media access protocols for WDM phontic networks,'' in {\em Proc. 26th
IEEE Simulation Symposium}, pages 68 -- 77, Mar. 1993.


\bibitem{Stichnoth93}
J. Stichnoth, D. O'Hallaron, and T. Gross ``Generating communication for array
    statements: Design, implementation, and evaluation,''
{\em Journal of Parallel and
    Distributed Computing}, vol. 21, no. 1, Apr, 1994, pp. 150-159. 

\bibitem{Subhlok94}
J. Subhlok, D. O'Hallaron, T. Gross, P. Dinda, J. Webb ``Communication and
    memory requirements as the basis for mapping task and data parallel 
    programs.'' {\em Proc. Supercomputing '94}, Washington, DC, Nov. 1994, 
    pp. 330-339. 

\bibitem{Subhlok93}
J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross ``Exploiting task and 
data  parallelism on a multicomputer,'' {\em Proceedings of the ACM 
SIGPLAN Symposium on Principles and Practice of Parallel Programming}, 
San Diego, CA, May, 1993, pp 13-22. 

\bibitem{Subramanian96}
S. Subramanian,  M. Azizoglu and A. Somani, ``Connectivity and Sparse
Wavelength Conversion in Wavelength-Routing Networks.''
{\em Proc. of INFOCOM'96}, pages 148--155,
1996.

\bibitem{SUIF}
Stanford Compiler Group ``The SUIF Library'', Stanford University.

\bibitem{Sussman92}
A. Sussman, G. Agrawal and J. Saltz, ``PARTI primitives for unstructured
and block structured problems.'' {\em Computing Systems in Engineering},
Vol. 3, No. 4, pages 73--86, 1992.

\bibitem{Tarjan74}
R.E. Tarjan ``Testing flow graph reducibility.'' 
{\em Journal of Computer and System Sciences}, 9:355-365, 1974.

\bibitem{Vengsarkar97}
A.M. Vengsarkar ``Optical Fiber Devices.'' {\em Photonic Networks}, Springer,
Pages 133--140, 1997.

\bibitem{Venkat96}
A. Venkateswaran and A. Sengupta ``On a Scalable Topology for Lightwave 
Networks.'' {\em IEEE INFOCOM'96}, Vol. 2, pages 4a.4.1---4a.4.8, 1996.

\bibitem{Yuan96b}
X. Yuan, R. Gupta and R. Melhem, ``Distributed Control in Optical 
WDM Networks,'' {\em
IEEE Conf. on Military Communications}(MILCOM), pages 100-104, McLean, VA, Oct.
21-24, 1996. 

\bibitem{Yuan96}
X. Yuan, R. Gupta and R. Melhem, "Demand-driven Data Flow Analysis for 
Communication Optimization," {\em Workshop on Challenges in Compiling for 
Scalable Parallel Systems}, New Orleans, Louisiana, Oct. 23-26, 1996.

\bibitem{Yuan96a}
X. Yuan, R. Melhem and R. Gupta ``Compiled Communication for All--optical
TDM Networks'', {\em Supercomputing'96}, Pittsburgh, PA, 
Nov. 1996.

\bibitem{Yuan97}
X. Yuan, R. Melhem and R. Gupta ``Distributed Path Reservation Algorithms
for Multiplexed All-optical Interconnection networks'' {\em the 
Third International
Symposium on High Performance Computer Architecture(HPCA 3)}, San Antonio,
Texas, Feb.1-5, 1997

\bibitem{Yuan97a}
X. Yuan, R. Gupta, and R. Melhem " An Array Data Flow Analysis based
Communication Optimizer," {\em Tenth Annual Workshop on Languages 
and Compilers for Parallel Computing} (LCPC'97), 
Minneapolis, Minnesota, August 1997 

\bibitem{Yuan97b}
X. Yuan, R. Gupta, and R. Melhem " Does Time Division Multiplexing 
Close the Gap Between Memory and Optical Communication Speeds?" 
{\em Workshop on Parallel Computing, Routing, and Communication} (PCRCW'97), 
Atlanta, Georgia, June 1997.

\bibitem{Yuan97c}
X. Yuan, C. Salisbury, D. Balsara and R. Melhem, ``A Load Balancing
Package on Distributed Memory System and its Application the
Particle-Particle Particle-Mesh (P3M) Methods.'' {\em Parallel Computing},
Vol. 23, No.19, pages 1525-1544, Oct. 1997.

\bibitem{Yuan98}
X. Yuan, R. Melhem and R. Gupta ``Performance of Multihop Communication
Using Logical Topologies on Torus Networks.'' {\em The Seventh 
International Conference on Computer Communications and Networks} (IC3N'98),
Lafayette, Louisiana, 1998.

\bibitem{Yuan98a}
X. Yuan and R. Melhem "Optimal Routing and Channel Assignment for 
Hypercube Communication on Optical Mesh-like Processor Arrays." 
{\em the Fifth International Conference on Massively Parallel
      Processing Using Optical Interconnections}(MPPOI'98), Las Vegas, 
      June 1998

\bibitem{Yuan98b}
X. Yuan, R. Melham, R. Gupta, Y, Mei and C. Qiao "Distributed Control
Protocols for Wavelength Reservation and Their Performance Evaluation"
Submitted to {\em IEEE trans. on Communications}, 1998.

\bibitem{zima88}
H. Zima, H. Bast and M. Gerndt. ``SUPERB: A tool for semi--automatic 
MIMD/SIMD parallelization.'' {\em Parallel Computing}, 6:1-18, 1988.

\bibitem{zhang94}
Z. Zhang and A. Acampora, ``A Heuristic Wavelength Assignment Algorithm for 
Multihop WDM Networks with Wavelength Routing and Wavelength Reuse.'' 
{\em Proc. IEEE Infocom'94}, pp 534-543, June 1994.

\end{thebibliography}
\end{document}










\chapter{Introduction}

% \subsection{Optical interconnection networks}

With the increasing computation power of parallel computers, 
interprocessor communication has become an important factor that
limits the performance of parallel computers.
Due to their capability of offering large bandwidth and low latency, 
optical interconnection networks are considered promising networks for future 
massively parallel computers. In all--optical networks,
communications are carried out in a pure circuit--switching 
fashion in order to avoid electronic/optical or optical/electronic
conversions at intermediate nodes. 
Specifically, packet switching techniques, which are usually used in electronic
multicomputer and multiprocessor interconnection networks are at a
disadvantage when
optical transmission is used. The absence of suitable photonic logic
devices makes it extremely inefficient to process packet routing information
in the photonic domain.
Moreover, conversion of this information into the electronic domain
increases the latency at intermediate nodes relative to the internode
propagation delay. Although  optical/electronic conversions may
be acceptable for large distributed networks \cite{Chlamtac92}, it represents a
disadvantage for multiprocessor networks in which internode propagation delays
are very small.

With the maturing of optical technology, transmission costs, and in 
particular the cost of high speed data links, has dropped tremendously in the 
last few years. 
The electronic processing capability of computers cannot match
the potentially very
high speed of  optical data transmission. Although in electronic
networks, the  processing speed is faster than 
the transmission speed, the transmission speed in optical networks
may be  much
higher than  the 
electronic processing speed which is typically limited to a few
gigabits per second. Thus,  
the communication bottleneck has  shifted from 
the transmission medium to the processing  needed to control that
medium in optical networks. This requires redesign
of network protocols to efficiently utilize the high speeds in optical
networks. 

Multiplexing techniques can be  used in optical
networks to fully utilize the large bandwidth of optics. 
Using multiplexing, 
multiple virtual channels are supported on a single link.
It is possible to concurrently establish multiple connections
using a single fiber in a multiplexed network.  Therefore, 
for a given topology, multiplexing increases the connectivity of the network.
Many research efforts have focussed on two
multiplexing techniques for optical interconnection networks,
namely, {\em time--division multiplexing} (TDM)
\cite{Melhem95,Qiao94,Qiao95,Qiao96} and {\em wavelength--division
multiplexing} (WDM) 
\cite{Brackett90,Chlamtac92,Ramaswami94}.
In TDM, each link is multiplexed 
by having different virtual channels on the link use different
{\em time slots}. In WDM each link is multiplexed by
having different virtual channels on the link use different 
{\em wavelengths}.

Regardless of the multiplexing technique, two approaches
can be used to establish connections in  multiplexed networks,
namely {\em link multiplexing} (LM) and {\em path multiplexing} (PM) 
\cite{Qiao95}.
In LM a connection which spans more than one communication
link is established by using possibly different channels on different links.
In PM a connection which spans more than one
communication link uses the same channel on all the links.
In other words, PM uses the same time-slot or the same wavelength on all
the links of a connection. On the other hand, 
LM can use different time-slots or different
wavelengths on different links, thus requiring time-slot interchange or
wavelength conversion capabilities at each intermediate node. 
Fig.~\ref{pmlm} shows the PM and LM connections at a switch.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/pmlm.eps,height=2.2in}}
\caption{Path multiplexing and link multiplexing}
\label{pmlm}
\end{figure}

Point--to--point networks, such as meshes, tori, rings and hypercubes,
are commonly used in commercial supercomputers. By exploiting space diversity
and traffic locality, they offer larger aggregate throughput and 
better scalability than shared media networks such as buses and stars.
Optical point--to--point networks can use either multi-hop packet routing 
(e.g.Shuffle Net
\cite{Acampora89})
or {\em deflection routing} \cite{Brassil94}. 
The performance of packet routing 
is limited by the speed of electronics since buffering and address decoding 
are usually performed in the electronic domain.
Thus, packet routing cannot efficiently utilize the potentially high bandwidth
that optics can provide.
With deflection routing, buffering of the optical data signals at intermediate
nodes is avoided by routing a packet through alternate
paths when multiple packets compete for the same output link.
While deflection routing requires simple network nodes and minimal 
buffering, a mechanism is necessary to guarantee bounded transfer
delays within the network.
As pointed out in \cite{As94}, although
point--to--point optical networks have intrinsically high aggregate throughput,
this advantage comes at the expense of additional control complexity 
in the form of routing and congestion control. New solutions should
exploit the inherent flexibility of dynamic reconfiguration of logical
topologies. 

While an all--optical network has the potential of providing large bandwidth,
network control for the all--optical network must not incur too much overhead
in order for the large bandwidth to be efficiently utilized.
In order for a data transmission 
to remain in the optical domain from the source node 
to the destination node, control mechanisms must be provided to establish
an all--optical path prior to  the data transmission. 
The efficiency of these path establishment mechanisms
is crucial for the communication performance.
This research
proposes to investigate network control schemes that establish
all--optical paths efficiently in optical TDM point--to--point networks
through architectural and compiler support. 
Specifically, two options for network control will be considered, 
the dynamic {\em distributed path reservation} scheme and 
the {\em compiled communication} scheme. 




To dynamically establish connections, either centralized 
or distributed control mechanisms can be used.
Centralized control mechanisms, such as 
wavelength assignment \cite{Ramaswami94} and 
time--slot assignment \cite{Qiao94},  collect all 
communication requests in a central controller and perform connection
scheduling in the controller. When the network size is large, 
the central controller becomes a  bottleneck. 
Thus, centralized control mechanisms 
are not scalable to large networks. Distributed path reservation protocols
distribute the work load of  path reservations to all the  nodes in the 
system. During distributed path reservation, a connection request, which 
arrives
at the network dynamically, will negotiate with each node along the path for
the establishment of a connection. The communication can  start after a
successful path reservation.
This research
 will focus on studying distributed path reservation protocols.
The protocols are generalizations of control protocols in 
non-multiplexed circuit--switching networks \cite{Nugent88}.
Multiplexing, however, introduces an additional complexity which requires
a careful consideration of many factors and parameters that affect the
efficiency of the protocols.
A distributed path reservation scheme can generally
be partitioned into two parts, selecting a physical
path and assigning virtual channels along the physical path to form 
a virtual path. To select a physical path, either deterministic routing
or adaptive routing can be used. Once a physical path is determined, a
path reservation scheme must decide how to select virtual channels on
the links along the physical path for an all--optical connection.
In addition, the signal propagation overhead must also be considered
in the designing of distributed reservation protocols. Many system paramters 
(system size, multiplexing degree, etc),
as well as  protocol parameters, contribute to the performance
of the protocols. The problem to be addressed is that, for a given
system, how to reserve paths without incurring too much protocol overhead.

\begin{figure}[htbp]
\begin{tabbing}
\hspace{1in} (1) ...\\
\hspace{1in} (2) barrier\\
\hspace{1in} (3) set network state to support the connections
                 used in $C_1$ and $C_2$\\
\hspace{1in} (4) barrier\\
\hspace{1in} (5) DO\=\ i = 1, 1000\\
\hspace{1in} (6) \> $C_1$: computation that uses communication pattern 1\\
\hspace{1in} (7) \> $C_2$: computation that uses communication pattern 2\\
\hspace{1in} (8) END DO
\end{tabbing}
\caption{An Example}
\label{CCOMM}
\end{figure}

Using distributed path reservation protocols, the
network control is performed in the electronic domain,
which is relatively slow in comparison to
the large bandwidth supported by optical data paths. Thus, the network
control overhead might become the bottleneck  in
all--optical networks, especially for communications with small message
sizes. Compiled communication is proposed to improve communication
performance by shifting the runtime control overhead 
into compile time processing. 
In compiled communication, the compiler determines the 
communication patterns in a program and inserts 
instructions that set the state of the network to support the 
communication patterns. 
Hence,
at runtime, an all--optical path for each communication in the program is  
established without  dynamic path reservation. 
Fig.~\ref{CCOMM} shows an example of compiled
communication. In the example, the compiler determines that two communication
patterns are needed inside the loop and inserts
codes in lines (2) --- (4) to set the network state to establish connections
in patterns $C_1$ and $C_2$. Hence, at runtime, the communications
inside the loop need not perform path reservation.

Beside eliminating dynamic control overhead, 
compiled
communication also offers a number of advantages over traditional dynamic
network control. First,   
off-line algorithms can be used to manage network
resources more efficiently. 
For example, off--line connection scheduling algorithms result in
better channel utilization than  simple on--line routing algorithms.
Since off--line algorithms are executed at compile time, 
the complexity of the algorithms will
not effect the runtime efficiency of the program. 
Second, compiled communication may simplify the 
software communication protocol since 
communications are deterministic
under compiled communication
\cite{Kumar92}. Third, 
since  routing decisions are made at compile time, 
compiled communication uses simpler hardware in the 
router and does not incur routing delay.
The major limitation of
 compiled communication is that it cannot efficiently handle  the
communication patterns that are unknown at compile time. 

A number of issues must be addressed
in order to apply compiled communication techniques. First, the compiler must
be able to synthesize communication patterns from application programs.
This task can be complex depending on the programming model and the 
machine model. Second, the compiler must
have  knowledge of the underlying networks. Many compilers generate code
using high level communication libraries, such as PVM and MPI, which
provide a very simple model for communications. However, this simple model
also hinders  compiled communication. A different
communication model
must be provided for the compiler to manage communications statically.
Third, efficient off--line algorithms must be designed for the compiler 
to manage network resources effectively. 
Further, if compiled 
communication is to be the only means for network control, mechanisms to
perform correct communications for the communication
patterns that are unknown at compile time
must be provided. One way to handle compiled time unknown communication
patterns in compiled communication is to use a pre--determined pattern
to route messages and emulate multi-hop communication.

%When dealing with compile time unknown patterns, 
%routing messages through a pre--determined
%communication pattern (logical topology) is an alternative
%to providing  distributed path reservation protocols. 
%Data transmissions are no longer all--optical, instead, there are 
%intermediate nodes at which routing must be performed.  
%Three factors contribute to the communication performance: 
%the number of intermediate hops, the processing time at an 
%intermediate hop and the multiplexing degree required to realize the
%logical topology.
%Consider an 
%$n\times n$ mesh connected network. A message has to be relayed, on average,
%at $n$
%intermediate routing hops. If a  hypercube logical topology is 
%realized on the mesh using virtual circuits, then a message will be relayed, 
%on average, at
%$\frac{lg(n)}{2}$ intermediate routing hops.
%This research will study 
%efficient logical topologies for  point--to--point physical networks.
%Notice that  two
%extremes for the choices of logical topologies are (1) 
%having no logical topology, which
%minimizes the multiplexing degree  while requiring a large number of 
%intermediate relays and (2) realizing
%a complete graph,  which eliminates all the intermediate relays 
%at the cost of a large multiplexing degree. A good logical topology may be
%a compromise between these two extremes.

This research will consider both dynamic network control and compiled 
communication in optical TDM point--to--point networks using path 
multiplexing.
To support this research,  a detailed 
network simulator that simulates all  proposed
control schemes will be 
designed and implemented. 
The simulator should be able to simulate communication with three different
control mechanisms,  (1) various dynamic path reservation protocols, 
(2) compiled communication for   patterns
known at compile time  
where communication paths are assumed to 
be realized using off--line algorithms, and (3)  compiled communication 
for  patterns unknown at compile time
where messages are routed through a logical topology.
Beside the network simulator, a tool
 must be developed to extract communication 
patterns from application programs. 
Traditional communication traces are not sufficient for
the study of compiled communication. Instead,  static communication
patterns must be distinguished from dynamic communication patterns. 
In order to obtain this
information, A communication analysis tool that analyzes
communication requirements at compiler time will be developed. 

In the rest of the proposal,  background information and previous work
related to this research will be presented in  Chapter 2. 
The research goals and  approaches will be described in
Chapter 3. Preliminary work that has been done will be reported in 
Chapter 4, and Chapter 5 concludes the proposal.

\newpage











\chapter{Introduction}

Fiber--optic technology has advanced significantly over the past
few years, so have the development of tunable lasers, filters, high--speed
transmitter and receiver circuits, optical amplifiers and photonic
switching devices \cite{Ikegami97,Vengsarkar97}. 
With the maturing of optical technology, 
transmission cost, and in particular the cost of high speed data links, 
has dropped tremendously. The electronic processing capability of 
computers cannot match the potentially very
high speed of  optical data transmission.
The communication bottleneck has shifted from 
the transmission medium to the processing  needed to control that
medium. 

In optical interconnection networks, each physical link can offer very high
bandwidth. In order to fully utilize the available bandwidth, an optical
link can be shared through {\em time--division multiplexing} (TDM)
\cite{ganz92,Melhem95,Qiao94} and/or {\em wavelength--division 
multiplexing} (WDM) \cite{Brackett90,Chlamtac92,Dowd93,Subramanian96}.
In TDM, optical links are multiplexed 
by assigning different virtual communication channels to different
{\em time slots}, while in WDM, optical  links are multiplexed by
assigning different virtual communication channels to different 
{\em wavelengths}. By using TDM, WDM or TWDM (a combination of TDM and WDM), 
each link can support multiple channels.

Point--to--point networks, such as meshes, tori, rings and hypercubes,
are used in commercial supercomputers. By exploiting space diversity
and traffic locality, they offer larger aggregate throughput and 
better scalability than shared media networks such as buses.
Optical point--to--point networks can be implemented by 
replacing electronic links
with optical links and operating in a packet switching fashion just like 
electronic networks. The performance 
of such networks is limited by the speed of electronics since 
buffering and address decoding are performed in the electronic domain.
Thus, these networks cannot efficiently utilize the potentially high bandwidth
that optics can provide. New generation optical point--to--point networks
exploit the {\em channel--routing} capability in optical switches.
In TDM networks, time--slot routing\cite{Qiao94} is used, while in 
WDM networks, wavelength--routing\cite{chlamtac93} is used. The 
channel routing in an optical switch routes messages from a channel of an 
input port to a channel of an output port without converting the messages
into the electronic domain. The switch states, however, are usually 
controlled by electronic signals.

Using channel routing, two approaches can be used to establish connections 
in  multiplexed optical networks, namely {\em link multiplexing} (LM) and 
{\em path multiplexing} (PM). These connections are called {\em lightpaths}
since the light signal travels through the connections that may span a number
of optical links and switches without being converted into the electronic 
domain. In LM, a connection which spans more 
than one communication link is established by using possibly different 
channels on different links. In PM, a connection which spans more than one
communication link uses the same channel on all the links.
In other words, PM uses the same time-slot or the same wavelength on all
links of a connection, while LM can use different time-slots or different
wavelengths, thus requiring time-slot interchange or
wavelength conversion capabilities at each intermediate optical switch. 
Since the technology to support PM is more mature than the one to support LM,
this thesis focuses on PM.

Since the electronic processing speed is relatively
slow compared to the optical data transmission speed,
optical point--to--point networks should ideally employ
{\em all--optical} communication in data transmission.
In all--optical communication, no electronic processing and
no electronic/optical (E/O)
or optical/electronic (O/E) conversions are performed
at intermediate nodes. Once converted into the 
optical domain, the signal remains there until it reaches the destination.
All--optical communication eliminates the electronic processing
bottleneck at intermediate nodes 
during data transmission and thus, exploits the large
bandwidth of optical links. This thesis considers all--optical 
networks where a  lightpath is established before a communication 
starts and the data transmission is carried out in a pure 
circuit--switching fashion. This type of communication is referred to 
as the {\em dynamic single--hop communication}. 
In such networks, electronic processing occurs only in 
the path reservation process and hence, using an efficient path reservation
protocol is crucial to obtain high performance. In this work, 
a number of path reservation algorithms that dynamically
establish lightpaths are designed and the impact of system parameters on the
algorithms is studied. These algorithms 
use a separate control network to exchange 
control messages and allow all--optical communication in the optical
data network.
 
Although dynamic single--hop networks achieve all--optical communication
in data transmission, the path reservation algorithms require
extra hardware support to exchange control messages
and result in large startup overhead, especially for small messages. 
An alternative is to use {\em dynamic multi--hop
communication}. In multi--hop networks, intermediate nodes are responsible
for routing packets  such that a packet sent from a 
sender will eventually reach its destination, possibly after being routed
through a number of intermediate nodes. Clearly,  multi--hop networks 
require E/O and O/E conversions at intermediate nodes. Thus, it is
important to reduce the number of hops that a packet visits. This reduction
may be achieved in an optical TDM network by 
combining the channel--routing technique and the 
packet switching technique. Specifically, packets may be routed 
through a  logical topology which has a small diameter 
as opposed to  the physical topology which may have a large diameter. 
The major issue in the multi--hop 
communication is to design appropriate logical topologies.
This thesis considers efficient schemes for realizing logical topologies
on top of physical mesh and torus networks using path multiplexing. 
Realizing logical
topologies on optical networks is different from traditional embedding 
techniques in that both routing and channel assignment 
options must be considered. An analytical model that 
models the maximum throughput and average package latency of 
multi--hop networks is developed and is used to evaluate the performance of 
logical topologies and identify the advantages of each logical topology.

While dynamic (single--hop or multi--hop) communications handle arbitrary
communication patterns,  their performance can be limited
by the electronic processing which occurs during path reservation 
in single--hop communication and during packet routing in 
multi--hop communication. 
{\em Compiled communication} overcomes this limitation for communication 
patterns that are known at compile time.
In compiled communication, the compiler analyzes a program and determines
its communication requirement. The compiler then
uses the knowledge of the underlying
architecture, together with the knowledge of the communication requirement,
to manage network resources statically. As a result,
runtime communication overheads, such as path reservation
and buffer allocation overheads, are reduced or eliminated,
and the communication performance is improved. However,
due to the limited network resources, the underlying network
cannot support arbitrary communication patterns.
Compiled communication
requires for the compiler to analyze a program and partition
it into phases such that each phase has a fixed, pre-determined
communication pattern that the underlying network can support.
The compiler inserts codes for performing network reconfigurations at
phase boundaries to support all connections in the next phase.
At runtime, a lightpath is available
for each communication without path reservation. Therefore,  
compiled communication accomplishes all--optical communication
without incurring extra hardware support and large start--up overheads.
This thesis studies the application of compiled communication 
to optical interconnection networks. Specifically, it considers
the communication analysis techniques needed to analyze the communication
requirement of a program. These analysis techniques are 
general in that
they can be applied to other communication optimizations and 
can be used for compiled communication in electronic networks. This thesis
also develops a number of  connection scheduling schemes which realize a given
communication pattern with a minimal multiplexing degree. Note that in 
optical TDM networks, communication time is proportional to the 
multiplexing degree. Finally, a communication phase analysis algorithm is 
developed to partition a program into phases so that each 
phase contains connections that can be supported by the underlying network.
All the algorithms are implemented in a compiler which is 
based on the Stanford SUIF
compiler\cite{Amar95}. This thesis evaluates the performance of the algorithms
in terms of both analysis cost and runtime efficiency.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
  & Single--hop & Multi--hop & Compiled\\
\hline
All--optical comm. & Yes & No & Yes\\
\hline
Startup overhead  & Yes & No & No\\
\hline
Extra hardware    & Yes & No & No\\
\hline
Arbitrary comm.   & Yes & Yes & No\\
\hline
\end{tabular}
\end{center}
\caption{General characteristics of the three schemes}
\label{generalchar}
\end{table}

Table~\ref{generalchar} summarizes the general characteristics of the three
schemes. In optical interconnection networks, 
the central problem to be addressed
is the reduction of  the amount of electronic processing needed for
controlling the 
communication. In dynamic single--hop networks, this problem is addressed 
by having efficient path reservation algorithms. In multi--hop networks,
this problem is tackled by designing efficient logical topologies to 
route messages. Compiled communication totally eliminates the electronic
processing in communications. However, it only applies to the communication
patterns that are known at compile time. While communications in optical
TDM point--to--point networks can be carried out by any of the three
communication schemes, it is necessary to understand the strengths and 
the limitations of each communication scheme in order to make appropriate
choices when
designing an optical interconnection network. 
In addition to considering the options within each communication scheme,
this thesis compares the performance of the three communication schemes 
using a number of benchmarks and real applications and identifies 
the situations in which each communication scheme has advantage over 
other schemes.

% ollowing section summarizes the contributions of the thesis.

%\section{Thesis contributions}
%
%This thesis makes contributions in the {\em design of control mechanisms}
%for time--multiplexed 
%optical interconnection networks. The contributions are in two areas:
%optical interconnection networks and compiler analysis techniques. In the 
%optical interconnection
%networks area, this thesis introduces efficient control schemes 
%for dynamic single--hop communication and dynamic multi--hop communication. 
%This thesis also 
%proposes and validates the idea of applying the 
%compiled communication technique 
%to optical interconnection networks.  In the compiler area, 
%this thesis addresses all the issues needed
%to apply the compiled communication paradigm 
%to optical interconnection networks,
%including communication optimization, 
%communication analysis, connection scheduling and 
%communication phase analysis. Some of the research presented in this 
%thesis has appeared in \cite{Yuan96,Yuan96a,Yuan97,Yuan97a,Yuan98,Yuan98a}.
%The main contributions of the thesis are detailed as follows.
%
%\begin{itemize}
%\item Dynamic single--hop communication.
%\begin{enumerate}
%  \item  Develop distributed path reservation protocols for optical TDM
%         point--to--point networks.
%  \item  Study the performance of the protocols and 
%         the impact of various
%         system parameters, such as system size, message size, etc, on 
%         the protocols. 
%\end{enumerate}
%\item Dynamic multi--hop communication.
%\begin{enumerate}
%  \item Develop efficient schemes to realize logical topologies on
%        top of physical optical torus and mesh networks.
%  \item Develop an analytical model to model the maximum throughput and the 
%        packet delay for multi--hop networks.
%  \item Study the performance of various logical topologies and identify
%        the advantages and the limitations of each logical topology.
%\end{enumerate}
%\item Compiled communication.
%\begin{enumerate}
%  \item Develop a communication analyzer that analyzes the communication 
%        requirement of a program.
%  \begin{itemize}
%    \item  Design a communication descriptor that describes
%           the communication requirement on virtual processor grids.
%    \item  Design data flow analysis algorithms for
%           communication optimizations to obtain communication
%           patterns in an optimized program.
%    \item  Design schemes that derive 
%           communication patterns on physical processors from the
%           communication descriptors.
%  \end{itemize}
%  \item Design communication scheduling algorithms that can be used by 
%        the compiler to schedule the communication patterns that
%        are known at compile time.
%  \item Design  a communication phase analysis algorithm
%        that partitions a program into phases such that the connections
%        within each phase can be supported by the underlying network.
%  \item Implement all the algorithms and evaluate these algorithms in terms
%        of both analysis cost and runtime efficiency.
%\end{enumerate}
%\item Simulators and Performance evaluation.
%\begin{enumerate}
%  \item Implement network simulators that simulate dynamic single--hop 
%        communication with various control protocols, dynamic multi--hop
%        communication with different logical topologies, and compiled
%        communication.
%  \item Study the performance of the three communication schemes via
%        benchmark and real application programs and identify the 
%        advantages and the limitations of each scheme.
%\end{enumerate}
%\end{itemize}
%
%Although all the techniques presented for optical TDM networks, many techniques
%may also be applied to optical WDM networks or to non--optical networks.

%\section{Thesis organization}

The remainder of the thesis is organized as follows.
Chapter 2 begins by describing background and thesis assumptions. This chapter
presents an overview of optical interconnection networks, discusses the 
TDM technique and introduces the {\em path multiplexing} (PM) and 
{\em link multiplexing} (LM) techniques
for establishing connections. This chapter also surveys the research related to
the three communication schemes. Finally, this background chapter surveys 
the traditional compilation techniques for distributed memory machines and
communication optimizations and discusses the 
difference between the traditional communication optimizations and the 
compiled communication technique.

Chapter 3 discusses the techniques used in dynamic single--hop communication.
Two types of distributed path reservation protocols, the {\em forward
reservation protocols} and the {\em backward path reservation protocols},
are described. This chapter also describes a  network simulator for
dynamic single--hop communication, evaluates the performance of 
the two types of protocols and  studies the impact of system
parameters on these protocols.

Chapter 4 discusses dynamic multi--hop communication. This chapter presents
efficient schemes for realizing logical topologies  on top of the 
physical torus networks,
describes an analytical model that models the maximum throughput and 
the average packet delay for the logical topologies and verifies 
the model with 
simulation. In addition, this chapter also describes the simulator for
dynamic multi--hop communication, evaluates 
the multi--hop communication with the logical topologies and identifies 
the advantages and the limitations of each logical topology.

Chapter 5 considers compiled communication. This chapter describes
the communication analyzer and discusses the communication descriptor used 
in the analyzer, the 
data flow analysis algorithms for communication optimizations, and
the actual communication optimization performed in the analyzer.
The chapter also describes off-line connection 
scheduling algorithms and a communication
phase analysis algorithm. Using these algorithms, the compiler analyzes 
the communication requirement of a program, partitions the program
into phases such that each phase contains connections that can be 
supported by the underlying network, and schedules the connections within 
each phase.  The chapter also presents the evaluation
of the compiler algorithms and studies their runtime efficiency.   

Chapter 6 compares the communication performance of the three
communication schemes. Three sets of application (benchmark) programs, 
including hand--coded parallel programs, HPF benchmark programs
and sequential programs from SPEC95, are used to evaluate the 
communication performance of the communication schemes. Different sets
of programs exhibit different communication characteristic. For example, 
the hand--coded programs are highly optimized for parallel execution, while
the programs from SPEC95 are not optimized for parallel execution.
This chapter compares the communication performance of the three
communication schemes and 
identifies the advantages of each scheme.

Finally, Chpater 7 summarizes the dissertation and suggests some directions
for future research.










\chapter{Dynamic multi--hop communication}
\label{multi}

Since by using time--division
multiplexing multiple channels are supported on an optical link,
more sophisticated logical topologies can be realized on top of 
a simpler physical network to improve the communication performance. These
logical topologies reduce the number of intermediate hops that a packet travels
at the cost of a larger multiplexing degree. 
On the one hand, the large multiplexing degree increases the packet 
communication time between hops. On the other hand, reducing the number 
of intermediate hops reduces the time spent at intermediate nodes. 
This chapter studies the trade--off between the  multiplexing degree
and the number of intermediate hops needed in logical topologies implemented
on top
of physical torus networks. Specifically,  four logical
topologies ranging from the most complex logical all--to--all 
connections to the simplest logical torus topology are examined.  
An analytical model for the
maximum throughput and the average packet delay is developed and verified
through simulations.
The performance 
and the impact of system parameters on the performance for
these four topologies are studied. Furthermore, the performance of 
the multi--hop communication  using an efficient logical topology is 
compared with that of the single--hop communication using a 
distributed path reservation protocol, and the advantages and 
the drawbacks of these two communication schemes are identified.

To perform multi--hop communication, packets may be routed through
intermediate nodes. Specifically, a communication
module at each node, which will be referred to as the {\em router} in this
chapter, is needed to route packets toward their destinations.
It is  assumed that each router 
contains a {\em routing buffer} that buffers all
incoming packets. For each
packet, the router determines whether to deliver the packet
to the local PE or to
the next link toward the packet destination.
A separate {\em output path buffer} is used 
for each outgoing path that buffers the packets
to be sent on that path and thus accommodates the speed mismatch between the 
electronic router and the optical path. 
Figure~\ref{ROUTER} depicts the structure of a router 
(see also Figure~\ref{ROUTER1}).
Note that the output paths are multiplexed in time over the physical links 
that connect the local PE to its corresponding switch. 
In the rest of the chapter, {\em routing delay} will be used 
to denote the time a packet spends in the routing buffer 
and the time for the router to make
a routing decision for the packet (packet routing time).
{\em Transmission delay} will be used to denote the time 
a packet spends on the path
buffer and the time it takes for the packet to be transferred on the path.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/queue.eps,width=5in}}
\caption{A router}
\label{ROUTER}
\end{figure}

\section{Realizing logical topologies on physical torus topology}

Four logical topologies are considered in this section, 
the logical torus topology, the logical hypercube topology, 
the logical all--to--all topology, and 
the logical allXY topology where all--to--all connections are established
along each dimension. Let us consider an example in which a packet 
is transmitted from node 0 to node 11 in 
the $4\times 4$ torus shown in Figure~\ref{DEF1}. Using the  
logical all--to--all topology,
the packet will go directly from node 0 to node 11. Using the logical allXY
topology, the packet will go from node 0 to node 3 to node 11. Using the
logical hypercube topology,
the packet will go from node 0(0000) to node 1(0001)
to node 3(0011) to node 11(1011). Using the logical torus topology, the packet
will go from node 0 to node 3 to node 7 to node 11. 


\begin{figure}
\centerline{\psfig{figure=fig/def1.eps,width=2in}}
\caption{Node numbering in a torus topology}
\label{DEF1}
\end{figure}

Traditional embedding techniques that
minimize the {\em congestion}  for a given communication 
pattern are not adequate for minimizing the number of virtual channels
needed to realize the communication in an optical network with path 
multiplexing. The congestion is usually not equal to the number of 
channels needed to realize a communication pattern. 
Consider the example in Figure~\ref{notequal} in which
the congestion in the network is 2, while 3 channels
are needed to realize the three connections. To efficiently realize a 
logical topology in an optical network, both routing and channel assignment
(RCA) options must be taken into consideration. 
Schemes to realize these four logical topologies on the physical 
torus topology will be discussed next. 


\begin{figure}
\centerline{\psfig{figure=fig/notequal.eps,width=2.4in}}
\caption{Difference between embedding and RCA}
\label{notequal}
\end{figure}


\subsection{Logical hypercube topology}

This subsection considers the optimal schemes to realize the logical
hypercube topology on top of the physical torus topologies. Since the
algorithm to realize the hypercube topology on the physical torus topologies
utilizes the algorithms to realize
the hypercube on top of physical mesh, ring and array topologies, 
algorithms to realize the hypercube topology on 
top of all these mesh--like topologies are discussed. 

Given networks of size $N$, it will be proven that 
$\lfloor\frac{2N}{3}\rfloor$ and
$\lfloor\frac{N}{3} + \frac{N}{4} \rfloor$ channels are the minimum required
to realize hypercube communication on array and ring topologies, 
respectively. Routing and 
channel assignment schemes that achieve these minimum 
requirements are developed, 
indicating that the bounds are tight and the  schemes are optimal.
These schemes are extended to mesh and torus topologies and it is proven 
that for a $2^k\times 2^{r-k}$ ($k\ge r-k$) mesh or torus, 
$\lfloor\frac{2\times2^k}{3} \rfloor$ and 
$\lfloor\frac{2^k}{3} + \frac{2^k}{4} \rfloor$ channels are the minimum 
required for realizing hypercube communication on these two topologies, 
respectively. Routing and channel assignment schemes 
are designed 
that use at most two more channels than the optimal to realize hypercube
communication on these topologies. In the following sections, first
the problem of routing and channel assignments for the hypercube
communication on the physical mesh--like topologies is formally defined, 
and then the algorithms are described.

\subsubsection{Problem definition} 

A network is modeled as a directed graph G(V, E), where nodes in 
V are  switches and edges in E are links.
Each node in a network is assigned
a node number starting from 0. It is assumed that
in arrays and rings the nodes are numbered
from left to right in ascending order, and that
the nodes are numbered in row major order for meshes and tori of size 
$n\times m$.  Thus, 
the node in the $i$th column
and the $j$th row is numbered as $j \times m + i$. 
%Fig.~\ref{DEF1} shows a $4\times 4$ torus topology.
This subsection focuses on studying the optimal RCA schemes for 
these traditional numbering schemes. 
Optimal node numbering (and its RCA) is a
much more complex problem and is not considered in this dissertation.
The number of nodes in a network is assumed to be $N=2^r$. 
For a mesh or a torus to contain $2^r$ nodes, each row and column must
contain a number of nodes that is a power of two. 
Hence, the size
of meshes and tori is denoted as $N=2^k\times 2^{r-k}$.
Without losing generality, it is always assumed that $k \ge r-k$.
The notations $ARRAY(N)$ and
$RING(N)$ are used to represent arrays and rings  of size N respectively,
and $MESH(2^k\times 2^{r-k})$ and $TORUS(2^k\times 2^{r-k})$ for
meshes and tori of size $2^k\times 2^{r-k}$ respectively. 

The connection from node $src$ to
node $dst$ is denoted as $(src, dst)$. A {\em communication pattern}
is a collection of connections. 
The {\em hypercube} communication pattern contains a connection $(src,dst)$ 
if and only if the binary representations of 
 $src$ and $dst$ differ in precisely one bit. A connection in the hypercube
communication pattern is called a {\em dimension l connection} if it connects
two nodes that differ in the $l$th bit position. In a network of 
size $N=2^r$, the set, $DIM_l$, where
$0\le l\le r-1$, is defined as  the set of all dimension $l$ connections
and ${H_r}$ is defined as the hypercube communication pattern. That is

\vspace{-0.1in}
\begin{tabbing}
\hspace{0.3in}$DIM_l=$\=$\{(i, i+2^l)\ |\ i\ mod\ 2^{l+1}\ < \ 2^l\} \cup$
                     $\{(i, i-2^l)\ |\ i\ mod\ 2^{l+1}\ \ge\ 2^l\}$\\
\\
\hspace{0.3in}${H_r} = \cup_{l=0}^{r-1}DIM_l $
\end{tabbing}
%\centerline{$DIM_l=\{(i, i+2^l)\ |\ i\ mod\ 2^{l+1}\ < \ 2^l\} \cup
%                   \{(i, i-2^l)\ |\ i\ mod\ 2^{l+1}\ \ge\ 2^l\}$}
%\vspace{0.15in}
%\centerline{${H_r} = \cup_{l=0}^{r-1}DIM_l $ }
%\vspace{0.15in}

\vspace{-0.1in}
It can be easily proven that removing any $DIM_l$, for any $l\le r-1$,
from ${H_r}$ leaves two disjoint sets of connections, each of which being
a hypercube pattern on $\frac{N}{2}$ nodes. 
For example, removing $DIM_0$ from ${H_r}$ results in an ${H_{r-1}}$
on the 
even--numbered nodes and another ${H_{r-1}}$ on the odd--numbered nodes once
the nodes are properly renumbered. Next, 
some definitions are introduced and the results of this section are 
summarized.

\begin{description}

\item
{\bf Definition:} $P(x, y)$ is a {\em directed path}
 in G from node x to node y. It
 consists of a set of consecutive edges beginning at x and ending at y.

\item
{\bf Definition:} Given a network G and a communication pattern  $I$, 
a {\em routing R(I)} of $I$
is a set of directed paths $R(I) = \{P(x, y) | (x, y) \in I\}$.

\item
{\bf Definition:} Given a network G, a communication pattern I and a 
routing R(I) for the communication pattern, 
the congestion of an edge $\alpha\in E$, denoted as $\pi(G, I, R(I), \alpha)$, 
is the number of
paths in R(I) containing $\alpha$. The {\em congestion}
of G in the routing R(I), denoted as $\pi(G, I, R(I))$, is the maximum congestion 
of any edge of G in the routing R(I), that is, 
$\pi(G, I, R(I)) = max_{\alpha}\{\pi(G, I, R(I), \alpha)\}$. 
The {\em congestion} 
of G for 
a communication pattern I, 
denoted as $\pi(G, I)$, is the minimum congestion of G in any 
routing R(I) for I, that is, $\pi(G, I) = min_{R}\{\pi(G, I, R(I))\}$.

\item
{\bf Definition:} Given a network G and a routing $R(I)$ for communication
 pattern I, an
{\em assignment function} $A: R\rightarrow INT$,
is a mapping from the set of paths to the set of integers $INT$, 
where an integer
corresponds to a channel.
A {\em channel assignment} for a routing $R(I)$ is an assignment
function $A$ that satisfies the following conditions: 

\begin{enumerate}
\item  If 
$P(x_1, y_1)$, $P(x_2, y_2)$ are different paths that share a common edge,
then\\ $A(P(x_1, y_1)) \ne A(P(x_2, y_2))$. This condition 
ensures that each channel on one link can only be assigned
to one connection (i.e., there are no link conflicts). 

\item $A(P(x, y_1)) \ne A(P(x, y_2))$ and $A(P(x_1, y)) \ne A(P(x_2, y))$.
This condition ensures that 
each node can only use
one channel at a time to send to or receive from 
other nodes (i.e., there are
no node conflicts). 
\end{enumerate}


%A channel assignment
%that violates condition (1) is said to have {\em link conflicts}, and a
%channel assignment that violates condition (2) is said to have
%{\em node conflicts}. 
$A(R)$ denotes the set of 
channels assigned to the paths in R and $|A(R)|$ is the size of $A(R)$. 
Let $w(G, I, R)$ denote  the 
minimum number of channels for the routing R, that is,
$w(G, I, R) = min_A\{|A(R)|\}$.  $w(G,I)$ denotes the smallest 
$w(G, I, R)$ over all R, i.e. $w(G,I) = min_R\{w(G, I, R)\}$

\item
{\bf Lemma 1:} $w(G,I) \ge \pi(G, I)$.\\
{\bf Proof: } Follows directly from the above definitions. $\Box$
\end{description}

\noindent
The following sections show that\\

%I further give a lower bound for  $\pi(MESH(2^k\times 2^{r-k}), H_r)$
%and $\pi(TORUS(2^k\times 2^{r-k}), H_r)$
%and show that

\begin{tabbing}
\hspace{0.3in}$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$\\
\\
\hspace{0.3in}$w(RING(N), H_r) = \pi(RING(N), H_r) = 
\lfloor \frac{N}{3} + \frac{N}{4}\rfloor$\\
\\
\hspace{0.3in}$w(MESH(2^k\times 2^{r-k}), H_r)
                \le \lfloor \frac{2\times 2^k}{3} \rfloor + 2$
              $\le \pi(MESH(2^k\times 2^{r-k}), H_r) + 2$\\
\hspace{0.3in}\\
\hspace{0.3in}$w(TORUS(2^k\times 2^{r-k}), H_r) 
              \le \lfloor \frac{2^k}{3} + \frac{2^k}{4}\rfloor + 2 $
              $\le \pi(TORUS(2^k\times 2^{r-k}), H_r) + 2$
\end{tabbing}

%\centerline{$w(MESH(2^k\times 2^{r-k}), H_r) \le 
%\lfloor \frac{2\times 2^k}{3} \rfloor + 2 \le 
%\pi(MESH(2^k\times 2^{r-k}), 
%H_r) + 2$}
%\centerline{ $w(TORUS(2^k\times 2^{r-k}), H_r) \le
%\lfloor \frac{2^k}{3} + \frac{2^k}{4}\rfloor + 2 \le
%\pi(TORUS(2^k\times 2^{r-k}), H_r) + 2$}

\subsubsection{Hypercube on linear array}

Since routing in a linear array is fixed, the RCA
problem is reduced to a channel assignment problem. 
Given a linear array of size $N=2^r$, it is proven that
 $\lfloor\frac{2N}{3}\rfloor$ channels is the lower
bound for realizing the hypercube communication  by showing  that 
$\pi(ARRAY(N), H_r) \ge \lfloor\frac{2N}{3}\rfloor$.
A channel assignment scheme is developed that uses
$\lfloor\frac{2N}{3}\rfloor$ channels for the hypercube communication. 
This proves that the bound is a tight lower 
bound and that the channel assignment scheme is optimal. 
%Array topology is a 
%relatively simple topology, there exist general 
%optimal channel assignment schemes on this topology\cite{qiao96}. 

\vspace{0.12in}
\noindent
{\bf A lower bound}
\vspace{0.12in}

Using Lemma 1, a lower bound is obtained
by proving that there exists a link in the linear
array that is used $\lfloor\frac{2N}{3}\rfloor$ times when realizing
${H_r}$. The following 
lemmas establish the bound.

\noindent
{\bf Lemma 2}: In a linear array of size $N=2^r$, where $r \ge 2$, 
there are $2^{r-1}$ connections in $DIM_{r-1}\cup DIM_{r-2}$ that
use the link $(n, n+1)$ for any specific $n$ satisfying 
$2^{r-2} \le n \le 2^{r-1}-1$.

\noindent
{\bf Proof}: The connections in $DIM_{r-1}$ and $DIM_{r-2}$
can be represented by

%\small
% \footnotesize
\begin{tabbing}
\hspace{0.1in}$DIM_{r-1}$\= =$\{(i, i + \frac{N}{2}) | 0\le i <\frac{N}{2}\}$ 
                 $\cup\{(i, i - \frac{N}{2}) | \frac{N}{2}\le i < N\}$\\
\\
\hspace{0.1in}$DIM_{r-2} =\{(i, i+\frac{N}{4})|0 \le i < \frac{N}{4}$ or $
                         \frac{N}{2}\le i < \frac{3N}{4}\}$\\
\hspace{0.1in}\>$\cup$\hspace{0.05in} $\{(i, i-\frac{N}{4})| \frac{N}{4} \le i < \frac{N}{2}$ or 
                         $\frac{3N}{4}\le i < N\}$
\end{tabbing}

\noindent
Consider the 
connections in $DIM_{r-1}$. All connections $(i, i+\frac{N}{2})$ with
$0 \le i\le n$ use link $(n, n+1)$, where $2^{r-2} \le n \le 2^{r-1}-1$. 
Hence, as shown in Fig.~\ref{LEMMA1}~(a),
there are  n+1 connections in $DIM_{r-1}$ that use link 
$(n, n+1)$. Similarly, in $DIM_{r-2}$,
all connections  $(i, i+\frac{N}{4})$, where
$n < i+\frac{N}{4} < \frac{N}{2}$, use  link $(n, n+1)$.
As shown in Fig.~\ref{LEMMA1}~(b), there are $2^{r-1} - n - 1$
such connections.
Hence, there are a total of $n+1 + 2^{r-1} - n - 1 = 2^{r-1}$ 
connections in $DIM_{r-1}$ and $DIM_{r-2}$ that use link $(n, n+1)$. $\Box$


\begin{figure*}
\centerline{\psfig{figure=fig/l1.eps,width=5.4in}}
\caption{Dimension $r-1$ and $r-2$ connections}
\label{LEMMA1}
\end{figure*}

\noindent
{\bf Lemma 3}: In a linear array of size $N=2^{r}$, there exists a link 
$(n, n+1)$ such that at least $\lfloor\frac{2N}{3}\rfloor$ connections in 
${H_r}$ use that link.

\noindent
{\bf Proof}: Let $T_i(2^r)$ be the number of connections in ${H_r}$
that use link $(i, i+1)$ and let $T(2^r) = max_i (T_i(2^r))$.
Thus $T(2^0) = 0$ and $T(2^1) = 1$.
From Lemma 2, one knows that
for $2^{r-2} \le n \le 2^{r-1}-1$, link $(n, n+1)$ is used
$2^{r-1}$ times  by connections in $DIM_{r-1}$ and $DIM_{r-2}$. Thus,
the links in the second quarter of the array (from node $2^{r-2}$ to node 
$2^{r-1}-1$) are used $2^{r-1}$ times by  dimension $r-1$ and 
dimension $r-2$ connections.
By the definition of hypercube communication, it is known that 
dimension 0 to dimension $r-3$ connections 
form a hypercube on this quarter of the array. Thus, 
Lemma 2 can be recursively applied 
 and the following inequality is obtained.\\

\centerline{$T(2^r) \ge 2^{r-1} + T(2^{r-2})$}

\noindent
It can be proven by induction that the above  inequality and the 
boundary conditions $T(2^0) = 0$, $T(2^1) = 1$, imply that 
$T(N) = T(2^r) \ge \lfloor \frac{2N}{3}\rfloor$.
%
%Base case: $T(2^0) = 0 \ge \lfloor \frac{2\times 0}{3} \rfloor$ and 
%           $T(2^1) = 1 \ge \lfloor \frac{2\times 2}{3} \rfloor$.
%
%Induction case: assuming $T(\frac{N}{4}) = T(2^{r-2}) \ge \lfloor 
%                          \frac{2\times 2^{r-2}}{3} \rfloor$,
%
%$T(N) = T(2^r) \ge 2^{r-1} + T(2^{r-2})
%           \ge 2^{r-1} + \lfloor \frac{2^{r-1}}{3} \rfloor
%           \ge \lfloor \frac{3\times 2^{r-1} + 2^{r-1}}{3} \rfloor
%           \ge \lfloor  \frac{2\times 2^{r}}{3} \rfloor
%           = \lfloor \frac{2N}{3} \rfloor$
%\noindent
Hence, there exists a link which is used at least
$\lfloor\frac{2N}{3}\rfloor$ times by connections in ${H_r}$. $\Box$

The proof of Lemma 3 is constructive in the sense that
the link that is used at least 
$\lfloor\frac{2N}{3}\rfloor$ times can be found. 
By recursively considering the second quarter of the linear array, 
one can conclude that the source node, $n$, of the link $(n, n+1)$ that is 
used at least $\lfloor\frac{2N}{3}\rfloor$ times in ${H_r}$ is 
$n = \frac{N}{4}+\frac{N}{16} + \frac{N}{64} + .. = 
 \lfloor \frac{N}{3} \rfloor$.
Hence, the link that is used at least 
$\lfloor\frac{2N}{3}\rfloor$ times in ${H_r}$ is 
$(\lfloor \frac{N}{3} \rfloor, \lceil \frac{N}{3} \rceil)$.

\noindent
{\bf Corollary 3.1} Give an array of size $N=2^r$, if the nodes in the 
array are partitioned into 2 sets $S_1=\{i|0\le i \le n\}$ and 
$S_2=\{i | n+1 \le i \le N\}$, where $n=\lfloor \frac{N}{3}\rfloor$,
then there are at least $\lfloor\frac{2N}{3}\rfloor$
connections  in ${H_r}$ 
from  $S_1$ to  $S_2$ and 
$\lfloor\frac{2N}{3}\rfloor$
connections
from $S_2$ to $S_1$. $\Box$

\noindent
{\bf Theorem 1}: $\pi(ARRAY(N), H_r) \ge \lfloor \frac{2N}{3} \rfloor$.

\noindent
{\bf Proof}: Directly from Lemma 3. $\Box$

\vspace{0.12in}
\noindent
{\bf An optimal channel assignment scheme}
\vspace{0.12in}

By the definition of hypercube communication, connections  
in ${H_r}$ can be partitioned
into three sets, $DIM_0$, $EVEN_r$ and $ODD_r$. $DIM_0$ contains the 
dimension 0 connections, $EVEN_r$ contains connections 
between nodes with even node numbers, and $ODD_r$ contains 
connections between nodes with odd node numbers. 
Each of $EVEN_r$ and $ODD_r$
forms a $r-1$ dimensional 
hypercube communication, ${H_{r-1}}$, if only the nodes involved in 
communications are considered
and that the nodes are renumbered accordingly. 
%Fig.~\ref{LEMMA4} shows the
%partition of ${H_3}$.  
Thus, channel assignment schemes for 
${H_{r-1}}$  can be extended to realize ${H_r}$ as shown in the 
following lemma.

%\begin{figure}
%\centerline{\psfig{figure=fig/l4.eps,width=4in}}
%\caption{$H_3 = EVEN_3 \cup ODD_3 \cup DIM_0$}
%\label{LEMMA4}
%\end{figure}

%A straight forward channel assignment scheme can be obtained from 
%the above partitioning of ${H_r}$.
%Assuming we know how to assign channels for 
% ${H_{r-1}}$ on an array of size $2^{r-1}$,
%we can assign channels for ${H_r}$ by 
%scheduling $EVEN_r$ on the $2^{r-1}$ even numbered nodes, $ODD_r$
%on the $2^{r-1}$ odd numbered nodes and using one more configuration to 
%schedule $DIM_0$
%(it can be easily  proven that $DIM_0$ forms a configuration).
%Let
%$D(N)$ be the number of configurations
% needed to realize hypercube communication 
%for array of size $N$. It can be expressed in the following equation.\\
%\centerline{$D(N) = 2D(N/2) + 1$}

\noindent
{\bf Lemma 4}: Assuming that ${H_{r-1}}$ can be
realized on an array of size $2^{r-1}$
using  $K$ channels, then ${H_r}$
can be realized on an array of size $2^{r}$ 
using $2K+1$ channels.

\noindent
{\bf Proof}: ${H_r} = EVEN_r\cup ODD_r \cup DIM_0$. From the above 
discussion and the
assumption, $EVEN_r$ and $ODD_r$ are ${H_{r-1}}$ (when nodes are properly
renumbered), $K$ channels can be used to realize 
$EVEN_r$ or $ODD_r$. Since it can be easily proven that 
$DIM_0$ can be realized with one channel,
a total of $2K+1$ channels can be used to realize ${H_r}$. $\Box$

Let $D(N)$ be the number of channels
needed for ${H_r}$ on an array of size $N = 2^r$. 
If a channel assignment scheme is used that is in accordance with the proof
of  Lemma 4, it can be shown that the  equation,
$D(N) = 2D(N/2) + 1$.
Given that no channel is needed to realize hypercube communication on 
a 1--node array, D(1) = 0. Solving for $D(N)$ results in 
$D(N) = N-1$, which is not optimal.
%means that using this simple scheme, 
%$N-1$ channels are needed to realize ${H_r}$ on a linear array of size 
%$N=2^r$. This  does not reach the lower bound. 
%Fig.~\ref{NOOPT} shows the 
%channel assignment for a 16 node array using this simple scheme.
The following lemma improves this simple channel assignment scheme.


%\begin{figure}
%\centerline{\psfig{figure=fig/noopt.eps,width=4in}}
%\caption{A non-optimal channel assignment
%         for ${H_4}$ on array uses 15 channels.}
%\label{NOOPT}
%\end{figure}

\begin{figure}
\centerline{\psfig{figure=fig/l6.eps,width=3in}}
\caption{Realizing $DIM_0\cup DIM_1$ of $H_3$}
\label{LEMMA6}
\end{figure}

\noindent
{\bf Lemma 5}: Assuming that ${H_{r-2}}$ can be realized on an array of size 
$2^{r-2}$ using $K$ channels, then ${H_r}$ can be realized on an 
array of size $2^r$ using $4K+2$ channels.

\noindent
{\bf Proof}: Consider ${H_r}$
without dimension 0 and dimension 1 connections. 
By the definition of ${H_r}$,
${H_r} - (DIM_0\cup DIM_1) = DIM_2\cup ... \cup DIM_{r-1}$ 
forms four hypercube patterns, each being an ${H_{r-2}}$ pattern on 
nodes $\{n\ |\ n\ mod\ 4 = i\}$ (with proper node renumbering), denoted
by $subarray_i$, for $i =$ 0, 1, 2 or 3. From 
the hypothesis, ${H_{r-2}}$ can be realized on an array of size $2^{r-2}$ 
using  $K$ channels.  The 
four sub--cube patterns can be realized in $4K$ channels.
The remaining connections to be considered are those in $DIM_0$ and $DIM_1$.
It can easily be proven that connections in $DIM_0$ and $DIM_1$
can be assigned to 2 channels as shown in Fig.~\ref{LEMMA6}.
%by
%mixing half the connections in $DIM_0$ with half the connections in $DIM_1$
%in one channel.
%Fig.~\ref{LEMMA6} shows an example for an 8--node array. As can be seen
%from the figure, similar channel assignment can be used to assign 2 channels
%to all $DIM_0$ and $DIM_1$ connections for any array of size $N=2^r$.
Hence, the hypercube communication ${H_r}$ can be realized using
a total of $4K+2$ channels. $\Box$

\begin{figure}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}\=Algorithm 1: Assign\_array($N = 2^r$) \\
\>(1)\hspace{0.1in}\=If $(r = 0)$ then return $\phi$\\
\>(2)\>If\=\ (r is odd) then\\
\>(3)\>\>/* applying Lemma 4 */\\
\>(4)\>\>recursively apply Assign\_array($N/2=2^{r-1}$)  for $EVEN_r$. \\
\>(5)\>\>recursively apply Assign\_array($N/2=2^{r-1}$)  for $ODD_r$.\\
\>(6)\>\>assign connections in  $DIM_0$ to one channel.\\
\>(7)\>Else /* r is even, apply Lemma 5 */\\
\>(8)\>\>Fo\=r i = 0, 1, 2, 3\\
\>(9)\>\>\>apply Assign\_array($N/4 = 2^{r-2})$
           for $subarray_i$. \\
\>(10)\>\>assign connections in $DIM_0 \cup DIM_1$ to 2 channels.
\end{tabbing}
\caption{The channel assignment algorithm}
\label{ALGO1}
\end{figure}

The channel assignment algorithm, {\em Algorithm 1}, 
is depicted in Fig.~\ref{ALGO1}. 
%Notice that 
%while the algorithm is described using a recursive notation for simplicity,
%it is easier to perform the channel assignment in a bottom-up fashion.
For the base case, when $N=2^0=1$, the hypercube pattern 
contains no connection.
To assign channels to connections in an array of size $N=2^r$, 
$r > 0$, there are two 
cases. If $r$ is even, then Lemma 5 is applied to use
$4K+2$ channels for the hypercube pattern, where $K$ is the 
number of channels needed
 to realize a hypercube pattern on an array of size $2^{r-2} = N/4$. 
If $r$ is odd, Lemma 4 is applied to use
$2K+1$ channels to realize the hypercube pattern, where $K$ is the
number of channels needed
to realize a hypercube pattern in an array of size $2^{r-1} = N/2$.
The example of using this algorithm to schedule ${H_4}$ in an array of size 
16 is shown in Fig.~\ref{OPT}.


\begin{figure}
\centerline{\psfig{figure=fig/opt.eps,width=3.6in}}
\caption{Optimal channel assignment for ${H_4}$}
\label{OPT}
\end{figure}

\noindent
{\bf Theorem 2}: {\em Algorithm 1}  uses $\lfloor\frac{2N}{3}\rfloor$
channels for  ${H_r}$ on a
linear array with $N=2^r$ nodes, thus 
$w(ARRAY(N), H_r) \le \lfloor\frac{2N}{3}\rfloor$.

\noindent
{\bf Proof}: Let $D_{odd}(2^r)$ and $D_{even}(2^r)$
denote the number of channels needed 
when $r$ is  odd and even, respectively.
The number of channels for the hypercube pattern using 
{\em Algorithm 1}
can be formulated as follows,

\hspace{0.1in}$D_{odd}(2^r) = 2D_{even}(2^{r-1}) + 1$, when $r$ is odd.

\hspace{0.1in}$D_{even}(2^r) = 4D_{even}(2^{r-2}) + 2$, when $r$ is even.

\noindent
Using the  boundary condition $D_{even}(1)= D_{even}(2^0) = 0$,
it can be proven by induction that  $D_{odd}(N) = \frac{2N}{3} - \frac{1}{3}$
and $D_{even}(N) = \frac{2N}{3} - \frac{2}{3}$.
%
%Base case: $D_{even}(2^0) = \frac{2\times 1}{3} - \frac{2}{3} = 0$.
%
%Induction case: Assuming $D_{even}(N) = \frac{2N}{3} - \frac{2}{3}$
%and $D_{odd}(N) = \frac{2N}{3} - \frac{1}{3}$.
%
%$D_{odd}(2N) = 2D_{even}(N) + 1 
%            = 2\times (\frac{2N}{3} - \frac{2}{3}) + 1
%            = \frac{2\times(2N)}{3} - \frac{1}{3}$
%$D_{even}(2N) =  4D_{even}(N/2)+2
%              = 4 (\times \frac{2\times N/2}{3} - \frac{2}{3}) + 2
%              = \frac{2\times (2N)}{3} - \frac{2}{3}$
%
%\noindent
Hence, $D_{odd}(N)$ and $D_{even}(N)$ are equal 
to $\lfloor\frac{2N}{3}\rfloor$. 
$w(ARRAY(N), H_r) \le \lfloor\frac{2N}{3}\rfloor$. $\Box$
%, which is equal to the lower bound in 
%Theorem 1. $\Box$

%Notice that for a linear array of size $2^r$, where $r$ is an odd number,
%Lemma 5 can also apply recursively without applying Lemma 4 first. However, 
%this will not lead to an optimal channel assignment. 
%Specifically, the number of channels required in this case can be determined
% from
% the formula $D(N) = 4D(N/4) + 2$, with
%boundary condition $D(2) = 1$. The solution
%of this equation is $D(N) = \frac{5N}{6}- \frac{2}{3}$, which is not optimal.

\noindent
{\bf Theorem 3}:\\ 
$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$, and Algorithm 1 is optimal.

\noindent
{\bf Proof: } Follows from Theorem 1, Theorem 2 and Lemma 1.$\Box$
%From Theorem 1, we have 
%$\pi(ARRAY(N), H_r) \ge \lfloor \frac{2N}{3} \rfloor$, from Theorem 2, 
%we have $w(ARRAY(N), H_r) \le \lfloor \frac{2N}{3} \rfloor$, and
%from Lemma 1, we have $w(ARRAY(N), H_r) \ge \pi(ARRAY(N), H_r)$.
%Combining these results, we obtain 
%$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$.
%Since the channel assignment algorithm, {\em Algorithm 1}, uses 
%$\lfloor \frac{2N}{3} \rfloor$ channels for ${H_r}$, it is optimal.
%$\Box$

\subsubsection{Hypercube connections  on rings}

By having links between node 0 and node $N-1$, two paths can be 
established from any node to any other node on a ring. It has been shown
\cite{beauquier97}
that even for a fixed routing, general optimal channel assignment problem is 
NP--complete. This section  focuses on the specific
problem of optimal RCA for ${H_{r}}$ on ring topologies, obtaining
a lower bound on the number of channels needed to realize ${H_{r}}$ and
developing an optimal routing and channel assignment algorithm 
that achieves this lower bound.

%Assuming that 
%a virtual circuit between two nodes is always established using a
% shortest path, the flexibility provided by 
% the extra links between node 0 and node $N-1$ affects only 
%the dimension $r-1$ connections that span $\frac{N}{2}$ nodes.  
%The following 
%lemma establishes  
%a lower bound for the multiplexing degree needed to 
%realize ${H_r}$ in a ring with $N=2^r$ nodes.

\noindent
{\bf Lemma 6}: $\pi(RING(N), H_r) \ge 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$.

\noindent
{\bf Proof}: This lemma is proven by showing that there exist 
two cuts on a ring
that partition the ring into two sets, $S_1$ and $S_2$, such that 
$ 2\times \lfloor\frac{N}{3} + \frac{N}{4}\rfloor$  connections in 
${H_r}$  originate at nodes  in $S_1$ and terminate at nodes in $S_2$.
Since there are only 2 links connecting  $S_1$ to
$S_2$, one of the 2 links must be used at least
$\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$ times, regardless of which routing
scheme is used.
Consider ${H_r}$ on a ring of size $N=2^r$.
The connections in $DIM_0\cup...DIM_{r-2}$
form two $r-1$ dimensional
hypercube patterns in two {\em arrays} of size $2^{r-1}$.
The first  array, denoted by $subarray_1$, contains  
nodes 0, ..,  $2^{r-1}-1$ and the second  array, denoted by 
$subarray_2$,  contains nodes $2^{r-1}$,..,  $2^r-1$.
From Corollary 3.1, it follows that
there exists a link in each $2^{r-1}$ node array such that  
$\lfloor\frac{N}{3}\rfloor$ connections in the hypercube pattern
 use that link in each
direction. From the discussion in previous section, the link is  
$(\lfloor \frac{N}{3} \rfloor, \lceil \frac{N}{3} \rceil)$ in $subarray_1$
 and $(\lfloor \frac{N}{3} \rfloor+2^{r-1}, 
\lceil \frac{N}{3} \rceil + 2^{r-1})$ in 
$subarray_2$. 
These two links partition  the ring 
into two sets 
$S_1 = \{i| 0\le i\le \lfloor \frac{N}{3} \rfloor\} 
\cup \{i|2^{r-1}+\lfloor \frac{N}{3} \rfloor+1\le i\le 
2^r-1\}$ and 
$S_2 = \{i| \lfloor \frac{N}{3} \rfloor+1 \le i 
\le 2^{r-1}+\lfloor \frac{N}{3} \rfloor\}$.
Hence, there are $\lfloor\frac{N}{3}\rfloor$ connections from 
$S_1\cap subarray_1$ to $S_2\cap subarray_1$ and 
$\lfloor\frac{N}{3}\rfloor$ connections from 
$S_1\cap subarray_2$ to $S_2\cap subarray_2$ in $DIM_0\cup...DIM_{r-2}$.
Thus, there are $2\times \lfloor\frac{N}{3}\rfloor$ connections 
in $DIM_0\cup..\cup DIM_{r-2}$ originating at nodes in $S_1$ and
terminating at nodes in $S_2$.
Fig.~\ref{LEMMA8} 
shows the cuts on a 16--node ring. 
The remaining connections of ${H_r}$
are in $DIM_{r-1}$. By partitioning
the ring into $S_1$ and $S_2$, each node in $S_1$ has a dimension $r-1$ 
connection to a node in $S_2$. Hence, there are  $N/2$ 
connections in $DIM_{r-1}$ between  $S_1$ and $S_2$. Therefore, a total of 
$2\times \lfloor\frac{N}{3}\rfloor + N/2 = 2\times 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$ connections in ${H_r}$ are 
from  $S_1$ to 
 $S_2$. Thus, $\pi(RING(N), H_r) \ge 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$.  $\Box$

\begin{figure}
\centerline{\psfig{figure=fig/l8.eps,width=3.2in}}
\caption{Hypercube on a ring}
\label{LEMMA8}
\end{figure}


%In the following, we show that using deterministic 
%odd--even shortest path
%routing and a channel assignment scheme derived from lemma 6, 
%this lower bound can be achieve. 
The RCA scheme uses an odd--even shortest path
routing. Given a ring of size $N=2^r$,
an odd--even shortest path routing  works as follows. 
A connection between two nodes is  established using a
shortest path. Connections that have two shortest
paths are of the forms $(i, i+2^{r-1})$  and $(i, i-2^{r-1})$. For these
connections, the clockwise path is used if  $i$ is even and the 
counter--clockwise path if $i$ is odd. 
%Note that using odd--even 
%shortest path routing, the extra links between node 0 and node 
%$N-1$ affect only 
%the dimension $r-1$ connections that span $\frac{N}{2}$ nodes in
%${H_r}$.

The channel assignment algorithm is derived from Lemma 6. There are
two parts in the algorithm, channel assignment for
connections in $DIM_{r-1}$ and 
channel assignment for connections in $DIM_0\cup .. \cup DIM_{r-2}$. 
%Using odd--even short path routing, 
Channel assignment for 
connections in $DIM_0\cup .. \cup DIM_{r-2}$
is equivalent to channel assignment for  two ${H_{r-1}}$ 
in two disjoint arrays, 
thus, using the channel assignment 
scheme (for array) described in the 
previous section, $\lfloor \frac{N}{3} \rfloor$ channels 
can be used to realize
these connections. For the connections in $DIM_{r-1}$,
using  odd--even shortest path routing, 
four connections in $DIM_{r-1}$,
$(i, i+2^{r-1})$, $(i+2^{r-1}, i)$, $(i+1,i+2^{r-1}+1)$, $(i+2^{r-1}+1, i+1)$,
can be realized using one channel. We denote by $CONFIG_i$ these four 
connections. Since
the union of all $CONFIG_i$, where $i = 0, 2, 4, ..., N/2-2$ is equal to 
$DIM_{r-1}$,  $N/4$  channels are 
sufficient to realize $DIM_{r-1}$. Fig.~\ref{ALGO2} shows  the channel
assignment
algorithm for ring topologies.



\begin{figure}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}\=Algorithm 2: Assign\_ring($N=2^r$)\\
\>\\
\>(1)\hspace{0.1in}\=Apply Assign\_array($N/2=2^{r-1}$) on $subarray_1$.\\
\>(2)\hspace{0.1in}Apply Assign\_array($N/2=2^{r-1}$) on  $subarray_2$.\\
\>\>Since $subarray_1$ and $subarray_2$ are disjoint, \\
\>\>channels can be reused in steps (1) and (2).\\
\>(3)\>fo\=r i = 0, N/2-2, step 2\\
\>\>\>Assign a channel to connections $(i, i+2^{r-1})$, $(i+2^{r-1}, i)$,\\
\>\>\>         $(i+1,i+2^{r-1}+1)$ and $(i+2^{r-1}+1, i+1)$\\
\end{tabbing}
\caption{The channel assignment for rings}
\label{ALGO2}
\end{figure}

\noindent
{\bf Theorem 4}: {\em Algorithm 2} uses 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$ channels
to realize ${H_r}$ in a ring of size $N=2^r$.

\noindent
{\bf Proof}: Follows from above discussion. $\Box$

\noindent
{\bf Theorem 5}: 
$w(RING(N), H_r) = \pi(RING(N), H_r) = 
\lfloor \frac{N}{3 } + \frac{N}{4} \rfloor$, and the odd--even shortest path
routing with Algorithm 2 is an optimal RCA scheme for hypercube
connection  on rings.

\noindent
{\bf Proof: } Follows from Lemma 1, Lemma 6 and Theorem 4.$\Box$
%From Lemma 6, we have 
%$\pi(RING(N), H_r) \ge \lfloor \frac{N}{3} + \frac{N}{4} \rfloor$,
% from Theorem 4, 
%we have $w(RING(N), H_r) \le \lfloor \frac{N}{3} + \frac{N}{4} \rfloor$,
%and from Lemma 1, we have $w(RING(N), H_r) \ge \pi(RING(N), H_r)$.
%Combining these results, we obtain
%$w(RING(N), H_r) = \pi(RING(N), H_r) = 
%\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$.
%Since with the odd--even shortest path routing, the 
%channel assignment algorithm, {\em Algorithm 2}, uses 
%$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$ channels 
%for ${H_r}$, the routing scheme together with the algorithm forms an 
%optimal RCA scheme for hypercube communication on rings. $\Box$

\subsubsection{Hypercube connections on meshes}

Given a $2^k\times 2^{r-k}$  mesh,
realizing the hypercube connections on the mesh is equivalent 
to realizing $H_{k}$ in each row and $H_{r-k}$ in each column. 
The following lemma gives the lower bound on the number of channels
required
to realize  hypercube communication patterns on  meshes. 
%Unlike the lower bound for the linear array
%and ring, we cannot prove that this lower bound is tight.

\noindent
{\bf Lemma 7}: $\pi(MESH(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2\times2^k}{3}\rfloor$, assuming  $k\ge r-k$.

\noindent
{\bf Proof}:  
The hypercube pattern on the mesh contains $2^{r-k}$  $k$--dimensional
hypercube patterns on  $2^k$ arrays in the $2^{r-k}$ rows. 
Consider a cut in edges
$(\lfloor \frac{2^k}{3} \rfloor, \lceil \frac{2^k}{3} \rceil)$ in every
row, which partitions the mesh into two parts. 
From Corollary 3.1, we know that for each row there are 
$\lfloor \frac{2\times 2^k}{3} \rfloor$ connections from the left of the
cut to the right of the cut, hence, there are a total of 
$2^{r-k} \times \lfloor \frac{2\times 2^k}{3} \rfloor$ connections
crossing
 the cut. Since there are $2^{r-k}$ edges in the cut, 
there exists at least one edge that is used at least 
$\lfloor\frac{2\times2^k}{3}\rfloor$ times. Thus,
 $\pi(MESH(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2\times2^k}{3}\rfloor$. $\Box$

%
%Without losing
%generality, we assume that      $k \ge r-k$ in the following discussions.
%The following lemma states the properties of this type of mesh. 
%
%\noindent
%{\bf lemma 8}: The hypercube pattern pattern in an $2^k\times 2^{r-k}$ mesh
%is equivalent to the hypercube pattern for array of size $2^k$ in every row in 
%x direction and hypercube pattern for array of size $2^{r-k}$ in every column
%in y direction.
%
%\noindent
%Proof: The lemma is proven by showing that connections sourced at
%any node in $i$th column and $j$th row forms the hypercube connections
%in $i$th column and hypercube connections in $j$th row that sourced at this
%node. Let number $i$ be represented in binary with $k$ bits,
%${k-1}i_{k-2}...i_0$ and $j$ be represented in binary with $r-k$ bits
%$j_{r-k-1}j_{r-k-2}...j_0$. Since the node in $i$th column and $j$th row
%are numbered as $n = i\times 2^{r-k} + j$, the binary representation of $n$
%is ${k-1}i_{k-2}...i_0j_{r-k-1}j_{r-k-2}...j_0$. By definition, the 
%hypercube connections sourced at this node in $i$th row are
%
%\centerline{\{${k-1}i_{k-2}...i_0a_{r-k-1}a_{r-k-2}...a_0 | $
%               there is one and only one $a_l$, where $0\le l\le r-k-1$,
%               differ from $j_l$\}}
%
%\noindent
%the hypercube connections source at this node in $j$th column are
%
%\centerline{\{$a_{k-1}a_{k-2}...a_0j_{r-k-1}j_{r-k-2}...j_0 | $
%               there is one and only one $a_l$, where $0\le l\le k-1$,
%               differ from $l$\}}
%\noindent
%By definition of hypercube communication, the hypercube pattern in the mesh
%sourced node\\
% ${k-1}i_{k-2}...i_0j_{r-k-1}j_{r-k-2}...j_0$ is equivalent to 
%the union of these two sets. Hence, the hypercube  pattern in 
%an $2^k\times 2^{r-k}$ mesh
%is equivalent to the hypercube pattern for array of size $2^k$ in every row in 
%x direction and hypercube pattern for array of size $2^{n-k}$ in every column
%in y direction. $\Box$
%
%lemma 8 states that establishing hypercube on 

Given a mesh of size 
$2^k\times 2^{r-k}$, the hypercube communication pattern 
in each 
row is denoted by ${H_k^{row}}$ 
and  the hypercube communication pattern in 
each column by ${H_{r-k}^{col}}$.
The RCA scheme uses X--Y shortest path routing.
Since we already know the optimal channel assignment for ${H_k^{row}}$ and 
${H_{r-k}^{col}}$, the challenge here is  to reuse channels on 
connections in two dimensions efficiently. 
Let us define an {\em array configuration} as the set of connections 
in a linear array that are assigned to the same channel. {\em Ring}, 
{\em mesh} and {\em torus configurations} are defined similarly.
Using the definition of configurations, a mesh configuration can be obtained
by combining array configurations in  the rows and the columns.
For example,
if an array configuration in x dimension and an array configuration in 
y dimension can be combined into a mesh configuration, the two array 
configurations can be realized in the mesh topology using one channel.
Notice that,  while there is no link conflict when assigning channels to 
 row and column connections, 
%${H_k^{row}}$ uses only x direction links while ${H_{r-k}^{col}}$
%uses only y direction links. However, 
node conflicts may occur and must be avoided.

\begin{figure}
\centerline{\psfig{figure=fig/l13.eps,width=3.0in}}
\caption{a Mesh configuration}
\label{LEMMA13}
\end{figure}

%Since we will try to combine array configurations
%into mesh configurations, 
Let us first take a deeper look at the 
array configurations for arrays of size $N=2^k$. Following the channel
 assignment algorithm, {\em Algorithm 1}, array
configurations can be classified into three categories;
 $E$--configurations that contain only connections
between even--numbered nodes,  $O$--configurations that contain  only
connections  between odd--numbered nodes,
and $EO$--configurations that contain  
dimension 0 (and/or) dimension 1 connections 
%(thus may connect  even
%number nodes and odd number nodes). 
As discussed in Section 3,
if $k$ is odd, there is only one 
$EO$--configuration for  connections in $DIM_0$,  
$(\lfloor \frac{2N}{3}\rfloor -1) / 2$ $E$--configurations for
connections in $EVEN_k$,
and $(\lfloor \frac{2N}{3}\rfloor -1) / 2$ $O$--configurations for
connections in $ODD_k$. Similarly, 
if  $k$ is even, there are two $EO$--configurations, 
 $(\lfloor \frac{2N}{3}\rfloor -2) / 2$ $E$--configurations 
and 
 $(\lfloor \frac{2N}{3}\rfloor -2) / 2$ $O$--configurations. 
The following lemma shows that $E$--configurations  and $O$--configurations 
in  rows and columns of the mesh 
can be combined.

\noindent
{\bf Lemma 8}: Given an  $E$--configuration, $E_x$, and an  
$O$--configuration, $O_x$, in the x direction and 
an $E$--configuration, $E_y$, and an $O$--configuration, $O_y$, in the 
y direction, 
$E_x$ and $O_x$ in all rows and $E_y$ and $O_y$ in all  columns can be
realized in two mesh configurations.

\noindent
{\bf Proof}: The proof is by constructing the two mesh 
configurations. In the first mesh configuration,
let all odd numbered rows realize $O_x$
and all even numbered row realize $E_x$.  
In this case, no connection starts or terminates at an 
odd numbered node in an even column or at 
an even numbered node in an odd column. Thus, in the same mesh configuration,
$E_y$ can be realized in odd
columns and $O_y$ can be realized in even columns.
The second mesh configuration realizes $E_x$ on odd numbered rows, 
$O_x$ on even numbered rows,
$E_y$ on even numbered columns and $O_y$ on odd numbered columns. These
two mesh configurations
 realize  $E_x$ and $O_x$ in all rows
 and $E_y$ and $O_y$ in all columns.
 Fig.~\ref{LEMMA13} shows the construction of a 
mesh configuration. $\Box$

Lemma 8 lays the foundation for the channel assignment algorithm. 
Let $a$ be the number of $E$--configurations 
and $O$--configurations in ${H_k^{row}}$,
$b$ be the  number of $EO$--configurations in ${H_k^{row}}$,
$c$ be the number of $E$--configurations
 and $O$--configurations in ${H_{r-k}^{col}}$,
and $d$ be the  number of $EO$--configurations in ${H_{r-k}^{col}}$.
From assumptions, it follows that  $k \ge r-k$, $a\ge c$, 
$a+b = \lfloor \frac{2\times 2^k}{3}\rfloor$ and 
$d \le 2$.
By combining $E$--configurations and $O$--configurations in rows and 
columns into mesh 
configurations, all the $E$--configurations and $O$--configurations 
in each row and
all the $E$--configurations and $O$--configurations 
in each column can be realized
using $a$ mesh configurations. 
Using an individual mesh 
configuration for each EO  configuration in the rows and the columns,
a total of 
$a + b + d \le \lfloor \frac{2\times 2^k}{3}\rfloor + 2$
configurations are sufficient 
to  realize the hypercube connections.
%communication pattern on the mesh. 
%This is stated in the following theorem.

\noindent
{\bf Theorem 4}: ${H_r}$ can be realized on a $2^k\times 2^{r-k}$ mesh, 
 where $k \ge r-k$, using 
$\lfloor \frac{2\times 2^k}{3}\rfloor + 2$ channels. $\Box$

%\noindent
%{\bf Proof:} Straight forward from the above discussion. $\Box$

\noindent
{\bf Corollary 4.1:} $w(MESH(2^k\times 2^{r-k}), H_r) \le 
\lfloor \frac{2\times 2^k}{3} \rfloor + 2 \le 
\pi(MESH(2^k\times 2^{r-k}), 
H_r) + 2$. $\Box$

\subsubsection{Hypercube connections on tori}
\label{hypercubeontori}

%Hypercube communication on torus topologies is 
%obtained from the hypercube communication on the ring topology.
As in the case of realizing $H_r$ on a mesh, 
${H_r}$ can be realized on a  $2^k\times 2^{r-k}$
torus by realizing ${H_{k}^{row}}$ in each row and ${H_{r-k}^{col}}$
in each column. The following lemma gives a lower bound
on the number of channels required to realize ${H_r}$
on a torus.

\noindent
{\bf Lemma 9}: $\pi(TORUS(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$, assuming  $k\ge r-k$.

\noindent
{\bf Proof:} 
The hypercube pattern on the torus contains $2^{r-k}$  $k$--dimensional
hypercube patterns on  $2^k$ rings in the $2^k$ rows. 
Considered two cuts in edges
$(\lfloor \frac{2^{k-1}}{3} \rfloor, \lceil \frac{2^{k-1}}{3} \rceil)$
and 
$(\lfloor \frac{2^{k-1}}{3} \rfloor + 2^{k-1}, 
\lceil \frac{2^{k-1}}{3} \rceil + 2^{k-1})$ in every
row which partition the torus into two parts. 
Following the same reasoning as in the proof of lemma 6, 
it is known that for each row there are 
$2 \times \lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor$ 
connections from one part to the other part, hence, there are a total of 
$2^{r-k} \times 2 \times \lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor$ 
connections
crossing the two parts. 
Since there are $2\times 2^{r-k}$ edges in the cut, regardless of the
routing scheme used, there exist at least one edge that is used at least 
$\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$ times. Thus,
 $\pi(TORUS(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$. $\Box$

X--Y routing between dimensions and odd--even shortest path routing
within each dimension are used to develop the RCA scheme. 
Next, the combination of 
ring configurations into torus configurations is considered.
As in the case of rings,
given a $2^k\times 2^{r-k}$ torus,  
the connections in ${H_r}$ are partitioned into two sets. The first
set includes all connections in $DIM_0\cup..\cup DIM_{k-2}$ in each row and 
all connections in $DIM_0\cup..\cup DIM_{r-k-2}$ in each column. The second
set includes the connections in $DIM_{k-1}$ in each row and 
the connections in $DIM_{r-k-1}$ in each column. The connections  
in $DIM_0\cup..\cup DIM_{k-2}$ in each row and the
connections in $DIM_0\cup..\cup DIM_{r-k-2}$ in each column form four 
hypercube patterns on  four disjoint $2^{k-1}\times 2^{r-k-1}$ 
sub--meshes in the torus.
A straight forward extension of
the channel assignment scheme
 in the previous section can be used to assign channels to these
connections with at most
$\lfloor \frac{2^k}{3} \rfloor + 2$ channels. 

To realize the connections in $DIM_{k-1}$ in each row and 
the connections in 
$DIM_{r-k-1}$ in each column, 
%we assume that $r-k \ge 3$, and thus, $k \ge 3$.
%Note that if $r-k < 3$, then the hypercube pattern 
%on each row have 1, 2 or 4 nodes
%and a multiplexing degree of 2 is sufficient to realize all $H_{r-k}^{col}$. 
%In this case, a simple scheme that realizes $H_k^{row}$ and 
%$H_{r-k}^{col}$
%individually yields a connections scheduling that results in at most
%2 more multiplexing degree than the minimum required (the multiplexing degree
%to realize $H_k^{row}$). 
%For connections in $DIM_{k-1}$ in each row and $DIM_{r-k-1}$ 
%in each column, 
The same partitioning for the 
ring topology discussed in section 4 is followed. Specifically, 
the following configurations are constructed 
in rows and columns respectively

\noindent
$row_i = \{(i, i+2^{k-1}), (i+2^{k-1}, i), (i+1, i+1+2^{k-1}), 
          (i+1+2^{k-1}, i+1)\}$

\noindent
$column_j = \{(j, j+2^{r-k-1}), (j+2^{r-k-1}, j), (j+1, j+1+2^{r-k-1}), 
          (j+1+2^{r-k-1}, j+1)\}$

\noindent
$DIM_{k-1}$ is  composed of
 the configurations
$row_i$, for  $i = 0, 2, ..., 2^{k-1}-2$ and $DIM_{r-k-1}$ 
 is composed of  the configurations 
$column_j$ for $j = 0, 2, ..., 2^{r-k-1}-2$.

\noindent
{\bf Lemma 10} For any $i_1$, $i_2$, 
where $i_1 \ne i_2$,  
$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
$column_{i_2}$ in each column can be realized in two torus configurations.

\noindent
{\bf Proof}: Similar to the proof of Lemma 8, omitted. $\Box$
%This lemma is proven by constructing the two torus configurations,
%while avoiding  node conflicts. 
%The first torus configuration is constructed in the following way.
%For each 
%row $i_1$, $i_1+1$, $i_1+2^{r-k-1}$ and $i_1+1+2^{r-k-1}$, configuration
%$row_{i_1}$ is realized, while in all other rows, $row_{i_2}$ is realized.
%Now, consider the columns. In each column $i_1$, $i_1+1$,
%$i_1+2^{k-1}$ and $i_1+1+2^{k-1}$, the $(i_2)$th, $(i_2+1)$th, 
%$(i_2+2^{r-k-1})$th
%and $(i_2+1+2^{r-k-1})$th nodes are not used. Hence, 
%$column_{i_2}$ can be established
%in these columns. A similar argument is used for establishing 
%$column_{i_1}$ on 
%all other columns. This completes the construction of the first torus
% configuration.
%The second torus configuration can be obtained from the first 
%torus configuration by swapping $row_{i_1}$ and
%$row_{i_2}$ in the rows and $column_{i_1}$ and $column_{i_2}$ in the
%columns. These two configurations include   
%$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
%$column_{i_2}$ in each column. Thus,  
%$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
%$column_{i_2}$ in each column can be realized in two torus configurations.
%$\Box$

%\begin{table}[htbp]
%\\small
%\footnotesize
%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%columns \& rows & configuration \\
%\hline
%row $i_1$ & $row_{i_1}$ \\
%\hline
%row $i_1+1$ & $row_{i_1}$ \\
%\hline
%row $i_1+2^{r-k-1}$ & $row_{i_1}$ \\
%\hline
%row $i_1+1+2^{r-k-1}$ & $row_{i_1}$ \\
%\hline
%other rows & $row_{i_2}$ \\
%\hline
%column $i_1$ & $column_{i_2}$ \\
%\hline
%column $i_1+1$ & $column_{i_2}$ \\
%\hline
%%column $i_1+2^{k-1}$ & $column_{i_2}$ \\
%\hline
%column $i_1+1+2^{k-1}$ & $column_{i_2}$ \\
%\hline
%other columns & $column_{i_1}$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Configuration 1}
%\label{CONF1}
%\end{table}


%\begin{table}[htbp]
%\small
%\footnotesize
%%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%columns \& rows & configuration \\
%\hline
%row $i_1$ & $row_{i_2}$ \\
%\hline
%row $i_1+1$ & $row_{i_2}$ \\
%\hline
%row $i_1+2^{r-k-1}$ & $row_{i_2}$ \\
%\hline
%row $i_1+1+2^{r-k-1}$ & $row_{i_2}$ \\
%\hline
%other rows & $row_{i_1}$ \\
%\hline
%column $i_1$ & $column_{i_1}$ \\
%\hline
%column $i_1+1$ & $column_{i_1}$ \\
%\hline
%column $i_1+2^{k-1}$ & $column_{i_1}$ \\
%\hline
%column $i_1+1+2^{k-1}$ & $column_{i_1}$ \\
%\hline
%other columns & $column_{i_2}$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Configuration 2}
%\label{CONF2}
%\end{table}
%
%\noindent
%{\bf Lemma 11}: Assuming that $r-k \ge 3$, 
%the dimension $k-1$ connections in each row and 
%dimension $r-k-1$ connections in each column can be realized in 
%$2^{k-2}$ torus configurations.
%
%\noindent
%{\bf Proof}: From Lemma 10,  configurations
%$row_i$, $i = 0, 2, ..., 2^{r-k-1}-2$ and configurations 
%$column_j$, $j = 0, 2, ..., 2^{r-k-1}-2$ can be realized in $2^{r-k-2}$
%torus configurations. Since $2^{k-2} - 2^{r-k-2}$ torus configurations
% can be
%used to realize $row_i$,  $ i = 2^{r-k-1}, .. 2^{k-1}$, $2^{k-2}$ 
%torus configurations can realize all the dimension $k-1$ connections 
%in each row and 
%dimension $r-k-1$ connections in each column. $\Box$

\noindent
{\bf Theorem 5}:  ${H_r}$ can be realized on a $2^k\times 2^{r-k}$ torus, 
where $k \ge r-k$, using
$\lfloor \frac{ 2^k}{3} + \frac{2^k}{4}\rfloor + 2$ channels.

\noindent
{\bf Proof}: As discussed above,  
$\lfloor \frac{ 2^k}{3}\rfloor + 2$ channels 
are sufficient to realize all 
connections in ${H_r}$, except the connections in  $DIM_{k-1}$ in each row and 
$DIM_{r-k-1}$ in each column, by realizing  four hypercube
communication patterns on the four disjoint sub--meshes. From Lemma 10, 
configurations
$row_i$, $i = 0, 2, ..., 2^{r-k-1}-2$ and configurations 
$column_j$, $j = 0, 2, ..., 2^{r-k-1}-2$ can be realized in $2^{r-k-2}$
torus configurations. Since $2^{k-2} - 2^{r-k-2}$ torus configurations
can be
used to realize $row_i$,  $ i = 2^{r-k-1}, 2^{r-k-1}+2, .., 2^{k-1}-2$, 
all the dimension $k-1$ connections in each row and 
dimension $r-k-1$ connections in each column can be 
realized in $2^{k-2}$ 
torus configurations. Hence, ${H_r}$ can be realized by  a total of 
$\lfloor \frac{ 2^k}{3}\rfloor + 2 + 2^{k-2}
= \lfloor \frac{ 2^k}{3} + \frac{2^k}{4}\rfloor + 2$ configurations.
$\Box$

\noindent
{\bf Corollary 5.1:} $w(TORUS(2^k\times 2^{r-k}), H_r) \le 
\lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor + 2 \le 
\pi(TORUS(2^k\times 2^{r-k}), 
H_r) + 2$. $\Box$

%\end{doublespace}

%\subsubsection{Conclusion}

%In this subsection, I studied optimal schemes to realize 
%hypercube connections on  mesh--like optical networks.
%I prove that 
%$\lfloor \frac{2N}{3}\rfloor$ and $\lfloor \frac{N}{3} + \frac{N}{4}\rfloor$
%are tight lower bounds of the number of channels needed to realize hypercube
%connections on linear arrays and rings of size $N$, respectively. I develop
%optimal RCA algorithms that achieve these lower bounds. Also,
%I study the mesh and torus topologies and develop RCA
%algorithms that use at most 2 more channels than the 
%optimal. 

\subsection{Logical torus, all--to--all and allXY topologies}
\label{otherontori}

The logical torus topology coincides with the physical network. Thus, when 
realizing logical torus topology, there are no link conflicts 
since the physical network can support all links in the logical 
network simultaneously. However, node conflicts may occur. 
Under our network model, each node in the network can only access
one channel at any given time slot. Hence, to support 4 out--going links at 
each node, at least 4 channels are needed. Using 4 channels, the logical 
torus topology can be realized as follows. All links in a torus can be
classified into four categories, the UP links, the DOWN links, the LEFT 
links and the RIGHT links. Each category can be realized using 1 channels
without incurring node conflicts and link conflicts as shown in 
Figure~\ref{logicaltorus}. Notice that all nodes can be sending and receiving 
messages in the figure. Hence, 4 channels are sufficient and necessary
to realize the logical torus topology on top of the physical torus 
topology.    

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/logicaltorus.eps,width=5in}}
\caption{Realizing logical torus topology}
\label{logicaltorus}
\end{figure}

Optimal schemes to realize all--to--all communication on ring and torus 
topologies can be found in \cite{Hinrichs94}. It is shown in 
\cite{Hinrichs94} that for an $N$ node ring, $N\ge 8$, 
the all--to--all communication can be realized with $N^2/8$ channels without
node conflicts. For
an $N\times N$ torus, the all--to--all communication can be realized with
$N^3/8$ channels. The connections on each channel  
to realize the all--to--all communication will be called 
an {\em AAPC configuration}.
Details about the connection scheduling can be found in 
\cite{Hinrichs94}.

The logical allXY topology realizes  all--to--all connections in
each dimension in the physical torus. For an $N\times N$ torus, each
node in the logical allXY topology logically connects to $2N-2$ nodes.
Using the AAPC configurations for rings,
techniques similar to the ones in section~\ref{hypercubeontori}
can be used to combine the ring configurations to form torus configurations
and realize the allXY on an $N\times N$ torus, where $N\ge 16$, resulting in 
a multiplexing degree of $N^2/8$. For an $N\times N$ torus with $N\le 8$,
$2N-2$ channels can be used to realize the allXY topology.
For example, 
using the 8 AAPC configurations for 8--node rings in \cite{Hinrichs94},
6 configurations along each dimension cannot be combined because
of node conflicts, while 2 configurations in each dimension can be
combined in the torus, resulting a multiplexing degree 
of $14=2\times 8-2$ for realizing the allXY topology.

\section{Performance of the logical topologies under light load}

This section  considers the communication performance of the 
logical topologies under light load such that the network contentions
on both channels and switches are negligible.
An analytical model will be described that takes the network 
contention effect into consideration later in this chapter.

Let us assume that a packet can be
transferred from source to destination on a path 
in one time slot and that the network has a multiplexing degree of  $d$.
It takes on average $\frac{d + 1}{2}$ time slots to transfer a packet from
a router to the next router. Thus, assuming that the packet routing time in 
each router (including the E/O, O/E conversions) is $\gamma$, the average
number of intermediate routing hops per packet is $h$, and the network
contention is negligible, 
the average delay time for each packet can be expressed as follows: 

\vspace{-0.15in}
\begin{center}
\[delay = (h + 2) * \gamma + (h + 1) * \frac{d + 1}{2}.\]
\end{center}

The first term, $(h+2) * \gamma$, is the average routing time that a packet 
spends at the  $h$ intermediate
routers and the 2 routers at the sending and receiving nodes.
The second term, $(h+1) * \frac{d + 1}{2}$, is the
average packet transmission time on paths plus the time that a packet waits 
in the output path buffers.
Thus, the average delay time is determined by three parameters, 
the multiplexing degree $d$, the packet routing time 
$\gamma$, and the average number of hops per 
packet transmission $h$.
We can assume that the packet routing 
time $\gamma$ is the same for all topologies.
Different logical topologies result in  
different number of intermediate hops, $h$, 
and different multiplexing degree, $d$.
Next, the performance of the four logical topologies is discussed.

Given an $N\times N$ torus, the logical all--to--all topology
establishes direct connections  between all pairs of nodes and thus,
totally eliminates the intermediate hops, resulting in 
$h= 0$. Using the algorithm in \cite{Hinrichs94},  a multiplexing degree
of $\frac{N^3}{8}$ can be used to realize the logical all--to--all topology. 
Thus $d = \frac{N^3}{8}$, and the delay time is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{all-to-all} = 2\times \gamma + (\frac{N^3}{8} + 1) / 2 = 
             O(\gamma + N^3). \]
\end{center}

Given an $N\times N$ torus, a  logical torus topology can be realized using 
a multiplexing degree of 4 (i.e., $d = 4$).
For a logical $N\times N$ topology, the average number of intermediate
hops is $h = \frac{N}{2} - 1$. Hence the delay time for the logical torus
topology is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{torus} = (\frac{N}{2} + 1)\times \gamma + 
                              \frac{N}{2} \times (4+1)/2 = O(N\times \gamma).\]
\end{center}

For $N = 2^r$,  the algorithm in section~\ref{hypercubeontori} can  realize 
a logical hypercube topology  on an $N\times N$ torus
using a multiplexing degree of 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 2$, if $r$ is odd, and 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 1$, 
if $r$ is even.
For a logical $N^2$ node hypercube, the average number of intermediate
hops is $h = \frac{lg(N^2)}{2} - 1 = lg(N) - 1$.
Hence,  the delay time (for an even r) is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{hypercube} = (lg(N) + 1)\times \gamma + 
     \lg(N) \times (\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 2) / 2
     = O(\gamma lg(N) + Nlg(N)).
\]
\end{center}

Finally, let us consider the logical {\em allXY} topology.
As discussed in section~\ref{otherontori}, 
when $N \le 8$,  the logical topology can be realized
using a multiplexing degree of $2N-2$. For $N > 8$, 
a multiplexing degree of  
$\frac{N^2}{8}$ is needed. Since 
for two nodes in the same column or row,
no intermediate hop is needed, while in other cases, 
one intermediate hop is required,
the average number of intermediate hops on the logical allXY topology is
given by:

\centerline{$\frac{2N-2}{N^2-1}\times 0 + \frac{(N^2-1) - (2N-2)}{N^2-1} 
            \times 1 = \frac{N^2-2N + 1}{N^2-1}.$}

Therefore, for $N > 8$, the average delay can be expressed as follows:

\vspace{-0.15in}
\begin{center}
\[delay_{all\_XY} = (2+\frac{N^2-2N+1}{N^2-1})\times \gamma + 
                    (1+\frac{N^2-2N+1}{N^2-1})\times 
                    (\frac{N^2}{8} + 1) / 2 = O(\gamma + N^2).\]
\end{center}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Logical  & Number of   & multiplexing & total number \\
topology & intermediate hops (h) & degree (d) & of connections (P)\\
\hline
all--to--all & $0$ & $\frac{N^3}{8}$ & $N^2(N^2-1)$ \\
\hline
all\_XY  & $\frac{N^2-2N+1}{N^2-1}$ & 
$\frac{N^2}{8}$\ $\dagger$ & $N^2(2N-2)$\\ 
\hline
hypercube & $lg(N) -1$ & $\lfloor \frac{N}{3}+\frac{N}{4} \rfloor + 1$\ $\ddagger$ &
$N^2lg(N)$\\
\hline
torus &  $\frac{N}{2} -1 $  &  4 & $N^2\times 4$\\
\hline
\end{tabular}
\end{center}

$\dagger$ Assuming that $N>8$. If $N < 8$, the value is $2N-2$.

$\ddagger$ Assuming that $r$ is even. If $r$ is odd, the value is 
$\lfloor \frac{N}{3}+\frac{N}{4} \rfloor + 2$.

\caption{Summary of logical topologies}
\label{toposummary}
\end{table}

Table~\ref{toposummary} summarizes the average number of intermediate 
hops ($h$),
the multiplexing degree ($d$) and the total number of logical 
connections ($P$) 
for the four topologies. 
%Since we will use physical $8\times 8$ and $16\times 16$ tori as our
%underlying networks, we evaluate $h$, $d$ and $P$ for these two
%physical topologies in Table~\ref{specifictopology}. 
Figure~\ref{DELAY} plots  the average delay as a function of the packet
routing time $\gamma$, for the four logical
topologies on a physical $16\times 16$ torus. When
$\gamma$ is very small compared to data transmission time ($\gamma\le 0.5$),
the logical torus topology
achieves the smallest delay time. When $0.5\le \gamma \le 4.25$, the 
logical hypercube
has the best performance. When $4.25\le \gamma \le 256.25$, the allXY topology
gives the best performance. When $\gamma
 > 256.25$, the all--to--all topology has the
smallest packet delay. 

The characteristics exhibited in Figure~\ref{DELAY} are true for any
network size. Specifically, for a given $N$, there is a value of $\gamma$ below
which routing on the torus is more efficient than routing on the logical
hypercube. Similarly, 
there is a value of $\gamma$, above which
routing on the allXY topology
is more efficient than routing on the logical hypercube.
Finally, there is a value of $\gamma$, above which
routing on the all--to--all  topology
is more efficient than routing on the allXY topology.
In Figure~\ref{BEST} these special values are plotted for different $N$
and the ($N, \gamma$) parameter space is divided into four regions.
Each region is labeled by the logical topology that results in the
lowest average packet delay.

These results are obtained by ignoring network traffic contention,
and thus  are valid only  under light load. 
In the next section, a queuing model is used to study the network
performance under high load.

%\begin{table}[htbp]
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
%physical & logical  & h & d & P \\
%topology & topology &  &  & \\
%\hline
% & all--to--all & 0 &  64 & $64\times 63$ \\
%\cline{2-5}
%$8\times 8$ & all\_XY  & 0.78 & 14 & $64\times 14$\\ 
%\cline{2-5}
%torus & hypercube & 2 & 6 & $64\times 6$\\
%\cline{2-5}
% & torus &  1  &  4 & $64\times 4$\\
%\hline
% & all--to--all & 0 &  512 & $256\times 255$ \\
%\cline{2-5}
%$16\times 16$ & all\_XY  & 0.88 & 32 & $256\times 30$\\ 
%\cline{2-5}
%torus & hypercube & 3 & 10 & $256\times 8$\\
%\cline{2-5}
% & torus &  1  &  4 & $256\times 4$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Logical topologies on $8\times 8$ and $16\times 16$ tori}
%\label{specifictopology}
%\end{table}

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}
{\psfig{figure=fig/delay.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}
{\psfig{figure=fig/delay1.eps,width=2.95in}}
\end{subfigure}
\end{subfigRow*}
\caption{Performance for logical topologies on $16\times 16$ torus}
\label{DELAY}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range.eps,width=3in}}
\caption{Logical topologies giving lowest packet delay for given $\gamma$ and $N$}
\label{BEST}
\end{figure}

\section{An analytical model and its verification}

This section describes an approximate 
analytical model that takes network contention into consideration.
This model is used to study the effect of the network load on 
the maximum throughput and the packet delay.
It is assumed that in each time slot, a 
packet can be sent from the source to the destination on a path. 
For example, if a 1Gbps channel is used with a 53--byte packet (or cell)
as defined in the ATM standard, then the slot duration is $0.424\mu s$.
All other delays in the system are normalized with respect 
to this slot duration.

The routers and the paths in a network are modeled as a network of queues.
As shown in Figure~\ref{ROUTER},  each router has a routing queue that 
buffers the packets to be processed. The  router places packets
either into one of the output path queues that buffer packets
waiting to be transmitted, or into the local processor. 
Both a router
and a path have a constant service time. The exact model for such network
is very difficult to obtain. The network is approximated by making the 
following assumptions: 1) each queue is independent of each other and 
2) each queue has a Poisson arrival and constant service time. These 
assumptions enable the derivation of  expressions for 
the maximum throughput and the average packet delay 
of the four logical topologies
by dealing with the M/D/1 queues independently. The simulation results
confirm that these approximations are reasonable.  The following 
notation is used in the model:

\begin{itemize}
\item $N$. Size of each dimension of the torus. Thus,
 the network has a total of 
$N^2$ nodes.

%\item $d$. Multiplexing degree in the network. Different logical topologies
%require different multiplexing degrees. The multiplexing degrees for the four
%logical topologies are summarized  in 
%Table~\ref{toposummary}. A {\em frame} consists of $d$ time slots.
%
%\item $P$. Number of connections in the logical topology. Different
%logical topologies contain different numbers of connections. 
%The numbers of connections in the four logical topologies  are summarized
%in Table~\ref{toposummary}.
%
%\item $h$. Average number of intermediate hops per packet transmission.
%This number depends on the logical topology and is summarized in 
%Table~\ref{toposummary}. 
%The average number of paths a packet goes
%through is equal to $h+1$. The average number of routers a packet goes
%through is $h+2$, intermediate hops plus the sending and receiving nodes.

\item $d$, $h$ and $P$ are defined in the previous section. 
A {\em frame} consists of $d$ time slots. Within a frame, one time slot
is allocated to each path. As discussed earlier,
the average number of paths that a packet traverses
is equal to $h+1$. The average number of routers that a packet traverses
is $h+2$.
 

\item $\lambda$. Average packet generation rate at each node per
time slot. This implies that the average generation rate of packets to the 
entire network is $N^2\lambda$. It is  assumed 
that the arrival process is Poisson
and is independently and identically distributed on all network nodes. 
Furthermore, it is  assumed that all packets are equally likely to be destined
to any one of the network nodes. At each router, the newly generated packets
and the packets arriving from other nodes
are maintained in an infinite routing buffer before being processed
as shown in Figure~\ref{ROUTER}.

\item ${\lambda}_s$. Average rate of packet arrival at a router per time slot,
including both generated packets and  packets received from other nodes. 
This composite arrival
rate, ${\lambda}_s$, may be derived as follows. In any time slot the total 
number of generated
packets that arrive at all the routing buffers is $\lambda N^2$. 
On average, each of 
these packets traverses  $h+2$ routers within the network. 
Therefore, under steady state condition, there will be $\lambda N^2 (h+2)$
packets in all the routers of the network 
in each time slot. Under the assumption that each packet is 
equally likely to be in each router, the total arrival rate is given by 
${\lambda}_s = {\lambda} (h+2)$.

\item ${\lambda}_p$. Average rate of packet arrival at a path buffer
per time slot.
This arrival rate, ${\lambda}_p$, can be derived as follows. 
Under steady state condition, in any time slot, 
the total number of  packets in all the routers in the network is  
$\lambda N^2 (h+2)$. 
Of all these packets, $\lambda N^2$ packet will exit the network and 
$\lambda N^2  (h+2) - \lambda N^2  = \lambda N^2  (h+1)$ packets will be 
transmitted through paths in the network. 
Under the assumption that sources and destinations are uniformly
distributed in the network, the average arrival
rate is given by ${\lambda}_p = \frac{\lambda N^2 (h+1)}{P}$. 

\item ${\gamma}$. The routing time per packet at a router. 
Since packets are
of the same length, the routing time is a constant value. The 
average packet departure rate from the routing buffer, denoted by
${\mu}_s$,  is ${\mu}_s = \frac{1}{{\gamma}}$.

\item ${\mu}_p$. The average packet departure rate from each path buffer
per time slot. Since in the model used,
each path will be served once in every frame, ${\mu}_p = \frac{1}{d}$. 
The average service time in each path is $S_p = \frac{1}{{\mu}_p} = d$.

\end{itemize}

\subsection*{Maximum throughput}

With the above notation, the maximum throughput and average
packet delay of the logical topologies can now be studied. First the theoretical
maximum throughput is considered and then the average packet delay. 
Two bottlenecks can
potentially limit the maximum throughput.

\begin{itemize}
\item If the average packet arrival rate at a routing buffer
 is larger than the average
packet departure rate, that is  if ${\lambda}_s \le {\mu}_s$, then 
the throughput will be limited by the router processing bandwidth.
The maximum packet generation rate allowed by the router bandwidth, 
${\lambda}^{max}_s$, can be derived as follows: ${\lambda}_s \le {\mu}_s$, or 
$(h+2)\lambda \le \frac{1}{{\gamma}}$, or $\lambda \le \frac{1}{{\gamma} (h+2)}$. Thus,

\vspace{-0.15in}
\begin{center}
\[{\lambda}^{max}_{s} = \frac{1}{{\gamma}(h+2)}\] 
\end{center}

\item If the average packet arrival rate at a path buffer
is larger than the average 
packet departure rate, that is ${\lambda}_p \le {\mu}_p$, then
the throughput will be limited by the path bandwidth. The maximum fresh packet
generation rate allowed by the path bandwidth, ${\lambda}^{max}_{p}$, can be 
derived as follows: ${\lambda}_p \le {\mu}_p$, or 
$\frac{(h+1)\lambda N }{P} \le \frac{1}{d}$, or 
$\lambda \le \frac{P}{(h+1)Nd}$. Thus, 

\vspace{-0.15in}
\begin{center}
\[{\lambda}^{max}_{p} = \frac{P}{(h+1)Nd}\]
\end{center}

\end{itemize}

The theoretical maximum throughput is the minimum of ${\lambda}^{max}_{s}$
and ${\lambda}^{max}_{p}$, that is, 
${\lambda}^{max} = min ({\lambda}^{max}_{s},  {\lambda}^{max}_{p})$. Given
a topology, ${\lambda}^{max} =  {\lambda}^{max}_{s}$ indicates that
the router speed is the bottleneck, while
${\lambda}^{max} =  {\lambda}^{max}_{p}$ indicates that the path speed is 
the bottleneck. 

\subsection*{Average packet delay}

As mentioned before, the packet delay is divided into the 
{\em routing delay}, 
which includes the time a packet spends on routing buffers and the time 
for routers to process the packets, and the {\em transmission delay}, which 
includes the time a packet spends on path buffers and the actual
packet transmission time on the paths. 

Let us first
consider the routing delay in each router.
It takes ${\gamma}$ timeslots for a
router to process the packet when the packet reaches the front of the 
routing buffer. As for the packet waiting time in the routing buffer,
since the routing buffer is modeled as an $M/D/1$
queue, the average queuing delay depends on the arrival rate
${\lambda}_s$ and is given by:
\vspace{-0.15in}
\begin{center}
\[ Q = \frac{{\lambda}_s ({\gamma})^2}{2(1-\frac{{\lambda}_s}{{\mu}_s})} \]
\end{center}
 
where ${\lambda}_s$ is the average packet arrival rate, ${\gamma}$ is the
expected service time, ${\mu}_s$ is the average packet departure rate.
Given that ${\mu}_s = \frac{1}{{\gamma}}$,  the total time
that a packet spends in each router is given by:
\vspace{-0.15in}
\begin{center}
\[ routing\ delay = {\gamma} + \frac{{\lambda}_s({\gamma})^2}{2(1 - {\lambda}_s{\gamma})} 
\hspace{1in}(1)\]
\end{center}

Consider the two components of 
the transmission delay on each path. 
The first component is 
the delay required by a packet to synchronize with
the appropriate outgoing slot in the frame 
on which the node transmits and the actual packet transmission time. 
The average value of this delay is 
$\frac{1 + 2 + ... + d}{d} = \frac{d+1}{2}$. The second component is the  
$M/D/1$ queuing delay that a packet 
experiences at the buffer before it reaches the head of the buffer. 
This follows the same formula as in the 
routing delay case, and is given by,
\vspace{-0.15in}
\begin{center}
\[ \frac{{\lambda}_p S^2_p}{2 (1 - \frac{{\lambda}_p}{{\mu}_p})} = 
   \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)}\]
\end{center}

The two components are combined to obtain 
the total delay a packet encounters on a path as follows,
\vspace{-0.15in}
\begin{center}
\[ transmission\ delay = \frac{d+1}{2} + \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)} \hspace{1in}(2) \]
\end{center}

As discussed earlier, each packet
takes $h+2$ hops and $h+1$ paths on average. Thus, given that on average,
a packet spends {\em  routing delay} in each router and 
{\em transmission delay} on each 
path, the average packet delay can be expressed as follows:

\centerline{$delay = (h+2) \times routing\ delay + (h+1) \times transmission\ delay.$}

Using formula (1) and (2),
the following average delay encountered by a packet from the source
to the destination is obtained.

\vspace{-0.15in}
\begin{center}
\[delay = (h+2)\times ({\gamma} + \frac{{\lambda}_s({\gamma})^2}{2(1 - {\lambda}_s{\gamma})})
           +(h+1)\times (\frac{d+1}{2} + 
                  \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)})\]
\end{center}

\subsection*{Model verification}

To verify the analytical model and to further study the performance of these
logical topologies, a network simulator was developed that simulates all four 
logical topologies on top of the torus topology. 
The simulator takes the following parameters.

\begin{itemize}
\item  {\em system size}, $N\times N$: This specifies the size of the network. Based on 
the logical topology, the system size also determines the multiplexing degree
in the system.

\item {\em packet generation rate}, ${\lambda}$: This is the rate at 
which fresh packets are 
generated in each node. 
It specifies the traffic on the network. The inter--arrival 
of packets follows a Poisson distribution. 
When a packet is generated at a node,
the destination is generated randomly among all other nodes in the system
with a uniform distribution.

\item {\em Packet routing time}, ${\gamma}$.

\end{itemize}

Fig~\ref{twothroughput} shows the maximum throughput obtained from the 
analytical model and from simulations.  Both $8\times 8$ and 
$16\times 16$ physical torus networks with different packet routing time
are examined.  
As can be seen from the figure, the analytical results and the simulation
results almost have a perfect match for all cases.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/thro.64.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/thro.256.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{predicted and simulated maximum throughput}
\label{twothroughput}
\end{figure}


Figure~\ref{twodelay1} and Figure~\ref{twodelay2} 
show the average packet delays obtained from the analytical model and from
simulations. Here, the packet routing time, $\gamma$,
 is equal to 1 time slot.
For $8\times 8$ torus, the analytical model matches the
simulation results fairly well 
for all topologies except when the generation rate is close
to saturation. The difference between the 
results from the analytical model and 
those from simulations is around 10\%. 
For the $16\times 16$ physical topology, 
the analytical model matches the simulations results for 
the all--to--all, allXY
and hypercube topologies. For the torus topology, the difference 
 is about 20\% due to the 
approximation.  Studies using other values of $\gamma$ have 
also been conducted. The 
analytical model and the simulation results on those studies
match slightly better than those shown in Figures \ref{twodelay1} and 
\ref{twodelay2}. Thus, overall the analytical model gives a good indication of
the actual performance.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/alltoall64.1.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/alltoall256.1.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delays for logical all--to--all topology ($\gamma = 1$)}
\label{twodelay1}
\end{figure}

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/three64.1.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/three256.1.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delays for logical allXY, hypercube and torus 
         topologies ($\gamma = 1$)}
\label{twodelay2}
\end{figure}

\section{Performance of the logical topologies}

In the previous section, an analytical model for performance
study for the logical topologies was developed and verified.
This section focuses on studying 
the performance of the logical topologies.
Since the simulation and the analytical model match reasonably well,
only the analytical model is used in this section 
to study the performance. 

Figure~\ref{throughput1} shows the impact of packet routing time 
on the maximum throughput. 
The underlying topology is a $32\times 32$
torus. For all logical topologies, increasing the speed of routers
increases the maximum throughput up to a certain limit.
For the all--to--all topology, the
router speed of 1 packet per 4 time slots is sufficient to overcome the
router performance bottleneck. 
Using faster router will not 
further 
improve the maximum throughput. For the allXY and hypercube topologies, the 
threshold is 1 packet per 2 time slots, and for the
 torus topology, the threshold
is 1 packet per time slot. When the routing speed is faster than the
threshold value, the maximum throughput is bound by the link speed
and the maximum throughput will not increase along with the increase in
 router speed.
Table~\ref{throughput2} shows the bandwidth limits
of routers and links for $N = 32$.

Figure~\ref{throughput1} also shows that the all--to--all
topology achieves higher maximum throughput
than the allXY topology, which in turn
achieves higher maximum throughput than the hypercube topology. The logical
torus has the worst maximum throughput. This observation holds for all
packet routing speeds. Under high workload, all paths
in the all--to--all and allXY topologies are utilized. The algorithms
to realize the all--to--all and allXY topologies guarantee that in each time
slot all links are used if all connections scheduled for that time slot are
in use, while the hypercube and torus topologies can not achieve this effect. 
Thus, it is expected that the all--to--all topology and the allXY topology
will outperform the hypercube and torus topologies in terms of maximum
throughput.
%This result indicates that
%using time division multiplexing to establish more connections improves the 
%link utilization under high workload.
 
\begin{figure}[htbp]
\centerline{\psfig{figure=fig/throughput1.eps,width=5in}}
\caption{Maximum throughput .vs. packet routing time $(N= 32)$}
\label{throughput1}
\end{figure}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
topology & bottleneck &  ${\gamma} = 0.5$ & ${\gamma} = 1$ &  ${\gamma} = 2$& ${\gamma}=4$\\
\hline
    &  ${\lambda}^{max}_{s}$ &  2.0     & 1.0     &    0.5   & 0.25\\
\cline{2-6}
all--to--all & ${\lambda}^{max}_{p}$ & 0.25  & 0.25 & 0.25  & 0.25\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.25  & 0.25  & 0.25 & 0.25\\
\hline
    &  ${\lambda}^{max}_{s}$ &  1.36     & 0.68  & 0.34   & 0.17\\
\cline{2-6}
allXY & ${\lambda}^{max}_{p}$ & 0.25  & 0.25 & 0.25  & 0.25\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.25  & 0.25  & 0.25 & 0.17\\
\hline
    &  ${\lambda}^{max}_{s}$ &  0.67     & 0.33     &    0.17   & 0.09\\
\cline{2-6}
hypercube & ${\lambda}^{max}_{p}$ & 0.1  & 0.1 & 0.1  & 0.1\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.1  & 0.1  & 0.1 & 0.09\\
\hline
    &  ${\lambda}^{max}_{s}$ &  0.24     & 0.12     &    0.06   & 0.03\\
\cline{2-6}
torus & ${\lambda}^{max}_{p}$ & 0.06  & 0.06 & 0.06  & 0.06\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.06  & 0.06  & 0.06 & 0.03\\
\hline
\end{tabular}
\end{center}
\caption{Maximum throughput for the logical topologies on $32\times 32$ torus}
\label{throughput2}
\end{table}

Figure~\ref{throughput3} shows the impact of network size on the maximum
throughput. The results in this figure are based 
upon a  packet routing time of one time slot.
Different packet routing times were also studied and 
similar trends were found.
In terms of maximum throughput, the all--to--all topology scales the best, 
followed
by the allXY topology, followed by the hypercube topology. 
The logical torus topology
scales worst among all these topologies. Figures~\ref{throughput1}
and \ref{throughput3} show that by using time--division multiplexing
to establish complex logical topology, the large aggregate
bandwidth in the network can be exploited to deliver higher throughput when the network
is under high workload. 
%In term of maximum throughput, the performance
%of the logical topologies are ordered as follows.
%\centerline{$all--to--all > allXY > hypercube > torus$}


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/throughput2.eps,width=5in}}
\caption{Maximum throughput .vs. network size (${\gamma} = 1$)}
\label{throughput3}
\end{figure}

%From the study of maximum throughput of the logical topologies, 
%we draw the conclusion
%that the complex logical topologies, such as the all--to--all topology
%and  the allXY topology
%are more efficient to exploit the bandwidth in the network. 

Although the all--to--all topology is the 
best in terms of the maximum throughput,
it suffers from large packet delay when the network is  not saturated.
Packet delay
is another performance metric to be considered.
For a network to be efficient, 
it must also be able to deliver packets with a small
delay. 
It is well known that time--division multiplexing results in larger
average packet delay due to the sharing of the links.
However, as discussed earlier, while using time--division multiplexing 
techniques
to establish logical topologies increases the per hop transmission time,
it reduces the average number of hops that a packet travels. Thus, the overall
performance   depends on  system parameters. Next, 
this effect for the logical topologies is studied.

Figure~\ref{latency1} shows the delay with regard to the fresh
packet generation 
rate. The underlying topology is a $16\times 16$
torus and $\gamma$  is  1 time slot. 
The figure shows that the all--to--all topology incurs 
very large delay
compared  to other logical topologies.  This is because of the large 
multiplexing degree needed to realize the logical all--to--all topology. 
Other 
topologies have similar delay when the generation rate is small, that is,
under low 
workload. However, the allXY topology has a larger saturation point than
the hypercube and torus topologies  and thus
has a small delay even when the network load is reasonably high (e.g. 
$\lambda = 0.25$). These results also hold for larger packet
routing times.
 
\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}
{\psfig{figure=fig/ss1.0.eps,width=3in}}
\end{subfigure}
\begin{subfigure}
{\psfig{figure=fig/ss1b.0.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delay as a function of packet generation rate 
         $({\gamma} = 1.0, N=16)$}
\label{latency1}
\end{figure}

Figure~\ref{latency2} shows the impact of packet routing time on the 
average packet delay. The results are based upon a $16\times 16$ 
torus network and a packet generation rate of 0.005. 
The packet routing speed
has an impact on the delay for all topologies. For very small packet
routing time
(${\gamma} = 0.25$), the torus topology has the smallest delay.
When the packet routing time increases, the delay in torus increases 
drastically, 
while the delays 
in the all--to--all and allXY topologies increase
slightly. In the all--to--all and allXY topologies a packet
travels through fewer number of routers
 than it does in the torus topology. Hence the
contention at  routers does not affect the delay in the all--to--all and 
allXY topologies as much as it does in the torus and hypercube topologies.
This study also implies
that to achieve good packet delay for logical torus topology, 
fast routers are crucial, while a fast router
is not as important in the all--to--all
and allXY topologies.
 
\begin{figure}[htbp]
\centerline{\psfig{figure=fig/delay2.eps,width=5.5in}}
\caption{Impact of  packet routing time on packet delay 
         ($\lambda = 0.005, N=16$)}
\label{latency2}
\end{figure}

Figure~\ref{latency3} shows the impact of network size on the packet delay
for the topologies. The results are based upon a packet routing time of
1 time slot and a packet generation rate of 0.01.
This figure shows the manner  in which the delay time grows with regard
to the network size. As discussed in section 2, ignoring network contention
for a physical $N\times N$
torus, the all--to--all topology results in
a packet delay of $O(\gamma+N^3)$, 
 the allXY topology has a delay of $O(\gamma +N^2)$,
hypercube has a delay of $O(\gamma lg(N) + Nlg(N))$, and torus has 
a delay of $O(\gamma N)$.
Thus, the all--to--all topology has  very large delay when the network size
is large. The delay differences among the other
three topologies are relatively 
small for reasonably large sized networks. When the packet
routing time is small (${\gamma}= 1.0$), the hypercube topology 
scales slightly 
better than the torus and the allXY topologies as shown in
Figure~\ref{latency3}~(a). When ${\gamma}$ is large (${\gamma}=4.0$), 
the hypercube topology and the allXY topology
are better than the other two topologies as shown in 
Figure~\ref{latency3}~(b). 
%We should note that the figure plots delays for network sizes 
%up to 1024 nodes. As the network size grows larger (given a certain
%${\gamma}$) the torus would have the best delay, since its asymptotic
%delay grows in the  order of  
%$O(N)$ which is the best among all these topologies. While theoretically
%the logical torus should scales better than other topologies,  
%realizing the logical hypercube
%and allXY topology is usually better than the logical torus for a network
%of practical size.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[${\gamma} = 1.0$]
{\psfig{figure=fig/delay3.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[${\gamma}= 4.0$]
{\psfig{figure=fig/delay3b.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{impact of network size on the delay ($\lambda = 0.01$)}
\label{latency3}
\end{figure}

From the above discussions, three parameters, $N$, $\gamma$ and 
$\lambda$ affect the average packet delay for all the logical topologies. 
Next, 
the regions in the $(N, \gamma, \lambda)$ parameter space, where a logical
topology has the lowest packet delay are identified. 
Figure~\ref{best1} shows the best topologies in 
the parameter space $(N, \gamma)$ with fixed $\lambda$.
Comparing Figure~\ref{BEST}, where the network contention
is ignored, with Figure~\ref{best1}, where the contention is taken into 
consideration, it  can be seen
 that the logical topologies with less connectivity
suffer more from network contention. As can be seen from 
Figure~\ref{best1}~(a), with small packet generation rate, 
all four logical topologies occupy part of the 
$(N, \gamma)$ parameter space, which indicates that under certain 
conditions, each of the four topologies out--performs the other three
topologies. While in the case of large packet generation rate as shown in 
Figure~\ref{best1}~(b), the logical torus topology is pushed out 
of the best topology picture.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[${\lambda} = 0.01$]
{\psfig{figure=fig/range1.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[${\lambda}= 0.06$]
{\psfig{figure=fig/range2.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Best logical topology for a given packet generation rate}
\label{best1}
\end{figure}

Figure~\ref{best2} shows the best logical topologies on the 
$(\gamma, \lambda)$ parameter space. Here, the underlying network
is a $16\times 16$ torus. Networks of different size exhibit similar
characteristics. The majority of the $(\gamma, \lambda)$ parameter space
is occupied by the logical hypercube and allXY topologies. 
The logical torus topology is good only when the 
$\lambda$ is small and $\gamma$ is small. The logical all--to--all 
topology out--performs other topologies only when the network is almost
saturated, that is, large $\lambda$ or large $\gamma$. This indicates
that in general, 
the logical hypercube and allXY topologies are better topologies 
than the logical torus and all--to--all topologies in terms of packet delay. 

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range4.eps,width=4in}}
\caption{Best logical topology for a $16\times 16$ torus}
\label{best2}
\end{figure}

Figure~\ref{best3} compares the performance of the logical
hypercube and allXY topologies.  Given a fixed $\gamma$, there
is a packet generation rate, $\lambda$, above which the 
allXY topology out--performs the logical hypercube topology.
When  $\gamma$ increases, the line in the figure moves down.
In other words, the hypercube topology is more sensitive to the
packet routing time $\gamma$.


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range3.eps,width=4in}}
\caption{Best logical topology for a given packet routing 
time ($\gamma = 1.0$)}
\label{best3}
\end{figure}


\section{Multi--hop communication vs single--hop communication}
\label{multisingle}

Previous sections considered the logical topologies that can be used 
to route packets and perform {\em multi--hop} communications.
As discussed in Chapter 3, another way to perform dynamic 
communication on multiplexed optical networks is to use a path 
reservation algorithm which reserves an optical path
from the source to the destination 
and then perform {\em single--hop} communications. 
The performance of these two communication schemes 
on a physical $16\times 16$ torus is compared in this
section. The logical allXY topology is used as the
logical topology for multi--hop communication since it offers large maximum
throughput and reasonably small average package delay for this 
size of networks. To obtain a fair comparison, the following 
assumptions are made:

\begin{itemize}
\item Both networks have the same multiplexing degree. For a $16\times 16$ 
torus, this means that both networks have a multiplexing degree of 32, which
is required for the logical allXY topology.
\item The data packet processing time in the multi--hop communication is 
equal to the control packet processing time in the path reservation
algorithm, since electronic processing is involved in both cases.
{\em Packet processing time}, $\gamma$, is used
 to represent both the data packet processing
time in the multi--hop communication and the control packet processing time
in the single--hop communication.
\item Control packet propagation time between two neighboring nodes 
is equal to data packet propagation time between the source and the 
destination, which is equal to 1 time slot.
\item It is assumed that a data {\em message} contains $s$ packets. 
Accordingly, the
{\em average message delay}, which is defined as the difference between the 
time the message is generated and the time when the whole message is received,
is measured 
instead of the {\em average packet delay}. The notation $\lambda_{msg}$ in 
this section represents the message generation rate per node per time slot. 
Since messages can be of different sizes, the {\em network load} is defined
to be\\
\centerline{$network\ load = s \times \lambda_{msg} 
            \times number\ of\ nodes,$} 
which is equal to the total number of packets injected into the network.
For the same reason, the throughput is measured
in terms of  packets delivered per time slot. 
\end{itemize}

The analytical model for the multi--hop communication 
cannot model the communication performance when packets in a message 
are sent to the same destination
since  destinations of packets are no longer distributed
uniformly among all nodes.
In some sense the number of packets in a message reflects the locality
of the communication traffic. All results in this section are obtained through
simulations. 


%\begin{figure}[htbp]
%\begin{subfigRow*}
%\begin{subfigure}[$throughput$]
%{\psfig{figure=fig/comp1b.eps,width=3in}}
%\end{subfigure}
%\begin{subfigure}[$delay$]
%{\psfig{figure=fig/comp1a.eps,width=3in}}
%\end{subfigure}
%\end{subfigRow*}
%\caption{Throughput and delay for message of size 8 ($\gamma = 1$)}
%\label{comp1}
%\end{figure}


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/allthro.eps,width=5.5in}}
\caption{Maximum throughput}
\label{comp1}
\end{figure}

Figure~\ref{comp1} shows the maximum throughput of the two schemes
with different message sizes, $s$, and packet processing times, $\gamma$. 
The packet processing time affects both the single--hop communication and the 
multi--hop communication, while the message size affects only single--hop
communication (larger message size leads to higher maximum throughput).
When the packet processing speed is fast, e.g.
$\gamma = 1$, such that the path bandwidth is the bottleneck in the 
communication, the multi--hop communication offers larger maximum throughput
than the single--hop communication. The reason is that multi--hop
communication utilizes the links in the network more efficiently 
when the network is saturated and 
does not incur additional control overhead. However, the multi--hop 
communication is more  sensitive to the packet processing time 
and the maximum throughput of the 
multi--hop communication decreases drastically when the packet 
processing time increases. In the single--hop communication,
the packet processing is only involved in the control network, thus, 
preserving the large bandwidth in the data network when the packet
processing time is large. This effect manifests itself when the message size 
is reasonably large and the extra control overhead is amortized over the
length of a message. Thus, 
the single--hop communication offers larger maximum throughput when
the packet processing time is large and the message size is sufficiently
large.
%In the multi--hop communication, message size does not affect the maximum
%throughput too much, while in the single--hop communication,
%large maximum throughput can only be achieved for large message sizes.
%This also implies that the multi--hop communication performs much 
%better then the single--hop communication when the message size is small.
Figures \ref{comp1a}~(a) and \ref{comp1a}~(b) show the maximum throughput 
with different message sizes for packet processing times of 1 and 4 
respectively. As can be seen from the figures, when the packet 
processing time is small ($\gamma = 1$), the multi--hop communication offers
larger maximum throughput for all message sizes. When the packet 
processing time is large ($\gamma = 4$), the single--hop communication
has a larger maximum throughput when the message size is sufficiently large.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[$\gamma = 1$]
{\psfig{figure=fig/comp4b.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[$\gamma = 4$]
{\psfig{figure=fig/comp4b.n.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Maximum throughput for different message sizes}
\label{comp1a}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/multisize.eps,width=5.5in}}
\caption{Impact of message size on the average message delay ($\gamma = 1$)}
\label{delay1}
\end{figure}

When the network is under light load, it is more meaningful to compare
the message delay. 
Figure~\ref{delay1} shows the  impact of the network load and the message size 
on the average message delay. In this figure,  $\gamma = 1$.
When the message size is small ($size = 4$), the multi--hop communication
has smaller message delay. When the message size is large ($size = 64$), the
single--hop communication offers smaller message delay. In both cases, 
the large network load amplifies the difference between single--hop and 
multi--hop communications. For  messages of medium size
($size = 16$), the multi--hop communication has smaller delay when the network
load is below a certain point. In general, small messages favor the multi--hop
communication while large messages  favor the  single--hop communication.


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/routing.eps,width=5.5in}}
\caption{Impact of packet processing time 
         on the average message delay ($\gamma = 1$)}
\label{delay2}
\end{figure}

The packet processing time affects the average message delay for both 
the single--hop communication and the multi--hop 
communication. In the single--hop 
communication, the packet processing time affects the path reservation
time only. Thus, given a fixed packet processing time, the extra
control overhead is almost the same for all message sizes. In 
the multi--hop communication, the extra overhead applies to each packet in a 
message, and thus the larger the message size, the larger the overhead. 
Figure~\ref{delay2} shows the impact of the packet routing time on the 
average packet delay. In this figure,  
the same network load of 10.24 is considered for different message sizes
(e.g. a generation rate of 0.01 for messages of size 4, 
$0.01\times 4\times 256 = 10.24$) with different message sizes. 
As can be seen from the figure, when the 
message size is small, the single--hop communication incurs larger 
message delay while for large message sizes, the multi--hop 
communication incurs larger message delay. 
The large packet processing time amplifies these effects. 

\section{Chapter summary}

This chapter considered the logical topologies for routing message
on top of torus topologies. Schemes for realizing the logical
torus, hypercube, allXY (where all--to--all connections along each dimension
are established) and all--to--all topologies on top of physical torus
networks were discussed. Optimal schemes for realizing hypercube
on top of physical arrays and rings were designed. Schemes that use
at most 2 more channels than the optimal for realizing 
hypercube on top of meshes and tori were presented.

An analytical model for the maximum throughput and the packet latency 
for multi--hop networks was developed and verified through simulations.
This analytical model was used to study the performance of the logical
topologies and to identify the cases where each logical topology 
out--performs the other topologies. 
In general, the performance of the logical
topologies with less connectivity, such as the torus and 
hypercube topologies,
are more sensitive to the network load and the router speed while
the logical topologies with more connectivity, such as the all--to--all and 
allXY topologies, are more sensitive to network size.
Logical topologies with dense connectivity achieve higher
maximum throughput than the topologies with less connectivity.
In addition, they also scale better 
with regard to the network size. In terms of the maximum throughput, 
the topologies can be ordered as follows:

\centerline{{\em all--to--all} $ > allXY > hypercube > torus$.}

In term of the average packet delay, the logical torus topology achieves best 
results only when the router is fast and the network is under light load,
while the logical all--to--all topology is best only when the router
is slow and the network is almost saturated. In all other cases, logical
hypercube and allXY topologies out--perform logical torus and 
all--to--all topologies. Comparing the logical allXY to the logical hypercube,
the allXY topology is better when the network is under high load. 
These results hold for all network sizes.

This chapter further compared  multi--hop communication with 
single--hop communication and identified the advantages and the limitations
of each communication scheme. The study in this chapter used randomly
generated communication traffic. Performance evaluation of these two 
schemes using communication patterns from real application programs, which 
confirms the results in this chapter, will be presented in Chapter 6.
Multi--hop communication is more efficient than 
single--hop communication in terms of maximum throughput when the packet 
processing speed is not a bottleneck in the system and 
when the message size is small.  When packet processing speed is slow, 
the single--hop  communication has higher maximum throughput when the 
message size is sufficiently large. In terms of the average message delay
when the network is under light load, large messages favor single--hop 
communication, while small messages favor multi--hop communication. 
The large packet processing time amplifies these effects.
Table~\ref{summary1} and Table~\ref{summary2} summarize these conclusions.


\begin{table} [htbp]
\begin{center}
\small
\caption{Maximum throughput on a $16\times 16$ torus}
\label{summary1}

\begin{tabular}{|c|c|c|}
\hline
       &  Small message size($4$) & Large message size($64$)\\
\hline
Small packet processing time & Multi--hop & Multi--hop\\
\hline 
Large packet processing time & Multi--hop & Single--hop\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table} [htbp]
\begin{center}
\small
\caption{Average message delay on a $16\times 16$ torus}
\label{summary2}

\begin{tabular}{|c|c|c|c|c|}
\hline
Network  & Packet processing   & \multicolumn{3}{c|}{Message size}\\
\cline{3-5}
load & time & Small(4)  & Medium(16) & Large(64)\\
\hline
Small & Small & Multi--hop & Multi--hop & Single--hop\\
\cline{2-5}
                    &  Large  & Multi--hop & Single--hop & Single--hop \\
\hline 
Large & Small & Multi--hop & Single--hop & Single--hop\\
\cline{2-5}
                    &  Large & Multi--hop & Single--hop & Single--hop\\
\hline
\end{tabular}
\end{center}
\end{table}

Both communication schemes suffer from the bottleneck of electronic processing,
which occurs in the path reservation in single--hop communication and
in the packet routing at intermediate nodes in multi--hop communication. 
Using the compiled communication technique discussed in the next chapter, 
this bottleneck can be removed. 

 



\chapter{Performance comparison}

This chapter evaluates the relative performance of the three
communication schemes presented in Chapters 3, 4, and 5
using real application programs. Three
sets of programs are used in the evaluation. The first set of programs
includes three hand--coded parallel programs, where communications 
are well defined and highly optimized for parallel execution. 
The second set of programs includes a number of HPF benchmark programs which
are tuned for parallel execution.
% and are concerned about
% the ease of programming. 
The third set of programs
includes a number of programs from SPEC95 which are not optimized for parallel
execution. 

The performance measurement is the communication time 
in the unit of time slots. A packet, 
which contains a number of words,
can be transmitted through a lightpath in a time slot.
In addition, {\em normalized time} is also used to compare the 
performance of the schemes. 
In normalized time, the best communication time
among all schemes is assigned a value of $1.0$
and communication times of all schemes are normalized with respect to 
the best communication time. The normalized time  shows
the best scheme for each program and how other schemes perform compared
to the best scheme. It is assumed that the  communication in  each pattern 
is performed in a synchronized manner. That is, the program synchronizes
before and after each communication pattern and thus no interleaving of 
communications and computations is allowed.  

Because the E--SUIF compiler does not handle the message passing paradigm,
the first set of experiment is carried out manually by extracting 
the communication patterns in the programs by hand.
The programs in the second and third sets are 
generated automatically by the E--SUIF compiler for the experiment. 
As discussed in Chapter 5,
the E--SUIF compiler first analyzes and optimizes the communications in a 
program and represents the communications using Section Communication 
Descriptors (SCDs). It then performs the communication phase analysis
and partitions the program into phases and schedules the communication 
pattern within each phase. Finally, the backend of the E--SUIF compiler
generates a library call, $lib\_comm$, for each SCD and another library
call, $lib\_phase$ for each phase. The $lib\_comm$ takes a SCD with all
runtime information as 
parameter. When the program is 
executed, the $lib\_comm$ procedure invokes a network 
simulator which simulates dynamic single--hop communication, 
dynamic multi--hop communication or compiled communication to obtain 
the communication time of the communication 
using one of the three communication schemes. 
The $lib\_phase$ is useful only when simulating compiled 
communication. It accesses to the 
communication requirement of each phase (that is obtained by the compiler),
and performs channel assignment for connections within each phase. Thus,
the communication performance of a program is obtained by running the
program generated by the E--SUIF compiler. 

The experiments use the following system settings.

\begin{itemize}
\item Physical network: $8\times 8$ torus.
\item Packet size: 4 words.
\item Routing algorithm: 
XY routing between dimensions and Odd--Even shortest--path routing within
each dimension.
\item Dynamic single--hop communication.
  \begin{itemize}
  \item Control protocol: Conservative backward reservation protocols ($cset$
        size is 1). As discussed in Chapter 3, the conservative backward
        reservation protocol almost has the best performance among all the
        path reservation protocols.  
  \item Control packet processing time: 1 time slot.
  \item Control packet propagation time: 1 time slot.
  \item Maximum control packet retransmission time: 5 time slot.
  \item Multiplexing degree: 1, 4, 14, 20.
  \end{itemize}
\item Dynamic multi--hop communication. 
  \begin{itemize}
  \item Logical topologies: torus, hypercube, allXY and all--to--all.
  \item packet switching time: 1 time slot.
  \end{itemize}
\item Compiled communication.
  \begin{itemize}
  \item Connection scheduling algorithms: combined algorithm for the first
        set of experiment, AAPC algorithm for the second and third 
        experiments.
  \end{itemize}
\end{itemize}  

\section{Hand--coded parallel programs}

This set of program includes three
hand--coded parallel programs, namely $GS$, $TSCF$ and $P3M$.
The $GS$ program uses Gauss--Siedel
iterations to solve Laplace equation on a discretized unit 
square with Dirichlet boundary conditions. It contains 
a nearest neighbor communication pattern with fairly large message size 
(64 packets messages).
The $TSCF$ program simulates the evolution of a
self--gravitating system using a self consistent field approach. It 
contains a hypercube communication pattern with small message 
size (1 packet message). $P3M$ performs particle--particle particle--mesh 
simulation \cite{Yuan97c}. This program contains five static communication
patterns.  Table~\ref{PATT}
describes the static communication patterns that arise in these programs.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Pattern & Type & Description\\
\hline
GS   & shared array ref. & PEs are logically linear array, Each PE \\
      &     & communicates with two PEs adjacent to it.\\
\hline
TSCF   & explicit send/recv & hypercube pattern\\
\hline
P3M  1 & data redistrib. & (:block, :block, :block) $\rightarrow$ (:, :, :block)\\
\cline{2-3}
P3M 2 & data redistrib. & (:, :, :block) $\rightarrow$ (:block, :block, :)\\
\cline{2-3}
P3M 3 & data redistrib. & (:block, :block, :) $\rightarrow$ (:, :, :block)\\
\cline{2-3}
P3M 4 & data redistrib. & (:, :, :block) $\rightarrow$ (:block, :block, :block)\\
\cline{2-3}
P3M 5 & shared array ref. & PEs are logically 3--D array, each PE\\
      &                   & communicates with 6 PEs surrounding it\\
\hline
\end{tabular}
\caption{Communication pattern description.}
\label{PATT}
\end{center}
\end{table}

Table~\ref{TP3M} shows the communication
time for these patterns in one main loop step in the programs. 
Table~\ref{TP3MN} shows the normalized time where
the best communication time is normalized to $1.0$.
In this experiment, it is assumed that there is sufficient multiplexing 
degree to support all the patterns in compiled communication. 
Thus, each phase contains one communication pattern and no
network reconfiguration is required to within each pattern. 
For dynamic single--hop communication, the communication time for
fixed multiplexing degrees of 1, 4, 14 and 20 is evaluated. 
For dynamic multi--hop
communication, the logical  torus, hypercube, allXY and
all--to--all topologies are considered. 
The following observations can be made from the results in Table~\ref{TP3MN}.

\begin{itemize}
\item Compiled communication out--performs
dynamic single--hop communication in all cases. The average
communication time for dynamic single--hop communication 
is 4.5 to 8.0 times greater than that
for compiled communication, depending on the multiplexing degree used
in dynamic single--hop communication.
Larger performance gains are observed for communications with
small message sizes (e.g., the $TSCF$ pattern) and dense communication 
(e.g., the $P3M\ 2$ pattern). Large multiplexing degree does not 
always improve the communication
performance for dynamic single--hop communication. For example, 
a multiplexing 
degree of 1 results in the best performance (for dynamic single--hop
communication) for the pattern in GS while a
degree of 14 has the best performance for the $P3M\ 5$ pattern.
\item Compiled communication out--performs  
dynamic multi--hop communication in all cases except for 
the $TSCF$ program where dynamic multi--hop communication has better 
communication time when using the logical hypercube topology. 
The reason is that
the $TSCF$ program only contains hypercube communication with message size 
equal to 1. Multi--hop communication 
achieves good communication performance when communication patterns in
a program matches the logical topology. However, on average, the
communication time for multi--hop communication is 3.0 to 7.6 times
larger than the communication time for compiled communication, depending 
on the logical topology used. 
\item Compiled communication achieves an 
average normalized time of 1.1 for all the communication patterns, which
indicates that compiled communication almost delivers optimal 
communication performance.
\item Comparing dynamic  multi--hop
communication with dynamic single--hop communication,
multi--hop communication has better performance
when the message size is small (e.g. $TSCF$, $P3M\ 5$), 
and when the communication requires dense connections 
(e.g. $P3M\ 2,3$), while
single--hop communication is better when the message size is large
(e.g. $GS$).
\end{itemize}

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & GS  & TSCF & P3M 1& P3M 2,3 & P3M 4& P3M 5\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 131 & 19 & 831 & 382 & 457 & 40 \\
\hline
     & torus & 404 & 30 & 3366 & 1656 &1632 & 127\\
%\cline{2-8}
Multihop & hypercube & 792 & 13 & 3371& 1338 &1499 & 74\\
%\cline{2-8} 
comm. & allXY & 990  & 17 & 3157 & 1058 &960 &121\\
%\cline{2-8}
      & alltoall & 4159 & 70 & 1326 &749 &1326 & 276\\
\hline
      & $d=1$  & 209 & 215 & 3194& 6655 &2091 & 378\\
Single--hop & $d=4$&  296 & 118 &2029 &2998 &1302 & 213\\
comm &        $d=14$ & 924 & 107 & 1713 &2171 &1508 & 196\\
  &          $d=20$ & 1296 & 108 &1702 & 2096 &1314 & 231\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Communication time (timeslots) for the hand--coded programs}
\label{TP3M}
\end{table} 

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & GS  & TSCF & P3M 1& P3M 2,3 & P3M 4& P3M5 & Average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 1.0 & 1.5 & 1.0 & 1.0 & 1.0 & 1.0 & 1.1 \\
\hline
     & torus & 3.1 & 2.3 & 4.1 & 4.3 &3.6 & 3.2 & 3.4\\
%\cline{2-8}
Multihop & hypercube & 6.0 & 1.0 & 4.1& 3.5 &3.3 & 1.9 & 3.3\\
%\cline{2-8} 
comm. & allXY & 7.6  & 1.3 & 3.8 & 2.8 &2.1 &3.0 & 3.4\\
%\cline{2-8}
      & alltoall & 31.7 & 5.4 & 1.6 &2.0 &2.9 & 6.9 & 8.4\\
\hline
      & $d=1$  & 1.6 & 16.5 & 3.9 &17.4 & 4.6 & 9.5 & 8.9\\
Single--hop & $d=4$&  2.3 & 9.1 &2.4 &7.8 &2.8 & 5.4 & 5.0\\
comm &        $d=14$ & 7.1 & 8.2 & 2.1 &5.7 &3.3 & 4.9 & 5.2\\
  &          $d=20$ & 9.9 & 8.2 &2.0 & 5.5 &2.9 & 5.8 & 5.7\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Normalized communication time for the hand--coded programs}
\label{TP3MN}
\end{table} 

  
In this study, two types of communication patterns are observed in 
a well designed parallel program, 
fine grain communications resulted from shared array references
and coarse grain communications resulted from data redistributions.
The fine grain communication  causes sparse connections with 
small message sizes, while the coarse grain communication results in 
dense connections with large message size.
For a communication system to efficiently support the 
fine grain communication,
the system must have small latency. Optical single--hop networks that use 
dynamic path reservation algorithms have a large 
startup overhead, thus cannot support this type of communication
efficiently. As shown in our simulation results, 
compiled communication where the startup overhead is eliminated
and dynamic multi--hop communication perform this type of 
communications efficiently.
For the coarse grain communication, the control overhead in the dynamic
communications  is not significant.
However, dense communication
results in a large number of conflicts in the system (path reservation in 
dynamic single--hop communication and packet routing in dynamic 
multi--hop communication), and the dynamic
control systems are not able to resolve these conflicts efficiently.
By using an off--line connection scheduling algorithm, compiled 
communication handles this type of communications efficiently.
The performance study confirms the conclusion in 
\cite{Hinrichs94} that static management of the dense
communication patterns results in large performance gains. 

%Compiled communication achieves high performance for 
%all types of static patterns in this set of experiments.
%Four factors contribute to the performance gain. First, 
%compiled communication eliminates dynamic control overhead. This is
%most significant for communication with small message sizes, where
%the overhead in the dynamic single--hop communication  is large 
%compared to the communication time.
%Second, compiled communication takes the whole 
%communication pattern into consideration, while dynamic communication, which
%considers the connection requests one by one, suffers from the 
%head--of--line effect \cite{Sivalingam93}.
%Third, the off--line message scheduling algorithm further
%optimizes the communication efficiency for compiled communication.
%Fourth, compiled communication allows the system to adapt to the
%communication requirement in a program. In dynamic single--hop system, 
%communication, control mechanism with variable multiplexing
%degree is very difficult to implement and
%results in large overhead, while in dynamic multi--hop systems, the system
%cannot choose an efficient logical topology without knowing the communication
%requirement of a program. Thus, an optical network with 
%dynamic control may provide good performance for some applications whose
%communication requirement matches the network capacity, but it will 
%not have good performance for other applications. 
%The compiler has the exact knowledge
%of the communication pattern in the three programs and thus, the compiled
%communication offers high performance. In the next set of experiments, I will
%consider some HPF benchmark programs where the compiler may not have the
%exact knowledge of the communication performance and study how much the
%communication performance for the compiled communication 
%is affected by the approximation in the compiler analysis. 

\section{HPF parallel benchmarks}

%\begin{table}
%\small
%\footnotesize
%\begin{center}
%\begin{tabular} {|c|c|}
%\hline
%benchmarks & description \\
%\hline
%0001 & Solution of 2-D Poisson Equation by ADI\\
%\hline
%0003 & 2-D Fast Fourier Transform \\
%\hline
%0004 & NAS EP Benchmark - Tabulation of Random Numbers \\
%\hline
%0008 & 2-D Convolution\\
%\hline
%0009 & Accept/Reject for Gaussian Random Number Generation \\
%\hline
%0011 & Spanning Percolation Cluster Generation in 2-D \\
%\hline
%0013 & 2-D Potts Model Simulation using Metropolis Heatbath\\
%\hline
%0014 & 2-D Binary Phase Quenching of Cahn Hilliard Cook Equation \\
%\hline
%0022 & Gaussian Elimination - NPAC Benchmark \\
%\hline
%0025 & N-Body Force Calculation - NPAC Benchmark  \\
%\hline
%0039 & Segmented Bitonic Sort \\
%\hline
%0041 & Wavelet Image Processing \\
%\hline
%0053 & Hopfield Neural Network \\
%\hline
%\end{tabular}
%\end{center}
%\caption{HPF Benchmarks and their descriptions}
%\label{desc1}
%\end{table}

This set of programs is from the Syracuse University HPF benchmark suite.
The benchmarks and their descriptions are 
listed in table~\ref{desc} in Section~\ref{evalphase} .
% The data 
%distributions are obtained from the original benchmark programs and are
%thus, optimized for the programs. 
The benchmarks include many different 
types of applications, however, all of the programs contain only 
regular computations.

The major difference between this experiment and the first
experiment is that, in this experiment,  compiled
communication is applied to the whole program instead of each individual
communication pattern. Assuming a multiplexing degree of 10,
the compiler tries to aggregate as many communications as possible into a 
phase  as opposed to the first experiment where 
compiled communication is assumed to have an infinite number of 
virtual channels  to handle each individual communication pattern 
in the programs. In addition, this set of programs contains 
communication patterns about which
the compiler cannot obtain precise information.
Two factors may degrade the performance of compiled communication.
First, compiler approximations may result in the waste of bandwidth for
establishing connections that are not used. Second, aggregating more 
communications in a phase reduces the number of network 
reconfigurations, but may result in larger communication time since
larger multiplexing degree is needed for more communications. This experiment
aims at studying the performance of compiled communication under these
limitations.

Table~\ref{HPFperformance} shows the communication time of the
programs using different communication schemes. 
Table~\ref{HPFperformanceN} shows the normalized time.
Even with the limitations discussed earlier, compiled communication in general 
out--performs dynamic communications to a large degree. 
The benefits of managing channels at compile time and 
eliminating the runtime path reservation overhead over--weights
the bandwidth losses through the imprecision of compiler analysis.
The average normalized time for compiled communication is 1.1 which
indicates that compiled communication almost delivers the best communication
performance for this set of programs. However, performance degradation
in compiled communication due to the conservative
approximation in compiler analysis is observed in some of the programs.
For example, compiler over--estimating the communication requirement 
is found in benchmarks 0009 and 0022. 
Note that the overall communication time for
the programs in Table~\ref{HPFperformance} may not show this, because each
program contains many communication patterns and the pattern that
is approximated may not dominate the overall communication time.
The performance loss due to aggregating communications, which results 
in larger multiplexing degree, is observed in benchmark 0025.
%It is desirable to develop more advanced communication phase analysis 
%techniques that can use different multiplexing degrees for different parts 
%of a program to achieve  best performance. 
Nonetheless, the 
overall trend of this experiment is very similar to that in 
the first experiment.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0001  & 0003 & 0004& 0008 & 0009 & 0011\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 45,624 &752  &1,368 &2,256 &2,394 & 105,252 \\
\hline
     & torus &197,760  &3,296  &1,776  &9,888  &3,108 & 158,594\\
%\cline{2-8}
Multihop & hypercube &159,840  &2,664  &1,032 &7,992  &1,806 & 147,496\\
%\cline{2-8} 
comm. & allXY &125,280&2,088  &1,704  & 6,439  &2,982 & 265,636\\
%\cline{2-8}
      & alltoall &87,960  &1,466  &4,944  & 4,398 &8,652 & 1,027,818\\
\hline
      & $d=1$  &888,240  &14,804  &1,920 &44,412 &3,360 & 141,052\\
Single--hop & $d=4$&357,600 &5,960  &2,208 &17,880 & 3,864&181,506 \\
comm &        $d=14$ &267,360  &4,456  &3,504 &13,368 &6,132 &372,678 \\
  &          $d=20$ &273,360 &4,556  &4,224 &13,668 &7,392 &484,374 \\
\hline
\end{tabular}

\vspace{0.5in}

\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0013  & 0014 & 0022& 0025 & 0039 & 0041\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &166,280 &63,400 &3,244,819 &29,854 &
68,704 &1,504  \\
\hline
     & torus &257,980  &129,800 &6,382,683  &25,470  & 106,525 &6,592 \\
%\cline{2-8}
Multihop & hypercube &363,340  &200,600  &9,509,070 &58,661  & 132,348 &5,328 \\
%\cline{2-8} 
comm. & allXY &748,220 &379,200  &5,922,920  & 63,264  &135,353 &4,176 \\
%\cline{2-8}
      & alltoall &3,368,600  &1,679,200  &6,379,275  &214,343  &393,166 &2,932 \\
\hline
      & $d=1$  &154,080  &71,200  &6,844,054 &23,440 &115,488 &29,608 \\
Single--hop & $d=4$&256,240 &125,200  &6,402,631 &31,221 &136,390 &11,920 \\
comm &        $d=14$ &779,920  &391,200  &6,516,485 &61,712 &214,042 &8,912 \\
  &          $d=20$ &1,086,160 &550,800  &6,925,278 &81,958 &261,832 &9,112 \\
\hline
\end{tabular}

\end{center}
\normalsize
\caption{Communication time for the HPF benchmarks.}
\label{HPFperformance}
\end{table} 


\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0001  & 0003 & 0004& 0008 & 0009 & 0011\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 1.0 &1.0  &1.3 &1.0 &1.3 & 1.0 \\
\hline
     & torus &4.3  &4.4  &1.7  &4.4  &1.7 & 1.5\\
%\cline{2-8}
Multihop & hypercube &3.5  &3.5  &1.0 &3.5  &1.0 & 1.4\\
%\cline{2-8} 
comm. & allXY &2.7&2.3  &1.7  & 2.9  &1.7 & 2.5\\
%\cline{2-8}
      & alltoall &1.9  &1.9  &4.8  & 2.0 &4.8 & 9.8\\
\hline
      & $d=1$  &19.3  &19.7  &1.9 &19.7 &1.9 & 1.3\\
Single--hop & $d=4$&7.8 &7.9  &2.1 &7.9 & 2.1&1.7 \\
comm &        $d=14$ &5.8  &5.9 &3.4 &5.9 &3.4 &3.5 \\
  &          $d=20$ &5.9 &6.0  &4.1 &6.0 &4.1 &4.6 \\
\hline
\end{tabular}

\vspace{0.5in}

\begin{tabular}{|c|c|r|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0013  & 0014 & 0022& 0025 & 0039 & 0041 &average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1.1 &1.0 &1.0 &1.3 &1.0 &1.0&1.1  \\
\hline
     & torus &1.7  &2.1 &2.0  &1.1  & 1.6 & 4.4 & 2.6 \\
%\cline{2-8}
Multihop & hypercube &2.4  &3.2  &2.9 &2.5  & 1.9 &3.5 & 2.5 \\
%\cline{2-8} 
comm. & allXY &4.9 &6.0  &1.8  & 2.7  &2.0 &2.8 & 2.8 \\
%\cline{2-}
      & alltoall &21.9  &26.7  &2.0  &9.1  &5.7 &1.9 & 7.6 \\
\hline
      & $d=1$  &1.0  &1.1  &2.1 &1.0 &1.7 &19.7 & 7.5 \\
Single--hop & $d=4$&1.7 &1.9  &2.0 &1.3 &2.0 &7.9 & 3.9 \\
comm &        $d=14$ &5.1  &6.2  &2.0 &2.7 &3.1 &5.9 & 4.4 \\
  &          $d=20$ &7.1 &8.7  &2.1 &3.6 &3.8 &6.1 & 5.2 \\
\hline
\end{tabular}

\end{center}
\normalsize
\caption{Normalized time for the HPF benchmarks.}
\label{HPFperformanceN}
\end{table} 

\section{Programs from SPEC95}

Four programs, ARTDIF (from HYDRO2D), TOMCATV, SWIM and 
ERHS (from APPLU) are used in this experiment.
These programs are also used in Section~\ref{evalanalyzer}, where
the descriptions of these programs can be found,  to
evaluate performance of the communication analyzer 
in the E--SUIF compiler.

%This set of programs includes 4 programs. 
%The first benchmark, ARTDIF, 
%is a kernel routine obtained from the HYDRO2D program, 
%which is an astrophysical program for the computation of galactical jets
%using hydrodynamical Navier Stokes equations. 
%The second benchmark, TOMCATV, does the mesh generation with 
%Thompson's solver. 
%The third program, SWIM, 
%is the SHALLOW weather prediction program.
%The fourth program, ERHS, is part of the
%APPLU program, which is the solver for five coupled 
%parabolic/elliptic partial differential equations. The programs, HYDRO2D,
%TOMCATV, SWIM and APPLU, are sequential programs 
%originally from the SPEC95 benchmark suite.
%The purpose of this experiment is to study the communication performance of
%the three communication mechanisms for 
%automatically parallelized programs. 



\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & ARTDIF  & TOMCATV & SWIM & ERHS\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1,224 &15,480 &2,708 &6,689 \\
\hline
     & torus &2,724  & 34,260 & 1,378  &4,380 \\
%\cline{2-8}
Multihop & hypercube &4,338 &57,240  &2,309 &6,482\\
%\cline{2-8} 
comm. & allXY &8,583  &108,900 &4,409 & 15,117\\
%\cline{2-8}
      & alltoall &38,772 &491,460  &19,169   &68,800\\
\hline
      & $d=1$  &666 &8,280  & 669 &  1,148\\
Single--hop & $d=4$&2,478  &31,260  &1,574  &4,382\\
comm &        $d=14$ &8,538 &108,060 &4,511  & 15,166\\
  &          $d=20$ &12,168 &154,140  &6,343  &21,614\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Communication time for SPEC95 benchmark programs.}
\label{SPEC95performance}
\end{table} 



\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & ARTDIF  & TOMCATV & SWIM & ERHS & average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1.8 &1.9 &4.0 &5.8 & 3.3 \\
\hline
     & torus &4.1  & 4.1 & 2.1  &3.8 & 3.5 \\
%\cline{2-9}
Multihop & hypercube &6.5 &6.9  &3.5 &5.6 & 5.6\\
%\cline{2-9} 
comm. & allXY &12.9  &13.1 &6.6 & 13.1& 11.4\\
%\cline{2-9}
      & alltoall &58.2 &59.2  &28.7   &59.9 & 51.5\\
\hline
      & $d=1$  &1.0 &1.0  & 1.0 &  1.0 & 1.0\\
Single--hop & $d=4$&3.7  &3.8  &2.3  &3.8 & 3.4\\
comm &        $d=14$ &12.8 &13.0 &6.7  & 13.2 & 11.4\\
  &          $d=20$ &18.2 &18.6  &9.4  &18.8 & 16.3\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Normalized communication time for SPEC95 benchmark programs.}
\label{SPEC95performanceN}
\end{table} 


Table~\ref{SPEC95performance} shows the communication performance of the
programs. Table~\ref{SPEC95performanceN} shows the normalized time.
The test inputs are used as the inputs to these program, which
determine the problem size. To reduce the simulation time, the main
iteration numbers in programs ARTDIF, SWIM and ERHS are reduced to one.
All programs ARTDIF, SWIM, TOMCATV and ERHS  only contains simple 
nearest neighbor communication patterns. Compiled communication performs
worse than dynamic single--hop communication with a multiplexing degree
of one because it  aggregates communications and uses larger
multiplexing degree than needed. 
Hence, it is desirable to develop more advanced communication phase analysis 
techniques that can use different multiplexing degrees for different parts 
of a program to achieve  best performance. 
However, considering all the programs evaluated,
compiled communication out--performs other schemes to a large degree
as shown in Table~\ref{overallN}.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Comm.    & Comp.  & \multicolumn{4}{c|}{Multi--hop} & 
\multicolumn{4}{c|}{Single--hop}\\
\cline{3-10}
schemes &  comm. &   torus & hype. & allXY & alltoall & $1$ &
$4$ & $14$ & $20$ \\
\hline
Norm. time & 1.5 & 3.0 & 3.3 & 4.6 & 16.1 & 6.6 & 4.0 & 5.9 & 7.4 \\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Average normalized communication time for each scheme.}
\label{overallN}
\end{table} 

\section{Chapter summary}

This chapter studied the communication performance for the three
communication mechanisms, dynamic single--hop communication,
dynamic multi--hop communication and compiled communication using
three sets of programs. The following conclusions were drawn from the
study.

\begin{itemize}

\item %Even after taking into considerations the limitations of 
%compiler analysis, 
Compiled communication 
out--performs dynamic  communications
to a large degree for applications with regular computations.

\item The performance of compiled communication can be further 
improved by incorporating 
more advanced communication phase analysis techniques that allow 
different multiplexing degrees in different parts of a program.

\item The major disadvantage of dynamic communications is that they cannot
adapt to different communication requirements. Thus, they support
some communication patterns efficiently while they are inefficient for
other communication patterns. 
Compiled communication efficiently 
supports all types of communication patterns that 
can be determined at compile time.
 
\item Comparing dynamic multi--hop communication 
and dynamic single--hop communication,
dynamic multi--hop communication achieves better performance when the
message size is small and when the communication is dense, while  dynamic
single--hop communication is better when the message size is large.
This result matches the results in Section~\ref{multisingle} where
dynamic single--hop communication is compared with dynamic
multi--hop communication using randomly generated communication patterns.  

\end{itemize}
  
\chapter{Conclusion}

While optical interconnection networks have the potential to provide very large
bandwidth, network control, which is performed in the electronic domain 
due to the lack of suitable photonic logic devices,
has become the communication bottleneck in such networks. In order
to design efficient optical networks where end users can utilize the 
large bandwidth, efficient network control mechanisms must be developed
to reduce the control overheads. This thesis addresses the 
network control bottleneck problem in optical networks by considering three
communication schemes, {\em dynamic single--hop communication}, {\em dynamic
multi--hop communication} and {\em compiled communication}. In addition
to developing techniques to improve communication performance in each scheme, 
this thesis also compares the communication performance
of the three schemes and identifies the advantages and the limitations of each
scheme.  In the following sections, 
the thesis contributions are summarized and directions for future research
are identified.


\section{Thesis contributions}

This thesis makes contributions in the {\em design of control mechanisms}
for time--multiplexed 
optical interconnection networks. The contributions are in two areas:
optical interconnection networks and compiler analysis techniques. In the 
optical interconnection
networks area, this thesis introduces efficient control schemes 
for dynamic single--hop communication and dynamic multi--hop communication. 
This thesis also 
proposes and validates the idea of applying the 
compiled communication technique 
to optical TDM  networks.  In the compiler area, 
this thesis addresses all the issues needed
to apply the compiled communication paradigm 
to optical interconnection networks,
including communication optimization, 
communication analysis, connection scheduling and 
communication phase analysis. 
The main contributions of the thesis are detailed as follows.

\begin{itemize}
\item {\bf Dynamic single--hop communication}. Two sets of efficient 
path reservation algorithms, 
{\em forward path reservation protocols} and  {\em backward 
path reservation protocols}, are designed. Variants of the protocols, 
including holding/dropping and aggressive/conservative schemes, 
are considered. The performance of the protocols and the impact
of system parameters on these protocols are evaluated. 
Forward path reservation protocols extend traditional path reservation
schemes for electronic networks and are simpler compared to  backward
path reservation protocols. However, these  
schemes suffer from either the over--locking problem for 
aggressive schemes or the low successful
reservation rate for  conservative schemes.  
Backward path reservation protocols overcome 
these problems by probing the network state before reserving channels. 
Performance study has established that in optical time--division 
multiplexing networks, backward path reservation protocols, 
though more complex than forward path reservation protocols, 
result in better communication performance when the corresponding 
system and protocol parameters are the same. It is also found that 
while some system or protocol parameters, such as the holding time for 
holding schemes, do not have a significant impact on the performance
of the protocols, other parameters, such as the aggressiveness of a protocol
and the speed of the control network, affect the performance drastically.
Similar techniques can be extended for the path reservation in 
WDM wide area networks \cite{Yuan96b,Yuan98b}.

\item {\bf Dynamic multi--hop communication}. 
Schemes for realizing 
four logical topologies, torus, hypercube, allXY and all--to--all, 
on top of the physical torus topologies are considered. 
Optimal and near optimal routing and channel assignment (RCA) schemes 
for realizing hypercube on array, ring, mesh and torus 
topologies are developed.
An analytical model for analyzing the maximum throughput and the 
average packet delay is developed and verified via simulation.
This model is used to study the performance of dynamic multi--hop 
communication using the four logical topologies. It is found that
in terms of the maximum throughput, the logical all--to--all topology
is the best while the logical torus topology has the lowest performance.
In terms of the average packet delay, the logical torus topology achieves best 
results only when the router is fast and the network is under light load,
while the logical all--to--all topology is best only when the router
is slow and the network is almost saturated. In all other cases, logical
hypercube and allXY topologies out--perform logical torus and 
all--to--all topologies. In addition, 
the impact of system parameters, such as the packet switching time, 
on these topologies are studied. In general,  the performance of the logical
topologies with low connectivity, such as the torus and 
hypercube topologies, are more sensitive to the network load 
and the router speed while the logical topologies with more connectivity, 
such as the all--to--all and allXY topologies, are more sensitive to 
network size. Some of the techniques developed for multi--hop 
communication in optical TDM networks can be applied to other areas. 
The optimal scheme to realize hypercube on mesh--like
topologies can be used to 
efficiently perform communications in algorithms that contain
hypercube communication patterns \cite{Leighton92}. 
The modeling technique can be extended to the modeling of WDM networks or
electronic networks that perform multi--hop communication.

\item {\bf Compiled communication}. 
This thesis considers all the issues necessary
to apply compiled communication
to optical TDM networks, including communication optimization,
communication analysis, connection scheduling and communication phase
analysis.

\begin{itemize}
\item {\it Communication optimization and communication analysis}. 
A communication descriptor called 
{\em Section Communication Descriptor} (SCD) that describes 
communications on virtual processor grids is developed. A 
communication analyzer which performs a number of communication 
optimizations, including message vectorization, redundant communication 
elimination and message scheduling, is presented. All the optimizations
use a demand driven global array data flow analysis framework. This framework
improves previous data flow analysis algorithms for 
communication optimizations by reducing analysis cost and increasing 
analysis precision. 
Algorithms are developed to derive communications on physical processors
from SCDs. 
These algorithms address the problem of effective approximations
in the cases when the information in a SCD is insufficient for deriving
precise communication on physical processors. 
The communication optimization technique is general
and can be implemented in a compiler that compiles HPF--like 
programs for distributed memory machines.   
The communication analysis technique can be used by a compiler 
that requires the knowledge of the communication requirement of a program
on physical processors. 

\item {\it Connection scheduling}. A number of heuristic connection scheduling
algorithms are developed to schedule connections on torus topologies. Some
of the algorithms can also be applied to other topologies. 

\item {\it Communication phase analysis}. 
A communication phase analysis algorithm
is designed to partition a program into phases such that each phase contains
communications that can be supported by the underlying network, while 
capturing the communication locality in the program 
to reduce the reconfiguration overheads. 
This algorithm can also be applied to compiled communication 
on electronic networks. 

\end{itemize}

\item{\bf Communication performance comparison}.
A number of benchmarks and real application programs, including 
hand--coded parallel programs, HPF kernel benchmarks and 
programs from SPEC95, are used to  compare the communication 
performance of the three communication schemes. The relative strengths and
weaknesses of the three schemes are evaluated. 
%Following conclusions are drawn. 
The study establishes that even
with the limitations of compiler analysis, compiler communication
generally out--performs dynamic communications.
%The major advantage of compiled communication is its ability to
%adapt to the communication requirement during different phases of a 
%program. 
It delivers high communication performance for all types 
of communication patterns that are known at compile time.
The dynamic single--hop communication and dynamic multi--hop 
communication both suffers from the 
inability to adapt to the communication requirement. Given
a fixed system setting, they provide good performance for some
communication patterns while fail to achieve high performance for other
communication patterns. Comparing these two communication schemes, 
multi--hop communication has the advantage when the message size is small
and when the communication requires dense connections, 
while single--hop communication
has the advantage when the message size is large. 
\end{itemize}

\section{Future research}

The research of this thesis can be extended in various ways. Some of the
algorithms  can be improved. Additional 
work may either extend the applicability of the techniques or improve
the techniques. Following are a number of future research directions that
are related to this thesis.

\begin{itemize}

\item {\bf Improving backward path reservation algorithms}.   
In the backward reservation, once a channel is reserved,
the reservation fails only when the network state changes. 
Due to the distributed manner of collecting channel states and 
reserving channels in backward path reservation algorithms,
the information for channels on links close to the source node is not as
accurate as the information for channels on links close to the 
destination node.
This problem can be severe when the network size is large. Two 
possible solutions to this problem are as follows. 
First,  a more efficient control network can be used 
to route control messages.
For example,  a Multistage Interconnection Network (MIN) with multi--cast
capability can be used to route control messages so that 
control messages can reach all nodes along the path at the same time.
This allows a protocol to collect the channel usage information 
more efficiently and increases the chance of successful reservation.
Second, assuming that the control network has the same topology as the 
data network,
the backward path reservation protocols can selectively collect the 
channel usage information. The idea behind this improvement
is that wrong information may be worse than no information. 
    
\item {\bf Path reservation with adaptive routing}. In the thesis, path
reservation algorithms assume a deterministic routing.
Preliminary research on extending the path reservation protocols 
with adaptive routing was carried out. The preliminary 
results show that using current
path reservation protocols (both forward and backward reservations), 
the adaptive routing yields lower maximum throughput on the physical torus 
topology for uniform communication traffics. Further research is 
needed to explain this phenomenon and to design path reservation
protocols that take advantage of adaptive routing.

\item {\bf Topologies for multi--hop communication}. In this thesis,
four logical topologies, torus, hypercube, allXY and alltoall,
on top of the physical torus topologies are considered. 
There are two ways to extend this work. 
First, a different physical topology can be considered. For instance,  
it would be interesting to consider efficiently 
realizing regular topologies, such as mesh, torus, on top of an irregular 
topology. Second, there are logical topologies other than the four 
logical topologies considered that can achieve good communication 
performance. Examples include the tree and the shuffle--exchange topologies.

\item {\bf Interprocedural communication optimization}. 
The communication analyzer in the thesis performs
a number of communication optimizations, including message vectorization,
global redundant communication elimination and global message scheduling, 
using intraprocedural array data flow analysis. By incorporating the
interprocedural array data flow analysis, more optimization opportunities
can be uncovered. The intraprocedural array data flow analysis framework
uses interval analysis. It can naturally be extended to 
interprocedual analysis
by treating a procedure as an interval. However, many details,
such as array reshaping at subroutine boundaries and its impact on
communications, must be considered in order for the interprocedural 
analysis to work. 

\item {\bf Improving communication phase analysis}.
The communication phase analysis algorithm in the thesis follows simple 
heuristics, it considers the control structures in a program using
post--order traversal. This enables the algorithm to consider
communications in innermost loops first, 
aggregate the communications out of loops to reduce
the reconfiguration overhead and capture the communication locality. 
However, while the algorithm is simple to implement, the phases it generates
are not optimal in the sense that there may exist other program partitioning
schemes that result in less phases in a program.
More advanced communication phase analysis algorithms based on 
better communication model \cite{Salisbury97}
may be developed by using a general control flow graph for program 
representation and by considering the communication requirement
of the whole procedure when generating phases. 

\item {\bf Compact communication descriptor}.
The communication descriptor in the compiler 
that describes communication patterns
on physical processors is a flat structure. It contains all pairs
of source and destination nodes. This descriptor is both large and hard
to manipulate. More compact communication descriptor is desirable for
the compiler. The challenge however, is that 
the descriptor must both be compact and easy to use by the 
analysis algorithms. 

\item {\bf Irregular communication patterns}

Many scientific codes contain irregular communication patterns that
can only be determined at runtime. This thesis has restricted the
compiled communication technique to be applied to the programs
that contain only regular computations. This restriction can be
relaxed by using a strategy similar to the Chaos runtime 
library\cite{Sussman92}.
This library performs an inspector phase that calculates the runtime
schedule once for many executions of the communication pattern. Similarly
the connection scheduling algorithms can gather communication 
information at runtime and assign channels to all connections 
within the next looping structure to be used for subsequent iterations.
  
\end{itemize}

\section{Impact of this research}

This thesis establishes that the compiled communication technique
is more efficient than both dynamic single--hop communication and 
dynamic multi--hop communication. The compiler algorithms that enable
the application of compiled communication on optical TDM networks, though
can be further improved, are available in this thesis. Although
the compiled communication technique can only apply to the communication 
patterns that are known at compile time, mechanisms that allow the compiler
to manage network resources so that compiled communication can be
supported must be incorporated in future optical TDM networks for 
multiprocessor systems to achieve high performance. Dynamic communication
schemes must be used to handle general communication in an optical TDM network.
Dynamic single--hop communication incurs large startup overhead and is thus
inefficient for small messages which occur frequently in parallel
applications. Dynamic multi--hop communication is efficient 
for small messages, however, it places electronic processing in the 
critical path of data transmission and cannot fully utilize the large
bandwidth in optical links when the optical data
transmission speed is significantly faster than the electronic 
processing speed. Hence, both schemes have their own advantages and the better
choice between these two schemes depends on the application programs and the
advances in optical networking technology.

This thesis develops techniques for efficient communication in optical TDM
networks. Many techniques developed  can be applied to other areas. The 
path reservation algorithms for dynamic single--hop communication can be
extended for WDM wide area networks. The efficient routing and channel
assignment algorithms for hypercube communication pattern 
can be used to efficiently perform communications in algorithms that contain
hypercube communication patterns. The modeling technique for multi--hop 
communication in optical TDM networks can be extended for WDM networks
and electronic networks with multi--hop communication. The communication 
optimization technique based on a demand driven data flow analysis technique
can be incorporated in a compiler that compiles a HPF--like language 
for distributed memory machines. The communication analysis technique can
be used by compilers that perform architectural dependent communication 
optimizations, or compiled communication on electronic networks.









\section{Preliminary work}
\label{prework}

This section describes the preliminary work that has been done toward 
achieving the goals described in Chapter 3. A number of 
distributed path reservation protocols for 
point--to--point optical TDM networks have been designed and 
 a network simulator that
simulates the performance of these protocols has been developed. I also
designed and implemented a demand driven communication optimization
data flow analyzer which performs message vectorization and redundant
communication elimination. The analyzer represents the  logical communication
patterns in programs in communication descriptors
 and can be used to generate physical communication patterns
for both optimized and non--optimized programs. In addition, connection
scheduling algorithms that can be used to schedule communication patterns
that are known at compile time are developed for the torus topology.

\subsection{Distributed path reservation}

\subsubsection{The protocols}
\label{protocol}

In order to support a distributed control mechanism for connection
establishment, it is  assumed that in addition to the optical data network,
there is a logical {\em shadow network} through which
all the control messages are communicated. 
The shadow network has the same physical topology as the data network.
The traffic on the shadow network consists of small control packets
and thus is much lighter than the traffic on the data network. 
The shadow network  operates in packet switching mode; routers at 
intermediate nodes examine the control packets and update local bookkeeping
information and switch states accordingly. 
The shadow network can be implemented as an 
electronic network or alternatively a virtual channel on the data network
can be reserved exclusively for exchanging control messages.
It is  also assumed that a node can send or receive messages through
different virtual channels simultaneously. 
%This is always true for a 
%TDM system. For WDM system, this implies that each nodes must
%have one sender and receiver for each wavelength it operates on. 
%%The protocols can be modified to 
%apply to the WDM system with single tunable
%sender and receiver in each node.

A path reservation protocol ensures that the path from a source node
to a destination node is reserved before the connection is used. A path 
includes the virtual channels on the links that form the connection, the
transmitter at the source node and the receiver at the destination node.
Reserving the transmitter and receiver is the same as reserving a
virtual channel on the link from a node to the switch attached to that
node.
Hence, only reservation of virtual channels on links forming a
connection with path multiplexing will be considered.
There are many options available with respect to different aspects of the 
 path reservation mechanisms, which are discussed next.

\begin{itemize}

%\noindent
%$\bullet$
\item {\em Forward reservation} versus {\em backward reservation}.
Locking mechanisms are needed by the distributed path reservation
protocols  to ensure the exclusive usage of a virtual channel for a connection.
This variation characterizes the timing at which
the protocols  perform the locking.
Under forward reservation, the virtual channels are locked 
by a control message that travels from
the source node to the destination node.
Under backward reservation, a control message travels to the
destination to probe the path, then virtual channels that are found to be
available are locked by another
control message which travels from the destination node to the source node.

%\noindent 
%$\bullet$
\item {\em Dropping} versus {\em holding}. This variation characterizes
the behavior of the protocol when it 
determines that a connection establishment does not progress.
Under the  dropping approach, once the protocol
determines that  the establishment of a connection is not progressing,
it releases the virtual channels  locked on the partially established
path and informs the source node that the reservation has failed.
Under the holding approach, when the protocol determines
that  the establishment of a connection is not progressing,
it keeps the virtual channels  on the partially established path locked for
some period of time, hoping that during this period, the reservation
will progress. If, after this timeout period, the reservation still does not
progress, the partial path is then released and the
source node is informed of the failure.
Dropping can be viewed as holding with holding time equal to 0.

%\noindent 
%$\bullet$
\item {\em Aggressive} reservation versus {\em conservative} reservation. This
variation characterizes the protocol's treatment of each reservation. Under
the aggressive reservation, the protocol tries to establish a connection
by locking as many virtual channels as possible during the reservation process.
Only one of the locked channels is then used for the connection, while the
others are released.
Under the  conservative reservation approach, the protocol
locks only one virtual channel during the reservation process.

\end{itemize}

\subsubsection*{Deadlock}

Deadlock in the control network can arise from two sources.
First, with limited number of buffers, a request loop can be formed within the
control network.
Second, deadlock can occur when a request is holding (locking)
virtual channels on some links while requesting other channels on other
links.
This second source of deadlock can be avoided by the dropping or holding mechanisms
described above.
Specifically, a request will give up all the locked channels if 
it does not progress within a certain timeout period.

Many deadlock avoidance or deadlock prevention techniques for 
packet switching networks proposed in the literature \cite{Dally87} 
can be used to deal with deadlock within the control network (the
first source of deadlock).
Moreover, the control network is under light traffic, and
each control message consists of only a single packet of small size 
(4 bytes). Hence, it is feasible to provide a large number of buffers in each 
router to reduce or eliminate the chance of deadlock.

\subsubsection*{States of virtual channels}

The control network router at each node maintains a state for each
virtual channel on links connected to the router. For forward reservation,
the control router maintains the states for the outgoing links, while
in backward reservation, the control router maintains the states
for the incoming links. As discussed later, 
this setting enables the router to have the information
needed for reserving virtual channels and updating the switch states.
A virtual channel, $V$, on link $L$, can be in one of the following states:

\begin{itemize}
\item $AVAIL$: indicates that the virtual channel $V$ on link $L$
is available and can be used to establish a new connection,
\item $LOCK$: 
indicates that $V$ is locked by some request in the process of establishing
a connection.
\item $BUSY$: indicates that $V$
is being used by some established connection to transmit data.
\end{itemize}

For a link, $L$, the set of virtual channels that are in the $AVAIL$ state is
denoted as $Avail(L)$. When a virtual channel, $V$, is not in $Avail(L)$,
an additional field, $CID$, is maintained to identify the connection request
locking  $V$, if $V$ is in the $LOCK$ state, or the connection using $V$, if $V$
is in the $BUSY$ state.

\subsubsection*{Forward reservation schemes}

In the connection establishment protocols,
each connection request is assigned a unique identifier, $id$, which
consists of the identifier of the source node and a serial number
issued by that node. 
Each control message related to the establishment of a connection carries its
$id$, which becomes the identifier of the connection when it is successfully
established. It is this $id$ that is maintained in the $CID$ field of
locked or busy virtual channels on links.
Four types of packets are used in the forward reservation
 protocols to establish a connection.

\begin{itemize}

%\noindent 
%$\bullet$
\item {\em Reservation packets} ($RES$), used to reserve virtual channels.
In addition to the connection $id$, a $RES$ packet contains a bit vector,
$cset$, of size equal to the number of virtual channels in each link.
The bit vector $cset$ is used to keep track of the set of virtual channels 
that can be used to satisfy the connection request carried by $RES$.
These virtual channels are locked
at intermediate nodes while the $RES$ message
progresses towards the destination node. The switch
states are also set to connect the locked channels on the input and output links.

%\noindent 
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$), used to inform source nodes of the
success of connection requests.
An $ACK$ packet contains a $channel$ field which indicates the virtual
channel selected for the connection.
As an $ACK$ packet travels from the destination to the source, it changes
the state of the virtual channel 
selected for the connection to $BUSY$, and unlocks
(changes from $LOCK$ to $AVAIL$)
all other virtual channels that were locked by the corresponding $RES$ packet.

%\noindent 
%$\bullet$
\item {\em Fail or Negative ack packets} ($FAIL/NACK$), used to inform source
nodes of the failure of connection requests. While traveling back to the source
node, a $FAIL/NACK$ packet unlocks all virtual channels that were locked by the
corresponding $RES$ packet.
 
%\noindent 
%$\bullet$
\item {\em Release packets} ($REL$),  used to release connections.
A $REL$ packet traveling from a source to a destination changes the
state of the virtual channel
reserved for that connection from $BUSY$ to $AVAIL$.

\end{itemize}

The protocols require that control packets from a destination, $d$, to a source, $s$,
follow the same paths (in opposite directions) as packets from $s$
to $d$.
The fields of a packet will be denoted by $packet.field$.
For example, $RES.id$ denotes the $id$ field of the $RES$ packet.

The forward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $RES$ packet with $RES.cset$ set to the
virtual channels that the node may use. This message is then routed to the
destination. When an intermediate node receives the $RES$ packet, 
it determines the next outgoing link, $L$, on the path to the destination, and
updates $RES.cset $ to $ RES.cset \cap Avail(L)$.
If the resulting $RES.cset$ is empty,
the connection cannot be established, and a $FAIL/NACK$ message is sent back to the
source node. The source node will retransmit the request after some
period of time.
This process of failed reservation is shown in Figure~\ref{FORWARD}(a). 
Note that if $Avail(L)$ is represented by a bit-vector, then
$RES.cset \cap Avail(L)$ is a bit-wise "$AND$" operation.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/forward.pstex,height=2.2in}}
\caption{Control messages in forward reservation}
\label{FORWARD}
\end{figure}

If the resulting $RES.cset$ is not empty, the router reserves all the 
virtual channels in $RES.cset$ on link $L$ by changing their states to $LOCK$
and updating $Avail(L)$.
The router will then set the switch state to connect the virtual channels in the
resulting $RES.cset$ of the corresponding incoming and outgoing links.
Maintaining the states of outgoing links is sufficient for these two
tasks.
The $RES$ message is then forwarded to the next node on the path to the destination.
This way,
as $RES$ approaches the destination, the 
path is reserved incrementally. Once $RES$ reaches the
destination with a non-empty $RES.cset$, the destination selects from 
$RES.cset$ a virtual channel to be used for the connection and informs
the source node that the channel is selected by sending an $ACK$ message 
with $ACK.channel$ set to the selected virtual channel.
The source can start sending data once it 
receives the $ACK$ packet. After all data is sent, the source
node sends a $REL$ packet to tear down the connection. This successful
reservation process is shown in Figure~\ref{FORWARD} (b). Note that although
in the algorithm described above, the switches are set during the processing
of the $RES$ packet, they can instead be set during the processing of
the $ACK$ packet.

\noindent
{\bf Holding}: The protocol described above can be modified to 
use the holding policy instead of the dropping policy.
Specifically, when an intermediate node
determines that the connection for a reservation cannot be established, 
that is when $RES.cset \cap Avail(L) = \phi$, the node buffers the $RES$ packet
for a limited period of time. If within
this  period, some virtual channels in the original $RES.cset$ become
available, the $RES$ packet can then continue its journey. Otherwise, 
the $FAIL/NACK$ packet is  sent back to the source.
Implementing the holding policy 
requires each node to maintain a holding queue and
to periodically check that queue to determine if any of the virtual channels 
has become available. In addition, some timing 
mechanism must be incorporated in the routers to timeout 
held control packets. This increases the hardware
and software complexities of the routers.

\noindent
{\bf Aggressiveness}: 
The aggressiveness
of the reservation is reflected in the size of the 
virtual channel set, $RES.cset$, initially chosen by the source node.
In the most aggressive scheme, the source node sets
$RES.cset$ to $\{0, ..., N-1\}$, where $N$ is the number of 
virtual channels in the system. This ensures that the reservation
will be successful if there exists an available virtual channel on the path.
On the other hand, 
the most conservative reservation assigns
$RES.cset$ to include only a single virtual channel. In this case, the
reservation can be successful only when the virtual channel chosen by the
source node is available in all the links on the path. Although 
the aggressive scheme seems to have advantage over the conservative scheme,
it results in excessive locking of the virtual channels in the system. Thus, in
heavily loaded networks, this is expected to decrease the overall throughput.
To obtain optimal performance, the aggressiveness of the protocol should be
chosen appropriately between the most aggressive and the most conservative extremes.

The retransmit time  is another protocol parameter.
In traditional non--multiplexed networks, the retransmit time
is typically chosen randomly from a range [0,MRT], where MRT
denotes some maximum retransmit time.
In such systems, MRT must be set to a reasonably
large value to avoid live-lock. However, this may increase the average
message latency time and decrease the throughput.
In a multiplexed network, the problem of live-lock only 
occurs in the most aggressive scheme (non--multiplexed circuit switching
networks can be considered as  having a multiplexing degree of 1 and 
using aggressive reservation). 
For less aggressive schemes, the
live-lock problem can be avoided by changing the virtual channels selected in
$RES.cset$ when $RES$ is retransmitted.
Hence, for these schemes, a small retransmit time can be used.

\subsubsection*{Backward reservation schemes}

In  the forward locking protocol, the initial decision concerning the 
virtual channels to be locked for a connection request is made in the 
source node without any information about network usage. The backward
reservation scheme tries to overcome this handicap by probing the network
before making the decision. In the backward reservation schemes,
a forward message is used to probe the availability of virtual channels.
After that,
the locking of virtual channels is performed by a backward message. 
The backward reservation scheme uses six types of control
packets, all of which carry the connection $id$, in addition to other
fields as discussed next:

\begin{itemize}
%\noindent
%$\bullet$
\item {\em Probe packets} ($PROB$) travel from sources to destinations 
gathering
information about virtual channel usage without locking any virtual channel.
A $PROB$ packet carries a bit vector, $init$,
to represent the set of virtual channels that are
available to establish the connection.

%\noindent
%$\bullet$
\item {\em Reservation packets} ($RES$) are similar to the $RES$ packets in the forward
scheme, except that they travel from destinations to sources, lock
virtual channels as they go through intermediate nodes, and set the
states of the switches accordingly.
A $RES$ packet contains a $cset$ field.

%\noindent
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$) are similar to $ACK$ packets in the forward
scheme except that they travel from sources to destinations.
An $ACK$ packet contains a $channel$ field.


%\noindent
%$\bullet$
\item {\em Fail packets} ($FAIL$) unlock the virtual channels locked by the
$RES$ packets in cases of failures to establish connections.

%\noindent
%$\bullet$
\item {\em Negative acknowledgment packets} ($NACK$) are
used to inform the source nodes of reservation failures.

%\noindent
%$\bullet$
\item {\em Release packets} ($REL$) are
used to release connections after the communication is completed.

\end{itemize}

Note that a $FAIL/NACK$ message in the forward scheme performs the functions
of both a $FAIL$ message and a $NACK$ message in the backward scheme. 

The backward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $PROB$ message with $PROB.init$ set to contain all
virtual channels in the system.
This message is then routed to the destination.
When an intermediate node receives the $PROB$ packet, 
it determines the next outgoing link, $L_f$, on the forward path to the
destination,  and updates $PROB.init $ to $PROB.init \cap Avail(L_f)$.
If the resulting $PROB.init$ is empty,
the connection cannot be established and a $NACK$ packet is sent back to the
source node.  The source node will try the reservation again after a certain 
retransmit time.
Figure~\ref{BACKWARD}(a) shows this failed reservation case.

If the resulting $PROB.init$ is not empty, the node 
forwards $PROB$ on $L_f$ to the next node. 
This way,
as $PROB$ approaches the destination, the virtual channels available
on the path are recorded in the $init$ set.
Once $PROB$ reaches the
destination,  the destination forms a $RES$ message with $RES.cset$
equal to a selected subset of $PROB.init$ and sends this message back
to the source node.
When an intermediate node receives the $RES$ packet, it determines the
next link, $L_b$, on the backward path to the source, and updates
$RES.cset $ to $RES.cset \cap Avail(L_b)$. 
If the resulting $RES.cset$ is empty, 
the connection cannot be established. In this case the node sends
a $NACK$ message to the source node to inform it of the failure,
and sends a $FAIL$ message to the 
destination to free the virtual channels locked
by $RES$. This process is shown in Figure~\ref{BACKWARD}(b).

\begin{figure}[htp]
\centerline{\psfig{figure=fig/back.pstex,height=2.2in}}
\caption{Control messages in backward reservation}
\label{BACKWARD}
\end{figure}

If the resulting $RES.cset$ is not empty,
the virtual channels in $RES.cset$ are locked, the switch is set accordingly
and $RES$ is forwarded on $L_b$
to the next node.  When $RES$ reaches the source with a non-empty
$RES.cset$,
the source  selects a
virtual channel from the $RES.cset$ for the connection and sends
an $ACK$ message to the destination with $ACK.channel$ set to the
selected virtual channel. This $ACK$ message unlocks all the virtual channels 
locked by $RES$, except the one in $channel$.
The source node can start sending data as soon as it sends the $ACK$ message.
After all data is sent, the source
node sends a $REL$ packet to tear down the connection.
The process of successful reservation is shown in Figure~\ref{BACKWARD}(c).

\noindent
{\bf Holding}: Holding can be incorporated in the backward reservation scheme
as follows.
In the protocol, there are two cases that cause the reservation to fail. 
The protocol may determine that the reservation fails when processing
the $PROB$ packet. In this case, no holding is necessary since 
no resources have yet been locked.
When the protocol determines that the 
reservation fails during the  processing of a
$RES$ packet, a holding mechanism
similar to the one used in the forward reservation scheme may be applied.

\noindent
{\bf Aggressiveness}:
The aggressiveness of the backward reservation protocols is reflected in the 
initial size of $cset$ chosen by the destination node.
The aggressive approach sets
$RES.cset$ equal to $PROB.init$, while the conservative
approach sets $RES.cset$ to contain a single virtual channel from $PROB.init$.
Note that if a protocol supports only the conservative scheme,
the $ACK$ messages may be omitted, and thus only five types of messages 
are needed. 
As in the forward reservation schemes, the 
retransmit time is a parameter in the backward schemes.

\subsubsection{A preliminary network simulator \& Experimental evaluation}
\label{simulator}

A preliminary network simulator has been developed to simulate the behavious
of multiplexed torus networks. The simulator models the network with 
various choices of system parameters and protocols. Specifically, 
the simulator provides the following options for protocol parameters.

\begin{itemize}
\item {\em forward and backward} reservations, this determines which
protocol to be simulated.

\item {\em initial $cset$ size}: This parameter determines the
initial size of $cset$ in the reservation packet. 
It restricts the set of virtual channels under
consideration for a reservation. For FD and FH, 
the initial $cset$ is chosen when the source node composes the RES packet.
Assuming that $N$ is the multiplexing degree in the system,
an $RES.cset$ of size $s$ is chosen by generating a random number,
$m$, in the range $[0,$N$ - 1]$, 
and assigning $RES.cset$ = $\{m\ mod\ N, m+1\ mod\ N..., N+s-1\ mod N\}$.
In the backward schemes, the initial $cset$ is set when
the destination node composes the $ACK$ packet. An $ACK.cset$ of size $s$ 
is generated in the following manner.
If the available set, $RES.INIT$,
has less available channels than $s$, the $RES.INIT$ is copied to $ACK.cset$.
Otherwise,  the available channels are represented in a linear
array and the method used in generating the $cset$  in the forward schemes
is used.

\item {\em timeout value}: This  value 
determines how long a reservation packet can be put in a waiting queue.
The dropping scheme can be considered as a holding scheme with timeout time
equal to 0.

\item {\em maximum retransmit time} (MTR): 
This specifies the period after which a node will retry a
failed reservation. As discussed earlier,
this value is crucial for avoiding live-lock
in the most aggressive schemes. The actual retransmit time
is chosen randomly between 0 and  $MRT -1$.
\end{itemize}

Besides the protocol parameters, the simulator also allows the 
choices of various system parameters.

\begin{itemize}

\item {\em system size}: This specifies the size of the network. All our
simulations are done on torus topology.

\item {\em multiplexing degree}. 
This specifies the number of virtual
channels supported by each link. In our simulation, the multiplexing degree
ranges from 1 to 32.

\item {\em message size}: The message size directly affects  the time that
 a connection is kept before it is released.
In our simulations,  fixed size messages are assumed.

\item {\em request generation rate at each node (r)}: This specifies the traffic on
the network. The connection requests at each node is assumed to have a Poisson
inter-arrival distribution. When a request is
generated at a node, the destination of the request is generated randomly
among the other nodes in the system. When a generated request is blocked,
it is put into a queue, waiting to be re-transmitted.

\item {\em control packet processing and propagation time}: This specifies the speed of the
control networks. The control packet processing time is the time for an
intermediate node to process a control packet. The control packet
propagation time is the time for a control packet to be transferred from one node
to the next. It is assumed
 that all the control packets have the same
processing and propagation time.
\end{itemize}

In the following discussion, $F$ is used to denote forward reservation,
$B$ denotes the backward reservation, $H$ denotes 
holding and $D$ denotes dropping
schemes. For example, $FH$ means the forward holding scheme.
I have implemented a network simulator with various  control mechanisms 
including FH, FD, BH and BD.
Although the simulator can simulate both WDM and TDM torus networks, 
only the results for TDM networks will be presented in this paper.
The results for WDM networks follow similar patterns.
In addition to the options of backward/forward reservation and holding/dropping
policy, the simulation uses the following parameters.

The average latency and throughput are used to evaluate the protocols.
The latency is the period between the time when a message is ready and the time
when the first packet of the message is sent.
The  throughput is the number of messages received per time unit.
Under light traffic, 
the performance of the protocols is measured by the average message latency,
while under heavy traffic, the throughput 
is used as  the performance metric.  
The simulation time is measured in time slots, where a time slot is the
time to transmit an optical data packet between any two nodes in the network.
Note that in multiprocessing applications, nodes are physically close
to each other, and thus signal propagation time is very small (1 foot per
nsec) compared to the length of a message.
Finally, deterministic XY--routing is assumed in the torus topology.


\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Throughput]
  {\psfig{figure=eps/CMP1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CMP2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Comparison of the reservation schemes with dropping}
\label{DFMUL}
\end{figure}


Figure~\ref{DFMUL} depicts the throughput and average latency as a function of
the request generation rate for six
protocols that use the dropping policy in a $16\times 16$ torus.
The multiplexing degree is taken to be 32, the
message size is assumed to be 8 packets and the control packets
processing and propagation time is assumed to be 2 time units. 
For each of the forward and backward schemes, three variations are considered 
with varying aggressiveness.
The conservative variation in which the
initial $cset$ size is 1, the most aggressive variation in which
the initial set size is equal to the multiplexing degree and an optimal variation 
in which the initial set size is chosen (by repeated trials) to maximize the
throughput.
The letters  $C$, $A$ and $O$ are used to
denote these three variations, respectively.
For example, $FDO$ means the forward dropping scheme with optimal $cset$ size.
Note that the use of the optimal $cset$ size reduces the delay in addition to
increasing the throughput. Note also that the network saturates when
the generation rate is between 0.006 and 0.018, depending on the protocol
used. The maximum saturation rate that the $16\times 16$ torus can achieve in the
absence of contention and control overhead can be calculated from
\[
\frac{number\ of\ links}{no.\ of\ PEs \times av. \ no.\ of\ links\ per\ msg
\times msg\ size} = \frac{1024}{256\times 8 \times 8} = 0.0625
\]
Hence, the optimal backward protocol can achieve
almost 30\% of the theoretical full utilization rate. 

Figure~\ref{DFMUL}(b) also reveals that,
when the request generation rate, $r$, is small, for example $r = 0.003$, 
the network is under light traffic and 
all the protocols achieve the same throughput, which is equal to $r$ times
the number of processors.
In this case, the performance of the network should be measured by the
average latency.
In the rest of the performance study,
the maximum throughput (at saturation) and the average latency
(at $r = 0.003$) will be used to measure the performance of the protocols.
Two sets of experiments are performed. The first set 
evaluates the effect of the protocol parameters on the network throughput and
delay, and the second set
evaluates the impact of system parameters on performance.

\subsubsection*{Effect of protocol parameters}

In this set of experiments,
the effect of the initial $cset$ size, the holding time and the retransmit time on the 
performance of the protocols are studied. 
the system parameters for this set of experiment are chosen as follows:
System size = $16\times 16$,
message size = 8 packets, control packet processing and propagation time = 2 time
units.

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET3.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET4.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on forward schemes}
\label{CSETF}
\end{figure}
\begin{figure}[hbtp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on backward schemes}
\label{CSETB}
\end{figure}

Figure~\ref{CSETF} shows the effect of the initial $cset$ size on the forward
holding scheme with different multiplexing degrees, namely
1, 2,  4, 8, 16 and 32.
The holding time is taken to be 10 time units and the MTR is 5 time units
for all the protocols with initial $cset$ size less than the multiplexing degree
and 60 time units for the most aggressive forward scheme.
Large MTR is used in the most aggressive forward scheme because it is 
observed that small MTR often leads to live-lock in that scheme.
only the protocols with the holding policy will be shown since using the
dropping policy leads to similar patterns. The effect of holding/dropping will
be considered in a later figure.
Figure~\ref{CSETB} shows the results for the backward
schemes with the dropping policy.

From Figure~\ref{CSETF} (a), it can be seen that when the multiplexing 
degree is larger than 8, both the most
conservative protocol and the most aggressive protocol
do not achieve the best throughput. Figure~\ref{CSETF}(b) shows that these
two extreme protocols do not achieve the smallest latency either.
The same observation applies to the backward schemes in Figure~\ref{CSETB}.
The effect of choosing the optimal initial $cset$ is significant on both
throughput and delay. That effect, however, is more significant in the
forward scheme than in the backward scheme. For example, with multiplexing
degree = 32,
choosing a non-optimal $cset$ size may reduce the throughput by 50\%
in the forward scheme and only by 25\% in the backward scheme. 
In general, the optimal initial $cset$ size is hard to find.
Table~\ref{OPTCSET} lists the optimal initial $cset$ size for each multiplexing
degree.
A rule of thumb to approximate the optimal $cset$ size is to use 1/3 and 1/10 of the
multiplexing degree for forward schemes and backward schemes, respectively.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Multiplexing & \multicolumn{2}{|c|}{Optimal $cset$ size}\\
\cline{2-3}
Degree & Forward & Backward\\
\hline
4 & 1 & 1\\
\hline
8 & 2 & 1\\
\hline
16 & 5 & 2\\
\hline
32 & 10 & 3\\
\hline
\end{tabular}
\end{center}
\caption{Optimal $cset$ size}
\label{OPTCSET}
\end{table}

Figure~\ref{HOLDSET} shows the effect of the holding time on the performance of
the protocols for a multiplexing degree of 32. 
As shown in Figure~\ref{HOLDSET}(a), the holding time has little
effect on the maximum throughput. It slightly increases the performance for the
forward aggressive and the backward aggressive schemes. As for the 
average latency at light working load, the holding time also has little effect
except for the forward aggressive scheme, where the
latency time decreases by about 20\% when
the holding time at each intermediate node increases from 0 to 30 time units.
Since holding requires extra hardware support compared to
dropping, it is  concluded that holding is not 
cost--effective for the reservation protocols. In the rest of the paper,
only protocols with dropping policies will be considered.


\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/HOLDSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/HOLDSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of holding time}
\label{HOLDSET}
\end{figure}

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/RETRYSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/RETRYSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of maximum retransmit time}
\label{RETRYSET}
\end{figure}



Figure~\ref{RETRYSET} shows the effect of the maximum
retransmit time (MRT)
on the performance. Note that the retransmit time is uniformly distributed
in the range $0..MRT-1$. As shown in Figure~\ref{RETRYSET} (a),
increasing MRT results in performance degradation in
all the schemes except FDA, in which the performance improves
with the MRT. This confirms that the MRT value is important to 
avoid live-lock in the network when aggressive reservation is used.
In other schemes this parameter is not important, because when 
retransmitting a failed request, virtual channels different than the ones
that have been tried may be included in $cset$.
This result indicates another drawback of the 
forward aggressive schemes: in order to avoid live-lock, the MRT
must be a reasonably large value, which decreases the overall performance.

The results of the above set of experiments may be summarized as follows: 

\begin{itemize}

\item With proper protocols, multiplexing results in higher
maximum throughput. Multiplexed networks are significantly more efficient than
non--multiplexed networks.

\item Both the most aggressive and the most
conservative reservations cannot achieve optimal performance. 
However, the performance of the forward schemes is more sensitive to the
initial $cset$ size than the performance of the backward schemes.

\item The value
of the holding time in the holding schemes does not have significant
impact on the performance. In general, however, dropping is more efficient
than holding.

\item  The retransmit time 
has little impact on all the schemes except the FDA scheme.

\end{itemize} 

In the next section,
only  dropping schemes with
MRT equal to 5 time units for all schemes except FDA will be considered.
The MRT for FDA schemes is set to 60.

\subsubsection*{Effect of other system parameters}

This set of  experiments focuses on studying the performance of the 
protocols under different multiplexing degrees, system sizes, message sizes and 
control network speeds.
Only one parameter is changed in each experiment, with the other
parameters set to the following default values (unless stated otherwise):
network size = $16\times 16$ torus, multiplexing degree = 
16, message size = 8 packets, 
control packet processing and propagation time = 2 time units.

\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
  {\psfig{figure=eps/MUL1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/MUL2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{The performance of the protocols for different multiplexing degree}
\label{DBMUL}
\end{figure}


Figure~\ref{DBMUL}
shows the performance of  the protocols for different multiplexing degrees. 
When the multiplexing degree is small,  BDO and FDO
have the same maximum bandwidth as BDC and FDC, respectively. When 
the multiplexing degree is large, BDO and FDO offers better throughput.
In addition, for all multiplexing degrees, BDO is the best among
all the schemes. As for the average latency, both FDA and BDA have significantly
larger latency than all other schemes. Also, FDO and BDO have the smallest latencies.
It can be seen from this experiment that the backward
schemes always provide the same or better performance (both maximum
throughput and latency) than their forward reservation counterparts for all
multiplexing degrees considered.

Figure~\ref{SIZE} shows the effect of the network size on the performance of
the protocols.
It can be seen from the figure that all the protocols, except the aggressive
ones, scale nicely with the network size.
This indicates that the aggressive protocols cannot 
take advantage of the spatial diversity of the communication. This is 
a result of excessive reservation of channels. When the network size
is small, there is little big difference in the performance of the protocols.
When the network size is larger, the backward schemes show their superiority.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/SIZE1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
 {\psfig{figure=eps/SIZE2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the network size}
\label{SIZE}
\end{figure}


\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/MSIZE1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
{\psfig{figure=eps/MSIZE2.eps,height=2.2in}}
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the message size}
\label{MSIZE}
\end{figure}



Figure~\ref{MSIZE} shows the effect of the message size on the protocols.
The multiplexing degree  in this experiment is 16.
The throughput in this figure is normalized to reflect the
number of packets that pass through the network, rather than the number
of messages.  That is,\\
\centerline{$normalized\ throughput\ =\ msg\ size \times \ throughput$}
Both the forward and backward locking schemes achieve higher
throughput for larger messages. When messages are sufficiently large,
the signaling overhead in the protocols is small and
all protocols have almost the same performance. 
However, when the message size is small, the BDO scheme achieves 
higher throughput
than the other schemes. This indicates that BDO incurs less 
overhead in the path reservation than the other schemes. 

The effect of message size on the latency of the protocols is interesting.
Forward schemes incur larger latency when the message size is large. 
By blindly chosing initial cset, forward schemes
do not avoid chosing virtual channels used in communications, which
increases the latency when the message size is large (so that connections are
hold longer for communications).
Backward schemes probe
the network before chosing the initial csets. Hence, the latency in backward
shemes does not degrade as
much as in forward schemes when message size increases. 
Another observation is that in both forward and backward protocols, 
aggressive schemes sustain the increment of message size better
than the conservative schemes. This is also because of the longer
communication time with larger message size. Aggressive schemes are more 
efficient in finding a path in case of large message size. Note that 
this merit of aggressive schems is offsetted by the 
over reservations.
Another interesting point is that
the latency for messages of  size 1 results in higher latency than 
messages of size 8 in BDA scheme. This can be explained by the overly crowded
control messages in the network
in the case when data message contains a single packet (and
thus can be transmitted fast). The conflicts of control messages result in
larger latency.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
{\psfig{figure=eps/CNS1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CNS2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the speed of the control network}
\label{CNS}
\end{figure}

Figure~\ref{CNS} shows the effect of the control network speed on performance. 
The multiplexing degree  in this experiment is 32.
The speed of the
control network is determined by the
time for a control packet to be transferred from one node to the next node,
and the time for the control router to process the control packet. From the 
figure, it can be seen that, when the control speed is slower, the maximum
throughput and the average latency degrade.
The most aggressive schemes in both 
forward and backward reservations, however, are more sensitive to the
control network speed. Hence, it is important to have a reasonably fast 
control network when these reservation protocols are used. 

The results of the above set of experiments may be summarized as follows:

\begin{itemize}
\item The performance of FDA is significantly worse than other protocols. 
Moreover, this protocol cannot take advantage of both larger multiplexing 
degree and larger network size.
\item The backward reservation schemes provide better performance than
the forward reservation schemes for all multiplexing degrees. 
\item The difference of the protocols does not affect the communication efficiency
when the network size is small. However, for large networks, the
backward schemes provide better performance.
\item The backward schemes provide better performance when the message size
is small. When the message size is large, all the protocols have similar 
performance.
\item The speed of the control network affects the performance of the 
protocols greatly.
\end{itemize}

In this section, I have discussed distributed path reservation algorithms 
to establish connections with path multiplexing for communication
requests that arrive at the network dynamically.  In the next section, 
preliminary work on the communication analysis for compiled communication
will be presented.

\subsection{Communication analysis for compiled communication}
\label{commlab}

In order for the compiler to perform compiled communication, data structures 
must be designed for the compiler to represent the communication requirement
in the program. The data structure must both be powerful enough to represent 
the communication reqirement and easy to be obtained from the source program.
A communication descriptor which is an extension of the {\em array
section descriptor} \cite{callahan88} is designed. 
The descriptor describes the communication 
pattern in logical processor space. It can be used for both communication 
optimization and to derived the communication pattern in physical processor
space. In the following, the communication descriptor and 
how to calculate the communication descriptor from the source program will be 
described.


\subsubsection{Section communication descriptor (SCD)}

In this section,  
{\em Section Communication Descriptor} 
(SCD), which can be used 
to represent logical communication patterns in a program, will be introduced. 
The calculation of initial SCDs in programs will also be described.
Further information about SCD and its operations can be found in \cite{Yuan96}.

\subsubsection*{The descriptor}

The processor space is considered as an unbounded grid of virtual processors.
The abstract processor space is similar to a {\em template} in High
Performance Fortran (HPF) \cite{HPF}, which is a grid over which 
different arrays are aligned. In the rest of this section,  
{\em communication} means communication on the virtual processor grid.

The Section Communication Descriptor(SCD) is composed of three parts, 
(1) an array region which describes the parts of arrays that are involved 
in the communication, (2) a communication mapping which describes the
source and destination (in virtual processor grid) 
relationship in the communication and (3) a
communication qualifier which describes the iterations in which
the communication should 
be performed. More specifically, a SCD is defined as 
$<N, D, M, Q>$, where $N$ is an array name, $D$ is the source array region
of the communication, and $M$ is the descriptor of source-destination mapping,
and $Q$ is a descriptor that indicates in which iterations (in the interval)
the communication should be performed. 

The {\em bounded regular section descriptor} (BRSD)\cite{callahan88} is used
to describe the source region of communications. As discussed in 
\cite{callahan88}, set operations can be efficiently performed over BRSDs.
The source region
D is a vector of subscript values such that each of its elements is either
(1) an expression of the form $\alpha*i + \beta$, where i is a loop 
index variable and $\alpha$ and $\beta$ are invariants, (2) a triple 
$l:u:s$, where $l$, $u$ and $s$ are invariants, or (3) $\perp$, indicating
no information about the subscript value.   

The source-destination mapping $M$ is denoted as
$<source, destination, qual>$. The $source$
is a vector whose elements are of the form  
$\alpha*i + \beta$, where $i$ is a loop 
index variable and $\alpha$ and $\beta$ are invariants. The $destination$ 
is a vector whose elements are of the form 
$\sum_{j=1}^{n} \alpha_{j}*i_j + \beta_j$, where $i_j$'s are 
loop index variables  and $\alpha_j$'s and $\beta_j$'s are
invariants. The {\em mapping qualifier} list, 
$qual$, is a list of elements whose
format is $i = l:u:s$, where
$i$ is a variable, $l$, $u$ and $s$ are invariants. $l:u:s$  
denotes the ranges of the variable $i$. 
The mapping qualifier is used to describe the broadcast
effect, which may be needed during message vectorization.

The qualifier $Q$ is of the form $i = l:u:s$, where $i$ is the
induction variable of the interval. The notation
$Q=\perp$ is used to indicate that 
the communication is to be performed 
in every iteration.
$Q$ will be referred to as the SCD's {\em communication 
qualifier} in the rest of the paper. Including $Q$ in the descriptor
enables SCD to describe the communications that can be partially
vectorized. 

Initially, communications are required
before assignment statements with remote references.
It is assumed that owner computes rule is enforced. The owner
computes rule requires each item referenced on the {\em right hand side} (rhs)
 of an 
assignment statement to be sent to the processor that owns the {\em left hand
side} (lhs).
Before  the calculation of SCD is presented, I will first 
describe the calculation of  the ownership of array elements.


\begin{figure}[tbph]
%\begin{singlespace}
\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{11.5cm}  ALIGN (i, j) with VPROCS(i, j) :: x, y, 
                                      z\\
\hspace{11.5cm}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
\hspace{11.5cm}  ALIGN (i) with VPROCS(i, 1) :: a, b\\
\hspace{11.5cm}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
\hspace{11.5cm}(s2)\hspace{0.1in}\>b(i-1) = a(i)...\\
\hspace{11.5cm}(s3)\hspace{0.1in}\>a(i+2) = ...\\
\hspace{11.5cm}(s4)\hspace{0.1in}\>b(i) = a(i+1) ...\\
\hspace{11.5cm}(s5)\hspace{0.1in}\>do\=\ j = 1, 100\\
\hspace{11.5cm}(s6)\hspace{0.1in}\>\>x(i, j) = w (i, j)\\
\hspace{11.5cm}(s7)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s8)\hspace{0.1in}end do\\
\hspace{11.5cm}(s9)\hspace{0.1in}if (...) then\\
\hspace{11.5cm}(s10)\hspace{0.1in}\>do i = 1, 100\\
\hspace{11.5cm}(s11)\hspace{0.1in}\>\>do\=\ j = 50, 100\\
\hspace{11.5cm}(s12)\hspace{0.1in}\>\>\>x(i+j-1, 2*i+2*j-3) = w (i, j)\\
\hspace{11.5cm}(s13)\hspace{0.1in}\>\>\>y(i, j) = w(i, j)\\
\hspace{11.5cm}(s14)\hspace{0.1in}\>\>end do\\
\hspace{11.5cm}(s15)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s16)\hspace{0.1in}\>do i = 1, 100\\
\hspace{11.5cm}(s17)\hspace{0.1in}\>\>y(i, 150) = x(i+1, 150)\\
\hspace{11.5cm}(s18)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s19)\hspace{0.1in}end if\\
\hspace{11.5cm}(s20)\hspace{0.1in}do i = 1, 100\\
\hspace{11.5cm}(s21)\hspace{0.1in}\>b(i) = a(i+1)\\
\hspace{11.5cm}(s22)\hspace{0.1in}\>do j = 1, 200\\
\hspace{11.5cm}(s23)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
\hspace{11.5cm}(s24)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s25)\hspace{0.1in}\>w(i+1, 200) = ...\\
\hspace{11.5cm}(s26)\hspace{0.1in}end do\\

\end{tabbing}
\end{minipage}

\begin{minipage}{20cm}
\begin{picture}(0,0)%
\special{psfile=fig/2.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0038in}%
%\begin{picture}(1080,1070)(385,-240)
%\end{picture}

\end{minipage}
\normalsize
\caption{An example program and its interval flow graph}
\label{EXAMPLE}
\end{figure}


\subsubsection*{Ownership}

It is assumed that the arrays are all aligned to a single virtual space by 
simple affine functions. 
The alignments allowed are scaling, axis alignment and 
offset alignment.  The mapping from a point $\vec{d}$ in the data space to the 
corresponding point $\vec{v}$ in the virtual processor grid can be specified by
an alignment matrix $M$ and an alignment offset vector $\vec{\alpha}$. 
Thus, the ownership information of an element $\vec{d}$ can be calculated
using the formula $\vec{v} = M \vec{d} + \vec{\alpha}$. 
For example, consider the 
alignments of array $w$ and $a$ in the example program in 
Fig.~\ref{EXAMPLE}, the alignment matrices and offset vectors for 
array $w$ and $a$ are the following:
\begin{center}
\small
\footnotesize
\[ M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{\alpha}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   M_a  = \left(
       \begin{array}{c} 1 \\ 0 \end{array} \right),\
   \vec{\alpha}_a = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right).
\]
\end{center}

\subsubsection*{Initial SCD calculation}

Once the ownership information is provided, initial SCDs can be calculated
from the program structure. Let  $<N, D, M, Q>$ be an initial $SCD$, where
$N$ is the array to be communicated.
The array region 
$D$ contains a single element which is determined by the index expressions.
Since initially communication is always performed in 
every iteration, $Q = \perp$.  Let $M = <src, dst, qual>$. Since
initially communication does not perform broadcast, 
$qual = \perp$. Next I will 
describe how to calculate the source processor, $src$,
 and destination processor,
$dst$, for the mapping $M$ from the program structure.

Let $\vec{i}$ be the vector of loop indices. When the subscript
expressions are affine functions of the loop indices, the array 
references can be
expressed as $N(G\vec{i} + \vec{g})$, where $N$ is the array name, $G$ is a matrix and $\vec{g}$ is  a vector. 
I call G the {\em data access matrix}
and $\vec{g}$ the {\em access offset vector}.  The 
matrix, $G$, and the vector, $\vec{g}$, describe
a mapping from each
point in the iteration space to the corresponding point in the data space.
Let $G_l$, $\vec{g}_l$, $M_l$, $\vec{\alpha}_l$ be the data access matrix,
access offset vector, alignment matrix and alignment vector 
for the left
hand side array, and  
$G_r$, $\vec{g}_r$, $M_r$, $\vec{\alpha}_r$ be the corresponding quantities
for the right hand side array. The source processor $src$, which represents
the processor of the array element in $rhs$, and destination
processor $dst$, which represents the processor of the element in $lhs$
can be obtained from the  following equations.\\
\centerline{$src = M_r(G_r\vec{i} + \vec{g}_r) + \vec{\alpha}_r,$ \hspace{1in}
            $dst = M_l(G_l\vec{i} + \vec{g}_l) + \vec{\alpha}_l$}

Consider the communication in statement $s11$ for Fig.~\ref{EXAMPLE}.
The compiler can obtain from the program the following  data 
access matrices,
access offset vectors, alignment matrices and alignment vectors. 
\begin{center}
\small
\footnotesize
\[ M_x  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{\alpha}_x = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{\alpha}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
%\end{center}
%\begin{center}
\[ G_l  = \left(
       \begin{array}{c} 1 \\ 2 \end{array}
       \begin{array}{c} 1 \\ 2 \end{array} \right),\
   \vec{g}_l = \left(
       \begin{array}{c} -1 \\ -3 \end{array} \right),\
   G_r  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_r = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right)
\]
\end{center}
Thus, the initial SCD for statement $s12$ is \\
\centerline{$<N=w, D=<(i, j)>, M=<(2*j, i+1), (i+j-1, 2*i+2*j-3), 
            \perp>, Q= \perp>$}
As an indication of the complexity of a SCD, the structure for this 
communication required 712 bytes to store.


\subsubsection*{Operations on SCD}

Operations, such as intersection, difference and union, on the SCD descriptors
are needed in our analysis. Since the analysis can not guarantee exact
results of the operations, {\em subset} and {\em superset} versions of these
operations are implemented. 
Based on the purpose of the operation, the compiler uses the proper
version to obtain a conservative approximation. These operations,
the union ($\cup$), the intersection ($\cap$) and the difference ($-$), 
are straight-forward extension of the operations on BRSD \cite{callahan88}.
Details can be found in \cite{Yuan96}. 

%Here, we will describe an important
%operation: testing whether a communication $SCD_1=<N_1, R_1, M_1, Q_1>$ is 
%a sub--communication of another communication $SCD_2=<N_2, R_2, M_2, Q_2>$.
%
%\noindent
%{\bf Range testing operations} Ranges are denoted as $l:u:s$, where
%$l$ is the lower bound, $u$ is the upper bound and $s$ is the step.
%Range testing operations check the relation between two ranges. 
%Most commonly used range
%testing operations include the subrange testing, which test whether one
%range is a subrange of another range. This testing is reduced to the
%testing of the relation of the $l$, $s$ and $u$ in the two ranges.
%
%\noindent
%{\bf Subset Mapping testing}. 
%Testing that a relation $M_1$ ($= <s_1, d_1, q_1>$) is a subset of
%another relation $M_2$ ($= <s_2, d_2, q_2>$) is
%done by checking if the equations $s_1 = s_2$ and $d_1 = d_2$ and
%subrange testing $q_1 \subseteq q_2$. The equations 
%$s_1 = s_2$ and $d_1 = d_2$ can easily be solved by 
%treating variables in $M_1$ as constants and 
%variables in $M_2$ as variables. 
%Note that since the elements
%in $s_1$ and $s_2$ are of the form $\alpha*i+\beta$, the equations can
%generally be solved efficiently.
%
%\noindent
%{\bf SCD subset testing}\hspace{0.2in}
%$SCD_1 \subseteq SCD_2 \Longleftrightarrow N_1 = N_2 \wedge
%R_1 \subseteq R2 \wedge M_1 \subseteq M_2 \wedge Q_1 \subseteq Q_2$

\subsubsection{Array data flow analysis for communication optimization}
\label{arraydflow}


Many communication optimization opportunities can be uncovered by
propagating the SCD for a statement globally. For example, if a SCD
can be propagated from a loop body to the loop header without being killed
in the process of propagation, the communication represented by the SCD
can be hoisted out of the loop body, i.e. the communication can be vectorized.
Another example is the redundant communication elimination.
While propagating $SCD_1$,  if $SCD_2$ is encountered such that $SCD_2$ is 
a  subset of the $SCD_1$, then the communication represented by $SCD_2$ can
be subsumed by the communication represented by $SCD_1$ and can be eliminated.
Propagating  SCDs backward can find the latest point to place the 
communication, while propagating SCDs forward can find the last point where
the effect of the communication is destroyed. Both these
two propagations are useful in communication optimization. 
Since forward and backward propagation
are similar, I will only focus on 
backward propagation of SCDs.

I present generic demand driven algorithms to propagate
SCDs through the Interval flow graph representations of
programs. The analysis techniques are
the reverse of the interval-analysis \cite{gupta93}.
Specially, by reversing the information flow associated with program points,
I derive a system of request propagation rules.
The SCD descriptors are propagated 
until they cannot be propagated any further, 
i.e. all the elements in the SCD are
killed. However, in practice, the compiler may choose to 
terminate the propagation prematurely to
save analysis time while there are still elements in SCDs. 
In this case, since the analysis starts from the 
points that contribute to the  optimizations,
the points that are textually close to the starting points, where
most of the optimization oppurtunities present, have been considered.
This gives the demand driven algorithm the ability to trade precision for
time.
In the propagation, at a certain time, only a single interval
is under consideration. Hence, the propagations are logically done in
an acyclic flow graph. During the propagation, a 
SCD may expand when it is propagated out of a loop. When a set of
elements of SCD is killed inside a loop, the set is propagated into the loop
to determine the exact point where the elements are killed. There are 
two types of propagations,
{\em upward} propagation, in which SCDs may need to be 
expanded, and {\em downward} propagation, in which SCDs may need to be 
shrunk. 


The format of a data flow {\em propagation request}
is  $<S, n, [UP|DOWN], level, cnum>$, where S is a SCD, n is a node
in the flow graph, constants $UP$ and $DOWN$ indicate the request is  
upward propagation  or downward propagation, $level$ indicates
at which level is the request and the value $cnum$ 
indicates which child node of 
$n$ has triggered the request. A special value $-1$ for $cnum$ is used as
the indication of the beginning of downward propagation.
The propagation request triggers 
some local actions and causes the propagation of a SCD from the node n. 
The propagation of SCD follows the following rules. It is  assumed that node 
$n$ has $k$ children.

\subsubsection*{Propagation rules}

{\bf RULE 1: upward propagation: regular node}. 
The request on a regular node takes an action based
 on SCD set $S$ and the local
information. It also propagates the information upward.
The request stops when S become empty. The rule is shown in the following 
pseudo code. In the code, functions $action$ and $local$
are depended on the type of optimization to be performed.
The $pred$ function finds all the nodes that are  predecessors in the 
interval flow graph and the 
set $kill_n$ includes all the elements defined in node
$n$. Note that $kill_n$ can be represented as an SCD.

\begin{tabbing}
\hspace{1in}re\=quest($<S_1, n, UP, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, UP, level, k>$) : \\
\hspace{1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{1in}\>action(S, local(n))\\
\hspace{1in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - kill_n, m, UP, level, j>$)\\
\end{tabbing}
A response to requests in a node $n$ 
occurs only when all its successors have been processed. This 
guarantees that in a acyclic flow graph
each node will only be processed once. The side effect is that 
the propagation will not pass beyond a branch point.
A more aggressive scheme can 
propagate a request through a node without checking whether
all its successors are processed. In that scheme, however, a nodes may need 
to be processed multiple times to obtain the final solution.    

{\bf RULE 2: upward propagation: same level loop header node}.
The loop is contained in the current level. The request needs to obtain the
summary information, $K_n$, for the interval, perform the action
based on $S$ and the summary information, propagate the information passed 
the loop and trigger a downward propagation to propagate the information
into the loop nest. Here, 
the summary function $K_n$, summarizes all the elements defined
in the interval. 
It can  be calculated either before hand or in a 
demand driven manner. I describe how to 
calculate the summary in a demand driven manner later. 
Note that a loop header can only have one successor besides the 
entry edge into the loop body. The $cnum$ of the downward request
is set to -1 to indicate that it is the start of the downward propagation.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{1in}\>action(S, $K_n$) \\
\hspace{1in}\>if\=\ ($S - K_n \ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - K_n, m, UP, level, j>$)\\
\hspace{1in}\>if ($S\cap K_n \ne \phi$) then\\
\hspace{1in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$)\\
\end{tabbing}



{\bf RULE 3: upward propagation: lower level loop header node}.

The relative level between the propagation request
and the node can be determined by 
comparing the level in the request and the level of the node. Once a request
reaches the loop header. The request will need to be expanded to be 
propagated in the upper level. At the mean time, this request triggers 
a downward propagation for the set that must stay in the loops. 
Assume that the loop index variables is $i$ with bounds $low$ and $high$.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{1in}\>calculate the summary of loop $n$\\
\hspace{1in}\>outside = $expand(S, i, low:high) - 
                     \cup_{def}expand(def, i, low:high)$\\ 
\hspace{1in}\>inside = $expand(S, i, low:high) \cap 
                     \cup_{def}expand(def, i, low:high)$\\
\hspace{1in}\>if\=\ (outside $\ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<outside, m, UP, level -1, j>$)\\
\hspace{1in}\>if (inside $\ne \phi$) then\\
\hspace{1in}\>\>request($<inside,  n, DOWN, level, -1>$)\\
\end{tabbing}

The variable $outside$ stores the elements that can be propagated out of
the loop, while the variable $inside$ store the elements that are killed
within the loop. The expansion function has the same definition as in 
\cite{gupta93}. For a SCD descriptor S, expand(S, k, low:high) is a 
function which replaces
all single data item references $\alpha*k+\beta$ used in any
array section descriptor D in S by the triple ($\alpha*low+\beta:
\alpha*high+\beta:\alpha$). 
The sets $def$ includes all the definition that are the source of a
flow-dependence.

{\bf RULE 4: downward propagation: lower level loop header node}.

This is the initial downward propagation. The loops index variable, $i$, 
is treated as a constant in the downward propagation. 
Hence, SCDs that are propagated into the loop body 
must be changed to be the initial
available set for iteration $i$, that is, subtract all the variables 
killed in the iteration i+1 to high and propagate the information from the 
tail node  to the head node. This propagation prepares the downward 
propagation into the loop body by shrinking the SCD for each iteration.

\begin{tabbing}
\hspace{1in}qu\=ery($<S, n, UP, level,cnum>$): \\
\hspace{1in}\>if\=\ $(cnum = -1)$ then\\
\hspace{1in}\>\>calculate the summary of loop $n$;\\
\hspace{1in}\>\>request($<S - \cup_{def}expand(def, k, i+1:high), 
                    l, DOWN, level-1, 1>$);\\
\hspace{1in}\>else\\
\hspace{1in}\>\> STOP /* interval processed */\\
\end{tabbing}

{\bf RULE 5: downward propagation: regular node}.
For regular node, the downward propagation is the same as the upward
propagation.

\begin{tabbing}
\hspace{1in}re\=quest($<S_1, n, DOWN, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, DOWN, level, k>$) : \\
\hspace{1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{1in}\>action(S, local(n))\\
\hspace{1in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - kill_n, m, DOWN, level, j>$)\\
\end{tabbing}


{\bf RULE 6: downward propagation: same level loop header node}.
When downward propagation reaches a loop header(not the loop header
whose body is being processing), it must generate further downward
propagation request to go deeper into the body.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, DOWN, level, 1>$): \\
\hspace{1in}\>action(S, summary(n)); \\
\hspace{1in}\>if\=\ ($S-K_n \ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - K_n, m, DOWN, level, j>$);\\
\hspace{1in}\>if\=\ ($S\cap K_n \ne \phi$) then\\
\hspace{1in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$);\\
\end{tabbing}

\subsubsection*{Summary calculation}

\begin{figure}[htbp]
\begin{tabbing}
\hspace{1in}(1)\hspace{0.5in}Su\=mmary\_kill(n)\\
\hspace{1in}(2)\hspace{0.5in}\>$K_{out}(tail)$ = $\phi$\\
\hspace{1in}(3)\hspace{0.5in}\>fo\=r all $m\in T(n)$ and 
                               level(m) = level(n)-1 in backward order\\
\hspace{1in}(4)\hspace{0.5in}\>\>if\=\ m is a loop header then\\
\hspace{1in}(5)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                    K_{in}(s)$\\
\hspace{1in}(6)\hspace{0.5in}\>\>\>$K_{in}(m)$ = summary\_kill(m) $\cup
                                   K_{out}(m)$\\
\hspace{1in}(7)\hspace{0.5in}\>\>else\\ 
\hspace{1in}(8)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                   K_{in}(s)$\\
\hspace{1in}(9)\hspace{0.5in}\>\>\>$K_{in}(m)$ = $kill(m) \cup K_{out}(m)$\\
\hspace{1in}(10)\hspace{0.5in}\>return (expand($K_{in}(header)$, i, low:high))\\
\end{tabbing}
\caption{Demand driven summary calculation}
\label{KILL}
\end{figure}

During the request propagation, the summary information of an interval is
needed when a loop header is encountered. In this section, I describe
an algorithm to obtain the summary information in a demand driven manner.
I use the calculation of kill set of the interval as an example. Let 
$kill(i)$ 
be the variables killed in node $i$, $K_{in}$  and $K_{out}$ 
be the variables killed before and after node respectively.
Fig.~\ref{KILL} depicts the demand driven algorithm. The 
algorithm propagates the data flow information from the tail node to the 
header node in the interval using the following data flow equation:\\

\centerline{$K_{out}(n) = \cup_{s\in succ(n)}K_{in}(s)$}
\centerline{$K_{in}(n) = kill(n)\cup K_{out}(n)$}

When inner loop header is
encountered, a recursive call is issued to get the summary information
for the inner interval. Once loop header is reached, the kill set need
to be expanded to be used by the outer loop.

\subsubsection{Implementation}

The analyzer is implemented on top of the Stanford SUIF compiler.
Since I do not have a backend code generator, I am not able to  
evaluate the optimization results in terms of execution time. 
Instead, I developed a communication emulation system,
which takes SCD descriptors as input and emulates the 
communications described by the SCDs in a multiprocessor system. 
The virtual to physical processor mapping is provided
to the emulation system to emulate the communication in physical
processors.
The emulation system provides an interface with C program as  library 
calls whose arguments include all information in a SCD.
A compiler backend automatically generates a library call for each 
SCD remaining in the program. In this way, the communication performance
can be evaluated in the emulation system
by running the program generated by the compiler backend.

The generation of a program used for evaluation is carried out in
the following steps. 
(1) A sequential program is compiled using SUIF frontend, $scc$, 
to generate the SUIF intermediate representation. 
(2) SUIF transformer, $porky$, is used to perform scalar optimizations,
forward propagation, dead code elimination and induction variable
detection. (3) A communication preprocessing phase annotates the
global arrays with data alignment information. (4) The
analyzer is invoked to analyze and optimize the communications required
in the program. After communication optimization, the backend of the
analyzer inserts a library call into the SUIF intermediate representation
for each SCD remaining in the program.  (5) The $s2c$ tool
is used to convert the SUIF intermediate representation into C program, which
is the one used for evaluation.

Six programs are used in the experiments. The first benchmark, 
L18, is the explicit hydrodynamics kernel in livermore loops (loop 18).
The second benchmark, ARTDIF, is a kernel routine obtained from 
HYDRO2D program, 
which is an astrophysical program for the computation of galactical jets
using hydrodynamical Navier Stokes equations. 
The third benchmark, TOMCATV, does the mesh generation with 
Thompson's solver. 
The fourth program, SWIM, 
is the SHALLOW weather prediction program.
The Fifth program, MGRID, is the simple multigrid solver for
computing a three
dimensional potential field. This sixth program, ERHS, is part of the
APPLU program, which is the solver for five coupled 
parabolic/elliptic partial differential equations. The programs, HYDRO2D,
TOMCATV, SWIM, MGRID and APPLU, originally come from SPEC95 benchmark suite.

Table~\ref{analysis} shows  the analysis cost of the
analyzer.  The analyzer, which applies the algorithms on all SCDs in the
programs,  is run on SPARC 5 with 32MB memory. 
Row 2 of the table shows the size of the programs and Row 3 shows the 
number of initial SCDs in the programs.
Row 4 shows the memory space requirement of the analyzer in SCD units. 
The value in bracket
is the maximum number of SCDs in a single node during the analysis. 
In the analyzer the size of the SCDs ranged from 0.6 to about 2 kbytes.
This memory does not include the memory to store the initial and final data 
flow information. By repeatedly using 
the same memory for propagating different
SCDs,  the analyzer requires very small amount of extra memory. Row 5 shows the
cumulative memory requirement, which is the sum of the number of SCDs passing 
through each node. This number is equivalent to the memory requirement of
traditional data flow analysis.
The value in bracket
is the maximum cumulative SCDs in a node.
Row 6 are the ratio between numbers in
row 5 and row 4, which is the ratio of the memory
memory requirements of the traditional method and this method. 
On an average this method reduces the memory requirement
by a factor of 26.
Row 7 gives the raw analysis times and row 8 shows the rate at which
the analyzer operates in units of source $lines/sec$. On an average
the analyzer compiles 166 lines per second for the six programs. 
Row 9 shows the total time,
which includes analysis time and the time to load and store the
SUIF structure, for reference.
This experiment shows that the analyzer is efficient in space and time.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Program &            L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
size(lines) &         83  &    101 &     190 &  429 &   486 & 1104\\
%\hline
%\# of tree nodes   &  85  &    110 &     337 &  543 &   660 &  958\\
\hline
\# of initial SCDs &  35  &     12 &     108 &   76 &   125 &  403\\
\hline
 memory requirement &  15(1)  &  16(1) & 74(3) &   60(1) &    71(1) &  232 (5)\\
\hline 
accu. memory req.  & 348(39) &  175(24) & 5078(156) &  767(27) &  1166(60) & 
                                                                    6029(173)\\
\hline
memory size ratio  & 23.2 &    10.9&    68.6 & 12.8 &  16.4 & 26.0\\
\hline
analysis time(sec) & 0.60 &   0.27 &    3.57 & 1.82 &  4.40 & 23.25\\
\hline 
lines / sec        & 138  &   374  &      94 & 235  &  110  & 47\\
\hline
total time(sec)    & 1.83 &   1.50 &    7.26 & 6.58 & 12.87 & 37.95\\
\hline
\end{tabular}
\end{center}
\caption{Analysis time}
\label{analysis}
\end{table}

Fig.~\ref{size} shows the performance of redundant communication optimization
of the analyzer. The performance metric is the number of elements in the 
communication. The results are obtained under the assumption that
the programs are executed on a 16 PE system and all arrays use 
the CYCLIC distribution.
% and the PEs are distributed as 16 for 1 dimension arrays, 
%$4\times 4$ for two dimensional arrays, $4\times 2\times 2 $ for three 
%dimensional arrays and $2\times 2\times 2 \times 2 \times 2$ for 4 dimensional
%arrays. 
This experiment is conducted using the test input size provided by
the SPEC95 benchmark for program TOMCATV, SWIM, MGRID, ERHS. Problem 
sizes of $6\times 100$ for L18 and $402\times 160$ for ARTDIF are used.
Some programs, such as MGRID, do not have any
optimization opportunities for redundant communication optimization. 
Others, such as the TOMCATV program, present many opportunities 
for optimizations. On an average, for the six programs, 30.5\% of
the total communication elements are reduced by the analyzer. This indicates 
that global communication optimization results in large performance gain and
the analyzer is effective in finding optimization opportunities.
%The performance evaluation for message vectorization optimization
%will be included in the full paper.

\begin{figure}
\centerline{\psfig{figure=fig/size.eps,width=5in}}
\caption{The reduction of the number of array elements in communications}
\label{size}
\end{figure}

%% TO BE WRITE LATER

This section introduced a communication descriptor that can represent
logical communication patterns in a program, describes the propagation rules
used in an array data flow analyzer for communication optimization and reports
the experimence with the analyzer. This tool is able to generate logical 
communication patterns in both optimized and non--optimized codes for a
program. Algorithms to derive physical communiation patterns from the
logical communication descriptors remain to be developed.
In the next section,  connection
scheduling algorithms for handling static patterns in compiled communication
are presented.


\subsection{Connection scheduling algorithms for compiled communication}

Compiled communication uses off--line algorithms to perform
connection scheduling. This section presents the connection scheduling 
algorithms and their performance evaluation. These algorithms assume
a torus topology. However, the foundamental principles of the algorithms
can be extended to other topologies. 

For a given network, a set of connections that do not share any link is called
a configuration. In an optical TDM network with path multiplexing, 
multiple configurations can be supported simultaneously. Specifically, for 
a network with multiplexing degree $K$, $K$ configurations can be 
established concurrently. Thus, for a given communication pattern,
realizing the communication 
pattern with a minimum multiplexing degree is equivalent to  determining the
minimum number of configurations that contain all the connections in 
the pattern. Next,
 some definitions will be presented to formally state 
the problem of
connection scheduling. A connection request from a source $s$ 
to a destination $d$ is denoted as $(s, d)$.

\begin{description}
\item A pair of connection requests $(s_1, d_1)$ and $(s_2, d_2)$ are said
to {\bf conflict}, if they cannot be simultaneously established because
they are using the same link.

\item A {\bf configuration} is a set of connection requests
$\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ such that
no requests in the set conflict.

\item Given a set of communication requests  
$R = \{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$,
the set $MC =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} is a
{\bf minimal configuration set} for $R$ iff: \\
$\bullet$ 
each $C_i \in MC$ is a configuration and each request 
$(s_{i}, d_{i}) \in R$ 
is contained in exactly one configuration in $MC$; and \\
$\bullet$
each pair of configurations $C_i,C_j \in MC$ contain requests 
$(s_i, d_i) \in C_i$ and $(s_j, d_j) \in C_j$ such that
$(s_i, d_i)$ conflicts with $(s_j, d_j)$.
\end{description}

Thus, the goal of connection scheduling heuristics is to compute a
minimal configuration set for a given request set $R$.
Next I present three such heuristics.

%
\subsubsection{Greedy algorithm}
%
In the greedy algorithm, a configuration is created by repeatedly 
putting connections into the configuration until no additional 
connection can be established in that configuration.
If additional requests remain, another configuration is created
and this process is repeated till all requests have been processed.
This algorithm is an modification of an algorithm proposed in \cite{Qiao94}.
The algorithm is shown in Fig.~\ref{SIMPLE}. The time complexity of
the algorithm is $O(|R|\times max_i(|C_{i}|)\times K)$, where $|R|$ 
is the number of the requests, $|C_{i}|$ is the number of connections
in configuration $C_{i}$ and $K$ is the number of configurations
generated.

\begin{figure}[htmb]
%\small
\begin{tabbing}
\hspace{1.0in}(1)\hspace{0.6in}MC = $\phi$, k = 1\\
\hspace{1.0in}(2)\hspace{0.6in}{\bf re}\={\bf peat}\\
\hspace{1.0in}(3)\hspace{0.6in}\> $C_{k} = \phi$\\
\hspace{1.0in}(4)\hspace{0.6in}\>{\bf for}\= {\bf each} $(s_{i}, d_{i}) \in R$\\
\hspace{1.0in}(5)\hspace{0.6in}\>\>{\bf if}\=\ $(s_{i}, d_{i})$ does 
                                   not conflict with any 
                                   connection in $C_{k}$ {\bf then}\\
\hspace{1.0in}(6)\hspace{0.6in}\>\>\>$C_{k} = C_{k} \bigcup$ { $(s_{i}, d_{i})$ }\\
\hspace{1.0in}(7)\hspace{0.6in}\>\>\>R = R - { $(s_{i}, d_{i})$ }\\
\hspace{1.0in}(8)\hspace{0.6in}\>\>{\bf end if}\\
\hspace{1.0in}(9)\hspace{0.6in}\>{\bf end for}\\
\hspace{1.0in}(10)\hspace{0.6in}\>MC = MC $\bigcup$ { $C_{k}$ }\\
\hspace{1.0in}(11)\hspace{0.6in}{\bf until} $R = \phi$\\
\end{tabbing}
\normalsize
\caption{The greedy algorithm.}
\label{SIMPLE}
\end{figure}

For example consider the linearly connected nodes shown in Fig.~\ref{EXAM}. 
The result for applying the greedy algorithm to schedule connection requests 
set \{(0, 2), (1, 3),(3, 4), (2, 4)\} is shown in Fig.~\ref{EXAM}(a). 
In this case, (0, 2) will be in time slot 1, (1, 3) in time slot 2, (3, 4) 
in time slot 1 and (2, 4) in time slot 3. 
Therefore, multiplexing degree 3 is needed to establish the paths for the 
four connections.  However,  as shown in Fig.~\ref{EXAM} (b), 
the optimal scheduling for the four connections, which can be obtained
by considering the connection in different order, is to schedule (0, 2) in 
slot 1, (1, 3) in slot 2, (3, 4) in slot 2 and (2, 4) in slot 1. 
The second assignment only use 2 time slots to establish all the connections. 

\begin{figure}[htbp]
\begin{center}
\begin{picture}(0,0)%
\special{psfile=/afs/cs.pitt.edu/usr0/xyuan/research/paper/write/962SC96/fig/961.3.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0050in}%
\begin{picture}(920,120)(75,640)
\end{picture}

\end{center}
\caption{Scheduling requests (0, 2), (1, 3),(3, 4), (2, 4)}
\label{EXAM}
\end{figure}


\subsubsection{Coloring algorithm}

The greedy algorithm  processes the requests in an arbitrary order.
In this section, I will describe an algorithm that applies a heuristic 
to determine the order in which the process the connection requests.
The heuristic assigns higher priorities to connection requests with fewer
conflicts. By giving the requests with less conflicts higher priorities, 
each configuration is likely to accommodate more requests and thus the
multiplexing degree needed for the patterns is likely to decrease. 

The problem of computing the minimal configuration set is formalized
as a graph coloring problem. A coloring of a graph is an assignment of 
a color to each node of the graph in such a manner that no two nodes 
connected by an edge have the same color. A conflict graph for a set of
requests is built in the following manner, (1) 
each node in the graph 
corresponds to a connection request and (2) an edge
is introduced between two nodes if the requestes represented by the 
two nodes are conflicted.
As stated by the theorem given below,
the number of colors used to color the graph is the number of 
configurations needed to handle the connection requests. 

\begin{description}
\item
{\bf Theorem:} Let $R=\{(s_{1}, d_{1}),(s_{2}, d_{2}),...,(s_{m}, d_{m})\}$
be the set of requests and $G = (V, E)$ be the conflict graph for $R$. 
There exists a configuration set $M = \{C_{1}, C_{2}, ..., C_{t}\}$
for $R$ if and only if $G$ can be colored with $t$ colors.
\end{description}

%Prove: ($\Rightarrow$) Assuming R has 
%configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. Let 
%$(s_{i}$, $d_{i}) \in C_{k}$, node $n_{i}$ can be colored by color $k$. 
%Therefore, there are totally $t$ colors in the graph. Now, we need to prove
%that for any two node $n_{i}$, $n_{j}$ such that $(i, j) \in E$, the two nodes
%are colored by different color. By the construction of conflict graph,
%if $(i, j) \in E$, node $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ share same links,
%hence, by the construction of configuration, $(s_{i}, d_{i})$ and
%$(s_{j}, d_{j})$  is in different configuration, thus $n_{i}$ and $n_{j}$ is
%colored by different colors. Hence, G can be colored by $t$ colors.
%
%($\Leftarrow$) Assuming G can be colored by $t$ colors. Let 
%$C_{i}$ = {$( s_{j}, d_{j})$ : $n_{j}$ is colored by color j}, 
%$M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. To prove 
%M is a configuration for R, we need to prove 1) for any $(s_{i}, d_{i}) \in R$,
%there exists a $C_{k}$ such that $(s_{i}, d_{i}) \in C_{k}$, and 2) $C_{k}$
% must
%be a configuration. The first condition is trivial. Now, let us consider
%the second condition. Let $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ belong to 
%$C_{k}$, by the construction the G, $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$
%do not share any links. Therefore, $C_{k}$ is a configuration. Hence, there
%exist configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} for R. $\Box$
%
%\begin{description}
%\item
%{\bf Corollary:} The optimal multiplexing degree for establishing 
%connections in $R$ is equivalent to the minimum number colors to 
%color graph $G$.
%\end{description}


Thus, our 
coloring algorithm attempts to minimize the number of colors used in 
coloring the graph. Since the coloring problem is known to be NP-complete, 
a heuristic is used for graph coloring. Our heuristic determines the order 
in which nodes are colored using the node priorities.
The algorithm is summarized in Fig~\ref{COLOR}. It should be noted that
after a node is colored, our algorithm updates the priorities of uncolored 
nodes. This is because in computing the degree of an uncolored node, 
only  the edges that connect the node to other uncolored nodes are 
considered. 
The algorithm finds a solution in linear time (with respect to the 
size of the conflict graph). The time complexity of the algorithm is 
$O(|R|^2\times max_i(|C_{i}|)\times K)$, where $|R|$ is the number of the 
requests, $|C_{i}|$ is the number of requests in configuration $C_{i}$ and 
$K$ is the total number of configurations generated.


\begin{figure}[htbp]
%\small
\begin{tabbing}
\hspace{1.0in}(1)\hspace{0.6in} Construct conflict graph G = (V, E)\\
\hspace{1.0in}(2)\hspace{0.6in} Calculate the priority for each node\\
\hspace{1.0in}(3)\hspace{0.6in} MC = $\phi$, k = 1\\
\hspace{1.0in}(4)\hspace{0.6in} NCSET = V\\
\hspace{1.0in}(5)\hspace{0.6in} {\bf re}\={\bf peat}\\
\hspace{1.0in}(6)\hspace{0.6in} \>Sort NCSET by priority\\
\hspace{1.0in}(7)\hspace{0.6in} \> WORK = NCSET\\
\hspace{1.0in}(8)\hspace{0.6in} \> $C_{k} = \phi$\\
\hspace{1.0in}(9)\hspace{0.6in} \>{\bf wh}\={\bf ile} (WORK $\ne \phi$)\\
\hspace{1.0in}(10)\hspace{0.6in} \>\> Let $n_{f}$ be the first 
                                      element in WORK\\
\hspace{1.0in}(11)\hspace{0.6in} \>\>$C_{k} = C_{k} \bigcup \{<s_{f}, d_{f}>\}$\\
\hspace{1.0in}(12)\hspace{0.6in} \>\>NCSET = NCSET $- \{n_{f}\}$\\
\hspace{1.0in}(13)\hspace{0.6in} \>\>{\bf fo}\={\bf r} {\bf each}  $n_{i} \in NCSET$ 
                                      and $(f, i) \in E$ {\bf do} \\
\hspace{1.0in}(14)\hspace{0.6in} \>\>\> update the priority of $n_{i}$\\
\hspace{1.0in}(15)\hspace{0.6in} \>\>\> WORK = WORK - $\{n_{i}\}$\\
\hspace{1.0in}(16)\hspace{0.6in} \>\>{\bf end for}\\
\hspace{1.0in}(17)\hspace{0.6in} \>{\bf end while}\\
\hspace{1.0in}(18)\hspace{0.6in} \>MC = MC + $\{C_{k}\}$\\
\hspace{1.0in}(19)\hspace{0.6in} {\bf until} NCSET = $\phi$
\end{tabbing}
\normalsize
\caption{The graph coloring heuristic.}
\label{COLOR}
\end{figure}

For torus and mesh networks, a suitable choice for priority for a
connection request is the ratio of the number of links in the path 
from the source to the destination and the degree of the node 
corresponding to the request in $G$. 
Applying the coloring algorithm to the example in Fig.~\ref{EXAM},
in the first iteration, the request is reordered as 
$\{(0, 2), (1, 3), (2, 4), (3, 4)\}$ and connections (0, 2), (2, 4) will be
put in time slot 1. In the second iteration, connections (1, 3), (3, 4) are
put in time slot 2. Hence, applying the 
coloring algorithm will use 2 time slots
to accommodate the requests.


\subsubsection{Ordered AAPC algorithm}

The graph coloring algorithm has better performance than the greedy heuristic.
However, for dense communication patterns the heuristics cannot guarantee that
the multiplexing degree found would be bounded by the minimum multiplexing 
degree needed to realize the all-to-all pattern. The algorithm described in 
this section targets dense communication patterns. By grouping the connection
requests
in a more organized manner, better performance can be achieved for dense 
communication.

The worst case of arbitrary communication is the {\em all-to-all personalized 
communication} (AAPC)  where each node sends a message to every 
other node in the system. Any communication pattern can be embedded in AAPC. 
Many algorithms \cite{Hinrichs94,Horie91} have been designed to 
perform AAPC efficiently for different topologies.
Among these algorithms, the ones that are of 
interests to us are the phased AAPC algorithms, in which the AAPC connections 
are partitioned into contention--free phases. A phase in this kind of AAPC 
corresponds to a configuration. Some phased AAPC algorithms are optimal in
that every link is used in each phase and every connection follows the
shortest path. Since all the connections in each AAPC phase are contention--free,
they form a configuration that uses all the links in the system. 
Each phase in the phased AAPC communication forms an {\em AAPC configuration}.
The set of {\em AAPC configurations} for AAPC communication pattern is 
called {\em AAPC configurations set}. 
The following theorem states the property 
of connection scheduling using AAPC phases.

\begin{description}
\item {\bf Theorem: } Let $R =
\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$be the 
set of requests, if $R$ can be partitioned into $K$ phases 
$P_1 = \{(s_{1}, d_{1}), ... , (s_{i_{1}}, d_{i_{1}})\}$, 
$P_2 = \{(s_{i_{1} + 1}, d_{i_{1} + 1}), ... , (s_{i_{2}}, d_{i_{2}})\}$,
... ,\\
 $P_K = \{(s_{i_{K-1} + 1}, d_{i_{K-1} + 1}), ... , 
(s_{i_{K}}, d_{i_{K}})\}$, such that $P_i$, $ 1 \le i \le K$, is a subset
of an AAPC configuration. Using the greedy algorithm to schedule the
connections results in multiplexing degree less than or equal to K.
\end{description}

The theorem states that if the connection requests
are reordered by the AAPC phases,
at most all AAPC 
phases are needed to realize arbitrary pattern using the
greedy scheduling algorithm. For example, following the algorithms in 
\cite{Hinrichs94}, $N^3/8$ phases are needed for a $N\times N$ torus. 
Therefore, in a $N\times N$ torus, $N^3/8$ degree is enough to satisfy
any communication pattern.

To obtain better performance on dense communication patterns, it is 
better to keep the connections in their AAPC format as much as possible. 
It is therefore better to schedule the phases with higher link 
utilization first. This heuristic is used in the ordered AAPC algorithm.
In ordered AAPC algorithm, the rank of the AAPC phases is calculated so 
that the phase that has higher utilization has higher rank. The phases 
are then scheduled according to their ranks. The algorithm is depicted in 
Figure~\ref{ORDAAPC}. The time complexity of this algorithm is
$O(|R|(lg(|R|) + max_i(|C_{i}|)\times K))$, where $|R|$ is the number of 
the requests, $|C_{i}|$ is the number of requests in configuration
$C_{i}$ and $K$ is the number of configurations needed. The advantage 
of this algorithm is that for this algorithm the multiplexing degree 
is bounded by $N^3/8$. Thus, in situations where the greedy or coloring
heuristics fail to meet this bound, AAPC can be used.  

\begin{figure}[ht]
%\small
\begin{tabbing}
\hspace{1in}(1)\hspace{0.6in}PhaseRank[*] = 0\\
\hspace{1in}(2)\hspace{0.6in}{\bf for}\= $(s_{i}, d_{i}) \in R$ {\bf do}\\
\hspace{1in}(3)\hspace{0.6in}\>let $(s_{i}, d_{i}) \in A_{k}$\\
\hspace{1in}(4)\hspace{0.6in}\>PhaseRank[k] = PhaseRank[k] + length($(s_{i}, d_{i})$)\\
\hspace{1in}(5)\hspace{0.6in}{\bf end for}\\
\hspace{1in}(6)\hspace{0.6in}sort phase according to PhaseRank\\
\hspace{1in}(7)\hspace{0.6in}Reorder R according the sorted phases.\\
\hspace{1in}(8)\hspace{0.6in}call greedy algorithm\\
\end{tabbing}
\caption{Ordered AAPC scheduling algorithm}
\label{ORDAAPC}
\normalsize
\end{figure}

\subsubsection{Performance for the scheduling algorithms}

In this section,  the performance of the connection scheduling
algorithms on $8\times 8$ torus topology is studied. 
The performances of the algorithms 
are evaluated using randomly generated communication patterns, patterns
encountered during data redistribution, and some frequently used 
communication patterns. The metric used to compare the algorithms is the 
multiplexing degree needed to establish the connections.
It should be noted that a dynamic scheduling algorithm will not perform
better than the greedy algorithm since it must establish the connections 
by considering the requests in the order that they arrive. 

A {\em random communication pattern} consists of a certain number of 
random connection  requests. A random connection request is obtained
by randomly generating a source and a destination. Uniform probability
distribution is used to generate the randomly sources and destinations.
The {\em data redistribution communication patterns} are obtained by
considering the communication results from array redistribution. In this
study,   data redistributions of a 3D array are considered. The array
has block--cyclic distribution in each dimension. The distribution of a
dimension can be specified by the block size and the number of processors
in the dimension.  A distribution is denoted  as {\em p:block(s)}, where
$p$ is the number of processors in the distribution and $s$ is the block size.
When the distribution of an
array is changed (which may results from the changing of the value $p$ or 
$s$), communication may be needed. 
Many programming
languages for supercomputers, such as CRAFT FORTRAN, allow an array to be
redistributed within the program. 

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
number of  & greedy  & coloring & AAPC & combined &improvement\\
connections. & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
100  & 7.0  & 6.7 & 6.9 & 6.6 & 6.3\%\\
\hline
400 & 16.5  & 16.1 & 16.5 & 15.9 & 3.8\%\\
\hline
800 & 27.2  & 25.9 & 26.5 & 25.6 & 6.3\%\\
\hline 
1200 & 36.3 & 34.5 & 35.3 & 34.2 & 6.1\%\\
\hline
1600 & 45.0  & 43.5 & 43.4 & 42.8 & 5.1\%\\
\hline
2000 & 53.4  & 50.4 & 50.4 & 49.7 & 7.4\%\\
\hline
2400 & 60.8  & 57.5 & 57.4 & 56.7 & 7.2\%\\
\hline
2800 & 68.8  & 64.4 & 62.4 & 62.4 & 10.2\%\\
\hline
3200 & 76.3  & 70.8 & 64 & 64 & 19.2\%\\
\hline
3600 & 83.9  & 76.8 & 64 & 64 & 31.1\%\\
\hline
4000 & 91.6  & 83 & 64 & 64 & 43.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Performance for random patterns}
\label{RANDOM}
\end{table}



Table~\ref{RANDOM} shows the multiplexing degree required to establish
connections for random communication patterns using the algorithms
presented.
The results in each row are the averages obtained from scheduling 100 different
randomly generated patterns with the specific number of connections.
The results in the column labeled {\em combined algorithm} are obtained by using 
the minimum of  the coloring algorithm and
the AAPC algorithm results.
Note that in compiled communication, more time can be spent
to obtain better runtime network utilization. Hence,  the
combined algorithm can be used to obtain better result by the compiler. The 
percentage  improvement shown in the sixth column
is achieved by the combined algorithm over the
dynamic scheduling. 
It is  observed that the coloring algorithm is always
better than the greedy  algorithm
and the AAPC algorithm is better than 
the other algorithms when the communication is dense. 
It can be seen that for sparse random patterns (100 - 2400
connections), the 
improvement range varies from 3.8\% to 7.2\%. Larger improvement
results for dense communication.  For example, the combined algorithm
uses 43.1\% less multiplexing degree than that of the greedy algorithm
for all--to--all pattern. 
This result confirms the result in \cite{Hinrichs94} that
it is desirable to use compiled communication for dense communication.


\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
No. of  & No. of  & greedy   & coloring & AAPC & combined &improvement\\
connections & patterns & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
0 - 100 & 34  & 1.2 &  1.2 & 1.2 & 1.2 & 0.0\%\\
\hline
101 - 200 & 50 & 5.9 & 4.9 & 4.8 & 4.6 & 28.3\%\\
\hline
200 - 400 & 54 & 10.6 &  9.7 & 10.0 &  9.5 & 11.6\%\\
\hline 
401 - 800 & 105 & 17.7 & 15.9 & 16.0& 15.5 & 14.2\%\\
\hline
801 - 1200 & 122 & 31.7 & 28.7 & 28.6 & 27.6 & 14.9\%\\
\hline
1201 - 1600 & 0  & 0      & 0    & 0    &0     &    0\%\\
\hline
1601 - 2000 & 15 & 46.3 &  42.8 & 35.1 & 35.1 & 31.9\%\\
\hline
2001 - 2400 & 77 & 55.5 &  51.5 & 51.9 & 50.4 & 10.1\%\\
\hline
2401 - 4031 & 0  & 0       & 0    & 0     &   0   &  0\% \\
\hline
4032     & 43 & 92  & 83 & 64 &  64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for data distribution patterns}
\label{REDIST}
\end{table}



To obtain more realistic results,  the performance is also evaluated using
the communication patterns for data redistribution and some
frequently used communication patterns.
Table~\ref{REDIST} shows the performance of the algorithms for data 
redistribution patterns. The communication patterns
 are obtained by extracting from the communication resulted from 
the random data redistribution of a 3D array of size
 $64 \times64 \times 64$. 
The  random data redistribution is created by randomly generating
the source data distribution and
the destination data distribution with regard to
the number of processors allocated to each dimension and the block size
in each dimension. Precautions are taken to make sure that the 
total processor number is 64 and the block size is not too large so that
some processors do not contain any part of the array.
The table lists the results for 500 random data redistributions. The first
column lists the range of the number of connection requests in each pattern.
The second column list the number of data redistrictions whose number of 
connection request fell into the range. For example, the second column in the
last row indicates that among the 500 random data redistributions, 43
results in 4032 connection requests. The third, fourth, fifth and sixth column
list the multiplexing degree required by the greedy algorithm, the coloring
algorithm, the AAPC algorithm and the 
combined algorithm respectively. The seventh 
column list the improve ratio by the combined algorithm over the greedy
algorithm.
The result shows that the 
 multiplexing degree required to establish connections resulting
from data redistribution is less than the random communication patterns. 
For the data redistribution pattern, the improvement ratio obtained by using
the combined algorithm  ranges from
10.1\% to 31.9\%, which is larger than the improvement ratio for the random 
communication patterns.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Pattern & No. of conn. & greedy  & coloring & AAPC & comb & ratio\\
\hline
ring    & 128 &  3  &  2 & 2  &  2   &   50\%\\
\hline
nearest neighbor & 256 & 6  & 4 & 4  &  4 & 50\%   \\
\hline
hypercube & 384 &  9 & 7 & 8 &  7 & 28.6\%\\
\hline
shuffle--exchange & 126 &  6& 4 & 5  & 4 & 50\% \\
\hline 
all--to--all & 4032 &  92 & 83 & 64 & 64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for frequently used patterns}
\label{FUSED}
\end{table}

Table~\ref{FUSED} shows the performance for some
frequently used communication patterns. In the ring and the
nearest neighbor patterns, no conflicts arise in the links. 
However, there are conflicts in the communication switches.
The performance gain is higher for these 
specific patterns when the combined algorithm is used.



\section{Conclusion and remaining work}

The advances of architectures that exploit massive parallelism has
created new requirements for interconnection networks. While optical networks
potentially offer large bandwidths, the slow network control has hindered the
full exploitation of the large bandwidth available in fiber optics. This
research 
proposes two methods to address this problem: (1)  efficient distributed
path reservation protocols that reduce control overhead via protocol design
and (2) compiled communication that reduces control overhead by shifting
the control overhead into compile time processing.

A summary of  what has been done and what remains to be
done in this research is listed below.

\begin{itemize}
\item Dynamic communication, work done.
  \begin{itemize}
    \item Various distributed path reservation algorithms have been
          developed. The algorithms assume deterministic routing. A 
          network simulator has been
          developed to evaluate the algorithms. The performance
          of the protocols and the impact of system parameters on
          the protocols have been studied. \cite{yuan97}
  \end{itemize}

\item Dynamic communication, work remaining.
  \begin{itemize}
    \item   Distributed path reservation schemes with adaptive routing and 
      their performance study remain to be done.
  \end{itemize}

\item Compiled communication, work done.
  \begin{itemize}
    \item A communication descriptor which can describe
      logical communication patterns
      in a program has been
      developed and a data flow analyzer for communication
      optimizations using that descriptor has been designed and implemented.
      \cite{Yuan96a}
    \item Connection scheduling algorithms for torus topologies have been
      developed and evaluated. \cite{Yuan96}
    \item A simulator which simulates compiled communication for the 
      communication
      patterns that are known at compile time has been developed.
  \end{itemize}
\item Compiled communication, work remaining.
  \begin{itemize}
    \item Algorithms to derive physical communication patterns from the
      communication descriptor are needed.  
    \item Interprocedural data flow analysis for communication optimization
      needs to be done. 
      Our previous work performs message vectorization and redundant 
      communication elimination within each procedure. This framework
      can be extended to incorporate more optimizations and to the
      interprocedural analysis.
    \item Efficient logical topologies for the communication patterns that 
          are unknown at compile time needs to be designed and evaluated.
    \item A network simulator which simulates multi--hop communication is to be
      designed to study the performance of the logical topologies. 
    \item Experiments to study the performance of compiled communication is 
     to be carried out.
  \end{itemize}
\item Dynamic communication versus compiled communication.
  A comparison between these two control mechanisms is to be carried out.
\end{itemize}



The study of distributed path reservation protocols will provide  general
guidelines for designing optical interconnection networks
 with dynamic network control.
The study of compiled communication will address issues involved when applying
compiled communication and give insights into the advantages and 
the limitations of compiled communication. Some results of this research
can also be applied
to electronic networks, since statically managing network resources
can also improve communication performance in those networks. In addition,
other factors  that affect the communication performance will
also be studied and understood in this research.















\chapter{Dynamic single--hop communication}
\label{single}

This chapter discusses the path reservation protocols for dynamic 
single--hop communication. Two types of 
distributed path reservation protocols, the {\em forward path reservation
protocols} and the {\em backward path reservation protocols},  
 have been designed for 
point--to--point optical TDM networks. A 
network simulator that simulates all the protocols has been developed 
and has been used to 
study the performance of the two types of protocols and to evaluate
the impact of system parameters such as the control packet processing
time and the message size on the protocols.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/singleexam.eps,width=4.5in}}
\caption{An optical network with distributed control.}
\label{singleexam}
\end{figure}

In order to support a distributed control mechanism for connection
establishment, it is assumed that in addition to the optical data network,
there is a logical {\em shadow network} through which
control messages are communicated. 
The shadow network has the same physical topology as the data network.
The traffic on the shadow network consists of small control packets
and thus is much lighter than the traffic on the data network. 
The shadow network  operates in packet switching mode; routers at 
intermediate nodes examine control packets and update local bookkeeping
information and switch states accordingly. 
The shadow network can be implemented as an 
electronic network or alternatively a virtual channel on the data network
can be reserved exclusively for exchanging control messages.
Figure~\ref{singleexam} shows the network architecture.
A virtual channel in the optical data network corresponds to a time slot. 
It is  also assumed that a node can send or receive messages through
different virtual channels simultaneously. 


A path reservation protocol ensures that the path from a source node
to a destination node is reserved before the connection is used. A path 
includes the virtual channels on the links that form the connection, the
transmitter at the source node and the receiver at the destination node.
Reserving the transmitter and the receiver is the same as reserving a
virtual channel on the link from a node to the switch attached to that
node. Hence, only the reservation of virtual channels on links forming a
connection with path multiplexing will be considered.
There are many options available with respect to different aspects of the 
path reservation mechanisms. These are discussed next.

\begin{itemize}

%\noindent
%$\bullet$
\item {\em Forward reservation} versus {\em backward reservation}.
Locking mechanisms are needed by the distributed path reservation
protocols  to ensure the exclusive usage of a virtual channel 
for a connection. This variation characterizes the timing at which
the protocols  perform the locking.
Under forward reservation, virtual channels are locked 
by a control message that travels from
the source node to the destination node.
Under backward reservation, a control message travels to the
destination to probe the path, then virtual channels that are found to be
available are locked by another
control message which travels from the destination node to the source node.

%\noindent 
%$\bullet$
\item {\em Dropping} versus {\em holding}. This variation characterizes
the behavior of the protocol when it 
determines that a connection establishment does not progress.
Under the  dropping approach, once the protocol
determines that  the establishment of a connection is not progressing,
it releases the virtual channels  locked on the partially established
path and informs the source node that the reservation has failed.
Under the holding approach, when the protocol determines
that  the establishment of a connection is not progressing,
it keeps the virtual channels  on the partially established path locked for
some period of time, hoping that during this period, the reservation
will progress. If, after this timeout period, the reservation still does not
progress, the partial path is then released and the
source node is informed of the failure.
Dropping can be viewed as holding with holding time equal to zero.

%\noindent 
%$\bullet$
\item {\em Aggressive} reservation versus {\em conservative} reservation. This
variation characterizes the protocol's treatment of each reservation. Under
the aggressive reservation, the protocol tries to establish a connection
by locking as many virtual channels as possible during the reservation process.
Only one of the locked channels is then used for the connection, while the
others are released.
Under the  conservative reservation approach, the protocol
locks only one virtual channel during the reservation process.

\end{itemize}

\subsection*{Deadlock}

Deadlock in the control network can arise from two sources.
First, with limited number of buffers, a request loop can be formed within the
control network.
Second, deadlock can occur when a request is holding (locking)
virtual channels on some links while requesting other channels on other
links.
This second source of deadlock can be avoided by the dropping or holding mechanisms
described above.
Specifically, a request will give up all the locked channels if 
it does not progress within a certain timeout period.

Many deadlock avoidance or deadlock prevention techniques for 
packet switching networks proposed in the literature \cite{Dally87} 
can be used to deal with deadlock within the control network (the
first source of deadlock).
Moreover, the control network is under light traffic, and
each control message consists of only a single packet of small size 
(4 bytes). Hence, it is feasible to provide a large number of buffers in each 
router to reduce or eliminate the chances of deadlocks.

\subsection*{States of virtual channels}

The control network router at each node maintains a state for each
virtual channel on links connected to the router. For forward reservation,
the control router maintains the states for the outgoing links.
% while
% in backward reservation, the control router maintains the states
% for the incoming links. 
As discussed later, this enables the router to have the information
needed for reserving virtual channels and updating the switch states.
A virtual channel, $V$, on link $L$, can be in one of the following states:

\begin{itemize}
\item $AVAIL$: indicates that the virtual channel $V$ on link $L$
is available and can be used to establish a new connection,
\item $LOCK$: 
indicates that $V$ is locked by some request in the process of establishing
a connection.
\item $BUSY$: indicates that $V$
is being used by some established connection to transmit data.
\end{itemize}

For a link, $L$, the set of virtual channels that are in the $AVAIL$ state is
denoted as $Avail(L)$. When a virtual channel, $V$, is not in $Avail(L)$,
an additional field, $CID$, is maintained to identify the connection request
locking  $V$, if $V$ is in the $LOCK$ state, or the connection using $V$, if $V$
is in the $BUSY$ state.

\section{Forward reservation schemes}

In the connection establishment protocols,
each connection request is assigned a unique identifier, $id$, which
consists of the identifier of the source node and a serial number
issued by that node. 
Each control message related to the establishment of a connection carries its
$id$, which becomes the identifier of the connection when it is successfully
established. It is this $id$ that is maintained in the $CID$ field of
locked or busy virtual channels on links.
Four types of packets are used in the forward reservation
 protocols to establish a connection.

\begin{itemize}

%\noindent 
%$\bullet$
\item {\em Reservation packets} ($RES$), used to reserve virtual channels.
In addition to the connection $id$, a $RES$ packet contains a bit vector,
$cset$, of size equal to the number of virtual channels in each link.
The bit vector $cset$ is used to keep track of the set of virtual channels 
that can be used to satisfy the connection request carried by $RES$.
These virtual channels are locked
at intermediate nodes while the $RES$ message
progresses towards the destination node. The switch
states are also set to connect the locked channels on the input and output links.

%\noindent 
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$), used to inform source nodes of the
success of connection requests.
An $ACK$ packet contains a $channel$ field which indicates the virtual
channel selected for the connection.
As an $ACK$ packet travels from the destination to the source, it changes
the state of the virtual channel 
selected for the connection to $BUSY$, and unlocks
(changes from $LOCK$ to $AVAIL$)
all other virtual channels that were locked by the corresponding $RES$ packet.

%\noindent 
%$\bullet$
\item {\em Fail or Negative ack packets} ($FAIL/NACK$), used to inform source
nodes of the failure of connection requests. While traveling back to the source
node, a $FAIL/NACK$ packet unlocks all virtual channels that were locked by the
corresponding $RES$ packet.
 
%\noindent 
%$\bullet$
\item {\em Release packets} ($REL$),  used to release connections.
A $REL$ packet traveling from a source to a destination changes the
state of the virtual channel
reserved for that connection from $BUSY$ to $AVAIL$.

\end{itemize}

The protocols require that control packets from a destination, $d$, to a source, $s$,
follow the same paths (in opposite directions) as packets from $s$
to $d$.
The fields of a packet will be denoted by $packet.field$.
For example, $RES.id$ denotes the $id$ field of the $RES$ packet.

The forward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $RES$ packet with $RES.cset$ set to the
virtual channels that the node may use. This message is then routed to the
destination. When an intermediate node receives the $RES$ packet, 
it determines the next outgoing link, $L$, on the path to the destination, and
updates $RES.cset $ to $ RES.cset \cap Avail(L)$.
If the resulting $RES.cset$ is empty,
the connection cannot be established
 and a $FAIL/NACK$ message is sent back to the
source node. The source node will retransmit the request after some
period of time.
This process of failed reservation is shown in Figure~\ref{FORWARD}(a). 
Note that if $Avail(L)$ is represented by a bit-vector, then
$RES.cset \cap Avail(L)$ is a bit-wise "$AND$" operation.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/forward.pstex,height=2.2in}}
\caption{Control messages in forward reservation}
\label{FORWARD}
\end{figure}

If the resulting $RES.cset$ is not empty, the router reserves all the 
virtual channels in $RES.cset$ on link $L$ by changing their states to $LOCK$
and updating $Avail(L)$.
The router will then set the switch state to connect the virtual channels in the
resulting $RES.cset$ of the corresponding incoming and outgoing links.
Maintaining the states of outgoing links is sufficient for these two
tasks.
The $RES$ message is then forwarded to the next node on the path to the destination.
This way,
as $RES$ approaches the destination, the 
path is reserved incrementally. Once $RES$ reaches the
destination with a non-empty $RES.cset$, the destination selects from 
$RES.cset$ a virtual channel to be used for the connection and informs
the source node that the channel is selected by sending an $ACK$ message 
with $ACK.channel$ set to the selected virtual channel.
The source can start sending data once it 
receives the $ACK$ packet. After all data is sent, the source
node sends a $REL$ packet to tear down the connection. This successful
reservation process is shown in Figure~\ref{FORWARD}~(b). Note that although
in the algorithm described above, the switches are set during the processing
of the $RES$ packet, they can instead be set during the processing of
the $ACK$ packet.

\noindent
{\bf Holding}: The protocol described above can be modified to 
use the holding policy instead of the dropping policy.
Specifically, when an intermediate node
determines that the connection for a reservation cannot be established, 
that is when $RES.cset \cap Avail(L) = \phi$, the node buffers the $RES$ packet
for a limited period of time. If within
this  period, some virtual channels in the original $RES.cset$ become
available, the $RES$ packet can then continue its journey. Otherwise, 
the $FAIL/NACK$ packet is  sent back to the source.
Implementing the holding policy 
requires each node to maintain a holding queue and
to periodically check that queue to determine if any of the virtual channels 
has become available. In addition, some timing 
mechanism must be incorporated in the routers to timeout 
held control packets. This increases the hardware
and software complexities of the routers.

\noindent
{\bf Aggressiveness}: 
The aggressiveness
of the reservation is reflected in the size of the 
virtual channel set, $RES.cset$, initially chosen by the source node.
In the most aggressive scheme, the source node sets
$RES.cset$ to $\{0, ..., N-1\}$, where $N$ is the number of 
virtual channels in the system. This ensures that the reservation
will be successful if there exists an available virtual channel on the path.
On the other hand, 
the most conservative reservation assigns
$RES.cset$ to include only a single virtual channel. In this case, the
reservation can be successful only when the virtual channel chosen by the
source node is available in all the links on the path. Although 
the aggressive scheme seems to have advantage over the conservative scheme,
it results in excessive locking of the virtual channels in the system. Thus, in
heavily loaded networks, this is expected to decrease the overall throughput.
To obtain optimal performance, the aggressiveness of the protocol should be
chosen appropriately between the most aggressive and the most conservative extremes.

The retransmit time  is another protocol parameter.
In traditional non--multiplexed networks, the retransmit time
is typically chosen randomly from a range [0,MRT], where MRT
denotes some maximum retransmit time.
In such systems, MRT must be set to a reasonably
large value to avoid live-lock. However, this may increase the average
message latency time and decrease the throughput.
In a multiplexed network, the problem of live-lock only 
occurs in the most aggressive scheme (non--multiplexed circuit switching
networks can be considered as  having a multiplexing degree of 1 and 
using aggressive reservation). 
For less aggressive schemes, the
live-lock problem can be avoided by changing the virtual channels selected in
$RES.cset$ when $RES$ is retransmitted.
Hence, for these schemes, a small retransmit time can be used.

\section{Backward reservation schemes}

In  the forward locking protocol, the initial decision concerning the 
virtual channels to be locked for a connection request is made in the 
source node without any information about network usage. The backward
reservation scheme tries to overcome this handicap by probing the network
before making the decision. In the backward reservation schemes,
a forward message is used to probe the availability of virtual channels.
After that,
the locking of virtual channels is performed by a backward message. 
The backward reservation scheme uses six types of control
packets, all of which carry the connection $id$, in addition to other
fields as discussed next:

\begin{itemize}
%\noindent
%$\bullet$
\item {\em Probe packets} ($PROB$) travel from sources to destinations 
gathering
information about virtual channel usage without locking any virtual channel.
A $PROB$ packet carries a bit vector, $init$,
to represent the set of virtual channels that are
available to establish the connection.

%\noindent
%$\bullet$
\item {\em Reservation packets} ($RES$) are similar to the $RES$ packets in the forward
scheme, except that they travel from destinations to sources, lock
virtual channels as they go through intermediate nodes, and set the
states of the switches accordingly.
A $RES$ packet contains a $cset$ field.

%\noindent
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$) are similar to $ACK$ packets in the forward
scheme except that they travel from sources to destinations.
An $ACK$ packet contains a $channel$ field.


%\noindent
%$\bullet$
\item {\em Fail packets} ($FAIL$) unlock the virtual channels locked by the
$RES$ packets in cases of failures to establish connections.

%\noindent
%$\bullet$
\item {\em Negative acknowledgment packets} ($NACK$) are
used to inform the source nodes of reservation failures.

%\noindent
%$\bullet$
\item {\em Release packets} ($REL$) are
used to release connections after the communication is completed.

\end{itemize}

Note that a $FAIL/NACK$ message in the forward scheme performs the functions
of both a $FAIL$ message and a $NACK$ message in the backward scheme. 

The backward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $PROB$ message with $PROB.init$ set to contain all
virtual channels in the system.
This message is then routed to the destination.
When an intermediate node receives the $PROB$ packet, 
it determines the next outgoing link, $L_f$, on the forward path to the
destination,  and updates $PROB.init $ to $PROB.init \cap Avail(L_f)$.
If the resulting $PROB.init$ is empty,
the connection cannot be established and a $NACK$ packet is sent back to the
source node.  The source node will try the reservation again after a certain 
retransmit time.
Figure~\ref{BACKWARD}(a) shows this failed reservation case.

If the resulting $PROB.init$ is not empty, the node 
forwards $PROB$ on $L_f$ to the next node. 
This way,
as $PROB$ approaches the destination, the virtual channels available
on the path are recorded in the $init$ set.
Once $PROB$ reaches the
destination,  the destination forms a $RES$ message with $RES.cset$
equal to a selected subset of $PROB.init$ and sends this message back
to the source node.
When an intermediate node receives the $RES$ packet, it determines the
next link, $L_b$, on the backward path to the source, and updates
$RES.cset $ to $RES.cset \cap Avail(L_b)$. 
If the resulting $RES.cset$ is empty, 
the connection cannot be established. In this case the node sends
a $NACK$ message to the source node to inform it of the failure,
and sends a $FAIL$ message to the 
destination to free the virtual channels locked
by $RES$. This process is shown in Figure~\ref{BACKWARD}~(b).

\begin{figure}[htp]
\centerline{\psfig{figure=fig/back.pstex,height=2.2in}}
\caption{Control messages in backward reservation}
\label{BACKWARD}
\end{figure}

If the resulting $RES.cset$ is not empty,
the virtual channels in $RES.cset$ are locked, the switch is set accordingly
and $RES$ is forwarded on $L_b$
to the next node.  When $RES$ reaches the source with a non-empty
$RES.cset$,
the source  selects a
virtual channel from the $RES.cset$ for the connection and sends
an $ACK$ message to the destination with $ACK.channel$ set to the
selected virtual channel. This $ACK$ message unlocks all the virtual channels 
locked by $RES$, except the one in $channel$.
The source node can start sending data as soon as it sends the $ACK$ message.
After all data is sent, the source
node sends a $REL$ packet to tear down the connection.
The process of successful reservation is shown in Figure~\ref{BACKWARD}(c).

\noindent
{\bf Holding}: Holding can be incorporated in the backward reservation scheme
as follows.
In the protocol, there are two cases that cause the reservation to fail. 
The protocol may determine that the reservation fails when processing
the $PROB$ packet. In this case, holding is not desirable because the PROB
packet is used to collect the channel usage information and holding could
reduce the precision of the information collected (the status of channels
on other links may change during the holding period).
%In this case, no holding is necessary since 
%no resources have yet been locked.
When the protocol determines that the 
reservation fails during the  processing of a
$RES$ packet, a holding mechanism
similar to the one used in the forward reservation scheme may be applied.

\noindent
{\bf Aggressiveness}:
The aggressiveness of the backward reservation protocols is reflected in the 
initial size of $cset$ chosen by the destination node.
The aggressive approach sets
$RES.cset$ equal to $PROB.init$, while the conservative
approach sets $RES.cset$ to contain a single virtual channel from $PROB.init$.
Note that if a protocol supports only the conservative scheme,
the $ACK$ messages may be omitted, and thus only five types of messages 
are needed. 
As in the forward reservation schemes, the 
retransmit time is a parameter in the backward schemes.

\section{Network simulator and experimental results}
\label{simulator}

A network simulator has been developed to simulate the behavior
of multiplexed torus networks. The simulator models the network with 
various choices of system parameters and protocols. Specifically, 
the simulator provides the following options for protocol parameters.

\begin{itemize}
\item {\em forward and backward} reservations, this determines which
protocol to be simulated.

\item {\em initial $cset$ size}: This parameter determines the
initial size of $cset$ in the reservation packet. 
It restricts the set of virtual channels under
consideration for a reservation. In forward schemes,
the initial $cset$ is chosen when the source node composes the RES packet.
Assuming that $N$ is the multiplexing degree in the system,
an $RES.cset$ of size $s$ is chosen by generating a random number,
$m$, in the range $[0,$N$ - 1]$, 
and assigning $RES.cset$ = $\{m\ mod\ N, m+1\ mod\ N..., N+s-1\ mod\ N\}$.
In the backward schemes, the initial $cset$ is set when
the destination node composes the $ACK$ packet. An $ACK.cset$ of size $s$ 
is generated in the following manner.
If the available set, $RES.INIT$,
has less available channels than $s$, the $RES.INIT$ is copied to $ACK.cset$.
Otherwise,  the available channels are represented in a linear
array and the method used in generating the $cset$  in the forward schemes
is used.

\item {\em timeout value}: This  value 
determines how long a reservation packet can be put in a waiting queue.
The dropping scheme can be viewed as a holding scheme with timeout time
equal to zero.

\item {\em maximum retransmit time} (MRT): 
This specifies the period after which a node will retry a
failed reservation. As discussed earlier,
this value is crucial for avoiding live-lock
in the most aggressive schemes. The actual retransmit time
is chosen randomly between 0 and  $MRT -1$.
\end{itemize}

Besides the protocol parameters, the simulator also allows the 
choices of various system parameters.

\begin{itemize}

\item {\em system size}: This specifies the size of the network. All our
simulations are done on torus topology.

\item {\em multiplexing degree}. 
This specifies the number of virtual
channels supported by each link. In our simulation, the multiplexing degree
ranges from 1 to 32.

\item {\em message size}: The message size directly affects  the time that
 a connection is kept before it is released.
In our simulations,  fixed size messages are assumed.

\item {\em request generation rate at each node (r)}: This specifies the traffic on
the network. The connection requests at each node are assumed to have a Poisson
inter-arrival distribution. When a request is
generated at a node, the destination of the request is generated randomly
among the other nodes in the system. When a generated request is blocked,
it is put into a queue, waiting to be re-transmitted.

\item {\em control packet processing and propagation time}: This specifies the speed of the
control networks. The control packet processing time is the time for an
intermediate node to process a control packet. The control packet
propagation time is the time for a control packet to be transferred from one node
to the next. It is assumed
 that all the control packets have the same
processing and propagation time.
\end{itemize}

In the following discussion, $F$ is used to denote forward reservation,
$B$ denotes the backward reservation, $H$ denotes 
holding and $D$ denotes dropping
schemes. For example, $FH$ means the forward holding scheme.
%A network simulator with various  control mechanisms 
%including FH, FD, BH and BD was implemented.
%Although the simulator can simulate both WDM and TDM torus networks, 
%only the results for TDM networks will be presented in this paper.
%The results for WDM networks follow similar patterns.
In addition to the options of backward/forward reservation and holding/dropping
policy, the simulation uses the following parameters.
The average latency and throughput are used to evaluate the protocols.
The latency is the period between the time when a message is ready and the time
when the first packet of the message is sent.
The  throughput is the number of messages received per time unit.
Under light traffic, 
the performance of the protocols is measured by the average message latency,
while under heavy traffic, the throughput 
is used as  the performance metric.  
The simulation time is measured in time slots, where a time slot is the
time to transmit an optical data packet between any two nodes in the network.
Note that in multiprocessor applications, nodes are physically close
to each other, and thus signal propagation time is very small (1 foot per
nsec) compared to the length of a message.
Finally, deterministic XY--routing is assumed in the torus topology.


\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Throughput]
  {\psfig{figure=eps/CMP1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CMP2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Comparison of the reservation schemes with dropping}
\label{DFMUL}
\end{figure}


Figure~\ref{DFMUL} depicts the throughput and average latency as a function of
the request generation rate for six
protocols that use the dropping policy in a $16\times 16$ torus.
The multiplexing degree is taken to be 32, the
message size is assumed to be 8 packets and the control packets
processing and propagation time is assumed to be 2 time units. 
For each of the forward and backward schemes, three variations are considered 
with varying aggressiveness.
The conservative variation in which the
initial $cset$ size is 1, the most aggressive variation in which
the initial set size is equal to the multiplexing degree and an optimal variation 
in which the initial set size is chosen (by repeated trials) to maximize the
throughput.
The letters  $C$, $A$ and $O$ are used to
denote these three variations, respectively.
For example, $FDO$ means the forward dropping scheme with optimal $cset$ size.
Note that the use of the optimal $cset$ size reduces the delay in addition to
increasing the throughput. Note also that the network saturates when
the generation rate is between 0.006 and 0.018, depending on the protocol
used. The maximum saturation rate that the $16\times 16$ torus can achieve in the
absence of contention and control overhead is given by
\[
\frac{number\ of\ links}{no.\ of\ PEs \times av. \ no.\ of\ links\ per\ msg
\times msg\ size} = \frac{1024}{256\times 8 \times 8} = 0.0625.
\]
Hence, the optimal backward protocol can achieve
almost 30\% of the theoretical full utilization rate. 

Figure~\ref{DFMUL}(b) also reveals that,
when the request generation rate, $r$, is small, for example $r = 0.003$, 
the network is under light traffic and 
all the protocols achieve the same throughput, which is equal to $r$ times
the number of processors.
In this case, the performance of the network should be measured by the
average latency.
In the rest of the performance study,
the maximum throughput (at saturation) and the average latency
(at $r = 0.003$) were used to measure the performance of the protocols.
Two sets of experiments are performed. The first set 
evaluates the effect of the protocol parameters on the network throughput and
delay and the second set
evaluates the impact of system parameters on performance.

\subsubsection*{Effect of protocol parameters}

In this set of experiments,
the effect of the initial $cset$ size, the holding time and the retransmit time on the 
performance of the protocols are studied. 
The system parameters for this set of experiments are chosen as follows:
system size = $16\times 16$,
message size = 8 packets, control packet processing and propagation time = 2 time
units.

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET3.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET4.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on forward schemes}
\label{CSETF}
\end{figure}
\begin{figure}[hbtp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on backward schemes}
\label{CSETB}
\end{figure}

Figure~\ref{CSETF} shows the effect of the initial $cset$ size on the forward
holding scheme with different multiplexing degrees, namely
1, 2,  4, 8, 16 and 32.
The holding time is taken to be 10 time units and the MRT is 5 time units
for all the protocols with initial $cset$ size less than the multiplexing degree
and 60 time units for the most aggressive forward scheme.
Large MRT is used in the most aggressive forward scheme because it is 
observed that small MRT often leads to live-lock in this scheme.
Only the protocols with the holding policy will be shown since using the
dropping policy leads to similar patterns. The effect of holding/dropping will
be considered in a later figure.
Figure~\ref{CSETB} shows the results for the backward
schemes with the dropping policy.

From Figure~\ref{CSETF} (a), it can be seen that when the multiplexing 
degree is larger than 8, both the most
conservative protocol and the most aggressive protocol
do not achieve the best throughput. Figure~\ref{CSETF}(b) shows that these
two extreme protocols do not achieve the smallest latency either.
The same observation applies to the backward schemes in Figure~\ref{CSETB}.
The effect of choosing the optimal initial $cset$ is significant on both
throughput and delay. That effect, however, is more significant in the
forward scheme than in the backward scheme. For example, with multiplexing
degree = 32,
choosing a non-optimal $cset$ size may reduce the throughput by 50\%
in the forward scheme and only by 25\% in the backward scheme. 
In general, the optimal initial $cset$ size is hard to find.
Table~\ref{OPTCSET} lists the optimal initial $cset$ size for each multiplexing
degree.
A rule of thumb to approximate the optimal $cset$ size is to use 1/3 and 1/10 of the
multiplexing degree for forward schemes and backward schemes, respectively.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Multiplexing & \multicolumn{2}{|c|}{Optimal $cset$ size}\\
\cline{2-3}
Degree & Forward & Backward\\
\hline
4 & 1 & 1\\
\hline
8 & 2 & 1\\
\hline
16 & 5 & 2\\
\hline
32 & 10 & 3\\
\hline
\end{tabular}
\end{center}
\caption{Optimal $cset$ size}
\label{OPTCSET}
\end{table}

Figure~\ref{HOLDSET} shows the effect of the holding time on the performance of
the protocols for a multiplexing degree of 32. 
As shown in Figure~\ref{HOLDSET}(a), the holding time has little
effect on the maximum throughput. It slightly increases the performance for the
forward aggressive and the backward aggressive schemes. As for the 
average latency under light work load, the holding time also has little effect
except for the forward aggressive scheme, where the
latency time decreases by about 20\% when
the holding time at each intermediate node increases from 0 to 30 time units.
Since holding requires extra hardware support compared to
dropping, it is  concluded that holding is not 
cost--effective for the reservation protocols. In the rest of the paper,
only protocols with dropping policies will be considered.


\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/HOLDSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/HOLDSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of holding time}
\label{HOLDSET}
\end{figure}

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/RETRYSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/RETRYSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of maximum retransmit time}
\label{RETRYSET}
\end{figure}



Figure~\ref{RETRYSET} shows the effect of the maximum
retransmit time
on the performance. Note that the retransmit time is uniformly distributed
in the range $0..MRT-1$. As shown in Figure~\ref{RETRYSET} (a),
increasing MRT results in performance degradation in
all the schemes except FDA, in which the performance improves
with MRT. This confirms that the MRT value is important to 
avoid live-lock in the network when aggressive reservation is used.
In other schemes this parameter is not important, because when 
retransmitting a failed request, virtual channels different than the ones
that have been tried may be included in $cset$.
This result indicates another drawback of the 
forward aggressive schemes: in order to avoid live-lock, the MRT
must be a reasonably large value, which decreases the overall performance.

\subsubsection*{Effect of other system parameters}

In this section,
only  dropping schemes with
MRT equal to 5 time units for all schemes except FDA will be considered.
The MRT for FDA schemes is set to 60.
This set of  experiments focuses on studying the performance of the 
protocols under different multiplexing degrees, 
system sizes, message sizes and 
control network speeds.
One parameter is changed in each experiment, with the other
parameters set to the following default values (unless stated otherwise):
network size = $16\times 16$ torus, multiplexing degree = 
16, message size = 8 packets, 
control packet processing and propagation time = 2 time units.




\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
  {\psfig{figure=eps/MUL1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/MUL2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{The performance of the protocols for different multiplexing degree}
\label{DBMUL}
\end{figure}


Figure~\ref{DBMUL}
shows the performance of  the protocols for different multiplexing degrees. 
When the multiplexing degree is small,  BDO and FDO
have the same maximum bandwidth as BDC and FDC, respectively. When 
the multiplexing degree is large, BDO and FDO offer better throughput.
In addition, for all multiplexing degrees, BDO is the best among
all the schemes. As for the average latency, both FDA and BDA have significantly
larger latency than all other schemes. Also, FDO and BDO have the smallest latencies.
It can be seen from this experiment that the backward
schemes always provide the same or better performance (both maximum
throughput and latency) than their forward reservation counterparts for all
multiplexing degrees considered.

Figure~\ref{SIZE} shows the effect of the network size on the performance of
the protocols.
It can be seen from the figure that all the protocols, except the aggressive
ones, scale nicely with the network size.
This indicates that the aggressive protocols cannot 
take advantage of the spatial diversity of the communication. This is 
a result of excessive reservation of channels. When the network size
is small, there is little difference in the performance of the protocols.
When the network size is larger, the backward schemes show their superiority.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/SIZE1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
 {\psfig{figure=eps/SIZE2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the network size}
\label{SIZE}
\end{figure}


\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/MSIZE1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
{\psfig{figure=eps/MSIZE2.eps,width=2.95in}}
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the message size}
\label{MSIZE}
\end{figure}



Figure~\ref{MSIZE} shows the effect of the message size on the protocols.
The multiplexing degree  in this experiment is 16.
The throughput in this figure is normalized to reflect the
number of packets that pass through the network, rather than the number
of messages,  that is,\\
\centerline{$normalized\ throughput\ =\ msg\ size \times \ throughput.$}
Both the forward and backward locking schemes achieve higher
throughput for larger messages. When messages are sufficiently large,
the signaling overhead in the protocols is small and
all protocols have almost the same performance. 
However, when the message size is small, the BDO scheme achieves 
higher throughput
than the other schemes. This indicates that BDO incurs less 
overhead in the path reservation than the other schemes. 

The effect of message size on the latency of the protocols is interesting.
Forward schemes incur larger latency when the message size is large. 
By blindly choosing initial cset, forward schemes
do not avoid choosing virtual channels used in communications, which
increases the latency when the message size is large (so that connections are
held longer for communications).
Backward schemes probe
the network before choosing the initial csets. Hence, the latency in backward
schemes does not increase as
much as in forward schemes when message size increases. 
Another observation is that in both forward and backward protocols, 
aggressive schemes sustain the increment of message size better
than the conservative schemes. This is also because of the longer
communication time with larger message sizes. Aggressive schemes are more 
efficient in finding a path in case of large message size. Note that 
this merit of aggressive schemes is offset by 
over reservation.
Another interesting point is that
the latency for messages of  size 1 results in higher latency than 
messages of size 8 in BDA scheme. This can be attributed to too many
control messages in the network
in the case when data message contains a single packet (and
thus can be transmitted fast). The conflicts of control messages result in
larger latency.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
{\psfig{figure=eps/CNS1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CNS2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the speed of the control network}
\label{CNS}
\end{figure}

Figure~\ref{CNS} shows the effect of the control network speed on performance. 
The multiplexing degree  in this experiment is 32.
The speed of the
control network is determined by the
time for a control packet to be transferred from one node to the next node
and the time for the control router to process the control packet. From the 
figure it can be seen that when the control speed is slower, the maximum
throughput and the average latency degrade.
The most aggressive schemes in both 
forward and backward reservations, however, are more sensitive to the
control network speed. Hence, it is important to have a reasonably fast 
control network when these reservation protocols are used. 


\section{Chapter summary}

This chapter presented the forward path reservation algorithms
and the backward path reservation algorithms to establish connections 
with path multiplexing for connection requests that arrive at the 
network dynamically. Holding and dropping variants of these protocols 
were described. A holding scheme holds the reservation packet for a period 
of time when it determines that there is no available channel on the next link
for the connection. A dropping scheme drops the reservation
packet and starts a new reservation once it finds that there is no 
available channel on the next link for the connection.  Protocols
with various aggressiveness were discussed. The most aggressive schemes
reserve as many channels as possible for each reservation to increase
the probability of a successful reservation, while the most
conservative schemes reserve one channel for each reservation to reduce
the over--locking problem. The performance of the protocols and 
the impact of system parameters on the protocols were studied. 
The major results obtained in the experiments are summarized as follows.

\begin{itemize}

\item With proper protocols, multiplexing results in higher
maximum throughput. Multiplexed networks are significantly more efficient than
non--multiplexed networks.

\item Both the most aggressive and the most
conservative reservations cannot achieve optimal performance. 
The performance of the forward schemes is more sensitive to the
aggressiveness than the performance of the backward schemes.

\item The value of the holding time in the holding schemes does 
not have significant impact on the performance. 
In general, however, dropping is more efficient than holding.

\item  The retransmit time 
has little impact on all the schemes except the forward aggressive
dropping scheme.

\item The performance of the forward aggressive dropping scheme
is significantly worse than other protocols. 
Moreover, this protocol cannot take advantage of both larger multiplexing 
degree and larger network size.

\item The backward reservation schemes provide better performance than
the forward reservation schemes for all multiplexing degrees. 

\item The difference of the protocols does not affect the communication 
efficiency when the network size is small. However, for large networks, the
backward schemes provide better performance.

\item The backward schemes provide better performance when the message size
is small. When the message size is large, all the protocols have similar 
performance.

\item The speed of the control network significantly 
affects the performance of the protocols.

\end{itemize}

These protocols achieve all--optical communication
in data transmission. However, they require extra hardware support to 
exchange control messages and incur large startup overhead. 
An alternative to the single--hop communication is 
the multi--hop communication. Multi--hop networks
do not require extra hardware support. They use intermediate hops
to route messages toward their destinations. In the next chapter, 
multi--hop networks are considered.









\section{Goals and Approaches}
\label{goal}

This research will consider network control for optical TDM
point--to--point networks in multiprocessor environments. There are 
three primary goals of this work: (1) to develop and evaluate
dynamic network control mechanisms which establish connections
(virtual paths)
for the connection requests that arrive at the network dynamically, 
(2) to investigate the feasibility
of applying compiled communication technique in optical multiplexed
interconnection networks, and to address  relevant issues when applying 
compiled communication, and (3) to compare the communication performance
of these two schemes.  Shared memory
programs compiled for  distributed memory machines and 
programs using explicit message passing will be used for the
study of compiled communication. The torus topology will be assumed to be
the underlying physical topology in this research for performance
evaluations. This topology  
has been demonstrated to be an efficient
topology for point--to--point networks
 and is used in many commercial supercomputers. 
Note, however, that many  techniques developed in
this research can also be applied to other topologies.

Optical interconnection networks offer large bandwidth and
low error rates. To fully exploit the large bandwidth in an optical network,
the control overhead must be reduced.
Since dynamic network control must be performed in the electronic domain, 
it is crucial to 
develop efficient network control mechanisms that 
 establish all--optical paths
for connection requests without incurring too much control overhead. 
The study of dynamic network control schemes is 
the first goal of this research. The results of the study will give
general guidelines for the design of dynamic network control schemes
in optical multiplexed interconnection networks. The study will also
lead to the 
understanding of  the impact of system parameters on the control schemes.

One way to reduce the control overhead is to use compiled communication.
Compiled communication reduces runtime control overhead by
managing communications at compile time.  The study of compiled communication
 is the second focus of this
research. A number of issues must be addressed in order to apply 
compiled communication. How to efficiently 
obtain  communication patterns in 
a program?  How to manage network resources when communication patterns
are known? How to handle communication patterns that are 
unknown at compile time?
These are some questions to be answered among others.
The study of compiled communication will try to give solutions to
the problems encountered when applying compiled communication.
The results of the study will give insights into compiled 
communication in optical TDM point--to--point networks as well as the
potential performance gain by compiled communication.

In order to achieve the above goals, the steps outlined below are proposed.

\begin{enumerate}
\item 
  Study  efficient dynamic network control mechanisms for  optical TDM
  interconnection networks.
  \begin{enumerate}
    \item Develop distributed path reservation protocols for optical TDM
     point--to--point networks.
    \item Develop a network simulator that simulates the network behavior
          under all designed protocols.
    \item Study the performance of the protocols and 
          the impact of various
          system parameters, such as system size, message size, etc, on 
          the protocols. Latency and maximum throughput will be used 
          as performance measures.
  \end{enumerate}
\item Design and implement a tool that synthesizes  communication
          patterns from application programs.
   \begin{enumerate}
     \item Design a communication descriptor that can describe
           the communication requirement (logical communication patterns)
           in a program.
     \item Design and implement data flow analysis algorithms for
           communication optimization to obtain communication
           patterns in the optimized code of a program.
     \item Design and implement schemes that derive phyiscal 
                communication patterns from logical communication patterns.
   \end{enumerate}
\item 
  Study   compiled communication and evaluate its 
  performance in optical TDM networks.
  \begin{enumerate}
    \item Design communication scheduling algorithms that can be used by 
          the compiler to schedule the communication patterns that
          are known at compile time.
    \item Design efficient logical topologies that can be used to 
          route messages in the communication patterns that are unknown at
          compile time.
    \item Develop a network simulator that simulates  network behavior
          for networks
          with compiled communication. The simulator should include  
          off--line
          connection scheduling algorithms to establish connections
          for the communication patterns that are known at compile time. The 
          simulator should also be able to simulate  multi--hop
          communication for handling the  communication
          patterns that are unknown at compile time.
    \item Evaluate the performance of compiled communication. 
          This includes the performance  for 
          static patterns,  dynamic patterns, and the overall
          performance for all patterns in programs.
  \end{enumerate}
\item
  Carry out a comparison of dynamic communication and compiled communication
  using actual application programs.
\end{enumerate}
  
\subsection{Dynamic network control}
 
In order to fully explore the potential of optical communication,
optical signals should be transmitted in a pure circuit--switching 
fashion in the optical domain (all--optical communication). No buffering and 
optical-to-electronic or electronic-to-optical conversions should be needed
at intermediate nodes. Dynamic network control processes the connection
requests arriving at the network dynamically and negotiates with the
nodes in the network to establish virtual paths for 
connection requests before the communications can start. In this work, 
distributed path reservation algorithms for 
the establishment of VPs are to be designed and evaluated.
The major issues to be considered are the control overhead, 
the efficiency of the  virtual channel utilization  and the 
hardware overhead.  Some trade--offs among these issues may be  needed to
obtain an efficient algorithm. 

A cycle level network
simulator which simulates the network behavior for networks with
different  
protocols is needed. 
In order to  study all  variants of the protocols, the 
simulator must provide the following capabilities.
\begin{itemize}
\item It must be able to simulate all different protocols designed.
\item It must be able to simulate networks with different parameters, such as
      network size and multiplexing degree.
\item It must be able to simulate protocols with different protocol
      parameters.
\end{itemize}
Using the network simulator, the performance of the 
protocols will be studied, 
the impact of system parameters and protocol parameters
on the protocols will be determined. 
Based upon the results, the protocol parameters can be tuned to obtain 
good performance.


\subsection{A tool to extract communication patterns in a program}

To apply compiled communication, the compiler must be able to obtain the
communication requirements in a program. Traditional methods
analyze communication requirements on the logical processor 
array (logical communication patterns) 
\cite{Hinrichs95,Gupta92} and represent the communications in 
mathematical notations, such as matrix notation in \cite{Hinrichs95}.
Based on the form of the mathematical notation,  communication
patterns on  physical
processors (physical communication patterns)  may be obtained. 
Obtaining physical communication patterns
in this manner may not be sufficient for compiled communication, since
in compiled communication, we would like to know all  communication
patterns that can be synthesized by the compiler. Note that 
communication patterns that are unknown at 
compile time 
result in much larger overhead when 
using compiled communication. 

In this research, a descriptor, which can describe  logical communication 
patterns, is developed. The 
descriptor can be easily calculated from program structures. Traditional
communication optimization algorithms will  be implemented using
this descriptor to represent communications in programs. Note that,
since communication optimization are commonly implemented in compilers,
implementing traditional communication optimizations
will give  more realistic communication patterns in programs.
Once the communication optimizer is implemented,
 logical communication patterns in both optimized and 
non-optimized programs can be obtained and represented in the descriptor.
An efficient scheme will then be designed to derive  physical
communication patterns from the logical communication patterns. 
 Note that brute force methods can be used to derive physical communication
patterns. However, this may result in large
analysis overhead and must be avoided if possible.

\subsection{Compiled communication}

Under compiled communication, the compiler will manage the communications in
a program. For a static pattern, the compile can  insert codes in the 
program to set the network state such that, at runtime, a path will
exist before a communication starts without  dynamic path reservation.
For a dynamic pattern, the compile will route messages in the pattern
through a logical topology.
The following steps will be carried
out to study compiled communication.

\subsubsection{Connection scheduling for static patterns}

Once the compiler knows the communication patterns in a  program, it 
employs  off--line connection scheduling algorithms to manage the 
communications.  Since the communication time of a communication pattern
in an optical TDM system is proportional
 to the multiplexing degree required for 
the communication pattern,
the primary goal of these 
algorithms is to achieve the minimum multiplexing degree that contains all
the connections in the pattern. Different scheduling schemes may be needed for
different patterns. For example, connection scheduling for dense communication
patterns may be different from that for sparse communication patterns.

\subsubsection{Efficient logical topology design for dynamic patterns}

When a logical topology is to be used to route
messages in the communication patterns
that are unknown at compile time, 
 the communication performance depends on three 
factors, the multiplexing degree needed to realize the logical topology, the 
average number of intermediate hops and the processing time in each 
intermediate node. When the logical topology is the same as the physical
topology, no multiplexing is needed (multiplexing degree $= 1$). However,
messages may need to be routed through a large number of intermediate nodes.
When the logical topology is the complete graph, no intermediate hops are
needed for a connection. However, realizing a complete graph may require
a large multiplexing degree. Other choices between these two extremes
may result in better communication performance. 
All possible candidates for the logical topologies (on top of a physical
torus topology) will be examined and compared. 

\subsubsection{Network simulator for compiled communication}

A network simulator will be designed and implemented to study
the communication performance of compiled communication.
The simulator will take the trace and/or communication patterns generated
from compiler analysis of a program as inputs
and simulate compiled communication.
When communication patterns are known at compile time,  off--line 
algorithms will be used to perform the connection scheduling. When 
communication patterns are unknown at compile time, a pre--determined
communication pattern will be used to route the messages. Hence, the
simulator must be able to simulate  single--hop communication for
static patterns and multi--hop communication for dynamic patterns.
Again, it should provide sufficient flexibility similar to the flexibility
of  the simulator for
dynamic communications.

\subsubsection{Performance of compiled communication}

With the simulator, the performance of various off--line
connection scheduling algorithms can be studied and compared.
The performance of various logical topologies can be evaluated. The impact
of various system parameters on compiled communication can also be examined.
Synthesis communication patterns, as well as communication
patterns that come from real application programs, will be used in the 
evaluations.

\subsection{Performance comparison of dynamic communication and compiled 
communication}

Once the above steps are completed, I will be able to compare the 
communication performance of dynamic communication and compiled 
communication using benchmark programs including the floating point benchmark
in SPEC95 and other application programs. This study will quantify
the performance gained by compiling communication for patterns that are known
at compiled time and the performance lost by  routing messages over logical
topologies for  patterns that are unknown 
at compiled time.

\newpage






%% background seciton
\chapter{Background and related work}
\label{backg}

\section{Optical TDM networks}
\label{opnet}

An optical point--to--point network 
consists of switches with a fixed number of input and output ports.
One input port and one output port are used to connect 
the switch to a local processing element and all remaining 
input and output ports of a switch are
used for connections to other switches.   
An example of such networks is the $4 \times 4$ torus shown in 
Figure~\ref{TORI}.
In these networks, each link in the network is time--multiplexed to support 
multiple virtual channels.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/tori.eps,width=3.5in}}
\caption{A torus connected network}
\label{TORI}
\end{figure}

Two approaches can be used to establish connections in multiplexed networks,
namely {\em link multiplexing} (LM) and {\em path multiplexing} (PM) 
\cite{Qiao95}. PM uses the same channel on all
the links along the path to form a connection. On the other hand,
LM may use different 
channels on different links along the path, 
thus requiring time-slot interchange in TDM networks at each 
intermediate node. Fig.~\ref{pmlm} shows the PM and LM connections at a 
$2\times 2$ switch where each link supports two channels.
LM is similar to the multiplexing technique in electronic networks where
a data packet can change channels when it passes a switch. Using LM for 
communication has many advantages over using PM. For example, the path
reservation for a LM connection is simpler than that for a PM connection, and
LM results in better channel utilization. However,
optical devices for LM are still in the research stage and 
are very expensive using current technology. Hence,
this thesis is concerned only with  path multiplexing because
the enabling technology is more mature. 

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/pmlm.eps,height=2.2in}}
\caption{Path multiplexing and link multiplexing}
\label{pmlm}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/pm.eps,width=4.5in}}
\caption{Path multiplexing in a linear array}
\label{PM}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/ts.eps,width=3.5in}}
\caption{Changing the state of a switch in TDM}
\label{TS}
\end{figure}

In order to time--multiplex a network with path multiplexing, 
a {\em time slot} is defined to be a fixed period of time and the time
domain is divided into a repeated sequence of $d$ time slots, where $d$
is  the {\em multiplexing degree}. Different virtual channels on each link
occupy different time slots.
Figures \ref{PM} and \ref{TS} illustrate path multiplexing on a linear
array. In these two figures,  two virtual channels are supported
on each link by dividing the time domain into two time slots, 
and using alternating time
slots for the two channels $c0$ and $c1$.
Let us use $(u,v)$ to denote a connection from node $u$ to node $v$.
Figure \ref{PM} shows four established connections over the two channels,
namely connections
$(0, 2)$ and $(2, 1)$ that are established using channel $c0$, and
connections $(2, 4)$ and $(3, 2)$ that are established using channel $c1$.
The switches, called {\em Time--Multiplexed Switches} (TMS),
are globally synchronized at time slot boundaries, and each switch
is set to alternate between the two states that are needed to realize the
established connections.
For example, Figure~\ref{TS} shows the two states
that the $3 \times 3$ switch attached to PE 2 must realize for the 
establishment of the connections shown in Figure~\ref{PM}.
Note that each switch can be an electro-optical switch
(Ti:LiNbO$_3$ switch, for example
\cite{hinton}) which connects optical inputs to optical outputs without
E/O and O/E conversions. The state of a switch is controlled by 
setting electronic bits in a {\em switch state register}.

The duration of a time slot may be equal to the duration over which 
several hundred bits may be transmitted.
For synchronization purposes, a guard band at each end of a time slot
must be used to allow for changing the state of switches (shifting
a shift register) and to accommodate possible drifting or jitter. For example, 
if the duration of a time slot is $276ns$, which includes a guard band of
$10ns$ at each end, then $256ns$ can be used to transmit data. If the
transmission rate is $1Gb/s$, then a packet of 256 bits can be transmitted
during each time slot. 
Note that
the optical transmission rate is not affected by the relatively
slow speed of changing the state of  switches ($10 ns$)
since that change is performed only every $276 ns$.

Communications in TDM networks can either be {\em single--hop} or 
{\em multi--hop}. In single--hop communication, circuit--switching
style communications  are carried out. A path for a 
communication must be established before the communication starts. 
In general, any $N \times N$ network, other than a completely 
connected network, has a limited connectivity in the sense
that only  subsets, $C = \{ (x, y) | 0\le x, y < N\}$, of the possible
$N^2$ connections can be established simultaneously without conflict.
For single--hop communication the network
must be able to establish any possible connection in one hop, without
intermediate relaying or routing.
Hence,  the network
must be able to change the connections it supports at different times.
This thesis considers switching networks in which the set of connections 
that may be established simultaneously (that is, the state of the network) 
is selected by changing the contents of hardware registers. The single--hop
communication can be achieved in two ways. First, a path reservation
algorithm can be used to dynamically establish and tear down all--optical
connections for arbitrary communications. Second, compiled communication 
uses the compiler to analyze the communication requirement of a program
and insert code to establish all--optical 
connections (at phase boundaries) before communications start.  
Unlike the case in a single--hop system where connections 
are dynamically established and torn down, connections in 
a multi--hop system are fixed and a message may travel through 
a number of lightpaths to reach its destination.
Dynamic single--hop communication,
dynamic multi--hop communication and compiled communication will be discussed
in some details next.

\section{Dynamic single--hop communication with PM}

To establish a connection in an optical TDM  network,
a physical path, $PP$, from the source to the destination is first chosen. 
Then, a virtual path, $VP$, consisting of a virtual channel in each link 
in $PP$ is selected and the connection is established.  The selection 
of $PP$ has been studied extensively and is well understood \cite{Leighton92}.
It can be classified into {\em deterministic} routing, where
$PP$ can be determined from the source node and the destination node
(e.g., X--Y routing on a mesh), 
or {\em adaptive} routing, where  $PP$ is selected from
a set of possible paths. 
Once $PP$ is selected, a time slot is used
in all the links along  $PP$. 

The control in optical TDM networks is responsible for the establishment of a
virtual path for each connection request. 
Due to the similarity of TDM and WDM networks, many techniques for virtual channel assignment in one of these two types of networks can also apply to
the other type.
Network control for multiplexed optical networks can be classified into
two categories, centralized control and  distributed control. 
Centralized control assumes a central controller which maintains the 
state of the whole network and schedules all communication requests. 
Many time slot assignment  and wavelength assignment
algorithms have been proposed for  
centralized control. In \cite{Qiao94} a number
of time slot assignment algorithms are proposed for TDM multi-stage 
interconnection networks. In \cite{Chlamtac92} wavelength assignment for
wide area networks is studied. A time wavelength assignment algorithm for 
WDM star networks is proposed in \cite{ganz92}. Theoretical study
for optimal routing and wavelength assignment for arbitrary networks is
presented in \cite{Ramaswami94}. 

Distributed control does not assume a central controller and thus is more
practical for large  networks. 
Little work has been done on distributed control for 
optical multiplexed networks.
In \cite{Qiao94} a distributed path reservation scheme
for optical Multistage Interconnection  Networks (MIN) is
proposed. Distributed path reservation methods for both path multiplexing and 
link multiplexing are  presented in \cite{Qiao96}. 
This thesis proposes distributed path reservation algorithms that are more
efficient than the previous algorithms, investigates variations in 
channel reservation methods 
and studies the impact of the system parameters on the 
protocols.

\section{Dynamic multi--hop communication with PM}

By using path multiplexing, efficient logical topologies can be established
on top of the physical topology. The connections in the logical topologies
are lightpaths that may span a number of links. In such systems, the 
switching architecture consists of an optical component and an electronic
component. The optical component is an all--optical switch, which can 
switch the optical signal from some input channels to output channels in
the optical domain (i.e., without E/O and O/E conversions), and which can
locally terminate some other lightpaths by directing them to the node's 
electronic component. The electronic component is an electronic packet
router which serves as a store--and--forward electronics overlaid
on top of the optical virtual topology. 
Figure~\ref{ROUTER1} provides a schematic diagram of the architecture of the 
nodal switch in a physical torus topology.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/multiswitch.eps,width=1.5in}}
\caption{A nodal switching architecture}
\label{ROUTER1}
\end{figure}

Since the electronic processing is slow compared to the optical
data transmission, it is desirable to reduce the number of intermediate
hops in a multi--hop network. This can be achieved by having a logical topology
whose connectivity is high. However, realizing a logical topology with a large 
number of connections requires a large multiplexing degree. In a TDM system,
large multiplexing degree results in a large time to transmit a 
packet through a lightpath because every light path is established only
for a fraction of the time. Hence, there exists a performance trade--off
in the logical topology design between a logical topology with large 
multiplexing degree and high connectivity  and  a logical topology with
small multiplexing degree and low connectivity. As will be shown in 
this dissertation, both topologies have advantages for certain types
of communication patterns and system settings.

Multi--hop networks have been extensively studied in the area  of
WDM wide area networks. The works in 
\cite{Bannister90,chlamtac93,Labour91,Mukherjee94,Venkat96} 
consider the realization of logical topologies on optical multiplexed 
networks. These works consider wide area networks and focus on 
designing efficient logical topologies on top of irregular networks. 
Since finding an optimal logical topology on irregular networks
is an NP--hard problem, heuristics and 
simulated annealing algorithms are used to find suboptimal schemes.
This dissertation considers regular networks in multiprocessor 
environments and derives optimal connection scheduling schemes for 
realizing hypercube communications. Besides logical topology design,
connection scheduling algorithms can also be used to realize 
logical topologies. In \cite{Qiao96} message scheduling for permutation
communication patterns in mesh--like networks is considered. 
In \cite{Qiao94} optimal schemes for realizing
all--to--all patterns in multi-stage networks are presented. 
In \cite{Hinrichs94} message scheduling for all--to--all communication 
in mesh--like topologies is described. 

The performance of multi--hop
networks has also been previously studied. However,  
most previous performance studies for optical multi--hop networks assume 
a broadcast based underlying WDM network, such as an optical star network 
\cite{kovacevic95,Sivarajan91}, 
where the major concerns are the number of transceivers in each
node and  the tuning speed of the
transceivers. This thesis studies the logical topologies
on top of a physical torus topology in a TDM network, 
where the major focus is the 
trade--off between the multiplexing degree and the connectivity of a topology.

\section{Compiled communication}

Compiled communication has recently drawn the attention of several 
researchers \cite{Cappello95,Hinrichs95}. Compiled communication
has been used in combination with message passing in the iWarp system 
\cite{Gross89,Gross94,Hinrichs95a}, where
it is used for  specific subsets 
of static patterns. All other communications are handled using  
message passing. The prototype system described in \cite{Cappello95} 
eliminates the cost of supporting multiple communication models. It relies
exclusively upon compiled communication. However, the performance of this
system is severely limited due to frequent dynamic reconfigurations of 
the network. 
%This frequency can be reduced in  optical networks through
%the use of TDM. 
Compiled communication is more beneficial in optical multiplexed 
networks. Specifically, it reduces the control overhead, which is 
one of the major factors that limit the communication performance 
in optical networks. Moreover, multiplexing, which is natural in optical
interconnection networks, enables a network to support simultaneously more
connections  than  a non--multiplexed  network, which
reduces the reconfiguration overhead in compiled communication.

The communication patterns in an application program can be broadly
classified into two categories: {\em static patterns} that can be
recognized by the compiler and {\em dynamic patterns} that are only 
known at run-time. For a static pattern, compiled
communication computes a minimal set of network configurations that 
satisfies the connection requirement of the pattern and thus,  
handles  static patterns with high efficiency.
Recent studies \cite{Lahaut94} have shown that about 95\% of the 
communication patterns in scientific programs are static patterns.
Thus, using the compiled communication technique to improve the 
communication performance for the static patterns is likely to 
improve the overall communication performance. 
Some advantages of using compiled communication for
handling static patterns are as follows.

\begin{itemize}
\item Compiled communication totally eliminates the
path reservation and the large startup overhead associated with
the path reservation. 

\item The connection scheduling algorithm is executed 
off-line by the compiler. Therefore,  complex strategies 
can be employed to improve network utilization. 

\item No routing decisions are made at runtime which means that 
the packet header can be shortened causing the network 
bandwidth to be utilized more effectively.

\item Optical networks efficiently support multiplexing
which reduces the chance of network reconfigurations due to the lack of
network capacity. 

\item Compiled communication adapts to the 
communication requirement in each phase. For example, it can use different
multiplexing degrees for different phases in a program. In contrast,
dynamic communications always use the same configuration to handle 
all communications in a program which may not be optimal.

\end{itemize}

In order to apply compiled communication to a large scale multiprocessor 
system, three main problems must be addressed: 

\begin{description}
\item
{\bf Communication Pattern Recognition:}
This problem has been considered by 
many researchers since information on communication patterns has been
previously used to perform communication optimizations
\cite{Bromley91,Gupta92,Hinrichs95,Li91}. The stencil compiler 
\cite{Bromley91} for CM-2 recognizes {\em stencil} communication
patterns. Chen and Li \cite{Li91} incorporated a pattern extraction 
mechanism in a compiler to support the use of collective communication 
primitives. Techniques for recognizing a broad set of communication 
patterns were also proposed in \cite{Gupta92}. 
However, most of these methods determine a specific subset of static
communication
patterns, such as the broadcast pattern and the nearest neighbor pattern, 
which is  not sufficient for compiled communication. Since the 
communication performance of compiled communication relies heavily
on the precision of the communication analysis, it is desirable to 
perform more precise analysis that can recognize arbitrary communication
patterns. Furthermore, compiled communication
requires the partitioning of a program into phases, such that each 
phase contains communications that can be supported by the underlying 
network, and the scheduling of connections within each phase. These are
new problems that must be addressed.

\item
{\bf Compiling Static Patterns:} Once the compiler determines a 
communication pattern within each phase, which is called a 
{\em static pattern}, the compiler must be able to schedule the
communication pattern on the multiplexed network. 
In TDM networks, communication performance
is proportional to the multiplexing degree. Given a 
communication pattern, the smaller the multiplexing degree, the
less time the communication lasts. Thus, connection scheduling
algorithms that schedule all connection requests in a phase with
a minimal multiplexing degree must be designed to handle the static
patterns. It has been shown that optimal message scheduling 
for arbitrary topologies is NP-complete \cite{Chlamtac92}.
Hence, heuristic  algorithms that provide good performance need to be 
developed.

\item
{\bf Handling Dynamic Patterns:} A number of techniques can be used to
handle dynamic communication patterns. 
One approach is to setup all-to-all connections among 
all nodes in the system. This way each node has a time slot to 
communicate with every other node. However, establishing paths for 
the all-to-all communication can be prohibitively expensive for large systems.
An alternative is to perform dynamic single--hop or multi-hop communications. 
The dynamic communications are not as efficient as compiled communication. 
However, since this method is not used frequently, its
effect on the overall performance is limited. 

\end{description}

\section{Programming and machine model}

Compiled communication requires the compiler to extract  communication
patterns from application programs. 
The method to extract communication patterns in a program
depends on both programming model and  machine model.
The programming model includes the ones using
explicit communication primitives and the ones that require implicit
communication through remote memory references. There are two 
different machine architectures, the shared memory machine and the
distributed memory machine. Communication requirements for these two
machine models are different for a program.  In 
shared memory machines with hardware cache coherence, communications
result from cache coherence traffic, while 
in distributed memory machines,
communications  result from data movements  between
processors. 

{\bf Explicit communication}:
Most of the current commercial distributed memory
supercomputers support the explicit communication programming
model. In such programs, programmers explicitly use  communication 
primitives to perform the communication required in a  program.
The communication primitives can be high level library routines, such 
as PVM \cite{PVM94} or MPI \cite{MPI93}, or low level communication primitives
such as the shared memory operations in the CRAY T3D \cite{Numrich94} and
the CRAY T3E. Communication
patterns in a  program with explicit communication
primitives can be obtained from the
analysis of the communication primitives in the program.

{\bf Implicit communication}: Managing explicit communication is tedious
and error-prone. This has motivated considerable research towards 
developing compilers that relieve programmers from  the burden of 
generating communication
\cite{amarasinghe93,banerjee95,gupta95,hiran92,rogers89,zima88}.
Such compilers take sequential or shared memory 
parallel programs and generate Single Program Multiple Data (SPMD)
programs with explicit message
passing. This type of programs will be referred to as 
{\em shared memory programs}.
Shared memory programs can be compiled for execution on both
distributed memory machines and shared memory
machines. In the case when a program is to be run on a distributed memory
machine,
the communication requirements of the program can be obtained from 
memory references. 
If a program is to be run on a shared memory machine,
the communication requirements
depend on the cache behavior. However,
a superset of the communication patterns may be obtained by examining the
memory references in the program. 
This work will consider data parallel
shared memory programs compiled for execution on
distributed memory machines. 
Compilers that exploit task parallelism \cite{Gross94a,Subhlok93}
are not considered. However, similar techniques may also apply to 
task parallel programs.

\section{Compilation for distributed memory machines}
\label{comp}

While communication requirements of a shared memory
program can be obtained by analyzing
the remote memory references in the program, the actual communication patterns
in the program depend on the compilation techniques used. 
To obtain realistic communication patterns,
 compilation techniques for compiling 
shared memory programs for  distributed memory machines
 must be considered. 
The most important issues to be addressed
when compiling for distributed memory machines
are data partitioning,  code generation and 
communication optimization. This section surveys previous work
 on these issues.

{\em Data partitioning} decides the distribution of array elements
to processors. There are
two approaches for handling the data partitioning problem. 
The first approach is to add user directives to programming languages
and let the users  
specify the data distribution. This approach is used in Fortran D 
\cite{hiran92}, Vienna Fortran \cite{Chapman92} and High Performance
Fortran (HPF) \cite{HPF} among others. 
It uses  human knowledge of  
application programs and simplifies the compiler design. However, using this
approach requires programmers to work at a low level abstraction 
(understanding the detail of memory layout). Since the best placement 
decision will vary between different architectures, with explicit user
placement, the programmer must reconsider the data placement for each
new architecture. Hence, many algorithms have been developed to
perform automatic data distribution. An algorithm has been designed 
for the CM Fortran compiler that attempts to minimize and identify 
alignment communications in data parallel Fortran programs \cite{Knobe90}.
Similar algorithms have been proposed in 
\cite{Chatterjee93,Gupta92a,Hinrichs95,Li91a}.
Data partitioning directly affects the communication
requirements in a program running on a distributed memory machine. Once 
data partitioning is decided, the minimum requirement of data movements in
a program, which results in communications, is fixed.

{\em Code generation} generates the communication code to ensure the
correctness of a program. The {\em Owner computes} rule is generally used 
for distributing the computation onto processors. 
Under owner computes rule, the owner of the array
element on the
left hand side of an assignment statement executes the statement. Thus, the
owner of an array element on the right hand side of the assignment statement
must send the element to the owner of the left hand side, which results
in communication. Without considering efficiency, a simple scheme
can be used to generate the correct SPMD
 code by inserting guarded communication
primitives \cite{rogers89}. 
However, the communication and synchronization overhead of this scheme can 
be so
large that there may be 
no benefit for running the program on a multiprocessor system.
Several researchers have 
proposed techniques for generating efficient code for array statements, 
given {\em block}, {\em cyclic} and {\em block--cyclic} distributions. 
In \cite{Koelbel90,Koelbel91} compile time analysis of array 
statement with block and cyclic distribution is presented.
In \cite{Chatterjee93a} Chatterjee et al. present a framework for compiling
array assignment statements in terms of constructing a finite state machine. 
This method handles block, cyclic and block--cyclic distributions. Method
in \cite{Stichnoth93} improves Chatterjee's method in terms of buffer space and
communication code generation overheads. Other compilers
\cite{amarasinghe93,banerjee95,gupta95,hiran92,rogers89,zima88} use
communication optimization to generate efficient code for programs
on distributed memory machines. Different ways of code 
generation result in different communication patterns at runtime.
 For example, the compiler may 
decide to send/receive all elements in an array to speed up the 
communication. It may also decide to send/receive one element
 at a time to save buffer space. 

{\em Communication optimizations} reduce the cost of  communication 
in a program.
Communication performance not only
affects the performance of a  parallel application but also limits
its scalability. Therefore, communication optimization is 
crucial for the performance of programs compiled for a
distributed memory machine. Many communication optimizations are applied
within a single loop using data dependence information. Examples of such
optimizations include message vectorization \cite{hiran92,zima88}, 
collective communication \cite{Gupta92,Li91}, message coalescing
\cite{hiran92} and 
message pipelining
\cite{gupta95,hiran92}. Earlier methods are based on
 {\em location based} data dependence, which is not precise since it 
only determines whether two references refer to the same memory 
location. Later schemes refine the information and use 
{\em value based} data dependence \cite{amarasinghe93}.  
In value based data dependence, a read
reference depends on a write reference only if the write provides 
the value
for the read reference.

Communication optimizations based only on  data dependence information
usually result in redundant communications \cite{Chakrabarti96}. 
The more recently developed
optimizations use data flow information to reduce redundant communication
and perform other optimizations. In \cite{Gong93} 
a data flow framework which can integrate
a number of communication optimizations is presented. However, the method
 can only apply to a very small subset of programs 
which are constrained
in the forms of loop nests and  array indices. In \cite{Gupta96} a unified 
framework which uses global array data flow analysis for communication 
optimizations is 
described. Since only a very simplified version of the analysis algorithm
is implemented, it is not clear whether this approach is practical for large
programs. In \cite{Chakrabarti96,Kennedy95} methods 
that combine traditional data flow analysis 
techniques with data dependence analysis for performing global
communication optimizations are described. 
These schemes are very efficient in terms of their 
analysis cost since bit vectors are used to represent  data flow 
information. However, they cannot obtain the array data flow information 
that is as 
precise as the information computed using
array data flow analysis approaches.
Communication optimization changes the communication behavior of a program.
Since many communication optimizations are commonly used in production
compilers, these optimizations
must be considered to obtain realistic communication patterns in a program.

%Notice that most of the communication optimizations
%described above  assume the traditional communication model, 
%which might not be true in compiled communication. 
%Therefore, it is interesting to investigate
%useful communication optimizations under the assumption of
%compiled communication.
%\newpage

%\section{Chapter summary}

%In this chapter, I describe the multiplexing techniques used in optical
%interconnection networks, discuss the issues in dynamic single--hop
%communication, dynamic multi--hop communication and compiled 
%communication and survey previous results in these areas. 
%I have also summarized the related research in the 
%compilation for distribution memory machines. This chapter gives 
%the background and surveys previous research related to the 
%thesis. In the next chapter, I will discuss the techniques for 
%dynamic single--hop communication. 



\chapter{Compiled communication}
\label{compiled}

%Recently many researchers have shown that communication performance
%of dense matrix  applications can be greatly improved 
%by allowing network resources to be managed by compiler and using 
%the {\em compiled communication} technique\cite{Cappello95,Hinrichs95,Yuan96}.
In compiled communication, the compiler analyzes a program to determine
its communication requirement. The compiler can then
use the knowledge of the underlying
architecture, together with the knowledge of the communication requirement,
to manage network resources statically. As a result, 
runtime communication overheads, such as the path reservation overhead 
and the buffer allocation overhead, can be reduced or eliminated, 
and the communication performance can be improved.
Due to the limited resources, the underlying network  
cannot support arbitrary communication patterns. 
Thus, compiled communication
requires the compiler to analyze a program and partition
the program into phases such that each phase has a fixed, pre-determined 
communication pattern that the underlying network can support.
The compiler inserts code to reconfigure the network at  
phase boundaries, uses the knowledge of the 
communication requirement within each
phase to manage network resources directly, 
and optimizes the communication 
performance. 

A number of compiler issues must be addressed 
in order to apply the compiled communication technique 
to optical TDM networks.  Specifically, 
given a multiplexing degree, the compiler must
partition a program into phases such that each phase contains connections that
can be realized by the underlying network with the given multiplexing degree.
To obtain good performance, each phase must contain as much communication
locality as possible so that less reconfiguration overhead will be incurred
at runtime. A compiler, called the E-SUIF (extended SUIF) compiler, 
is implemented to support compiled communication.
The structure of the compiler is shown in Figure~\ref{overall}. 
There are four major components in the 
system. The first component is the {\em communication analyzer} that
analyzes a program and obtains
its communication requirement on virtual processor
grids. The second component is the {\em virtual to physical processor 
mapping} subsystem that computes the 
communication requirement of a program on physical processors.
The third component is the {\em communication phase analysis} subsystem that
partitions the program into phases such that each phase contains 
communications that the underlying network
can support. The communication phase analysis utilizes a fourth component
of the system, the {\em connection scheduling algorithms}, to realize 
a given communication pattern with a minimal number of channels. 


\begin{figure}
\centerline{\psfig{figure=fig/overall.eps,height=3.5in}}
\caption{The major components in the E--SUIF compiler}
\label{overall}
\end{figure}
 
Next, the programming model of the compiler will be discussed, followed
by the four components needed to support compiled communication.

\section{Programming model}

The E--SUIF compiler considers structured HPF--like programs 
that contain conditionals 
and nested loops, but no arbitrary goto statements. The programmer
explicitly specifies the data alignments and distributions. 
For simplicity, this chapter assumes that all arrays are aligned to a 
single virtual processor grid template, and the data distribution is 
specified through the distribution of the template.
However, the implementation of the communication analyzer
handles multiple virtual processor grids. 
Arrays are aligned to the virtual processor grid by 
simple affine functions. The alignments allowed are scaling, 
axis alignment and  offset alignment.  The mapping from a point 
$\vec{d}$ in data space to the
corresponding point $\vec{e}$ on the virtual processor grid is specified by
an alignment matrix $M$ and an alignment offset vector $\vec{v}$. 
$\vec{e} = M \vec{d} + \vec{v}$. 
The alignment matrix $M$ specifies the scaling and the axis alignment, thus
it is a permutation of a diagonal matrix. 
The distribution of the virtual 
processor grid can be cyclic, block or block--cyclic. Assuming that there
are $p$ processors in a dimension, and the block size of that 
dimension is $b$, the virtual processor $e$ is in physical processor
$e\ mod\ (p*b) / b$. For cyclic distribution, $b=1$. For block distribution,
$b=n/p$, where $n$ is the size of the virtual processes along the dimension. 


The communication analyzer performs communication optimizations
on each subroutine. 
A subroutine is represented by an {\em
interval flow graph} $G = (N, E)$, with nodes N and edges E.
The communication optimizations are 
based upon a variant of Tarjan's intervals \cite{Tarjan74}.
The optimizations require that there are no {\em critical edges}
which are edges that 
connect a node with multiple outgoing edges to a node with multiple 
incoming edges. The critical edges can be eliminated by edge splitting 
transformation\cite{Gupta96}. Figure~\ref{EXAMPLE} shows
an example code and its corresponding interval flow graph.
%Notice that intervals can be easily identified
%in SUIF's hierarchical intermediate representation.


\begin{figure}[tbph]
%\begin{subfigRow*}
\begin{minipage}{10cm}
%\small
%\footnotesize
\begin{tabbing}
\hspace{0.5in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
\hspace{0.5in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
\hspace{0.5in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
\hspace{0.5in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
\hspace{0.5in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
\hspace{0.5in}(s4)\hspace{0.1in}\>enddo\\
\hspace{0.5in}(s5)\hspace{0.1in}enddo\\
\hspace{0.5in}(s6)\hspace{0.1in}do i = 1, 100\\
\hspace{0.5in}(s7)\hspace{0.1in}\>do j = 1, 100\\
\hspace{0.5in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
\hspace{0.5in}(s9)\hspace{0.1in}\>enddo\\
\hspace{0.5in}(s10)\hspace{0.1in}enddo\\
\hspace{0.5in}(s11)\hspace{0.1in}do i = 1, 100\\
\hspace{0.5in}(s12)\hspace{0.1in}\>do j = 1, 100\\
\hspace{0.5in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
\hspace{0.5in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
\hspace{0.5in}(s15)\hspace{0.1in}\>end do\\
\hspace{0.5in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
\hspace{0.5in}(s17)\hspace{0.1in}end do\\
\end{tabbing}

\end{minipage}

\begin{minipage}{10cm}
\centerline{\psfig{figure=fig/3.eps,width=4in}}
\end{minipage}
%\end{subfigRow*}
\normalsize
\caption{An example program and its interval flow graph}
\label{EXAMPLE}
\end{figure}

\section{The communication analyzer}
\label{commlab}

The communication analyzer analyzes the communication
requirement on virtual processor grids and performs a number of common
communication optimizations. This section
presents the data flow descriptor used in the analyzer 
to describe communication, the general data flow algorithms to propagate
the data flow descriptor, and the communication optimizations
performed by the analyzer. 

\subsection{Section communication descriptor (SCD)}

In order for the compiler to analyze the communication requirement of a 
program, data structures must be designed for the compiler to 
represent the communications in the program. 
The data structures must both be powerful enough to represent 
the communication requirement and simple enough to be  manipulated easily.
%A communication descriptor, call {\em Section Communication Descriptor} (SCD),
%is designed for the analyzer.
%It can be used for both communication 
%optimization and to derived the communication pattern in physical processor
%space. 

\subsubsection*{The descriptor}

The communication analyzer represents communication using
{\em Section Communication Descriptor} (SCD).
% This subsection
%describes the format of the descriptor.
A $SCD = <A, D, CM, Q>$ consists of
 three components.
The first component is the array region that is involved in the
communication. This includes the array name $A$ and the array region
descriptor $D$. The second component is the communication mapping descriptor
$CM$, which describes the source--destination relationship of 
the communication.
The third component is a qualifier descriptor $Q$, which specifies the time
when the communication is performed.

The {\em bounded regular section descriptor} (BRSD)\cite{callahan88} is used
as the region descriptor. The region $D$ is a vector of subscript values.
Each element in the vector is either
(1) an expression of the form $\alpha*i + \beta$, where
$\alpha$ and $\beta$ are invariants and i is a loop
index variable, or (2) a triple
$l:u:s$, where $l$, $u$ and $s$ are invariants. The triple,
 $l:u:s$, defines a set of values, $\{l$, $l+s$, $l+2s$, ..., $u\}$, as used
in the array statement in HPF.

The source--destination mapping $CM$ is denoted as
$<src, dst, qual>$. The source, $src$, is a vector whose elements are
of the form $\alpha*i + \beta$, where $\alpha$ and $\beta$ are
invariants and $i$ is a loop index variable. The destination,
$dst$, is a vector whose elements are of the form
$\gamma*j + \delta$, where
 $\gamma$ and $\delta$ are
invariants and $j$ is a
loop index variable. The {\em mapping qualifier} list,
$qual$, is a list of range descriptors. Each range descriptor
is  of the form $i = l:u:s$, where $l$, $u$ and $s$ are invariants and
$i$ is a loop index variable.
The notation $qual=NULL$ and $qual = \perp$ denote that
no mapping qualifier is needed.
The mapping qualifier specifies the range of a variable in $dst$ that
does not occur in $src$ to express the broadcast effect.

The qualifier $Q$ is a range descriptor of the form $i = l:u:s$,
where $i$ is the loop index variable of the loop that directly
encloses the SCD. This qualifier is used to indicate the
iterations of the loop in which the SCD should be performed.
If the SCD is to be performed in every iteration in the loop,
$Q=NULL$ or $Q = \perp$. $Q$ will be referred to as the
{\em communication qualifier}.
Notice that the qualifiers in most SCDs are NULL.

\subsubsection*{Operations on SCD}

Operations, such as intersection, difference and union, on SCD descriptors
are defined next. Since in many cases, operations do not have sufficient
information to yield  exact results, {\em subset} and {\em superset} versions 
of these operations are implemented. The analyzer uses a proper
version to obtain conservative approximations. These operations
are extensions of the operations on BRSD. 
%Since SCD 
%descriptors are composed of three parts, their operations usually reduce to
%operations on their components. 

\noindent
{\bf Subset Mapping testing}. Testing whether a mapping is a subset of another
mapping is one of the most commonly used operations in the analyzer.
Testing that a mapping relation $CM_1$ ($= <s_1, d_1, q_1>$) is a subset of
another mapping relation $CM_2$ ($= <s_2, d_2, q_2>$) is
done by checking for a solution of equations $s_1 = s_2$ and $d_1 = d_2$, 
where variables in $CM_1$ are treated as constants and variables in $CM_2$ 
as variables,  and
subrange testing $q_1 \subseteq q_2$. 
%The equations 
%$s_1 = s_2$ and $d_1 = d_2$ can easily be solved by 
%treating variables in $M_1$ as constants and 
%variables in $M_2$ as variables. 
Note that since the elements
in $s_1$ and $s_2$ are of the form $\alpha*i+\beta$, the equations can
generally be solved efficiently. Two mappings, $CM_1$ and $CM_2$ are 
{\em related}
if $CM_1 \subseteq CM_2$ or $CM_2 \subseteq CM_1$. Otherwise, they are
unrelated.

\noindent
{\bf Subset SCD  testing}. Let
$S_1=<A_1, D_1,CM_1, Q_1>$, $S_2=<A_2, D_2,CM_2, Q_2>$,
$SCD_1 \subseteq SCD_2 \Longleftrightarrow A_1 = A_2 \wedge
D_1 \subseteq D_2\wedge CM_1 \subseteq CM_2 \wedge Q_1 \subseteq Q_2$.

\noindent
{\bf Intersection Operation}. The intersection of two SCDs represents the 
elements constituting the common part of their array sections that have the 
same mapping relation. The following algorithm describes the subset version of
the intersection operation. 
%The superset version can just return any of the two SCDs.
Note that the operation requires the qualifier $Q_1$ to be equal to $Q_2$ to 
obtain a non empty result. $\phi$ denotes an empty set.
This approximation will not hurt the 
performance significantly since most 
SCDs have $Q=\perp$.

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> \cap <A_2, D_2, CM_2, Q_2>$\\ 
\hspace{0.2in}= $\phi$,  if $A_1 \ne A_2$ or $CM_1$ and $CM_2$ are 
                             unrelated or 
                            $Q_1 \ne Q_2$\\
\hspace{0.2in}= $<A_1, D_1\cap D_2, CM_1, Q_1>$,  if $A_1 = A_2$ and $CM_1 
                                            \subseteq CM_2$ and $Q_1 = Q_2$\\
\hspace{0.2in}= $<A_1, D_1\cap D_2, CM_2, Q_1>$,  if $A_1 = A_2$ and $CM_1 
                                            \supseteq CM_2$ and $Q_1 = Q_2$
\end{tabbing}

\noindent
{\bf Difference Operation}. The difference
 operation causes a part of the array 
region associated with the first operand to be invalidated at all the 
processors where it was available. In the analysis, the difference operation
is only used to subtract  elements killed (by a statement, or by a 
region), which means that the SCD to be subtracted always has 
$CM=\top$ and  $Q=\perp$. 

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> - <A_2, D_2, \top, \perp>$\\
\hspace{0.2in}= $<A_1, D_1, CM_1, Q_1>$,  if $A_1 \ne A_2$\\
\hspace{0.2in}= $<A_1, D_1 - D_2, CM_1, Q_1>$, if $A_1 = A_2$.
\end{tabbing}


\noindent
{\bf Union operation.} The union of two SCDs represents the 
elements that can be in either part of their array section.
This operation is given by:

\begin{tabbing}
\hspace{0.2in}$<A_1, D_1, CM_1, Q_1> \cup <A_2, D_2, CM_2, Q_2>$\\
\hspace{0.2in}= $<A_1, D_1\cup D2, CM_1, Q_1>$, if $A_1 = A_2$ and 
                                            $CM_1 = CM_2$ and $Q_1=Q_2$\\
\hspace{0.2in}= list($<A_1, D_1, CM_1, Q_1>$, $<A_2, D_2, CM_2, Q_2>$), 
                otherwise.
\end{tabbing}

\subsection{A demand driven array data flow analysis framework}
\label{arraydflow}

Many communication optimization opportunities can be uncovered by
propagating SCDs globally. For example, if a SCD
can be propagated from a loop body to the loop header without being killed
in the process of propagation, the communication represented by the SCD
can be hoisted out of the loop body, that is,
 the communication can be vectorized.
Another example is the redundant communication elimination.
While propagating $SCD_1$,  if $SCD_2$ is encountered such that $SCD_2$ is 
a  subset of the $SCD_1$, then the communication represented by $SCD_2$ can
be subsumed by the communication represented by $SCD_1$ and can be eliminated.
Propagating  SCDs backward can find the earliest point to place the 
communication, while propagating SCDs forward can find the latest point where
the effect of the communication is destroyed. Both these
two propagations are useful in communication optimizations. 
Since forward and backward propagation
are quite similar, only backward propagation will be presented next.

Generic demand driven algorithms are developed to propagate
SCDs through interval flow graph. The analysis technique is
the reverse of the interval-analysis \cite{gupta93}.
Specially, by reversing the information flow associated with program points,
a system of request propagation rules is designed.
SCDs are propagated 
until they cannot be propagated any further, 
that is, all the elements in the SCDs are
killed. However, in practice, the compiler may choose to 
terminate the propagation prematurely to
save analysis time while there are still elements in SCDs. 
In this case, since the analysis starts from the 
points that contribute to the  optimizations,
the points that are textually close to the starting points, where
most of the optimization opportunities are likely to be 
present, are considered.
This gives the demand driven algorithm the ability to trade precision for
time. In the propagation, at a given time, only a single interval
is under consideration. Hence, the propagations are logically done in
an acyclic flow graph. During the propagation, a 
SCD may expand when it is propagated out of a loop. When a set of
elements of SCD is killed inside a loop, the set is propagated into the loop
to determine the exact point where the elements are killed. There are 
two types of propagations,
{\em upward} propagation, in which SCDs may need to be 
expanded, and {\em downward} propagation, in which SCDs may need to be 
shrunk. 

The format of a data flow {\em propagation request}
is  $<S, n, [UP|DOWN], level, cnum>$, where S is a SCD, n is a node
in the flow graph, constants $UP$ and $DOWN$ indicate whether the request is  
upward propagation  or downward propagation, $level$ indicates
at which level is the request and the value $cnum$ 
indicates which child node of 
$n$ has triggered the request. A special value $-1$ for $cnum$ is used as
the indication of the beginning of downward propagation.
The propagation request triggers 
some local actions and causes the propagation of a SCD from the node n. 
The propagation of SCDs follows the following rules. It is  assumed that node 
$n$ has $k$ children.

\subsubsection*{Propagation rules}

\noindent
{\bf RULE 1: upward propagation: regular node}. 
The request on a regular node takes an action based
 on SCD set $S$ and the local
information. It also propagates the information upward.
The request stops when S become empty. The rule is shown in the following 
pseudo code. In the code, functions $action$ and $local$
are depended on the type of optimization to be performed.
The $pred$ function finds all the nodes that are  predecessors in the 
interval flow graph and the 
set $kill_n$ includes all the elements defined in node
$n$. Note that $kill_n$ can be represented as an SCD.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S_1, n, UP, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, UP, level, k>$) : \\
\hspace{0.2in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.2in}\>action(S, local(n))\\
\hspace{0.2in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - kill_n, m, UP, level, j>$)\\
\end{tabbing}

A response to requests in a node $n$ 
occurs only when all its successors have been processed. This 
guarantees that in an acyclic flow graph
each node will only be processed once. The side effect is that 
the propagation will not pass beyond a branch point.
A more aggressive scheme can 
propagate a request through a node without checking whether
all its successors are processed. In that scheme, however, a nodes may need 
to be processed multiple times to obtain the final solution.    

\noindent
{\bf RULE 2: upward propagation: same level loop header node}.
The loop is contained in the current level. The request needs to obtain the
summary information, $K_n$, for the interval, perform the action
based on $S$ and the summary information, propagate the information past 
the loop and trigger a downward propagation to propagate the information
into the loop nest. Here, 
the summary function $K_n$, summarizes all the elements defined
in the interval. 
It can  be calculated either before hand or in a 
demand driven manner. The method to 
calculate the summary in a demand driven manner will be described later. 
Note that a loop header can only have one successor besides the 
entry edge into the loop body. The $cnum$ of the downward request
is set to -1 to indicate that it is the start of the downward propagation.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{0.2in}\>action(S, $K_n$) \\
\hspace{0.2in}\>if\=\ ($S - K_n \ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - K_n, m, UP, level, j>$)\\
\hspace{0.2in}\>if ($S\cap K_n \ne \phi$) then\\
\hspace{0.2in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$)\\
\end{tabbing}

\noindent
{\bf RULE 3: upward propagation: lower level loop header node}.
The relative level between the propagation request
and the node can be determined by 
comparing the level in the request and the level of the node. Once a request
reaches the loop header. The request will need to be expanded to be 
propagated in the upper level. At the same time, this request triggers 
a downward propagation for the set of elements that are killed in the loop. 
Assume that the loop index variable is $i$ with bounds $low$ and $high$.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{0.2in}\>calculate the summary of loop $n$\\
\hspace{0.2in}\>outside = $expand(S, i, low:high) - 
                     \cup_{def}expand(def, i, low:high)$\\ 
\hspace{0.2in}\>inside = $expand(S, i, low:high) \cap 
                     \cup_{def}expand(def, i, low:high)$\\
\hspace{0.2in}\>if\=\ (outside $\ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<outside, m, UP, level -1, j>$)\\
\hspace{0.2in}\>if (inside $\ne \phi$) then\\
\hspace{0.2in}\>\>request($<inside,  n, DOWN, level, -1>$)\\
\end{tabbing}

The variable $outside$ contains the elements that can be propagated out of
the loop, while the variable $inside$ contains the elements that are killed
within the loop. The expansion function has the same definition as in 
\cite{gupta93}. For a SCD descriptor S, expand(S, k, low:high) is a 
function which replaces
all single data item references $\alpha*k+\beta$ used in any
array section descriptor D in S by the triple ($\alpha*low+\beta:
\alpha*high+\beta:\alpha$). 
The set $def$ includes all the definitions that are the source of a
flow-dependence.

\noindent
{\bf RULE 4: downward propagation: lower level loop header node}.
This is the initial downward propagation. The loops index variable, $i$, 
is treated as a constant in the downward propagation. 
Hence, SCDs that are propagated into the loop body 
must be changed to be the initial
available set for iteration $i$, that is, subtract all the variables 
killed in the iteration i+1 to high and propagate the information from the 
tail node  to the head node. This propagation prepares the downward 
propagation into the loop body by shrinking the SCD for each iteration.

\begin{tabbing}
\hspace{0.2in}qu\=ery($<S, n, UP, level,cnum>$): \\
\hspace{0.2in}\>if\=\ $(cnum = -1)$ then\\
\hspace{0.2in}\>\>calculate the summary of loop $n$;\\
\hspace{0.2in}\>\>request($<S - \cup_{def}expand(def, k, i+1:high), 
                    l, DOWN, level-1, 1>$);\\
\hspace{0.2in}\>else\\
\hspace{0.2in}\>\> STOP /* interval processed */\\
\end{tabbing}

\noindent
{\bf RULE 5: downward propagation: regular node}.
For regular node, the downward propagation is similar to the upward
propagation.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S_1, n, DOWN, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, DOWN, level, k>$) : \\
\hspace{0.2in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.2in}\>action(S, local(n))\\
\hspace{0.2in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - kill_n, m, DOWN, level, j>$)\\
\end{tabbing}

\noindent
{\bf RULE 6: downward propagation: same level loop header node}.
When downward propagation reaches a loop header (not the loop header
whose body is being processing), it must generate further downward
propagation request to go deeper into the body.

\begin{tabbing}
\hspace{0.2in}re\=quest($<S, n, DOWN, level, 1>$): \\
\hspace{0.2in}\>action(S, summary(n)); \\
\hspace{0.2in}\>if\=\ ($S-K_n \ne \phi$) then\\
\hspace{0.2in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{0.2in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{0.2in}\>\>\>request($<S - K_n, m, DOWN, level, j>$);\\
\hspace{0.2in}\>if\=\ ($S\cap K_n \ne \phi$) then\\
\hspace{0.2in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$);\\
\end{tabbing}

\subsubsection*{Summary calculation}

During the request propagation, the summary information of an interval is
needed when a loop header is encountered. 
An algorithm is described 
to obtain the summary information in a demand driven manner.
The calculation of kill set of the interval is used as an example. Let 
$kill(i)$ 
be the variables killed in node $i$, $K_{in}$  and $K_{out}$ 
be the variables killed before and after the node respectively.
Figure.~\ref{KILL} depicts the demand driven algorithm. The 
algorithm propagates the data flow information from the tail node to the 
header node in the interval using the following data flow equation:\\

\centerline{$K_{out}(n) = \cup_{s\in succ(n)}K_{in}(s)$}
\centerline{$K_{in}(n) = kill(n)\cup K_{out}(n)$}

When an inner loop header is
encountered, a recursive call is issued to get the summary information
for the inner interval. Once a loop header is reached, the kill set needs
to be expanded to be used by the outer loop.


\begin{figure}[htbp]
\begin{tabbing}
\hspace{0.2in}(1)\hspace{0.5in}Su\=mmary\_kill(n)\\
\hspace{0.2in}(2)\hspace{0.5in}\>$K_{out}(tail)$ = $\phi$\\
\hspace{0.2in}(3)\hspace{0.5in}\>fo\=r all $m\in T(n)$ and 
                               level(m) = level(n)-1 in backward order\\
\hspace{0.2in}(4)\hspace{0.5in}\>\>if\=\ m is a loop header then\\
\hspace{0.2in}(5)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                    K_{in}(s)$\\
\hspace{0.2in}(6)\hspace{0.5in}\>\>\>$K_{in}(m)$ = summary\_kill(m) $\cup
                                   K_{out}(m)$\\
\hspace{0.2in}(7)\hspace{0.5in}\>\>else\\ 
\hspace{0.2in}(8)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                   K_{in}(s)$\\
\hspace{0.2in}(9)\hspace{0.5in}\>\>\>$K_{in}(m)$ = $kill(m) \cup K_{out}(m)$\\
\hspace{0.2in}(10)\hspace{0.5in}\>return (expand($K_{in}(header)$, i, low:high))\\
\end{tabbing}
\caption{Demand driven summary calculation}
\label{KILL}
\end{figure}


\subsection{The analyzer}

The analyzer performs message vectorization, redundant communication 
elimination and communication scheduling using algorithms based upon
the demand driven algorithms described in the previous section. 
The analyzer performs the following steps:

\begin{enumerate}
\item {\em Initial SCD calculation}. 
Here the analyzer calculates the 
communication requirement for each statement that contains remote
memory references. Communications required
by each statement
are called {\em initial SCDs} for the statement and 
are placed preceding the statement.

\item {\em Message vectorization and available communication summary 
calculation}. The analyzer propagates
initial SCDs to the outermost loops in which they
can be placed. 
In addition to message vectorization optimization, 
this step also calculates
the summary of communications that are available after each loop. This 
information is used in the next step
for redundant communication elimination.

\item {\em Redundant communication elimination}. 
The analyzer performs redundant communication elimination using a demand 
driven version
of availability communication analysis \cite{gupta93}, which computes 
communications that are available before each statement. A communication
in a statement 
is redundant if it can be subsumed by available communications at
the statement. The analyzer also eliminates partially redundant 
communications. 

\item {\em Message scheduling}. 
The analyzer schedules messages
within each interval by placing messages with the same communication patterns
together and combining the messages to reduce the number
of messages. 
\end{enumerate}

\subsubsection*{Initial SCD Calculation}

The {\em owner computes} rule is assumed which 
requires each remote item referenced on the right handside of an 
assignment statement to be sent to the processor that owns the left
handside variable.
Initial SCDs for each statement represent this data movement.
Since the ownership of array elements determines  communication patterns,
the ownership of array elements will be described before the initial
SCD calculation step is presented.

\subsubsection*{Ownership.}

All arrays are aligned to a single virtual processor grid by 
affine functions. 
The alignments allowed are scaling, axis alignment and 
offset alignment.  The mapping from a point $\vec{d}$ in data space to a 
corresponding point $\vec{e}$ on the virtual processor grid (the owner of 
$\vec{d}$) can be specified by
an alignment matrix $M$ and an alignment offset vector $\vec{v}$
such that
$\vec{e} = M \vec{d} + \vec{v}$.  Using the alignment matrix and the 
offset vector, the owner of a data element
can be determined. Consider the 
 array $w$ in the example program in 
Figure~\ref{EXAMPLE}, the alignment matrix and the offset vector
 are given below.
\begin{center}
\small
\[ M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{v}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
\end{center}

\subsubsection*{Initial SCD Calculation.}

Using the ownership information, the initial SCDs are calculated
as follows. Let us consider each component in an 
initial $SCD = <A, D, CM, Q>$. $A$ is the array to be communicated.
The  region 
$D$ contains a single index given by the array subscript expression. 
The qualifier $Q = \perp$ since initial communications
must be performed in every iteration. Let $CM = <src, dst, qual>$. Since
initially communication does not perform broadcast, 
$qual = \perp$. Hence, the calculation of  $src$ and $dst$, 
which will be discussed in the following text, 
 is the only non-trivial computation in the calculation of initial
SCDs. 

Let $\vec{i}$ be the vector of loop induction variables. When  subscript
expressions are affine functions, an array reference can be
expressed as $A(G\vec{i} + \vec{g})$, where $A$ is the array name, 
$G$ is a matrix and $\vec{g}$ is  a vector. 
$G$ is called the {\em data access matrix}
and $\vec{g}$ the {\em access offset vector}.  The data access
matrix, $G$, and the access offset vector, $\vec{g}$, describe
a mapping from a point in the iteration space to a point in the data space.
Let $G_l$, $\vec{g}_l$, $M_l$, $\vec{v}_l$ be the data access matrix,
the access offset vector, the alignment matrix and the alignment vector 
for the {\em lhs} array reference, and  
$G_r$, $\vec{g}_r$, $M_r$, $\vec{v}_r$ be the corresponding quantities
for the {\em rhs} array reference. 
The source processor $src$ and destination processor $dst$
are given by:

\centerline{$src = M_r(G_r\vec{i} + \vec{g}_r) + \vec{v}_r,$ \hspace{1in}
            $dst = M_l(G_l\vec{i} + \vec{g}_l) + \vec{v}_l$}

Consider the communication of $w(i,j)$ in statement $s13$ 
in Figure~\ref{EXAMPLE}.
The analyzer can obtain from the program the data 
access matrices, access offset vectors, alignment matrices 
and alignment vectors and from them the  SCD for the communication 
given below.
As an indication of the complexity of a SCD, the structure for this 
communication required 524 bytes to store.
\begin{center}
\small
\[ M_z  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{v}_z = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{v}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
\[ G_l  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_l = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   G_r  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_r = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right)
\]
\end{center}
\centerline{$<A=w, D=(i, j), CM=<(2*j, i+1), (i, j), 
            \perp>, Q= \perp>$}

\subsubsection*{Message Vectorization and Available Communication Summary}

In this phase, the analyzer computes 
{\em backward exposed} communications, which 
are SCDs that can be hoisted out of a loop,
and  {\em forward exposed} communications, which are SCDs that are
available after the loop.
Backward exposed communications represent 
actual communications vectorized from inside the loop. When a SCD is 
vectorized, the initial SCD at the assignment statement
are replaced by SCDs
for  backward exposed communications at loop headers. 
Forward exposed communications
represent the  communications that are performed inside a loop and 
are still alive after the loop. 
Hence they can be used to subsume communications appearing
after the loop. By using data dependence information,  backward and
forward exposed communications are calculated by
propagating  SCDs from inner loop bodies to loop headers using a 
simplified version of the rules discussed in previous section.
% The rule is simplified in that the DOWN propagate of SCDs back into
%the loop body is no longer performed. 

%The calculations of forward exposed communications do not effect the
%SCDs in the original statement, since  the available communication
%only represents the effect
%of communications. 
Algorithms for the forward and backward exposed communication calculation
are  described in Figure~\ref{phase1}~(a) and (b). 
%Notation 
%$Request(S, n, UP|DOWN)$ denotes placing an UP or DOWN propagation of S after
%node n.  
Since only UP propagation
is needed,  $Request(S, n)$ is used to denote placing a
propagation of $S$ after node $n$.
In the algorithms,  $S$ is a
SCD occurring inside the interval whose header is node $n$ and whose induction
variable is $i$ with lower bound $1$ and upper bound $h$, 
$anti\_def$ is the set of definitions in the interval that have 
anti--dependence relation with the original array reference that causes  the
communication $S$, 
$flow\_def$ is the set of definitions in the interval that have 
flow--dependence relation with the original array reference that causes the
communication $S$. 
%The expansion function has the similar definition as in 
%\cite{gupta93}. 
For a SCD, S, $expand(S, i, 1:h)$ first 
determines which portion of the $S=<A, D, CM, Q>$ to be expanded.
If $D$ is to be expanded, that is, $i$ occurs in D, 
the function will replace
all single data item references $\alpha*i+\beta$ used in D
by the triple $\alpha+\beta:\alpha*h+\beta:\alpha$. 
If $D$ cannot be expanded, that is, after expansion $D$ is not in the 
allowed form, then the communications will stay inside the loop.
If $CM=<src, dst, qual>$ 
is to be expanded, that is,  $i$ occurs in $dst$ but not in $src$ and $D$,
the function will add $i=1:h:1$ 
into the mapping qualifier list $qual$. 

The algorithms determine the part of communications $Outside$, that 
can be hoisted out of a loop, and $Inside$, that cannot be hoisted
out of the loop. In forward exposed communication calculation, 
the analyzer makes $Outside$ as the forward exposed communication and ignores
the $Inside$ part. 
%Note again that forward exposed 
%communication calculation only calculates the communications that are alive
%after a loop and does not actually change the communications in a program.
In backward exposed communication calculation, the analyzer
makes $Outside$ as backward exposed communication. In addition, the analyzer
must also change the original SCD according to contents of $Inside$. In
the case when the SCD can be fully vectorized, 
%$Inside$ will be empty and 
%all communications in the SCD inside the loop are summarized by $Outside$ and 
%are hoisted out of the loop. 
the SCD in the original statement is 
removed. In the case when the SCD cannot be fully vectorized, part of the
communication represented by 
$Outside$ is hoisted  out of the loop, while other part represented by
$Inside$ stays at the original statement. Thus, the SCD in the original
statement must be modified by a  communication qualifier to indicate that
the SCD only remains in iterations that generate communications 
in $Inside$.
%Note that in forward exposed communication calculation,
%communications are killed by anti--dependence while in backward exposed 
%communication calculation,  communications are killed by flow--dependence.

\begin{figure}[htbp]
\small
\footnotesize
\begin{subfigRow*}
\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}re\=qu\=es\=t$(S, n)$ : \\
\>$Outside = expand(S, i, 1:h) -$\\  
\>\>$\cup_{anti\_{def}}expand(anti\_def, i, 
             1:h)$\\
\>if $(Outside \ne \phi)$ then\\
\>\>record $Outside$ as\\ 
\>\>\>forward exposed in node n \\
\>\>Let m be the header of the\\
\>\>\>interval including node $n$\\
\>\>$request(Outside, m)$;\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
(a) Forward exposed communication
\end{tabbing}
\end{minipage}

\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}re\=qu\=es\=t$(S, n)$ : \\
\>$Outside = expand(S, i, 1:h) -$\\
\>\>$\cup_{flow\_def}expand(flow\_def, i, 
             1:h)$\\
\>$Inside = expand(S, i, 1:h) \cap$\\
\>\>$\cup_{flow\_def}expand(flow\_def, i, 1:h)$\\
\>if $(Outside \ne \phi)$ then\\
\>\>convert $Inside$ in terms of $S$\\
\>\>\>with qualifier, denoted as $D$\\
\>\>if (conversion not successful) then\\
\>\>\>stop /* fail */\\
\>\>else\\
\>\>\>ch\=ange the S into D\\
\>\>\>record $Outside$ as backward\\
\>\>\>\>exposed comm. at node $n$.\\
\>\>\>Let m be the header of the\\
\>\>\>\>interval including node $n$\\
\>\>\>$request(Outside, m)$;\\
\\
(b) Backward exposed communication
\end{tabbing}
\end{minipage}
\end{subfigRow*}
\caption{Algorithms for the forward and backward exposed communication}
\label{phase1}
\end{figure}


%\begin{figure}
%\begin{subfigRow*}
%\small
%\footnotesize
%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} DO\=\ i = 1, 100\\
%              \>COMM: $C_1= <A, (1), <(1), (i), \perp>,\perp>$\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2= <B, (i-1), <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (a)
%\end{tabbing}
%\end{minipage}
%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} FORWARD COMM: $C_1^f=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} FORWARD COMM: $C_2^f=<B, [0:99], <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in} DO\=\ i = 1, 100\\
%              \>COMM: $C_1= <A, (1), <(1), (i), \perp>,\perp>$\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2= <B, (i-1), <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (b)
%\end{tabbing}
%\end{minipage}

%\begin{minipage}{8cm}
%\begin{tabbing}
%\hspace{0.2in} FORWARD COMM: $C_1^f=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} FORWARD COMM: $C_2^f=<B, [0:99], <(i-1), (i), \perp>, \perp>$\\
%\hspace{0.2in} BACKWARD COMM: $C_1^b=<A, (1), <(1), (i), i=1:100:1>, 
%                              \perp>$\\
%\hspace{0.2in} BACKWARD COMM: $C_2^b=<B, (0), <(i-1), (i), \perp>, 
%                               \perp>$\\
%\hspace{0.2in} DO\=\ i = 1, 100\\
%\hspace{0.2in}\>d(i) = a(1)\\
%              \>COMM: $C_2^s= <B, (i-1), <(i-1,1), (i,1), \perp>, i=2:100:1>$\\
%\hspace{0.2in}\>b(i) = b(i-1)\\
%\hspace{0.2in}  ENDDO\\
%\hspace{1.5in} (c)
%\end{tabbing}
%\end{minipage}
%\end{subfigRow*}
%\caption{Message vectorization}
%\label{VEC}
%\end{figure}

\begin{figure}[tbph]
\centerline{\psfig{figure=fig/vexam.eps,width=3.5in}}
\caption{Calculating backward exposed communications}
\label{VEC}
\end{figure}

Consider communications in the loop in 
Figure~\ref{VEC}. Assume that arrays $a$, $b$ and $d$ are identically aligned
to the virtual processor grid, initial SCDs, $C1$ and $C2$,  are shown in 
Figure~\ref{VEC}.
$C3$, $C4$ and $C5$ are the communications after the backward exposed 
communication calculation. 
Calculating the backward exposed communication for $C1$
results in  communication $C3$ 
in the loop header and the removal of  the communication $C1$ from its
original statement.
Calculating  the backward exposed communication for $C2$
puts $C4$ in the loop header and changes $C2$ into $C5$. 
Note that, there is a  flow--dependence relation from b(i) to 
 b(i-1). In calculating the 
backward exposed communication for SCD $C2$, 
$Inside = <b, (1:99:1), <(i-1,1), (i,1), \perp>, \perp>$. Converting $Inside$
back in terms of $C2$ results in $C5$.


%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{1.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{1.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{1.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{1.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{1.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{1.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_2=<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_3=<w, (2:101:1, 1:99:1), <(2*j, i+1), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_4=<y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 100)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{1.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{1.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{1.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{1.2in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
%\hspace{1.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after vectorization}
%\label{EXAMVEC}
%\end{figure}


%\begin{figure}[htbp]
%\small
%\footnotesize
%\begin{tabbing}
%\hspace{1in}\hspace{0.5in}Do\=\ i=1, 100\\
%\hspace{1in}s1: \>D[i] = A[1]\\
%\hspace{1in}s2:\>B[i] = B[i-1] + 1\\
%\hspace{1in}\hspace{0.5in}End Do\\
%\hspace{1in}\hspace{0.5in}Do i=1, 100\\
%\hspace{1in}s3:\>C[i] = B[i-1] + 1\\
%\hspace{1in}\hspace{0.5in}End Do
%\end{tabbing}
%\caption{backward and forward exposed communication}
%\label{BACKWARD}
%\end{figure}

\subsubsection*{Redundant Communication Elimination}

This phase calculates  available communications before each statement,
and eliminates a communication at the statement if the communication
is available. This optimization is done by propagating SCDs forward until
all elements are killed. During the propagation, if another SCD that can be
subsumed is encountered, that SCD is redundant and can be eliminated.

\begin{figure}[htbp]
\small
\footnotesize
\begin{subfigRow*}
\begin{minipage}{10cm}
\begin{tabbing}
\hspace{0.1in}re\=quest($S_1, n, UP$) $\wedge$ ... \\
\hspace{0.1in}$\wedge$request($S_k, n, UP$) : \\
\hspace{0.1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{0.1in}\>if\=\ (SCDs in n is a subset of S) then\\
\hspace{0.1in}\>\>remove the SCDs\\
\hspace{0.1in}\>if $(S-kill_n \ne \phi)$ then\\
\hspace{0.1in}\>\>fo\=r all $m\in succ(n)$\\
\hspace{0.1in}\>\>\>request($S - kill_n, m, UP$)\\
\\
\hspace{0.1in}{(a) Actions on nodes within an interval}\\
\end{tabbing}
\end{minipage}

\begin{minipage}{10cm}
\begin{tabbing}
\hspace{0.1in}re\=quest($S, n, UP$): \\
\hspace{0.1in}\>calculate the summary of loop $n$\\
\hspace{0.1in}\>In\=side= $expand(S, i, 1:i-1) \cap$\\
\hspace{0.1in}\>\> $(\cup_{def}expand(def, i, 1:i-1))$\\
\hspace{0.1in}\>if\=\ (inside $\ne \phi$) then\\
\hspace{0.1in}\>\>Let $l$ be the first node.\\
\hspace{0.1in}\>\>request($Inside,  l, DOWN$)\\
\\
\\
\hspace{0.1in}{(b) Actions on a loop header}\\
\end{tabbing}
\end{minipage}
\end{subfigRow*}
\vspace{-0.15in}
\caption{Actions in forward propagation}
\label{FORWARD1}
\vspace{-0.15in}
\end{figure}

Using the interval analysis
technique \cite{gupta93}, two passes are needed to obtain
the data flow solutions in an interval. 
Initially, UP propagations are performed.
Once the UP propagations reach interval headers, summaries of the SCDs
are calculated and  DOWN propagations of the summaries
are triggered. Note that since 
the data flow effect of propagating SCDs between
intervals is captured in the message vectorization
 phase of the analyzer, both the UP
and DOWN propagations are
performed within an interval in this phase.

Assuming that node $n$ has $k$ predecessors. 
When propagating SCDs within an interval in forward propagation, 
actions in a node
will be triggered only when all its predecessors place requests. The nodes
calculate the SCD available by performing intersection on all
SCDs that reach it, check whether communications within the node can 
be subsumed, and propagate the live communications  forward. 
 Figure~\ref{FORWARD1}~(a) describes actions 
on the nodes inside the interval in an UP forward propagation.
When the UP propagation reaches an interval boundary,
the summary information is calculated by obtaining all the elements that
are available in iteration $i$, and a DOWN propagation is triggered.
Note that in forward propagation,   communications
can be safely assumed to be performed in every iteration ($Q=\perp$), 
since the effect of the communication
must guarantee that the valid values are at the proper processors for the 
computation. Figure~\ref{FORWARD1}~(b)
shows actions at interval boundaries. The
propagation of a DOWN request is similar to that of an UP request except that
a DOWN propagation stops at interval boundaries.
% The results of redundant 
%communication elimination in the program in Figure~\ref{EXAMVEC} is shown in 
%Figure~\ref{EXAMRED}. As can be seen from the figure, the communication
%$C_3$ in Figure~\ref{EXAMVEC} is subsumed by communication $C_1$.

%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{1.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{1.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{1.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{1.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{1.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{1.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{1.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{1.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{1.4in}COMM: $C_2=<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.4in}COMM: $C_4=<y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>$\\
%\hspace{1.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 100)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{1.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{1.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{1.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{1.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{1.2in}(s16)\hspace{0.1in}\>w(i+1, 100) = ...\\
%\hspace{1.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after redundant communication elimination}
%\label{EXAMRED}
%\end{figure}


\subsubsection*{Global Message Scheduling}

After the redundant communication elimination phase, the analyzer further
reduces the number of messages using a global message scheduling
algorithm  proposed by Chakrabarti et al. in \cite{Chakrabarti96}.
The idea of this optimization 
is to combine messages that are of the same communication
pattern into a single message to reduce the number of messages in 
a program.  In order to perform message scheduling, the analyzer first
determines the earliest and latest points for each communication.
Placing the communication in any point between the earliest and 
the latest points that dominates the latest point 
always yields correct programs.
Thus, the analyzer can 
schedule the placement of messages  such that messages of 
same communication
patterns are placed together and are combined to reduce the number of
messages.

The latest point for a communication is the place of the SCD 
after redundant communication elimination.
Note that after message vectorization, SCDs are  placed
in the outermost loops that can perform the communications.
The earliest point for a SCD can be found by propagating
the SCD backward. 
As in \cite{Chakrabarti96}, it is  assumed 
that communication
for a SCD is performed at a single point. Hence, the backward
propagation will stop after an assignment statement, a loop header or a
branch statement where part of the SCD is killed. Since the propagation of
SCDs stops at a loop header node, only the UP propagation is needed.
Once the earliest and latest points for each communication are known, 
the greedy heuristic in \cite{Chakrabarti96} is used to perform the 
communication scheduling.
% The greedy algorithm
%propagates SCDs from their earliest points to the latest points, 
%along the path,
%if some communications are of the same pattern, the communications will
%be combined. The combined communication will be further propagated to the 
%latest point common to all communications combined.
%Consider communications $C_2$ and $C_4$ in  Figure~\ref{EXAMRED}.
%The earliest and the latest points for $C_2$
%are before statement $s6$ and $s11$,
%respectively. The earliest and the latest points for $C_4$ are before 
%statement $s11$. Since these two communications are of the same communication
%pattern, they can be placed before statement $s11$ and combined.


%\begin{figure}[tbph]
%
%\small
%\footnotesize
%\begin{tabbing}
%
%\hspace{0.2in}  ALIGN (i, j) with VPROCS(i, j) :: x, y, z\\
%\hspace{0.2in}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
%\hspace{0.2in}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
%\hspace{0.2in}(s2)\hspace{0.1in}\>do\=\ j = 1, 100\\
%\hspace{0.2in}(s3)\hspace{0.1in}\>\>x(i,j)=...\\
%\hspace{0.2in}(s4)\hspace{0.1in}\>enddo\\
%\hspace{0.2in}(s5)\hspace{0.1in}enddo\\
%\hspace{0.4in}COMM: $C_1=<w, (1:100:1, 1:100:1), <(2*j, i+1), (i,j), \perp>,
%                     \perp>$\\
%\hspace{0.2in}(s6)\hspace{0.1in}do i = 1, 100\\
%\hspace{0.2in}(s7)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{0.2in}(s8)\hspace{0.1in}\>\>y(i,j)=w(i,j)\\
%\hspace{0.2in}(s9)\hspace{0.1in}\>enddo\\
%\hspace{0.2in}(s10)\hspace{0.1in}enddo\\
%\hspace{0.4in}COMM: $C_{24} =\{<x, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>,  $\\
%\hspace{0.7in}  $ <y, (2:101:1, 1:100:1), <(i+1, j), (i, j), \perp>,
%                     \perp>\}$\\
%\hspace{0.2in}(s11)\hspace{0.1in}do i = 1, 100\\
%                                 \>$C_5=<w, (i, 200)
%                                    <(i+1, j), (i, j), \perp>, \perp>$\\
%\hspace{0.2in}(s12)\hspace{0.1in}\>do j = 1, 100\\
%\hspace{0.2in}(s13)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
%\hspace{0.2in}(s14)\hspace{0.1in}\>\>z(i, j) = z(i, j)* y(i+1, ,j)\\
%\hspace{0.2in}(s15)\hspace{0.1in}\>end do\\
%\hspace{0.2in}(s16)\hspace{0.1in}\>w(i+1, 200) = ...\\
%\hspace{0.2in}(s17)\hspace{0.1in}end do
%\end{tabbing}
%\caption{Communications after message scheduling}
%\label{EXAMCOM}
%\end{figure}

\subsection{Evaluation of the analyzer}
\label{evalanalyzer}

The analyzer is implemented as part of the E--SUIF compiler
which is developed to support compiled communication on optical TDM networks.
The E--SUIF compiler is based on the Stanford SUIF compiler \cite{SUIF}.
The generation of a program used for evaluations is carried out in
the following steps. First,
a sequential program is compiled using SUIF frontend, $scc$, 
to generate the SUIF intermediate representation. Next, 
the SUIF transformer, $porky$, is used to perform 
a number of scalar optimizations including
copy propagation, dead code elimination and induction variable
elimination. The communication preprocessing phase is used to annotate
global arrays with data alignment information. 
The analyzer is then invoked to analyze and optimize  communications 
in the program. After communication optimizations, the backend of the
compiler inserts a library call into the SUIF intermediate representation
for each SCD remaining in the program. Finally, the $s2c$ tool
is used to convert the SUIF intermediate representation into C program, which
is the one that is executed for evaluation.

To evaluate performance of the analyzer, 
a communication emulation system is developed. 
The system takes SCDs as input, emulates the 
communications described by the SCDs and collects statistics about 
the required communications, such as
the total number of elements communicated and the 
total number of messages communicated.
The emulation system provides an interface to C program in the form of 
a library call whose arguments include all information in a SCD.
The compiler backend in E--SUIF 
automatically generates the library call for each 
SCD remaining in the program. 
In this way, the communication performance of a program
can be evaluated in the emulation system
by running  programs generated by the E--SUIF compiler.

%Once this program is generated, it can be
%compiled with the emulation library and run to collect the communication 
%performance statistics. 
%In our experiments, we used the number of elements
%to be communicated as the performance measure.

Six programs, L18, ARTDIF, TOMCATV, SWIM, MGRID and ERHS
are used in the experiment. Programs ARTDIF, TOMCATV, SWIM, MGRID and ERHS
are from the SPEC95 benchmark suite. 
The descriptions of the programs are as follows.
\begin{enumerate}
\item L18 is the explicit hydrodynamics kernel in livermore loops (loop 18).
\item ARTDIF is a kernel routine obtained from  HYDRO2D program, 
which is an astrophysical program for the computation of galactical jets
using hydrodynamical Navier Stokes equations. 
\item TOMCATV does the mesh generation with Thompson's solver. 
\item SWIM is the SHALLOW weather prediction program.
\item MGRID is the simple multigrid solver for computing a three
dimensional potential field. 
\item ERHS is part of the APPLU program, which is the solver for five 
coupled \\
parabolic/elliptic partial differential equations. 
\end{enumerate}

Table~\ref{analysis} shows  the analysis cost of the analyzer. 
The analyzer, which implements all the optimization
algorithms on all SCDs in the programs,  was run on a 
SPARC 5 machine with 32MB memory. 
Row 2 and Row 3 shows the program sizes.
Row 4 shows the
cumulative memory requirement, which is the sum of number of SCDs passing 
through each node. This number is approximately equal 
to the memory requirement of
traditional data flow analysis.
The value in parenthesis
is the maximum number of cumulative SCDs in a node, which is the extra memory
needed by the analyzer.
In the analyzer, the size of a SCD ranges from 0.6 to about 3 kbytes.
The results show that traditional analysis method will require large
amount of memory when a program is large, while the analyzer uses
little extra memory. 
Row 5 gives the raw analysis times and row 6 shows the rate at which
the analyzer operates in units of  $lines/sec$. On an average,
the analyzer compiles 172 lines per second for the six programs. 
Row 9 shows the total time,
which includes analysis time and the time to load and store the
SUIF structure, for reference. In most cases, the analysis time is only
a fraction of the load and store time.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Program &            L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
size(lines) &         83  &    101 &     190 &  429 &   486 & 1104\\
%\hline
%\# of tree nodes   &  85  &    110 &     337 &  543 &   660 &  958\\
\hline
\# of initial SCDs &  35  &     12 &     108 &   76 &   125 &  403\\
\hline
accu. memory req.  & 348(1) &  175(1) & 5078(3) &  767(1) &  1166(1) & 
                                                                    6029(5)\\
\hline
analysis time(sec) & 0.62 &   0.32 &    3.47 & 1.87 &  1.92 & 20.92\\
\hline 
lines / sec        & 133  &   316  &      54 & 229  &  253  & 52 \\
\hline
total time(sec)    & 2.00 &   1.75 &    6.95 & 6.65 & 12.52 & 35.42\\
\hline
\end{tabular}
\end{center}
\caption{Analysis time}
\label{analysis}
\end{table}

Table~\ref{elements} and Table~\ref{messages} show the effectiveness
of the optimizations in the analyzer. 
Table~\ref{elements} shows the reduction of
the 
total number of elements to be communicated and Table~\ref{messages} shows
the reduction of the total number of messages. 
Both cyclic and block distributions on 16 PE systems are considered.
This experiment is conducted using the test input  provided by
the SPEC95 benchmark for programs TOMCATV, SWIM, MGRID ERHS. The outermost
iteration number in MGRID is reduced to 1 (from 40). Problem 
sizes of $6\times 100$ for L18 and $402\times 160$ for ARTDIF are used.
The number of elements and number of messages
communicated after all optimizations is compared
to those after message vectorization optimization.
Table~\ref{elements} shows that for cyclic distribution, an
average reduction of 31.5\% of the total communication elements is achieved.
The block distribution greatly reduces the number of elements to be
communicated and affects the optimization performance of the analyzer.
For block distribution, the average reduction is 23.1\%. 
Table~\ref{messages} shows that the analyzer reduces the 
total number of messages
by 36.7\% for cyclic distribution and by 35.1\% for block
distribution. These results indicate
that global communication optimization opportunities are quite common and
the analyzer developed is effective in finding these opportunities.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dist. & Opt. &        L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
      &      &       $\times10^4$& $\times10^5$ & $\times10^8$  & 
$\times10^7$ & $\times10^7$ & $\times10^6$\\
\hline
      & Vector. &    1.38  & 7.01   & 1.38    & 6.38   & 5.69 & 3.62\\
\cline{2-8}
cyclic & Final  &   0.96  & 5.73   & 0.34 &  4.58   & 5.69 & 2.29 \\
\cline{3-8}
       &        &   69.6\% & 81.7\% & 24.6\% & 71.8\% & 100\%& 63.3\%\\
\hline
\hline
       &        &   $\times10^3$ & $\times10^4$ & $\times10^6$ & 
$\times10^6$ & $\times10^6$ & $\times10^6$\\
\hline
       & Vector. &  3.26    & 7.17 & 5.74 & 3.38 & 8.49 & 3.11\\
\cline{2-8}
block  & Final &  2.57 &  6.97 & 5.12 & 1.08 & 8.49 & 1.65\\
\cline{3-8}
       &       &  78.8\% & 97.2\% & 89.1\% & 32.0\% & 100\% & 53.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Total number of elements to be communicated}
\label{elements}
\end{table}

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dist. & Opt. &        L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
   & Vector. &    368  & 400   & 68555  & 3892   & 17662 & $1.14\times10^6$\\
\cline{2-8}
cyclic & Final  & 96  &  336   & 41075  & 1807   & 17662 & $0.72\times10^6$\\
\cline{3-8}
       &        &   26.1\% & 84.0\% & 59.9\% & 46.4\% & 100\%& 63.1\%\\
\hline
\hline
       & Vector. &  330  & 185 & 16750 & 3894 & 14650 & $9.20\times10^5$\\
\cline{2-8}
block  & Final &  90 &  161 & 10915 & 2209 & 14650 & $4.89\times10^5$ \\
\cline{3-8}
       &       &  27.3\% & 87\% & 65.2\% & 56.7\% & 100\% & 53.2\%\\
\hline
\end{tabular}
\end{center}
\caption{Total number of messages}
\label{messages}
\end{table}

%\begin{figure}
%\centerline{\psfig{figure=fig/size.eps,width=6in}}
%\caption{The reduction of the number of array elements in communications}
%\label{size}
%\end{figure}
%\vspace{-0.2in}

\section{Virtual to physical processor mapping}

%The communication analyzer analyzes the communication requirement on
%virtual processor grids. 
In order to support compiled communication,
communication patterns on physical processors must be computed.
This section assumes that the physical processor grid has the same
number of dimensions as the logical processor grid. Notice that this is
not a restriction because a dimension in the physical processor grid
can always be collapsed by assigning a single processor to that dimension.
This section presents algorithms to compute  communications
on physical processors from SCDs. The computation
may not always be precise due to  symbolic 
constants in the SCD that are unknown at compile time.
The algorithms employ multi--level approximation schemes to 
obtain best information. 
%In this case, approximations are needed.

Given a $SCD = <A, D, CM = <src, dst, qual> , Q>$, let us first consider
the case where $A$ is an one-dimensional array and the virtual processor
grid is also one-dimensional. Let $src = \alpha*i+\beta$ and
$dst = \gamma*i + \delta$, $\alpha \ne 0$, $\gamma \ne 0$, and $qual = NULL$. 
$qual \ne NULL$ will be considered later
when multi-dimensional arrays and multi-dimensional virtual processor
grids are discussed. 
Let the alignment matrix and the offset vector be $M_A$ and $v_A$, that 
is, element A[n] is owned by virtual processor $M_A*n + v_A$. 
Let us assume that the number
of physical processors is $p$ and the block size of the distribution of
virtual processor grid is $b$. For an element A[n], the 
physical source processor of the communication can be computed as follows.
 
\centerline{$(M_A*n+ v_A)\ mod\ (p*b) / b$}

\noindent
The virtual destination processor can be computed by
first solving the equation \\
\centerline{$(M_A*n + v_A) = \alpha*i+\beta$ 
to obtain $i = (M_A*n+v_A-\beta) / \alpha$}

\noindent
and then replacing the value of $i$ in  $dst$ to obtain the 
virtual destination processor $\gamma*(M_A*n+v_A-\beta)/\alpha + \delta$. 
Thus, the
physical destination processor is given by\\
\centerline{$ (\gamma*(M_A*n+v_A-\beta)/\alpha + \delta)\ mod\ (p*b) / b$.} 

\noindent
The physical communication pattern for the SCD can be obtained
by considering all elements in $D$. 
However, there are situations that the exact
region $D$ cannot be determined at compile time. It is desirable to 
have a good approximation scheme that computes
the communication patterns when $D$ cannot be determined at compile time.

Before the approximation scheme is presented, let us first examine
the relation between communications on physical processors and 
that on virtual processors. Let us use notation $src\rightarrow dst$ 
to represent
a communication from $src$ to $dst$. Given a data region 
$D=l:u:s$, the communications on virtual processors can be derived as follows.
By mapping $D$ to the virtual processor grid, the source processors
of the communications can be obtained. Since the mapping from data space to 
the virtual processor grid is linear, the set of source processors can 
be represented as a triple $vs_l:vs_u:vs_s$, that is, the 
source processors on the virtual processor grid are $vs_l$, $vs_l+vs_s$, 
$vs_l+2*vs_s$, ..., $vs_u$. Due to the way in which 
$CM.src = \alpha*i + \beta$ 
is computed, 
equations $vs_l+i*vs_s = CM.src$, $i = 0, 1, 2, ...$,  
always have integer solutions.
Since $CM.dst$ is of the form $\gamma*i+\delta$, 
where $\gamma$ and $\delta$ are constants, 
the destination processors on the virtual processor grid can 
also be represented
as a triple $vd_l:vd_u:vd_s$, where 
$vd_l = \gamma*((vs_l-\beta)/\alpha) + \delta$, 
$vd_u = \gamma*((vs_u-\beta)/\alpha) + \delta$ and $vd_s = \gamma*vs_s/\alpha$. Notice that because of the way in which
$CM$ is computed, all the division operations in the formula result in 
integers. Thus, communications on the virtual processor grid can be
represented as $vs_l\rightarrow vd_l:vs_u\rightarrow vd_u:vs_s\rightarrow vd_s$, meaning the set\\
\centerline{$\{vs_l\rightarrow vd_l, vs_l+vs_s\rightarrow vd_l+vd_s, ..., vs_u\rightarrow vd_u\}$.}

\noindent
Communications on physical processors are obtained by mapping virtual 
processors onto  physical processors.  Given a block--cyclic distribution 
with block size $b$ and processor number $p$, a sequence of processors
on the virtual processor grid $l, l+s, l+2*s, ...$ will be mapped
to a sequence of physical processors repeatedly. For example, assuming that 
$p=2$ and $b=2$, the sequence of virtual processors 
$2, 2+3=5, 2+2*3=8, 2+3*3=11, ....$ will be mapped to physical processors
$1,0,0,1$ repeatedly as shown in Figure~\ref{virtualspace}. As will be seen
later, this characteristic can be utilized to develop an approximation 
algorithm for the cases when $D$ is unknown at compile time.  
A point $e$ in the virtual processor grid can be represented by 
two components $(pp, o)$, where
$pp = e\ mod\ (p*b) / b $ is the physical processor that contains 
$e$ and $o = e\ mod\ b$ is the offset of $e$ within the processor.
Let $(pp_k, e_k)$ correspond to $l+k*s$, $k = 0, 1, ...$.
It can be easily shown that \\
\centerline{$pp_i = pp_j \wedge e_i=e_j$ implies 
            $pp_{i+1} = pp_{j+1} \wedge e_{i+1}=e_{j+1}$}

\noindent
Since in the $(pp, o)$ space, there are $p$ choices for $pp$ and $b$ choices
for $o$, Thus, there exists a $k$, $k \le p*b$, such that $pp_k=p_0$ and 
$e_k=e_0$, which determines a repetition point. In the previous 
example, consider the 
sequence\\
\centerline{$2 =(1,0), 5=(0,1), 8=(0,0), 11=(1,1), 14=(1,0) ...$.}

\noindent
Thus, the physical processors repeat the sequence $1,0,0,1$.

Communications on physical processor contains two processors, the source
processor and the destination processor. Thus, in order for the 
communications to repeat, both source and destination processors must
repeat. Following the above discussion, the communication on the virtual 
processor grid, $src\rightarrow dst$, can be represented by four components 
$(spp, so, dpp, do)$, where $spp$ is the physical processor that
contains $src$, $so$ is the offset of $src$ within the processor, 
$dpp$ is the physical processor that contains $dst$, $do$ is the 
offset of $dst$ within the processor. Assuming that the source array and the 
destination array are mapped to the same virtual processor grid, 
there are $p$ choices for $spp$ and $dpp$, and  $b$ choices for $so$ and $do$.
Thus, there exists $k$, $k\le p^2b^2$, such that both source and destination 
processors, and thus the communication pattern, will repeat themselves.
The following lemma summarizes these results. Using this lemma,
communication patterns can be obtained by considering the elements in $D$ 
until the repetition point or the end of $D$, whichever occurs first.


\begin{figure}
\centerline{\psfig{figure=fig/virtualspace.eps,width=3.5in}}
\caption{Virtual processor space}
\label{virtualspace}
\end{figure}

\noindent
{\bf Lemma: } Assume that the virtual processor grid is distributed over $p$
processors with block size $b$.
Let $SCD = <A, D=l:u:s, CM = <src, dst, qual> , Q>$, 
assuming u is infinite, there exist a value
$k$, $k\le p^2b^2$, such that the communication for all 
$m \ge k$, $A[l+m*s]$ has the same
source and destination as the communication for $A[l+(m-k)*s]$.

\noindent
{\bf Proof}: Follows from above discussions. $\Box$

The implication of the lemma is that the algorithm to determine the
communication pattern for the SCD can stop when the
repetition point occurs. In other words, when the upper bound of 
$D$ is unknown, the communication pattern can be approximated by using
the repetition point. Figure~\ref{algo1} shows the algorithm to compute
the physical communication pattern for a 1--dimensional array and 
a 1--dimensional virtual processor grid. 
The algorithm first checks the SCD.
Let $D=l:u:s$ and $CM=<\alpha+\beta*i, \gamma+\delta*i, \perp>$.
If $l$ contains variables or the mapping is not clean ($\alpha$, $\beta$, 
$\gamma$ or $\delta$ are symbolic constants), the communication is 
approximated  with all--to--all connections.
Note that
by the semantics of array sections, when $l$ is unknown, the compiler
cannot determine the actual sequence of elements in an array section.
%The algorithm then
%deals with the situation when array region $D$ contains
%variables.  
When $s$ contains variables, it will be 
approximated by 1, that is, $D$ is approximated by a superset $l:u:1$.  
When  $u$ contains variables, 
the physical communication is approximated by
considering all elements until the repetition point. Note that when  
$u$ contains a variable, the sequence in $D$ is 
$l$, $l+s$, $l+2*s$, .... Although
the upper bound of the sequence is unknown to the compiler, 
the repetition point can be used
to approximate the communication pattern. 

\begin{figure}
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Co\=mpute\_1--dimensional\_pattern($D$, $CM.src$, 
                                                       $CM.dst$)\\
\\
              \>Let $D=l:u:s$, $CM.src=\alpha*i+\beta$, $CM.dst=\gamma*i+\delta$\\
              \>{\bf if}\=\ ($l$ contains variables) {\bf then}\\ 
              \>\>{\bf return} all--to--all connections\\
              \>{\bf end if}\\
              \>{\bf if}\=\ ($\alpha$, $\beta$, $\gamma$ or $\delta$ 
                  are variables) {\bf then}\\ 
              \>\>{\bf return} all--to--all connections\\
              \>{\bf end if}\\
              \>$pattern = \phi$\\
              \>{\bf for each} element $i$ in $D$ {\bf do} \\
              \>\>$pattern = pattern + communication\ of\ i$\\
              \>\>{\bf if}\=\ (communication repeated) {\bf then}\\
              \>\>\>{\bf return} $pattern$\\
              \>\>{\bf end if}\\
              \>{\bf end for}
\end{tabbing}
\end{center}
\caption{Algorithm for 1-dimensional arrays and 1-dimensioanl virtual processor grid}
\label{algo1}
\end{figure}

Now let us consider multi-dimensional arrays and multi--dimensional
virtual processor grids. In an 
n--dimensional virtual processor grid,  a
processor is represented by a $n$--dimensional coordinate
$(p_1, p_2, ..., p_n)$. The algorithm to compute the communication pattern
finds all pairs of source and destination processors that require
communication. This is done by considering the dimensions in virtual 
processor grid one at a time.
A set of $src=(sp_1,sp_2,...,sp_n)\rightarrow dst=(dp_1,dp_2,...,dp_n)$ 
pairs is used to represent the communications. 
A wild--card, $*$, is used to represent the dimension within a tuple
that has not been considered. Initially the communication set 
contains a single element where 
all dimensions are wild--cards. When one dimension is considered, 
it generates a 1-dimensional communication pattern for a specific dimension
in the source and the destination, denoted as $src\_dim$ and $dst\_dim$
respectively. This 1-dimensional pattern may degenerate to contain only source
processors or destination processors.
A cross product operation is defined to merge the 1-dimensional 
communication patterns into the $n$-dimensional communication. 
This operation is similar to the cross product of  sets
except that  specific dimensions are involved in the operation.
For the degenerate form of the 1-dimensional pattern,
the operation only involves source processors or destination processors.

For example, consider the communication for\\ 
\centerline{$SCD = <y, (1:4:1,1:4:1), <src = (i,j), dst = (j,i), qual =NULL>, NULL>$.}

\noindent
Further assume
that the virtual processor grid is distributed on 2 processors
with block size of 2 in each dimension 
and array $y$ is identically mapped to the virtual 
processor grid. Initially, the communication set contains a single element
$(*,*) \rightarrow  (*,*)$, indicating that all dimensions in the source and destination
processor have not been considered. Considering the first dimension 
in the data space, which is identically mapped to the first dimension
of the virtual grid. Hence, $src\_dim = 1$. From the mapping relation
$CM.src$ and $CM.dst$, it is can found that dimension 2 in the destination
processor correspond to dimension 1 in the source processor. 
Hence, $dst\_dim = 2$. Applying the algorithm for the 1--dimensional 
communication pattern obtains the communication to be $\{0\rightarrow 0, 1\rightarrow 1\}$ with
$src\_dim=1, dst\_dim=2$.
Taking the cross product of this pattern 
with the 2-dimensional communication set 
$\{(*,*) \rightarrow  (*,*)\}$ yields $\{(0,*)\rightarrow (*,0), (1,*)\rightarrow (*,1)\}$. Considering the
second dimension of the data space, the 1--dimensional communication set
is $\{0\rightarrow 0, 1\rightarrow 1\}$ with $src\_dim=2, dst\_dim=1$. 
Taking the cross product of this pattern set
to the 2--dimensional communication set gives 
$\{(0,0)\rightarrow (0,0), (0, 1)\rightarrow (1, 0), (1, 0)\rightarrow (0, 1), (1, 1)\rightarrow (1, 1)\}$, which is
the physical communication for the $SCD$. 

The above example does not take constant mappings and non--NULL 
qualifiers into consideration.
The algorithm to compute communication patterns for multi-dimensional
arrays that is  shown in Figure~\ref{algo2} considers all these situations.
The algorithm first checks whether the mapping relation can be processed.
If one loop induction variable occurs in two or more dimensions 
in $CM.src$ or $CM.dst$, the algorithm cannot
find the correlation between dimensions in source and destination processors,
and the communication pattern for the SCD is 
approximated by all--to--all connections. 
If the SCD passes the mapping relation test, the algorithm determines for each
dimension in the data space the corresponding dimension $sd$ in the source 
processor grid.
If it does not exist, the data dimension is not distributed
and need not be considered. 
If there exists such a dimension, the algorithm
then tries to find the corresponding dimension $dd$ in the destination
processor grid by checking whether there is a dimension $dd$ such that
$CM.dst[dd]$ contains the same looping index variable as the source 
dimension $CM.src[sd]$.
If such dimension exists, 
the algorithm computes 1-dimensional communication pattern
between dimension $sd$ in the source processor and dimension $dd$ in 
the destination processor, then cross--products the 1-dimensional 
communication pattern into the $n$-dimensional communication pattern.
When $dd$ does not exist, the algorithm determines a degenerate 1-dimensional
pattern, where only source processors are considered, and cross-products
the degenerate 1-dimensional pattern into the communication pattern.
After all dimensions in the data space are considered, there may still
exist dimensions in the source processor (in the virtual processor grid)
that have not been considered. These dimensions should be constants and
are specified by the alignment matrix and the alignment offset vector.
The algorithm fills in the constants in the source processors.
Dimensions in destination processor may not be fully considered, either.
When $CM.qual \ne NULL$, the algorithm  finds for each
item in $CM.qual$ the corresponding dimension, computes all possible 
processors in that dimension and cross--products the list into the 
communication list. Finally, the algorithm fills in all constant
dimensions in the destination.


%\begin{figure}
%\begin{center}
%\begin{tabbing}
%\hspace{0.1in}Co\=mpute communication pattern\\
%              \>{\bf if}\=\ (the mapping relation is not good) {\bf then}\\
%              \>\> approximate with all-to-all connections and stop.\\
%              \>{\bf for each} dimension in the array {\bf do}\\
%              \>\>determine the corresponding dimension in source and
%                  destination processor grids.\\ 
%              \>\>compute 1-dimensional communication pattern.\\
%              \>\>cross product this one-dimensional mapping to previous
%                    mappings.\\
%              \>fills in all other dimensions in the source processor if
%                necessary.\\
%              \>{\bf for each} elements in the mapping qualifier {\bf do}\\
%              \>\>determine the corresponding destination dimension.\\
%              \>\>compute all possible processors in dimension.\\
%              \>\>cross product the destination processors to 
%                  the previous mappings.\\
%              \>fills in all other dimensions in the destination processor if
%                necessary.\\
%\end{tabbing}
%\end{center}
%\caption{Algorithm for multi--dimensional array}
%\label{algo2}
%\end{figure}

\begin{figure}
\begin{center}
\begin{tabbing}
\hspace{0.1in}Co\=mpute communication pattern(SCD)\\
              \>Let $SCD=<A,D,CM,Q>$\\
              \>{\bf if}\=\ (the format of $CM$ is not good) {\bf then}\\
              \>\> {\bf return}  all-to-all connections\\
              \>{\bf end if}\\
              \>$pattern = \{(*,*,..., *)\}$\\
              \>{\bf for each} dimension $i$ in the array {\bf do}\\
              \>\>Let $sd$ be the corresponding dimension in source processor
                  grids.\\
              \>\>Let $dd$ be the corresponding dimension in destination
                  processor grids.\\ 
              \>\>1dpattern = compute\_1-dimensional\_pattern($D[i]$,
                  $CM.src[sd]$, $CM.dst[dd]$)\\
              \>\>pattern = cross\_product(pattern, 1dpattern)\\
              \>{\bf end for}\\
              \>pattern = source\_processor\_constants(pattern)\\
              \>{\bf for each} element $i$ in the mapping qualifier {\bf do}\\
              \>\>Let $dd$ be the corresponding destination processor 
                  dimension.\\
              \>\>1dpattern = compute\_1-dimensional\_pattern($CM.qual[i]$,
                              $\perp$, $CM.dst[dd]$)\\
              \>\>pattern = cross\_product(pattern, 1dpattern)\\
              \>{\bf end for}\\
              \>pattern = destination\_processor\_constants(pattern)\\
              \>{\bf return} pattern
\end{tabbing}
\end{center}
\caption{Algorithm for multi--dimensional array}
\label{algo2}
\end{figure}

\begin{figure}
\small
\footnotesize
\begin{center}
\begin{tabbing}
\hspace{1.5in}\=ALIGN (i, j) with VPROCS(2*j, i+2, 1) :: x\\
\>ALIGN (i) with VPROCS(1, i+1, 2) :: y\\
\>DO i = 1, 5\\
\>DO\=\ j = 1, 5\\
\>\>x(i, j) = y(i) + 1\\
\>END DO\\
\>END DO\\
\end{tabbing}
\end{center}
\caption{An example}
\label{ANEXAM}
\end{figure}


An example in Figure~\ref{ANEXAM} illustrates how 
communications on physical processors are derived.
In the program, the virtual processor grid is $3$-dimensional and 
the alignment array and 
the alignment offset vector
for arrays $x$ and $y$ are as follows:

\begin{center}
\[ M_x  = \left(
       \begin{array}{c} 0 \\ 1 \\ 0 \end{array}
       \begin{array}{c} 2 \\ 0 \\ 0\end{array} \right),\
   \vec{v}_x = \left(
       \begin{array}{c} 0 \\ 2 \\ 1 \end{array} \right)
%\]
%\[
   M_y  = \left(
       \begin{array}{c} 0 \\ 1 \\ 0 \end{array}
        \right),\
   \vec{v}_y = \left(
       \begin{array}{c} 1 \\ 1 \\ 2\end{array} \right)
\].
\end{center}

Let us assume that the virtual processor grid, $VPROCS$, is distributed as 
$p= (2,2,1)$,
which means 2 processors in dimension 0, 2 processors in dimension 1 and 
1 processor in dimension 2, and $b= (2,2,1)$, which means 
the block size 2 in dimension 0,  2 in dimension 1 and 1 in 
dimension 2. After communication analysis, the SCD to represent
the communication is as follows:\\
\centerline{\small $SCD = <y, (1:5:1), <src = (1, i+1, 2), 
dst = (2*j, i+2, 1), qual = \{j=1:5:1\}>, NULL>$.}

The communication on physical processors is computed as follows. 
First consider
the dimension 0 in the array $y$. From the alignment, the algorithm 
knows that dimension 1
in the virtual processor grid corresponds to this dimension in the data space. 
Checking $dst$ in $M$, the algorithm can find that dimension 1 in destination 
corresponds to dimension 1 in source processors. 
Applying the 1-Dimensional
mapping algorithm, an 1--dimensional communication pattern
$\{0\rightarrow 1, 1\rightarrow 0\}$ with $src\_dim=1$
and $dst\_dim=1$ is obtained. 
Thus the communication list becomes $\{(*, 1, *)\rightarrow (*, 0, *),
(*, 0, *)\rightarrow (*, 1, *)\}$ after taking the cross product with the 
1--dimensional pattern. 
Next, the other dimensions in source
processors, including dimension 0 that is always mapped to processor 0 and
dimension 2 that is always mapped to processor 1 are considered.
After filling in the physical processor in these dimensions in 
source processors, the communication pattern 
becomes $\{(0, 1, 1)\rightarrow (*, 0, *), (0, 0, 1)\rightarrow 
(*, 1, *)\}$. Considering the $qual$ in $M$,  the dimension 0 of
the destination processor can be either 0 or 1. Applying the cross product
operation, the new communication list 
$\{(0, 1,1)\rightarrow (0, 0, *),\ (0, 1, 1,)\rightarrow (1, 0, *),\ (0, 0, 1)\rightarrow (0, 1, *),\
   (0, 0, 1)\rightarrow (1, 1, *)\}$ is obtained. Finally, 
the dimension 2 in the destination processor is always mapped to processor 0,
Thus, the final mapping is  
$\{(0, 1,1)\rightarrow (0, 0, 0), (0, 1, 1s)\rightarrow (1, 0, 0), (0, 0, 1)\rightarrow (0, 1, 0),
(0, 0, 1)\rightarrow (1, 1, 0)\}$.


There are several levels of approximations in the algorithm. 
First, 
when the algorithm cannot correlate the source and destination processor
dimensions from the mapping relation, the algorithm
uses an  approximation
of all--to--all connections. If the mapping relation contains
sufficient information to distinguish the relation of 
the source and destination processor dimension, 
computing the communication pattern for 
a multi-dimensional array reduces to computing 1-dimensional 
communication patterns, thus the approximations within each 
dimension are isolated to that dimension and will not affect the 
patterns in other dimensions. Using this multi-level approximation
scheme, some information is obtained when the compiler does not have
sufficient information for a communication. 

\section{Connection scheduling algorithms}
\label{cscheduling}

Once the communication requirement on physical processors is obtained,
the compiler uses off--line algorithms to perform
connection scheduling and determines the communication phases in a program. 
This section presents the connection scheduling 
algorithms and their performance evaluation. These algorithms assume
a torus topology.
%However, the fundamental principles of the algorithms
%can be extended to other topologies. 

For a given network, a set of connections that do not share any link is called
a configuration. In an optical TDM network with path multiplexing, 
multiple configurations can be supported simultaneously. Specifically, for 
a network with multiplexing degree $d$, $d$ configurations can be 
established concurrently. Thus, for a given communication pattern,
realizing the communication 
pattern with a minimum multiplexing degree is equivalent to  determining the
minimum number of configurations that contain all the connections in 
the pattern. Next,
 some definitions will be presented to formally state 
the problem of
connection scheduling. A connection  from a source $src$ 
to a destination $dst$ is denoted as $(src, dst)$.

\begin{description}
\item A pair of connections  $(s_1, d_1)$ and $(s_2, d_2)$ are said
to {\bf conflict}, if they cannot be simultaneously established because
they use the same link.

\item A {\bf configuration} is a set of connections
$\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ such that
no connections in the set conflict.

\item Given a set of connections  
$Comm = \{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$,
the set $MC =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} is a
{\bf minimal configuration set} for $Comm$ iff: \\
$\bullet$ 
each $C_i \in MC$ is a configuration and each connection 
$(s_{i}, d_{i}) \in R$ 
is contained in exactly one configuration in $MC$; and \\
$\bullet$
each pair of configurations $C_i,C_j \in MC$ contain connections 
$(s_i, d_i) \in C_i$ and $(s_j, d_j) \in C_j$ such that
$(s_i, d_i)$ conflicts with $(s_j, d_j)$.
\end{description}

It has been shown that optimal message scheduling 
for arbitrary topologies is NP-complete \cite{Chlamtac92}.
Therefore these algorithms are heuristics that are demonstrated to
provide good performance. Three 
connection scheduling heuristic algorithms that  compute a
minimal configuration set for a given connection set $Comm$ are described next.

%
\subsection{Greedy algorithm}
%
In the greedy algorithm, a configuration is created by repeatedly 
putting connections into the configuration until no additional 
connection can be established in that configuration.
If additional connections remain, another configuration is created
and this process is repeated till all connections have been processed.
This algorithm is a modification of an algorithm proposed in \cite{Qiao94}.
The algorithm is shown in Figure~\ref{SIMPLE}. The time complexity of
the algorithm is $O(|Comm|\times max_i(|C_{i}|)\times d)$, where $|Comm|$ 
is the number of the connections, $|C_{i}|$ is the number of connections
in configuration $C_{i}$ and $d$ is the number of configurations
generated.

\begin{figure}[htmb]
%\small
\begin{tabbing}
\hspace{0.5in}(1)\hspace{0.3in}MC = $\phi$, k = 1\\
\hspace{0.5in}(2)\hspace{0.3in}{\bf re}\={\bf peat}\\
\hspace{0.5in}(3)\hspace{0.3in}\> $C_{k} = \phi$\\
\hspace{0.5in}(4)\hspace{0.3in}\>{\bf for}\= {\bf each} $(s_{i}, d_{i}) 
                                              \in Comm$\\
\hspace{0.5in}(5)\hspace{0.3in}\>\>{\bf if}\=\ $(s_{i}, d_{i})$ does 
                                   not conflict with any 
                                   connection in $C_{k}$ {\bf then}\\
\hspace{0.5in}(6)\hspace{0.3in}\>\>\>$C_{k} = C_{k} \bigcup$ { $(s_{i}, d_{i})$ }\\
\hspace{0.5in}(7)\hspace{0.3in}\>\>\>Comm = Comm $-$ { $(s_{i}, d_{i})$ }\\
\hspace{0.5in}(8)\hspace{0.3in}\>\>{\bf end if}\\
\hspace{0.5in}(9)\hspace{0.3in}\>{\bf end for}\\
\hspace{0.5in}(10)\hspace{0.3in}\>MC = MC $\bigcup$ { $C_{k}$ }\\
\hspace{0.5in}(11)\hspace{0.3in}{\bf until} $Comm = \phi$\\
\end{tabbing}
\normalsize
\caption{The greedy algorithm.}
\label{SIMPLE}
\end{figure}

For example consider the linearly connected nodes shown in Figure~\ref{EXAM}. 
The result for applying the greedy algorithm to schedule connections 
set \{(0, 2), (1, 3),(3, 4), (2, 4)\} is shown in Figure~\ref{EXAM}(a). 
In this case, (0, 2) will be in time slot 1, (1, 3) in time slot 2, (3, 4) 
in time slot 1 and (2, 4) in time slot 3. 
Therefore, multiplexing degree 3 is needed to establish the paths for the 
four connections.  However,  as shown in Figure~\ref{EXAM} (b), 
the optimal scheduling for the four connections, which can be obtained
by considering the connection in different order, is to schedule (0, 2) in 
slot 1, (1, 3) in slot 2, (3, 4) in slot 2 and (2, 4) in slot 1. 
The second assignment only use 2 time slots to establish all the connections. 

\begin{figure}[htbp]
\begin{center}
\begin{picture}(0,0)%
\special{psfile=../962SC96/fig/961.3.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0050in}%
\begin{picture}(920,120)(75,640)
\end{picture}

\end{center}
\caption{Scheduling connections (0, 2), (1, 3),(3, 4), (2, 4)}
\label{EXAM}
\end{figure}


\subsection{Coloring algorithm}

The greedy algorithm  processes the connections in an arbitrary order.
This subsection describes an algorithm that applies a heuristic 
to determine the order to process the connections.
The heuristic assigns higher priorities to connections with fewer
conflicts. By giving the connections with less conflicts higher priorities, 
each configuration is likely to accommodate more connections and thus the
multiplexing degree needed for the patterns is likely to decrease. 

The problem of computing the minimal configuration set is formalized
as a graph coloring problem. A coloring of a graph is an assignment of 
a color to each node of the graph in such a manner that no two nodes 
connected by an edge have the same color. A conflict graph for a set of
connections is built in the following manner, (1) 
each node in the graph 
corresponds to a connection and (2) an edge
is introduced between two nodes if the connections represented by the 
two nodes are conflicted.
As stated by the theorem given below,
the number of colors used to color the graph is equal to the number of 
configurations needed to handle the connections. 

%\begin{description}
%\item
\noindent
{\bf Theorem:} Let $Comm=\{(s_{1}, d_{1}),(s_{2}, d_{2}),...,(s_{m}, d_{m})\}$
be the set of connections and $G = (V, E)$ be the conflict graph for $Comm$. 
There exists a configuration set $M = \{C_{1}, C_{2}, ..., C_{t}\}$
for $R$ if and only if $G$ can be colored with $t$ colors.
%\end{description}

\noindent
{\bf Proof}: Since connections that correspond to the nodes with the same 
color do not conflict with each other, they can be placed in 
one configuration. $\Box$

%Prove: ($\Rightarrow$) Assuming R has 
%configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. Let 
%$(s_{i}$, $d_{i}) \in C_{k}$, node $n_{i}$ can be colored by color $k$. 
%Therefore, there are totally $t$ colors in the graph. Now, we need to prove
%that for any two node $n_{i}$, $n_{j}$ such that $(i, j) \in E$, the two nodes
%are colored by different color. By the construction of conflict graph,
%if $(i, j) \in E$, node $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ share same links,
%hence, by the construction of configuration, $(s_{i}, d_{i})$ and
%$(s_{j}, d_{j})$  is in different configuration, thus $n_{i}$ and $n_{j}$ is
%colored by different colors. Hence, G can be colored by $t$ colors.
%
%($\Leftarrow$) Assuming G can be colored by $t$ colors. Let 
%$C_{i}$ = {$( s_{j}, d_{j})$ : $n_{j}$ is colored by color j}, 
%$M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. To prove 
%M is a configuration for R, we need to 
%prove 1) for any $(s_{i}, d_{i}) \in R$,
%there exists a $C_{k}$ such that $(s_{i}, d_{i}) \in C_{k}$, and 2) $C_{k}$
% must
%be a configuration. The first condition is trivial. Now, let us consider
%the second condition. Let $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ belong to 
%$C_{k}$, by the construction the G, $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$
%do not share any links. Therefore, $C_{k}$ is a configuration. Hence, there
%exist configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} for R. $\Box$
%
%\begin{description}
%\item
%{\bf Corollary:} The optimal multiplexing degree for establishing 
%connections in $R$ is equivalent to the minimum number colors to 
%color graph $G$.
%\end{description}


Thus, the
coloring algorithm attempts to minimize the number of colors used in 
coloring the graph. Since the coloring problem is known to be NP-complete, 
a heuristic is used for graph coloring. The heuristic determines the order 
in which nodes are colored using the node priorities.
The algorithm is summarized in Fig~\ref{COLOR}. It should be noted that
after a node is colored, the algorithm updates the priorities of uncolored 
nodes. This is because in computing the degree of an uncolored node, 
only  the edges that connect the node to other uncolored nodes are 
considered. 
The algorithm finds a solution in linear time (with respect to the 
size of the conflict graph). The time complexity of the algorithm is 
$O(|Comm|^2\times max_i(|C_{i}|)\times d)$, where $|Comm|$ is the 
number of the 
connections, $|C_{i}|$ is the number of connections
 in configuration $C_{i}$ and 
$d$ is the total number of configurations generated.


\begin{figure}[htbp]
%\small
\begin{tabbing}
\hspace{0.5in}(1)\hspace{0.3in} Construct conflict graph G = (V, E)\\
\hspace{0.5in}(2)\hspace{0.3in} Calculate the priority for each node\\
\hspace{0.5in}(3)\hspace{0.3in} MC = $\phi$, k = 1\\
\hspace{0.5in}(4)\hspace{0.3in} NCSET = V\\
\hspace{0.5in}(5)\hspace{0.3in} {\bf re}\={\bf peat}\\
\hspace{0.5in}(6)\hspace{0.3in} \>Sort NCSET by priority\\
\hspace{0.5in}(7)\hspace{0.3in} \> WORK = NCSET\\
\hspace{0.5in}(8)\hspace{0.3in} \> $C_{k} = \phi$\\
\hspace{0.5in}(9)\hspace{0.3in} \>{\bf wh}\={\bf ile} (WORK $\ne \phi$)\\
\hspace{0.5in}(10)\hspace{0.3in} \>\> Let $n_{f}$ be the first 
                                      element in WORK\\
\hspace{0.5in}(11)\hspace{0.3in} \>\>$C_{k} = C_{k} \bigcup \{<s_{f}, d_{f}>\}$\\
\hspace{0.5in}(12)\hspace{0.3in} \>\>NCSET = NCSET $- \{n_{f}\}$\\
\hspace{0.5in}(13)\hspace{0.3in} \>\>{\bf fo}\={\bf r} {\bf each}  $n_{i} \in NCSET$ 
                                      and $(f, i) \in E$ {\bf do} \\
\hspace{0.5in}(14)\hspace{0.3in} \>\>\> update the priority of $n_{i}$\\
\hspace{0.5in}(15)\hspace{0.3in} \>\>\> WORK = WORK - $\{n_{i}\}$\\
\hspace{0.5in}(16)\hspace{0.3in} \>\>{\bf end for}\\
\hspace{0.5in}(17)\hspace{0.3in} \>{\bf end while}\\
\hspace{0.5in}(18)\hspace{0.3in} \>MC = MC + $\{C_{k}\}$\\
\hspace{0.5in}(19)\hspace{0.3in} {\bf until} NCSET = $\phi$
\end{tabbing}
\normalsize
\caption{The graph coloring heuristic.}
\label{COLOR}
\end{figure}

For torus and mesh networks, a suitable choice for priority for a
connection is the ratio of the number of links in the path 
from the source to the destination and the degree of the node 
corresponding to the connection in $G$. 
Applying the coloring algorithm to the example in Figure~\ref{EXAM},
in the first iteration, the connections are reordered as 
$\{(0, 2), (1, 3), (2, 4), (3, 4)\}$ and connections (0, 2), (2, 4) will be
put in time slot 1. In the second iteration, connections (1, 3), (3, 4) are
put in time slot 2. Hence, applying the 
coloring algorithm will use 2 time slots
to accommodate the connections.

\subsection{Ordered AAPC algorithm}

The graph coloring algorithm has better performance than the greedy heuristic.
However, for dense communication patterns the heuristics cannot guarantee that
the multiplexing degree found would be bounded by the minimum multiplexing 
degree needed to realize the all-to-all pattern. The algorithm described in 
this section targets dense communication patterns. By grouping the connections
in a more organized manner, better performance can be achieved for dense 
communication.

The worst case of arbitrary communication is the {\em all-to-all personalized 
communication} (AAPC)  where each node sends a message to every 
other node in the system. Any communication pattern can be embedded in AAPC. 
Many algorithms \cite{Hinrichs94,Horie91} have been designed to 
perform AAPC efficiently for different topologies.
Among these algorithms, the ones that are of 
interests to us are the phased AAPC algorithms, in which the AAPC connections 
are partitioned into contention--free phases. A phase in this kind of AAPC 
corresponds to a configuration. Some phased AAPC algorithms are optimal in
that every link is used in each phase and every connection follows the
shortest path. Since all the connections in each AAPC phase are 
contention--free,
they form a configuration that uses all the links in the system. 
Each phase in the phased AAPC communication forms an {\em AAPC configuration}.
The set of {\em AAPC configurations} for AAPC communication pattern is 
called {\em AAPC configurations set}. 
The following theorem states the property 
of connection scheduling using AAPC phases.

%\begin{description}
%\item 

\noindent{\bf Theorem: } Let $Comm =
\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ be the 
set of connections, if $Comm$ can be partitioned into $K$ phases 
$P_1 = \{(s_{1}, d_{1}), ... , (s_{i_{1}}, d_{i_{1}})\}$,\\ 
$P_2 = \{(s_{i_{1} + 1}, d_{i_{1} + 1}), ... , (s_{i_{2}}, d_{i_{2}})\}$,
... ,
 $P_K = \{(s_{i_{K-1} + 1}, d_{i_{K-1} + 1}), ... , 
(s_{i_{K}}, d_{i_{K}})\}$, such that $P_i$, $ 1 \le i \le K$, is a subset
of an AAPC configuration. Using the greedy algorithm to schedule the
connections $(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})$
results in a multiplexing degree less than or equal to K.

\noindent
{\bf Proof}: The theorem is proven  by contradiction 
that for any $\alpha$, 
$1\le \alpha \le m$, let $(s_\alpha, d_\alpha) \in P_\beta$, 
$1\le \beta \le K$, connections $(s_1, d_1)$, ..., $(s_\alpha, d_\alpha)$
can be scheduled by the greedy algorithm using a multiplexing degree
less than or equal to $\beta$.

Let $(s_\alpha, d_\alpha) \in P_\beta$ be the first connection that does
not satisfy the above proposition. That is,  
$(s_1, d_1)$, ..., $(s_{\alpha-1}, d_{\alpha-1})$ are scheduled using 
a multiplexing degree of $\beta$ and $(s_\alpha, d_\alpha)$ cannot be
accommodated in configuration $\beta$. Since the connections 
in $P_\beta$ do not conflict with each other, 
another connection that belongs to  $P_\gamma$, $\gamma < \beta$ must be
scheduled in configuration $\beta$. Hence, $(s_\alpha, d_\alpha)$ is not the
first connection that does not satisfy the proposition, which contradicts
the assumption. $\Box$

The theorem states that if the connections
are reordered by the AAPC phases,
at most all AAPC 
phases are needed to realize arbitrary pattern using the
greedy scheduling algorithm. For example, following the algorithms in 
\cite{Hinrichs94}, $N^3/8$ phases are needed for a $N\times N$ torus. 
Therefore, in a $N\times N$ torus, $N^3/8$ degree is enough to satisfy
any communication pattern.

To obtain better performance on dense communication patterns, it is 
better to keep the connections in their AAPC format as much as possible. 
It is therefore better to schedule the phases with higher link 
utilization first. This heuristic is used in the ordered AAPC algorithm.
In ordered AAPC algorithm, the rank of the AAPC phases is calculated so 
that the phase that has higher utilization has higher rank. The phases 
are then scheduled according to their ranks. The algorithm is depicted in 
Figure~\ref{ORDAAPC}. The time complexity of this algorithm is
$O(|Comm|(lg(|Comm|) + max_i(|C_{i}|)\times K))$, where $|Comm|$ 
is the number of 
the connections, $|C_{i}|$ is the number of connections in configuration
$C_{i}$ and $K$ is the number of configurations needed. The advantage 
of this algorithm is that for this algorithm the multiplexing degree 
is bounded by $N^3/8$. Thus, in situations where the greedy or coloring
heuristics fail to meet this bound, AAPC can be used.  

\begin{figure}[ht]
%\small
\begin{tabbing}
\hspace{1in}(1)\hspace{0.3in}PhaseRank[*] = 0\\
\hspace{1in}(2)\hspace{0.3in}{\bf for}\= $(s_{i}, d_{i}) \in Comm$ {\bf do}\\
\hspace{1in}(3)\hspace{0.3in}\>let $(s_{i}, d_{i}) \in A_{k}$\\
\hspace{1in}(4)\hspace{0.3in}\>PhaseRank[k] = PhaseRank[k] + length($(s_{i}, d_{i})$)\\
\hspace{1in}(5)\hspace{0.3in}{\bf end for}\\
\hspace{1in}(6)\hspace{0.3in}sort phase according to PhaseRank\\
\hspace{1in}(7)\hspace{0.3in}Reorder $Comm$ according the sorted phases.\\
\hspace{1in}(8)\hspace{0.3in}call greedy algorithm\\
\end{tabbing}
\caption{Ordered AAPC scheduling algorithm}
\label{ORDAAPC}
\normalsize
\end{figure}

\subsection{Performance of the scheduling algorithms}

In this section,  the performance of the connection scheduling
algorithms on $8\times 8$ torus topology is studied. 
The performances of the algorithms 
are evaluated using randomly generated communication patterns, patterns
encountered during data redistribution, and some frequently used 
communication patterns. The metric used to compare the algorithms is the 
multiplexing degree needed to establish the connections.
It should be noted that a dynamic scheduling algorithm will not perform
better than the greedy algorithm since it must establish the connections 
by considering the connections in the order that they arrive. 

A {\em random communication pattern} consists of a certain number of 
random connections. A random connection is obtained
by randomly generating a source and a destination. Uniform probability
distribution is used to generate the sources and destinations.
The {\em data redistribution communication patterns} are obtained by
considering the communication results from array redistribution. In this
study,   data redistributions of a 3D array are considered. The array
has block--cyclic distribution in each dimension. The distribution of a
dimension can be specified by the block size and the number of processors
in the dimension.  A distribution is denoted  as {\em p:block(s)}, where
$p$ is the number of processors in the distribution and $s$ is the block size.
When the distribution of an
array is changed (which may result from the changing of the value $p$ or 
$s$), communication may be needed. 
Many programming
languages for supercomputers, such as CRAFT FORTRAN, allow an array to be
redistributed within a program. 

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
number of  & greedy  & coloring & AAPC & combined &improvement\\
connections. & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
100  & 7.0  & 6.7 & 6.9 & 6.6 & 6.3\%\\
\hline
400 & 16.5  & 16.1 & 16.5 & 15.9 & 3.8\%\\
\hline
800 & 27.2  & 25.9 & 26.5 & 25.6 & 6.3\%\\
\hline 
1200 & 36.3 & 34.5 & 35.3 & 34.2 & 6.1\%\\
\hline
1600 & 45.0  & 43.5 & 43.4 & 42.8 & 5.1\%\\
\hline
2000 & 53.4  & 50.4 & 50.4 & 49.7 & 7.4\%\\
\hline
2400 & 60.8  & 57.5 & 57.4 & 56.7 & 7.2\%\\
\hline
2800 & 68.8  & 64.4 & 62.4 & 62.4 & 10.2\%\\
\hline
3200 & 76.3  & 70.8 & 64 & 64 & 19.2\%\\
\hline
3600 & 83.9  & 76.8 & 64 & 64 & 31.1\%\\
\hline
4000 & 91.6  & 83 & 64 & 64 & 43.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Performance for random patterns}
\label{RANDOM}
\end{table}

Table~\ref{RANDOM} shows the multiplexing degree required to establish
connections for random communication patterns using the algorithms
presented.
The results in each row are the averages obtained from scheduling 100 different
randomly generated patterns with the specific number of connections.
The results in the column labeled {\em combined algorithm} are obtained 
by using 
the minimum of  the coloring algorithm and
the AAPC algorithm results.
Note that in compiled communication, more time can be spent
to obtain better runtime network utilization. Hence,  the
combined algorithm can be used to obtain better result by the compiler. The 
percentage  improvement shown in the sixth column
is achieved by the combined algorithm over the
dynamic scheduling. 
It is  observed that the coloring algorithm is always
better than the greedy  algorithm
and the AAPC algorithm is better than 
the other algorithms when the communication is dense. 
It can be seen that for sparse random patterns (100 - 2400
connections), the 
improvement range varies from 3.8\% to 7.2\%. Larger improvement
results for dense communication.  For example, the combined algorithm
uses 43.1\% less multiplexing degree than that of the greedy algorithm
for all--to--all pattern. 
This result confirms the result in \cite{Hinrichs94} that
it is desirable to use compiled communication for dense communication.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
No. of  & No. of  & greedy   & coloring & AAPC & combined &improvement\\
connections & patterns & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
0 - 100 & 34  & 1.2 &  1.2 & 1.2 & 1.2 & 0.0\%\\
\hline
101 - 200 & 50 & 5.9 & 4.9 & 4.8 & 4.6 & 28.3\%\\
\hline
200 - 400 & 54 & 10.6 &  9.7 & 10.0 &  9.5 & 11.6\%\\
\hline 
401 - 800 & 105 & 17.7 & 15.9 & 16.0& 15.5 & 14.2\%\\
\hline
801 - 1200 & 122 & 31.7 & 28.7 & 28.6 & 27.6 & 14.9\%\\
\hline
1201 - 1600 & 0  & 0      & 0    & 0    &0     &    0\%\\
\hline
1601 - 2000 & 15 & 46.3 &  42.8 & 35.1 & 35.1 & 31.9\%\\
\hline
2001 - 2400 & 77 & 55.5 &  51.5 & 51.9 & 50.4 & 10.1\%\\
\hline
2401 - 4031 & 0  & 0       & 0    & 0     &   0   &  0\% \\
\hline
4032     & 43 & 92  & 83 & 64 &  64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for data distribution patterns}
\label{REDIST}
\end{table}

To obtain more realistic results,  the performance is also evaluated using
the communication patterns for data redistribution and some
frequently used communication patterns which occurs in the programs 
analyzed by the E--SUIF compiler.
Table~\ref{REDIST} shows the performance of the algorithms for data 
redistribution patterns. The communication patterns
are  extracted from the communication resulting from 
the random data redistribution of a 3D array of size
$64 \times64 \times 64$. 
The  random data redistribution is created by randomly generating
the source data distribution and
the destination data distribution with regard to
the number of processors allocated to each dimension and the block size
in each dimension. Precautions are taken to make sure that the 
total processor number is 64 and the block size is not too large so that
some processors do not contain any part of the array.
The table lists the results for 500 random data redistributions. The first
column lists the range of the number of connections in each pattern.
The second column lists the number of data redistrictions whose number of 
connections fell into the range. For example, the second column in the
last row indicates that among the 500 random data redistributions, 43
results in 4032 connections. Columns three to six
list the multiplexing degree required by the greedy algorithm, the coloring
algorithm, the AAPC algorithm and the 
combined algorithm respectively. The seventh 
column lists the percentage improvement
 by the combined algorithm over the greedy
algorithm.
The result shows that the 
 multiplexing degree required to establish connections resulting
from data redistribution is less than that resulting 
from the random communication patterns. 
For the data redistribution pattern, the percentage improvement 
obtained by using
the combined algorithm  ranges from
10.1\% to 31.9\%, which is larger than the improvement for the random 
communication patterns.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Pattern & No. of conn. & greedy  & coloring & AAPC & comb & percentage\\
\hline
ring    & 128 &  3  &  2 & 2  &  2   &   50\%\\
\hline
nearest neighbor & 256 & 6  & 4 & 4  &  4 & 50\%   \\
\hline
hypercube & 384 &  9 & 7 & 8 &  7 & 28.6\%\\
\hline
shuffle--exchange & 126 &  6& 4 & 5  & 4 & 50\% \\
\hline 
all--to--all & 4032 &  92 & 83 & 64 & 64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for frequently used patterns}
\label{FUSED}
\end{table}

Table~\ref{FUSED} shows the performance for some
frequently used communication patterns. In the ring and the
nearest neighbor patterns, no conflicts arise in the links. 
However, there are conflicts in the communication switches.
The performance gain is higher for these 
specific patterns when the combined algorithm is used.

\section{Communication Phase analysis} 

Armed with the connection scheduling algorithms, the compiler can
determine when two communication patterns can be combined so that
the underlying network can support both patterns simultaneously and
thus, can partition a program into phases such that each phase contains
connections that can be supported by the underlying network. 
This section considers the compiler algorithm to partition a program.
%into phases so that each phase will contains communications
%that can be supported by the underlying network. 
%Besides using  the 
%algorithms described in the previous section to obtains communication 
%patterns (on physical processors), we also use the algorithms in 
%\cite{Yuan96} to obtain the number of channels needed to 
%support a communication pattern on torus networks.

The communication phase analysis is carried out in a recursive manner on 
the high level SUIF representation of a program, 
which is similar to an abstract syntax tree. 
%Here, the high level SUIF
%will be briefly introduced. 
%Before I present the communication phase analysis algorithm, I 
%will first briefly describe the high level SUIF representation. 
%Details about 
%the SUIF representation can be found in \cite{SUIF}.
SUIF represents a program in a hierarchical manner. 
A procedure contains a list of SUIF nodes, where each
node can be of different types and can contain sub--lists. Some important
SUIF node types include TREE\_FOR, TREE\_LOOP,
TREE\_IF, TREE\_BLOCK and TREE\_INSTR. 
A TREE\_FOR node represents a for--loop structure. 
It contains four sub--lists, $lb\_list$ which contains the SUIF
to compute the 
lower bound, $ub\_list$ which contains the nodes to compute the upper bound,
$step\_list$ which contains the nodes to compute the step, and 
$body$ which contains the 
loop body. A TREE\_LOOP node represents a 
while--loop structure. It contains two sub--lists, $test$ and $body$.
A TREE\_IF node represents an if--then--else structure. It contains
three sub--lists, $header$ which is the test part, $then\_part$ which contains
the nodes in the then part, and the $else\_part$. A TREE\_BLOCK node
represents a block of statements, it contains a sub--list $body$.
A TREE\_INSTR nodes represents a statement.

%Each structure has two variables associated with it, the 
%communication pattern, $pattern$, which is the communication patterns 
%expose in the header before any, and the phase indicator, $phase$ to indicate
%whether there are phases (network reconfiguration) within the structure. 

%\subsection{Phase analysis algorithms}

Given a SUIF representation of a program, which contains a list of nodes,
the communication phase analysis algorithm determines the communication
phases for each sub--lists in the list and then determines the 
communication phases of the list. 
In addition to the annotations for communications, a {\em composite node}, 
which contains sub--lists, is associated with
two variables, $pattern$, which is the communication pattern that is exposed
from the sub--lists, and the $kill\_phase$, which has a boolean value, 
indicating whether its sub--lists contain phases.

\begin{figure}[tbph]
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Communication\_Phase\_Analysis(list)\\
\hspace{0.5in}Input: $list$: a list of SUIF nodes\\
\hspace{0.5in}Output:\=\ $pattern$: communication pattern exposed out of the 
                      list\\ 
              \>$kill\_phase$: whether there are phases within the list\\
\\
\hspace{0.5in}\=An\=al\=yze communication phases for each node in the list.\\
       \>$c\_pattern = NULL, kill\_phase = 0$\\
       \>{\bf For each} node $n$ in list in backward order {\bf do}\\
       \>\> {\bf if} ($n$ is annotated with $kill\_phase$) {\bf then}\\
       \>\>\>Generate a new phase for $c\_pattern$ after $n$.\\
       \>\>\>$c\_pattern = NULL, kill\_phase = 1$\\
       \>\>{\bf end if}\\
       \>\> {\bf if} ($n$ is annotated with communication pattern $a$) 
           {\bf then}\\
       \>\>\>$new\_pattern = c\_pattern + a$\\
       \>\>\>{\bf if}\=\ ($multiplexing\_degree(new\_pattern) \le d$) 
             {\bf then}\\
       \>\>\>\>$c\_pattern$ = new\_pattern\\
       \>\>\>{\bf else}\\
       \>\>\>\>Generate a new phase for $c\_pattern$ after $n$.\\
       \>\>\>\>$c\_pattern = a, kill\_phase = 1$\\
       \>\>\>{\bf end if}\\
       \>\>{\bf end if}\\
       \>{\bf end for}\\
       \>{\bf return} $c\_pattern$ and $kill\_phase$
\end{tabbing}
\end{center}
\caption{Communication phase analysis algorithm}
\label{PHASE}
\end{figure}





\begin{figure}[tbph]
%\small
%\footnotesize
\begin{center}
\begin{tabbing}
\hspace{0.5in}Communication\_Phase\_Analysis for TREE\_IF\\
\\
\hspace{0.5in}\=Analyze the $header$ list.\\
             \>Analyze the $then\_part$ list.\\
             \>Analyze the $else\_part$ list.\\
             \>Let $comb =$ the combined communications from the three 
               sub--lists.\\
             \>{\bf If}\=\ (there are phase changes in the sub--lists) 
               {\bf then}\\
             \>\>Generate a phase in each sub--list for the 
                 communication exposed.\\
             \>\>$pattern = NULL, kill\_phase = 1$\\
             \>{\bf if} ($multiplexing\_degree(comb) > d$) {\bf then}\\
             \>\>Generate a phase in each sub--list for the 
                 communication exposed.\\
             \>\>$pattern = NULL, kill\_phase = 1$\\
             \> {\bf else}\\
             \>\> $pattern = comb, kill\_phase = 0$\\
             \>{\bf end if}\\
             \>Annotate the TREE\_IF node with $pattern$ and $kill\_phase$.
\end{tabbing}
\end{center}
\caption{Communication phase analysis for TREE\_IF nodes}
\label{treeif}
\end{figure}

The algorithm to analyze communication phases in a program
for a node list is shown in Figure~\ref{PHASE}. 
The algorithm assumes that the multiplexing degree for the 
system is $d$. It also uses one of the algorithms discussed in 
section~\ref{cscheduling}, denoted as\\
$multiplexing\_degree(Comm)$,
to compute the multiplexing degree required to 
realize communication pattern $Comm$. Given a node list, the algorithm 
first recursively examines the sub--lists of all nodes 
and annotates the nodes with $pattern$ and $kill\_phase$. 
This post--order traversal of the
SUIF program accumulates the communications in the innermost loops first,
and thus can capture the communication locality when it exists and is 
supported by the underlying
network. Figure~\ref{treeif} describes the operations for
TREE\_IF nodes. The algorithms for TREE\_IF node computes
the phases for the three sub--lists. In the cases when there are phases 
within the sub--lists and  when the network does not have enough capacity
to support the combined communication, a phase is created in 
each of the sub--list to accommodate the corresponding communication
from that sub--list. Otherwise, the TREE\_IF node is annotated with 
the combined communication indicating the communication requirement of
the IF statement. Algorithms for processing other node types are 
similar. After all sub--lists in all nodes in the list are analyzed, 
the node list contains a straight line program, whose nodes are annotated with
communication, $pattern$ and $kill\_phase$.
The algorithm examines all these annotations in each node
from back to front. A variable $c\_pattern$ is
used to maintain all communications currently accumulated. 
There are two cases when a phase is generated. First, 
once a $kill\_phase$ annotation is encountered, which indicates there are 
phases in the sub--lists,  thus, it does not make sense to maintain 
a phase passing the node since there are phase changes during the
execution of the sub--lists, a new
phase is created to accommodate the connection requirement after the node. 
Second, in the cases when adding a new communication pattern 
into the current (accumulated) pattern exceeds the network capacity, 
a new communication phase is needed.
% Notice that a SCD can represent a communication pattern that is
%as complex as all to all connections. It might require 
% several communication phases for a single communication.


\begin{figure}
\centerline{\psfig{figure=fig/phaseexam.eps,width=5.9in}}
\caption{An example for communication phase analysis}
\label{phaseexam}
\end{figure}

Figure~\ref{phaseexam} shows an example for the communication phase analysis.
The program in the example contains six communications, $C0$, $C1$,
$C2$, $C3$, $C4$, $C5$ and $C6$, an IF structure and a DO structure.
The communication phase analysis algorithm first analyzes
the sub--lists in the IF and DO structures. Assuming the combination
of  $C1$ and $C2$ can be supported by the underlying network, while 
combining communications $C1$, $C2$ and $C3$ exceeds the network 
capacity, which results in the two phases in the IF branches and the 
$Kill\_phase$ is set for the IF header node. Assuming that all communications
of $C5$ within the DO loop can be supported by the underlying network,
Figure~\ref{phaseexam}~(a) shows the results after the sub--lists are analyzed.
The algorithm then analyzes the list by considering each node from back
to forth, it combines communications $C4$ and $C5$. Since the IF header
node is annotated with $kill\_phase$. A new phase is generated for 
communications $C4$ and $C5$ after the IF structure. The algorithm then
proceeds to create a phase for communication $C0$. Figure~\ref{phaseexam}~(c)
shows the final result of the communication phase analysis for this example.

%Here let me construct a communication phase analysis example.
%Let me think of a good example.

\subsection{Evalutation of the communication phase analysis algorithm}
\label{evalphase}

This section  presents the performance evaluation of the E--SUIF compiler
for compiled communication.
The compiler is evaluated with respect to the analysis time and 
the runtime performance.  The E--SUIF compiler 
analyzes the communication requirement of a program
and partitions the program into phases 
such that each phase contains a communication pattern that can be realized 
by a multiplexing degree of $d$, where $d$ is a parameter. In addition, the
compiler also gives channel assignments for connections in each phase.
It is  assumed that the underlying network is a $8\times 8$ torus.

\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|}
\hline
Prog. & Description & Distrib.\\
\hline
0001 & Solution of 2-D Poisson Equation by ADI & (*, block)\\
\hline
0003 & 2-D Fast Fourier Transform & (*, block)\\
\hline
0004 & NAS EP Benchmark - Tabulation of Random Numbers & (*, block)\\
\hline
0008 & 2-D Convolution & (*, block)\\
\hline
0009 & Accept/Reject for Gaussian Random Number Generation & (block) \\
\hline
0011 & Spanning Percolation Cluster Generation in 2-D & (*, block)\\
\hline
0013 & 2-D Potts Model Simulation using Metropolis Heatbath & (*, block)\\
\hline
0014 & 2-D Binary Phase Quenching of Cahn Hilliard Cook Equation & (*, block) \\
\hline
0022 & Gaussian Elimination - NPAC Benchmark & (*, cyclic) \\
\hline
0025 & N-Body Force Calculation - NPAC Benchmark & (block, *) \\
\hline
0039 & Segmented Bitonic Sort & (block) \\
\hline
0041 & Wavelet Image Processing & (*, block)\\
\hline
0053 & Hopfield Neural Network & (*, block) \\
\hline
\end{tabular}
\end{center}
\caption{Benchmarks and their descriptions}
\label{desc}
\end{table}

Programs from the HPF benchmark suite
at Syracuse University are used to evaluate the algorithms. 
The benchmarks and their descriptions are listed in Table~\ref{desc}. 
The table also shows the data
distribution of the major arrays in the programs. These 
distributions are obtained from the original benchmark programs.
Table~\ref{analysistime} breaks down the analysis
time. The table shows the time for overall analysis, the logical
communication analysis and the communication phase analysis.
The overall analysis includes
the time to load and store the program, 
the time to analyze communication requirement on the
virtual processor grid, 
the time to derive communication requirement on the
physical processor grid and the time for communication phase analysis.
The communication phase analysis time accounts for a significant portion
of the overall analysis time for all the programs. 
This is because the communication phase operates on large sets of data
(communication pattern). %Hence, to apply this technique for large programs,
%the analysis cost must be improved by carefully designing the analysis 
%algorithms and data structures. 
However, for medium size programs, such as the benchmarks used,
the analysis time is not significant. 

\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|c|c|}
\hline
benchmarks & size (lines) & overall & logical communication & phase analysis\\
\hline
0001 & 545 & 11.33 & 0.45 & 8.03 \\
\hline
0003 & 372 & 24.83 & 0.50 & 11.80\\
\hline
0004 & 599 & 19.08 & 0.42 & 15.02\\
\hline
0008 & 404 & 27.08 & 0.68 & 13.28\\
\hline
0009 & 491 & 46.72 & 4.45 & 19.65\\
\hline
0011 & 439 & 14.78 & 0.57 & 11.37\\
\hline
0013 & 688 & 23.08 & 1.07 & 17.30\\
\hline
0014 & 428 & 15.58 & 1.03 & 11.38 \\
\hline
0022 & 496 & 22.57 & 0.77 & 18.35 \\
\hline
0025 & 295 & 5.77 & 0.78 & 3.35 \\
\hline
0039 & 465 & 16.08 & 0.38 & 13.13 \\
\hline
0041 & 579 & 9.93 & 0.28 & 6.62\\
\hline
0053 & 474 & 7.39 & 0.35 & 4.33\\
\hline
\end{tabular}
\end{center}
\caption{Communication phase analysis time}
\label{analysistime}
\end{table}


\begin{table}
\small
\footnotesize
\begin{center}
\begin{tabular} {|c|c|c|c|c|c|c|}
\hline
benchmark & \multicolumn{3}{c|}{connections per phase} &  \multicolumn{3}{c|}{
channels per phase}\\
\cline{2-7}
programs  & actual & compiled & percentage & actual & compiled & percentage\\
\hline
0001 & 564.4 & 564.4 & 100\% & 9.1 & 9.1 &100\% \\
\hline
0003 & 537.6 & 537.6 & 100\% & 8.6 & 8.6 & 100\% \\
\hline
0004 & 116.3 & 116.3 & 100\% & 5.5 & 5.5 & 100\% \\
\hline
0008 & 562.6 & 562.6 & 100\% & 8.9 & 8.9 & 100\% \\
\hline
0009 & 91.2 & 230.7 & 39.6\% & 4.3 & 6.6 & 65.1\% \\
\hline
0011 & 126.3 & 126.3 & 100\% & 5.2 & 5.2 & 100\% \\
\hline
0013 & 67.3 & 67.3 & 100\% & 3.1 & 3.1 & 100\% \\
\hline
0014 & 126.4 & 126.4 & 100\% & 4.0 & 4.0 & 100\% \\
\hline
0022 & 13.1 & 413.2 & 3\% & 4.6 & 8.9 & 52.7\% \\
\hline
0025 & 80.0  &80.0 & 100\% & 3.0 & 3.0 & 100\% \\
\hline
0039 &125.7 & 125.8 & 99.9\% & 8.8 & 8.8 & 99.9\%\\
\hline
0041 & 556.1 & 556.1 & 100\% & 8.8 & 8.8& 100\% \\
\hline
0053 & 149.2& 575.2 & 25.9\% & 9.0 & 9.1 & 98.9\\
\hline
\end{tabular}
\end{center}
\caption{Analysis precision}
\label{precision}
\end{table}


Table~\ref{precision} shows the precision of the analysis. It compares
the average number of channels and connections per phase obtained from
our algorithms with  those in  actual executions. The number of channels and 
connections per phase in actual executions is obtained by accumulating the
connections within each phase, which is determined by the compiler. When 
a phase change occurs, the statistics about the number of connections within
each phase is collected and the connection scheduling algorithm is invoked
to compute the number of channels needed for the connections in that phase.
For most programs, the analysis results match the actual program executions,
which indicates that approximations are seldom used. 
For the programs where
 approximations occur, the channel approximation is better
than the connection approximation as shown in benchmark 0022. This is mainly
due to the approximation of the communications that are not vectorized.
For such communications, if the underlying network can support all connections
 in a loop, the phase will contain the loop and use the channels
for all communications in the loop. However, for the connections, the 
compiler approximates each individual communication inside the loop with
all communications of the loop. Since the number of channels for a
communication pattern determines the communication performance, this 
type of approximation does not hurt the communication performance.

\section{Chapter summary}

This chapter addressed the compiler issues for applying compiled 
communication. 
In particular, algorithms for communication analysis were presented
which take into consideration common communication optimizations
including message vectorization, redundant communication elimination and 
message scheduling. A demand
driven array data flow framework, which improves over previous communication
optimization algorithms by reducing the analysis cost and improving the 
analysis precision,  was developed 
for the communication optimizations. 
Three off-line connection
scheduling algorithms were described that realize 
a given communication pattern with a minimal multiplexing degree. 
A communication phase analysis algorithm, which partitions a program 
into phases such that each phase contains communications that can be
supported by the underlying network, was developed. 
The algorithm also exploits 
communication locality to reduce the amount of reconfiguration overhead
during program execution.

A compiler, called the E--SUIF compiler, implements all the above
algorithms and thus, supports  compiled communication.  
The E--SUIF compiler 
compiles a HPF--like program, analyzes its communication requirement,
partitions the program into phases such that each phase contains 
connections that can be supported by the underlying network,
assigns channels for connections in each phase, and outputs a C program
with the communication and phase annotations such that when 
the program is executed, the communications (and phases) 
in the program can be simulated.
All the algorithms were evaluated in the compiler. It was found that
the communication optimization algorithms are efficient in terms of 
the analysis cost and are  effective in finding the optimization 
opportunities. The communication phases analysis algorithm 
generally captures the program runtime behavior accurately. 

In the last three chapters, techniques for the 
three communication schemes are discussed.
Next chapter evaluates the three 
communication schemes and compares their performance using real application
programs. 










\documentstyle[11pt,psfig,setspace,subfigureh,psubfigure,fancyheadings]{thesis}
\newcounter{DefCount} 

\begin{document}

\maketitlepage

\makesignaturepage

\makecopyrightpage

\abstract
{
%With the increasing computation power of parallel computers, 
%interprocessor communication has become an important factor that
%limits the performance of supercomputing systems.
Optical interconnection networks are promising networks for future 
supercomputers due to their large bandwidths. However, the speed mismatch
between the fast optical data transmission and the relatively
slow electronic control components poses challenges for 
designing an optical network whose large bandwidth can be utilized 
by end users. The Time--Division--Multiplexing (TDM) technique 
alleviates this mismatch problem by sacrificing part of the large
optical bandwidth for efficient network control. 
This thesis studies efficient control mechanisms for optical
TDM point--to--point networks. Specifically, 
three communication schemes are considered, 
dynamic single--hop communication, dynamic 
multi--hop communication and compiled communication. 

%\thispagestyle{fancy}
%\setlength{\headrulewidth}{0pt}
%\rhead{Dr. Rami Melhem, Dr. Rajiv Gupta}
%\markright{\hspace{3.5in}Dr. Rami Melhem, Dr. Rajiv Gupta}

Dynamic single--hop communication uses a path reservation protocol to 
establish all--optical paths for connection requests that arrive at
the network dynamically. 
%In this 
%scheme, once a message is converted into the optical domain, 
%it remains there until it reaches the destination.
%No electronic/optical (E/O) and optical/electronic (O/E) conversions, 
%or electronic processing and buffering are performed 
%at intermediate nodes during data transmission. 
%In this communication scheme, 
%the electronic processing
%is isolated in the path reservation process. Hence, 
An efficient path 
reservation protocol is essential for this scheme to achieve high 
performance. In this thesis, a number of efficient distributed path
reservation protocols are designed and the impact of 
system parameters on these protocols is studied. 

Dynamic multi--hop communication allows intermediate hops to 
route messages toward their destinations.
% and thus, requires
%E/O and O/E conversions and electronic processing at intermediate
%hops. 
%
% Since the routing at 
%intermediate hops requires E/O and O/E conversions, it is
%important to reduce the number of intermediate hops in this 
%scheme. 
In optical TDM networks,
efficient dynamic multi--hop communication can be achieved
by routing messages through a logical topology that is more efficient than 
the physical topology. 
This thesis studies efficient schemes to realize logical topologies 
on top of physical torus topologies, presents an analytical model
for analyzing the maximum throughput and the average packet delay  
for multi--hop networks, and evaluates the performance of the optical 
communication on the logical 
topologies.

%In compiled communication, the compiler analyzes the communication
%requirement of a program and  partitions the program into phases 
%such that each phase contains communications 
%that can be supported by the underlying network. Using the 
%knowledge of the underlying network and the communication requirement, 
%the compile manages network resources, such as virtual
%channels, statically. By using the compiled communication technique, 
%runtime communication overheads, such as the path reservation overhead, 
%can be reduced or eliminated, and the communication 
%performance is improved. This technique is particularly attractive to 
%optical interconnection networks since optical networks have large control 
%overheads compared to electronic networks. 

Compiled communication eliminates the runtime communication overheads
of the dynamic communications by managing network resources 
at compile time. 
%Many compiler issues must be addressed in order to apply
%this technique. 
This thesis considers
issues for   applying  the compiled communication technique to
optical TDM networks, including communication analysis, connection scheduling
and communication phase analysis. 
%Specifically, it describes algorithms to 
%analyze the communication requirement of a program, proposes off--line 
%connection scheduling schemes that schedule connections using 
%a minimal multiplexing degree, and describes a 
%communication phase analysis algorithm which 
%partitions the program into phases such that each phase 
%contains communications that can be supported by the underlying network. 
A compiler, called the E--SUIF compiler, is implemented to support
compiled communication on optical TDM networks.
%All the algorithms are implemented and evaluated in a compiler based on the 
%Stanford SUIF compiler. 

%Communication in optical interconnection networks can be carried out 
%using any of the three communication schemes. 
Each communication scheme has its 
advantages and limitations
and is more efficient for some types of
communication patterns. 
%Dynamic single--hop communication 
%achieves all--optical communication during data transmission, 
%however, it requires extra hardware support
%to exchange control messages and results in large startup overhead.
%Dynamic multi--hop communication does not have large startup overhead,
%yet it requires electronic processing during data transmission. Compiled
%communication accomplishes all--optical communication without extra
%hardware support and large startup overhead. However, the performance
%of compiled communication relies heavily on the precision of compiler 
%analysis. It cannot be applied to applications whose communication patterns
%are known only at runtime time. 
This thesis compares the performance 
of the three communication schemes using a number of benchmarks and real
application programs and identifies the situations where each communication 
scheme out--performs the other schemes. 
}

\tableofcontents
\listoffigures
\listoftables

\bibliographystyle{plain}

\input{intro}

\input{background}

\input{singlehop}
\input{multihop}
\input{compiled}
\input{perform}

%\newpage

\begin{thebibliography}{9}

\bibitem{Acampora89}
A.S. Acampora and M.J. Karol, ``An Overview of Lightwave Packet Network.''
{\em IEEE Network Mag.}
3(1), pages 29-41, 1989.

\bibitem{amarasinghe93}
S. P. Amarasinghe and M. S. Lam ``Communication Optimization and Code
Generation for Distributed Memory Machine.'' In {\em Proceedings ACM SIGPLAN'93
Conference on Programming Languages Design and Implementation}, June 1993.

\bibitem{Amar95}
S. P. Amarasinghe, J. M. Anderson, M. S. Lam and C. W. Tseng,
``The SUIF Compiler for Scalable Parallel Machines.'' 
{\em Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing}, February, 1995.

\bibitem{As94}
H.R. As, ``Media Access Techniques: the Evolution towards Terabit/s LANs
and MANs.'' {\em Computer Networks and ISDN Systems},
26(1994) 603--656.

\bibitem{banerjee95}
P. Banerjee, J. A. Chandy, M. Gupta, E. W. Hodges IV, J. G. Holm, A. Lain, D. J. Palermo, S. Ramaswamy, and E. Su.
``The PARADIGM Compiler for Distributed-Memory Multicomputers.''
     in {\em IEEE Computer}, Vol. 28, No. 10, pages 37-47, October 1995.

\bibitem{Bannister90}
J.A. Bannister, L. Fratta and M. Gerla ``Topological Design of the
Wavelength--Division Optical Network.'' {\em IEEE INFOCOM'90}, 
pages 1005---1013, 1990.


\bibitem{Barry95}
R.A. Barry and P.A. Humblet. ``Models of Blocking Probability in All--optical
Networks with and without Wavelength Changers.'' In {\em Proceeding of
IEEE Infocom}, pages 402-412, April 1995.

\bibitem{beauquier97} B. Beauquier, J. Bermond, L. Gargano, P. Hell,
S. Perennes and U. Vaccaro ``Graph Problems Arraying from Wavelength--Routing
in All--Optical Networks.'' {\em Workshop on Optics and 
Computer Science}, 1997. 

\bibitem{Brackett90}
C. A. Brackett, ``Dense wavelength division multiplexing networks:
Principles and applications,'' {\em IEEE Journal on Selected Areas of 
Communications}, Vol. 8, pp. 948-964, Aug. 1990.

\bibitem{Brassil94} 
J. Brassil, A. K. Choudhury and N.F. Maxemchuk, ``The Manhattan Street 
Network: A High Performance, Highly Reliable Metropolitan Area Network,''
{\em Computer Networks and ISDN Systems},
26(6-8), pages 841-858, 1994.


\bibitem{Bromley91} 
M. Bromley, S. Heller, T. McNerney and G. L. Steele, Jr. ``Fortran at
Ten Gigaflops: the Connection Machine Convolution Compiler'.'' In
{\em Proc. of SIGPLAN'91 Conf. on Programming Language Design and 
Implementation}. June, 1991.

\bibitem{callahan88}
D. Callahan and K. Kennedy ``Analysis of Interprocedural Side Effects in a 
Parallel Programming Environment.'' {\em Journal of Parallel and Distributed
Computing}, 5:517-550, 1988.

\bibitem{Cappello95}
F. Cappelllo and C. Germain. ``Toward high 
communication performance through compiled communications on a 
circuit switched interconnection network.'' In {\em Proceedings of the Int'l
Symp. on High Performance Computer Architecture}, pages 44-53, Jan. 1995.

\bibitem{Chakrabarti96}
S. Chakrabarti, M. Gupta and J. Choi ``Global Communication 
Analysis and Optimization.'' {\em Proceedings of the ACM SIGPLAN'96
Conference on Programming Language Design and Implementation} (PLDI),
Pages 68 --- 78, Philadelphia, PA, May, 1996.

\bibitem{Chapman92}
B. Chapman, P. Mehrotra and H. Zima ``Programming in Vienna Fortran.''
{\em Scientific Programming}, 1:31--51, Fall 1992.

\bibitem{Chatterjee93}
S. Chatterjee, J. R. Gilbert, R. Schreiber and S. Teng ``Automatic Array 
Alignment in Data--Parlllel Programs.'' {\em Proceedings of the 20th Annual
ACM Symposium on Principles of Programming Languages}, Charleston, SC, Jan.
 1993.

\bibitem{Chatterjee93a}
S. Chatterjee, J. Gilbert, F. J. E. Long, R. Schreiber and S. Teng 
``Generating local addresses and communication sets for data--parallel
programs.'' In {\em Proc. of PPoPP}, pages 149--158, San Diego, CA, May 1993.


\bibitem{chen96}
C. Chen and s. Banerjee, ``A New Model for Optimal Routing and Wavelength
Assignment in Wavelength Division Multiplexed Optical Networks,'' {\em 
Proc. IEEE Infocom'96}, pages 164--171, 1996.

\bibitem{Chlamtac92}
I. Chlamtac, A. Ganz and G. Karmi. ``Lightpath 
Communications: An Approach to High Bandwidth Optical WAN's'' {\em 
IEEE Trans. on
 Communications}, Vol. 40, No. 7, July 1992.

\bibitem{chlamtac93}
I.Chlamtac, A. Ganz and G. Karmi ``Lightnets: Topologies for High--Speed
Optical Networks.'' {\em Journal of Lightwave Technology}, Vol. 11,
No. 5/6, pages 951---961, May/June 1993.


\bibitem{Dally87}
W. Dally and C. Seitz, ``Deadlock--Free Message Routing in Multiprocessor
Interconnection Networks.'' {\em IEEE trans. on Computers}, Vol. C--36,
No. 5, May 1987.

\bibitem{Dowd93}
P. Dowd, K. Bogineni and K. Ali,
``Hierarchical Scalable Photonic Architectures for High-Performance Processor Interconnection'',
{\em IEEE Trans. on Computers},
vol. 42, no. 9, pp. 1105-1120, 1993.

\bibitem{ganz92}
A. Ganz and Y. Gao,
``A Time-Wavelength assignment algorithm for WDM Start Networks'',
{\em Proc. of IEEE INFOCOM},
1992.

\bibitem{Gong93}
C. Gong, R. Gupta and R. Melhem. ``Compilation Techniques for
Optimizing Communication on Distributed-Memory System''. {\em International
conference on Parallel Processing}. Vol. II, pages 39-46, 
August 1993.

\bibitem{Gross89}
T. Gross. ``Communication in iWarp Systems.'' In {\em Proceedings 
Supercomputing}'89, pages 436--445, ACM/IEEE, Nov. 1989.

\bibitem{Gross94} 
T. Gross, A. Hasegawa, S. Hinrichs, D. O'Hallaron, and T. Stricker
``Communication Styles for Parallel Systems.'' {\em IEEE Computer}, 
vol.27, no. 12, December, 1994, pp. 34-44.

\bibitem{Gross94a}
T. Gross, D. O'Hallaron, and J. Subhlok ``Task parallelism in a High 
Performance Fortran framework.'' {\em IEEE Parallel \& Distributed Technology},
 vol 2, no 2, 1994, pp 16-26.

\bibitem{Gupta92}
M. Gupta and P. Banerjee. ``A Methodology for High--Level Synthesis of 
Communication on Multicomputers.'' In {\em International Conference on
Supercomputing}, Pages 357--367, 1992.

\bibitem{Gupta92a}
M. Gupta and P. Banerjee. ``Demonstration of Automatic Data Partitioning
Techniques for Parallelizing Compilers on Multicomputers.'' {\em IEEE
Trans. on Parallel and Distributed Systems}, 3(2)179-193, 1992.

\bibitem{gupta93}
M. Gupta and E. Schonberg ``A Framework for Exploiting Data Availability to
Optimize Communication.'' In {\em 6th International Workshop on Languages
and Compilers for Parallel Computing}, LNCS 768, pp 216-233, August 1993.

\bibitem{gupta95}
M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K.Y. Wang, D. Shields,
W.M. Ching and T. Ngo. ``An HPF compiler for the IBM SP2.'' In 
{\em proc. Supercomputing'95}, San Diego, CA, Dec. 1995.

\bibitem{Gupta96}
M. Gupta, E. Schonberg and H. Srinivasan ``A Unified Framework for
Optimizing Communication in Data-parallel Programs.'' In {\em IEEE trans.
on Parallel and Distributed Systems}, Vol. 7, No. 7, pages 689-704,
July 1996.

\bibitem{Hinrichs94}
S. Hinrichs, C. Kosak, D.R. O'Hallaron, T. Stricker and 
R. Take. ``An Architecture for Optimal All--to--All Personalized
Communication.'' In {\em 6th Annual ACM Symposium on Parallel Algorithms and
Architectures}, pages 310-319, June 1994.

\bibitem{Hinrichs95}
S. Hinrichs. ``Compiler Directed Architecture--Dependent Communication
Optimization.'' Ph.D dissertation, School of Computer Science, 
Carnegie Mellon University, 1995.

\bibitem{Hinrichs95a}
S. Hinrichs ``Simplifying Connection--Based Communication.'' {\em IEEE Parallel
and Distributed Technology}, 3(1)25--36, Spring 1995.

\bibitem{hinton}
H. Scott Hinton,
``Photonic Switching Using Directional Couplers'',
{\em IEEE Communication Magazine},
Vol 25, no 5, pp 16-26, 1987.

\bibitem{hiran92}
S. Hiranandani, K. Kennedy and C. Tseng ``Compiling Fortran D for MIMD
Distributed--memory Machines.'' {\em Communications of the ACM}, 35(8):66-80,
August 1992.

\bibitem{Horie91}
T. Horie and K. Hayashi. ``All--to--All Personalized 
Communication on a wrap--around Mesh.'' In {\em Proceedings of CAP Workshop},
November, 1991.

\bibitem{HPF}
High Performance Fortran Forum. {\em High Performance Fortran Language 
Specification Version 1.0.}, May 1993.

\bibitem{Ikegami97}
T. Ikegami ``WDM Devices, State of the Art.'' {\em Photonic Networks},
Springer, pages 79--90, 1997.

\bibitem{Kennedy95}
K. Kennedy and N. Nedeljkovic ``Combining dependence and data-flow analyses
to optimize communication.'' In {\em Proceedings of the 9th International
Parallel Processing Symposium}, Santa Barbara, CA, April 1995.

\bibitem{Knobe90}
K. Knobe, J.D. Lukas and G.L. Steele, Jr. ``Data Optimization:
Allocation of Arrays to Reduce Communication on SIMD Machines.''
{\em Journal of Parallel and Distributed Computing}, 8:102-118, 1990.

\bibitem{Koelbel90}
C. Koelbel ``Compiling Programs for Nonshared Memory
Machines.'' Ph.D thesis, Purdue University, August 1990.

\bibitem{Koelbel91}
C. Koelbel and P. Mehrotra ``Compiling global name--space parallel loops
for distributed execution.'' {\em IEEE Trans. on Parallel and Distributed 
Systems}, 2(4):440-451, Oct. 1991.

\bibitem{kovacevic95}
M. Kovacevic, M. Gerla and J.A. Bannister, ``On the performance of 
shared--channel multihop lightwave networks,'' {\em
Proceedings IEEE INFOCOM'95}, Boston, MA, pages 544--551, April 1995.

\bibitem{Kumar92}
M. Kumar. ``Unique Design Concepts in GF11 and Their Impact on
Performance''. {\em IBM Journal of Research and Development}. Vol. 36
No. 6, November 1992.

\bibitem{Labour91}
J. P. Labourdette and A. S. Acampora ``Logically Rearrangeable Multihop
Lightwave Networks.'' {\em IEEE Trans. on Communications}, Vol. 39, No. 8,
pages 1223---1230, August 1991.


\bibitem{Lahaut94}
D. Lahaut and C. Germain, ``Static Communications in 
Parallel Scientific Programs.'' In {\em 
Parallel Architecture \& Languages}, Europe,
pages 262--274, Athen, Greece, July 1994.

\bibitem{lee95}
S. Lee, A. D. Oh and H.A. Choi ``Hypercube Interconnection in TWDM
Optical Passive Star Networks'', {\em Proc. of the 2nd  International 
Conference on Massively Parallel Processing Using Optical Interconnections.}
San Antonio, Oct. 1995.

\bibitem{Leighton92}
F. Leighton, {\em Introduction to parallel algorithms and architecture: arrays,
trees, hypercubes.} Morgan Kaufmann, 1992.

\bibitem{Li91}
J. Li and M. Chen. ``Compiling  Communication --efficient Programs for 
Massive Parallel Machines.'' {\em IEEE Trans. on Parallel and Distributed 
Systems}, 2(3):361-376, July 1991.

\bibitem{Li91a}
J. Li and M. Chen ``The Data Alignment Phase in Compiing  Programs for
Distributed Memory Machines.'' {\em Journal of Parallel and Distributed 
Computing}, 13(2):213--221, October 1991.


\bibitem{Melhem95}
R. Melhem, ``Time--Multiplexing Optical Interconnection
Network; Why Does it Pay Off?'' In {\em Proceedings of the 1995 ICPP 
workshop on Challenges for Parallel Processing}, pages 30--35, August 1995.

\bibitem{MPI93} 
``The Message Passing Interface Forum''. {\em Draft Document for
a Standard Message Passing Interface}, November 1993.

\bibitem{Mukherjee92a}
 B. Mukherjee, ``WDM--based local lightwave networks --- Part I: Single--hop 
systems,'' {\em IEEE Network Magazine}, vol. 6, no. 3, pp. 12--27, May 1992.

\bibitem{Mukherjee92b} 
 B. Mukherjee, ``WDM--based local lightwave networks --- Part II: Multihop
systems,'' {\em IEEE Network Magazine}, vol. 6, no. 4, pp. 20--32, July 1992.

\bibitem{Mukherjee94}
B. Mukherjee, S. Ramamurthy, D. Banerjee and A. Mukherjee ``Some Principles
for Designing a Wide--Area Optical Network.'' {\em IEEE INFOCOM'94}, Vol. 1,
pages 1d1.1---1d1.10, 1994.



\bibitem{Nugent88} 
S. Nugent, ``The iPSC/2 direct--connect communications technology.'' In
{\em Proceedings of the 3rd conference on Hypercube Concurrent Computers and 
Application}, Volume 1, Jan. 1988.

\bibitem{Numrich94} R. W. Numrich, P.L. Springer and J.C. Peterson, 
``Measuerment of Communication Rates on the CRAY-T3d Interprocessor
Network''. In {\em Proceedings of High Performance Computing and Networking},
LNCS 797.

\bibitem{PVM94}
R. Manchek, ``Design and Implementation of PVM version 3.0'',
Technique report, University of Tennessee, Knoxville, 1994.


\bibitem{Qiao94}
C. Qiao and R. Melhem, ``Reconfiguration with Time Division Multiplexed 
MIN's for Multiprocessor Communications.'' {\em IEEE Trans. on Parallel and
Distributed Systems}, Vol. 5, No. 4, April 1994.

\bibitem{Qiao95}
C. Qiao and R. Melhem. ``Reducing Communication Latency with Path Multiplexing
in Optically Interconnected Multiprocessor Systems.'' In {\em Proceedings
of the International Symposium on High Performance Computer Architecture},
pages 34-43, January 1995.

\bibitem{Qiao96}
C. Qiao and Y. Mei, ``On the Multiplexing Degree Required to Embed 
Permutation in a Class of Networks with Direct Interconnects.'' 
In {\em IEEE Symp. on High Performance Computer Architecture}, Feb. 1996.

\bibitem{Ramaswami94}
R. Ramaswami and K. Sivarajan, ``Optimal Routing and Wavelength Assignment
in All--Optical Networks.'' {\em IEEE INFOCOM'94}, vol. 2, pages 970--979,
June 1994.

\bibitem{rogers89}
A. Rogers and K. Pingali ``Process decomposition through locality of
reference.'' In {\em Proc. SIGPLAN'89 conference on Programming Language
Design and Implementation}, pages 69-80, June 1989.

\bibitem{Salisbury97}
C. Salisbury and R. Melhem ``Modeling Communication Costs in Multiplexed 
Optical Switching Networks'', The {\em 
International Parallel Processing Symposium}, Geneva, 1997.

\bibitem{Sivarajan91}
K.Sivarajan and R. Ramaswami, ``Multihop networks based on de bruiji graphs,''
{\em Proceedings IEEE INFOCOM'91}, Bal Harbour, FL, pages 1001--1011, 
April 1991.

\bibitem{Sivalingam93}
K.M. Sivalingam and P.W. Dowd, ``Latency hiding strategies of pre--allocation
based media access protocols for WDM phontic networks,'' in {\em Proc. 26th
IEEE Simulation Symposium}, pages 68 -- 77, Mar. 1993.


\bibitem{Stichnoth93}
J. Stichnoth, D. O'Hallaron, and T. Gross ``Generating communication for array
    statements: Design, implementation, and evaluation,''
{\em Journal of Parallel and
    Distributed Computing}, vol. 21, no. 1, Apr, 1994, pp. 150-159. 

\bibitem{Subhlok94}
J. Subhlok, D. O'Hallaron, T. Gross, P. Dinda, J. Webb ``Communication and
    memory requirements as the basis for mapping task and data parallel 
    programs.'' {\em Proc. Supercomputing '94}, Washington, DC, Nov. 1994, 
    pp. 330-339. 

\bibitem{Subhlok93}
J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross ``Exploiting task and 
data  parallelism on a multicomputer,'' {\em Proceedings of the ACM 
SIGPLAN Symposium on Principles and Practice of Parallel Programming}, 
San Diego, CA, May, 1993, pp 13-22. 

\bibitem{Subramanian96}
S. Subramanian,  M. Azizoglu and A. Somani, ``Connectivity and Sparse
Wavelength Conversion in Wavelength-Routing Networks.''
{\em Proc. of INFOCOM'96}, pages 148--155,
1996.

\bibitem{SUIF}
Stanford Compiler Group ``The SUIF Library'', Stanford University.

\bibitem{Sussman92}
A. Sussman, G. Agrawal and J. Saltz, ``PARTI primitives for unstructured
and block structured problems.'' {\em Computing Systems in Engineering},
Vol. 3, No. 4, pages 73--86, 1992.

\bibitem{Tarjan74}
R.E. Tarjan ``Testing flow graph reducibility.'' 
{\em Journal of Computer and System Sciences}, 9:355-365, 1974.

\bibitem{Vengsarkar97}
A.M. Vengsarkar ``Optical Fiber Devices.'' {\em Photonic Networks}, Springer,
Pages 133--140, 1997.

\bibitem{Venkat96}
A. Venkateswaran and A. Sengupta ``On a Scalable Topology for Lightwave 
Networks.'' {\em IEEE INFOCOM'96}, Vol. 2, pages 4a.4.1---4a.4.8, 1996.

\bibitem{Yuan96b}
X. Yuan, R. Gupta and R. Melhem, ``Distributed Control in Optical 
WDM Networks,'' {\em
IEEE Conf. on Military Communications}(MILCOM), pages 100-104, McLean, VA, Oct.
21-24, 1996. 

\bibitem{Yuan96}
X. Yuan, R. Gupta and R. Melhem, "Demand-driven Data Flow Analysis for 
Communication Optimization," {\em Workshop on Challenges in Compiling for 
Scalable Parallel Systems}, New Orleans, Louisiana, Oct. 23-26, 1996.

\bibitem{Yuan96a}
X. Yuan, R. Melhem and R. Gupta ``Compiled Communication for All--optical
TDM Networks'', {\em Supercomputing'96}, Pittsburgh, PA, 
Nov. 1996.

\bibitem{Yuan97}
X. Yuan, R. Melhem and R. Gupta ``Distributed Path Reservation Algorithms
for Multiplexed All-optical Interconnection networks'' {\em the 
Third International
Symposium on High Performance Computer Architecture(HPCA 3)}, San Antonio,
Texas, Feb.1-5, 1997

\bibitem{Yuan97a}
X. Yuan, R. Gupta, and R. Melhem " An Array Data Flow Analysis based
Communication Optimizer," {\em Tenth Annual Workshop on Languages 
and Compilers for Parallel Computing} (LCPC'97), 
Minneapolis, Minnesota, August 1997 

\bibitem{Yuan97b}
X. Yuan, R. Gupta, and R. Melhem " Does Time Division Multiplexing 
Close the Gap Between Memory and Optical Communication Speeds?" 
{\em Workshop on Parallel Computing, Routing, and Communication} (PCRCW'97), 
Atlanta, Georgia, June 1997.

\bibitem{Yuan97c}
X. Yuan, C. Salisbury, D. Balsara and R. Melhem, ``A Load Balancing
Package on Distributed Memory System and its Application the
Particle-Particle Particle-Mesh (P3M) Methods.'' {\em Parallel Computing},
Vol. 23, No.19, pages 1525-1544, Oct. 1997.

\bibitem{Yuan98}
X. Yuan, R. Melhem and R. Gupta ``Performance of Multihop Communication
Using Logical Topologies on Torus Networks.'' {\em The Seventh 
International Conference on Computer Communications and Networks} (IC3N'98),
Lafayette, Louisiana, 1998.

\bibitem{Yuan98a}
X. Yuan and R. Melhem "Optimal Routing and Channel Assignment for 
Hypercube Communication on Optical Mesh-like Processor Arrays." 
{\em the Fifth International Conference on Massively Parallel
      Processing Using Optical Interconnections}(MPPOI'98), Las Vegas, 
      June 1998

\bibitem{Yuan98b}
X. Yuan, R. Melham, R. Gupta, Y, Mei and C. Qiao "Distributed Control
Protocols for Wavelength Reservation and Their Performance Evaluation"
Submitted to {\em IEEE trans. on Communications}, 1998.

\bibitem{zima88}
H. Zima, H. Bast and M. Gerndt. ``SUPERB: A tool for semi--automatic 
MIMD/SIMD parallelization.'' {\em Parallel Computing}, 6:1-18, 1988.

\bibitem{zhang94}
Z. Zhang and A. Acampora, ``A Heuristic Wavelength Assignment Algorithm for 
Multihop WDM Networks with Wavelength Routing and Wavelength Reuse.'' 
{\em Proc. IEEE Infocom'94}, pp 534-543, June 1994.

\end{thebibliography}
\end{document}










\chapter{Introduction}

% \subsection{Optical interconnection networks}

With the increasing computation power of parallel computers, 
interprocessor communication has become an important factor that
limits the performance of parallel computers.
Due to their capability of offering large bandwidth and low latency, 
optical interconnection networks are considered promising networks for future 
massively parallel computers. In all--optical networks,
communications are carried out in a pure circuit--switching 
fashion in order to avoid electronic/optical or optical/electronic
conversions at intermediate nodes. 
Specifically, packet switching techniques, which are usually used in electronic
multicomputer and multiprocessor interconnection networks are at a
disadvantage when
optical transmission is used. The absence of suitable photonic logic
devices makes it extremely inefficient to process packet routing information
in the photonic domain.
Moreover, conversion of this information into the electronic domain
increases the latency at intermediate nodes relative to the internode
propagation delay. Although  optical/electronic conversions may
be acceptable for large distributed networks \cite{Chlamtac92}, it represents a
disadvantage for multiprocessor networks in which internode propagation delays
are very small.

With the maturing of optical technology, transmission costs, and in 
particular the cost of high speed data links, has dropped tremendously in the 
last few years. 
The electronic processing capability of computers cannot match
the potentially very
high speed of  optical data transmission. Although in electronic
networks, the  processing speed is faster than 
the transmission speed, the transmission speed in optical networks
may be  much
higher than  the 
electronic processing speed which is typically limited to a few
gigabits per second. Thus,  
the communication bottleneck has  shifted from 
the transmission medium to the processing  needed to control that
medium in optical networks. This requires redesign
of network protocols to efficiently utilize the high speeds in optical
networks. 

Multiplexing techniques can be  used in optical
networks to fully utilize the large bandwidth of optics. 
Using multiplexing, 
multiple virtual channels are supported on a single link.
It is possible to concurrently establish multiple connections
using a single fiber in a multiplexed network.  Therefore, 
for a given topology, multiplexing increases the connectivity of the network.
Many research efforts have focussed on two
multiplexing techniques for optical interconnection networks,
namely, {\em time--division multiplexing} (TDM)
\cite{Melhem95,Qiao94,Qiao95,Qiao96} and {\em wavelength--division
multiplexing} (WDM) 
\cite{Brackett90,Chlamtac92,Ramaswami94}.
In TDM, each link is multiplexed 
by having different virtual channels on the link use different
{\em time slots}. In WDM each link is multiplexed by
having different virtual channels on the link use different 
{\em wavelengths}.

Regardless of the multiplexing technique, two approaches
can be used to establish connections in  multiplexed networks,
namely {\em link multiplexing} (LM) and {\em path multiplexing} (PM) 
\cite{Qiao95}.
In LM a connection which spans more than one communication
link is established by using possibly different channels on different links.
In PM a connection which spans more than one
communication link uses the same channel on all the links.
In other words, PM uses the same time-slot or the same wavelength on all
the links of a connection. On the other hand, 
LM can use different time-slots or different
wavelengths on different links, thus requiring time-slot interchange or
wavelength conversion capabilities at each intermediate node. 
Fig.~\ref{pmlm} shows the PM and LM connections at a switch.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/pmlm.eps,height=2.2in}}
\caption{Path multiplexing and link multiplexing}
\label{pmlm}
\end{figure}

Point--to--point networks, such as meshes, tori, rings and hypercubes,
are commonly used in commercial supercomputers. By exploiting space diversity
and traffic locality, they offer larger aggregate throughput and 
better scalability than shared media networks such as buses and stars.
Optical point--to--point networks can use either multi-hop packet routing 
(e.g.Shuffle Net
\cite{Acampora89})
or {\em deflection routing} \cite{Brassil94}. 
The performance of packet routing 
is limited by the speed of electronics since buffering and address decoding 
are usually performed in the electronic domain.
Thus, packet routing cannot efficiently utilize the potentially high bandwidth
that optics can provide.
With deflection routing, buffering of the optical data signals at intermediate
nodes is avoided by routing a packet through alternate
paths when multiple packets compete for the same output link.
While deflection routing requires simple network nodes and minimal 
buffering, a mechanism is necessary to guarantee bounded transfer
delays within the network.
As pointed out in \cite{As94}, although
point--to--point optical networks have intrinsically high aggregate throughput,
this advantage comes at the expense of additional control complexity 
in the form of routing and congestion control. New solutions should
exploit the inherent flexibility of dynamic reconfiguration of logical
topologies. 

While an all--optical network has the potential of providing large bandwidth,
network control for the all--optical network must not incur too much overhead
in order for the large bandwidth to be efficiently utilized.
In order for a data transmission 
to remain in the optical domain from the source node 
to the destination node, control mechanisms must be provided to establish
an all--optical path prior to  the data transmission. 
The efficiency of these path establishment mechanisms
is crucial for the communication performance.
This research
proposes to investigate network control schemes that establish
all--optical paths efficiently in optical TDM point--to--point networks
through architectural and compiler support. 
Specifically, two options for network control will be considered, 
the dynamic {\em distributed path reservation} scheme and 
the {\em compiled communication} scheme. 




To dynamically establish connections, either centralized 
or distributed control mechanisms can be used.
Centralized control mechanisms, such as 
wavelength assignment \cite{Ramaswami94} and 
time--slot assignment \cite{Qiao94},  collect all 
communication requests in a central controller and perform connection
scheduling in the controller. When the network size is large, 
the central controller becomes a  bottleneck. 
Thus, centralized control mechanisms 
are not scalable to large networks. Distributed path reservation protocols
distribute the work load of  path reservations to all the  nodes in the 
system. During distributed path reservation, a connection request, which 
arrives
at the network dynamically, will negotiate with each node along the path for
the establishment of a connection. The communication can  start after a
successful path reservation.
This research
 will focus on studying distributed path reservation protocols.
The protocols are generalizations of control protocols in 
non-multiplexed circuit--switching networks \cite{Nugent88}.
Multiplexing, however, introduces an additional complexity which requires
a careful consideration of many factors and parameters that affect the
efficiency of the protocols.
A distributed path reservation scheme can generally
be partitioned into two parts, selecting a physical
path and assigning virtual channels along the physical path to form 
a virtual path. To select a physical path, either deterministic routing
or adaptive routing can be used. Once a physical path is determined, a
path reservation scheme must decide how to select virtual channels on
the links along the physical path for an all--optical connection.
In addition, the signal propagation overhead must also be considered
in the designing of distributed reservation protocols. Many system paramters 
(system size, multiplexing degree, etc),
as well as  protocol parameters, contribute to the performance
of the protocols. The problem to be addressed is that, for a given
system, how to reserve paths without incurring too much protocol overhead.

\begin{figure}[htbp]
\begin{tabbing}
\hspace{1in} (1) ...\\
\hspace{1in} (2) barrier\\
\hspace{1in} (3) set network state to support the connections
                 used in $C_1$ and $C_2$\\
\hspace{1in} (4) barrier\\
\hspace{1in} (5) DO\=\ i = 1, 1000\\
\hspace{1in} (6) \> $C_1$: computation that uses communication pattern 1\\
\hspace{1in} (7) \> $C_2$: computation that uses communication pattern 2\\
\hspace{1in} (8) END DO
\end{tabbing}
\caption{An Example}
\label{CCOMM}
\end{figure}

Using distributed path reservation protocols, the
network control is performed in the electronic domain,
which is relatively slow in comparison to
the large bandwidth supported by optical data paths. Thus, the network
control overhead might become the bottleneck  in
all--optical networks, especially for communications with small message
sizes. Compiled communication is proposed to improve communication
performance by shifting the runtime control overhead 
into compile time processing. 
In compiled communication, the compiler determines the 
communication patterns in a program and inserts 
instructions that set the state of the network to support the 
communication patterns. 
Hence,
at runtime, an all--optical path for each communication in the program is  
established without  dynamic path reservation. 
Fig.~\ref{CCOMM} shows an example of compiled
communication. In the example, the compiler determines that two communication
patterns are needed inside the loop and inserts
codes in lines (2) --- (4) to set the network state to establish connections
in patterns $C_1$ and $C_2$. Hence, at runtime, the communications
inside the loop need not perform path reservation.

Beside eliminating dynamic control overhead, 
compiled
communication also offers a number of advantages over traditional dynamic
network control. First,   
off-line algorithms can be used to manage network
resources more efficiently. 
For example, off--line connection scheduling algorithms result in
better channel utilization than  simple on--line routing algorithms.
Since off--line algorithms are executed at compile time, 
the complexity of the algorithms will
not effect the runtime efficiency of the program. 
Second, compiled communication may simplify the 
software communication protocol since 
communications are deterministic
under compiled communication
\cite{Kumar92}. Third, 
since  routing decisions are made at compile time, 
compiled communication uses simpler hardware in the 
router and does not incur routing delay.
The major limitation of
 compiled communication is that it cannot efficiently handle  the
communication patterns that are unknown at compile time. 

A number of issues must be addressed
in order to apply compiled communication techniques. First, the compiler must
be able to synthesize communication patterns from application programs.
This task can be complex depending on the programming model and the 
machine model. Second, the compiler must
have  knowledge of the underlying networks. Many compilers generate code
using high level communication libraries, such as PVM and MPI, which
provide a very simple model for communications. However, this simple model
also hinders  compiled communication. A different
communication model
must be provided for the compiler to manage communications statically.
Third, efficient off--line algorithms must be designed for the compiler 
to manage network resources effectively. 
Further, if compiled 
communication is to be the only means for network control, mechanisms to
perform correct communications for the communication
patterns that are unknown at compile time
must be provided. One way to handle compiled time unknown communication
patterns in compiled communication is to use a pre--determined pattern
to route messages and emulate multi-hop communication.

%When dealing with compile time unknown patterns, 
%routing messages through a pre--determined
%communication pattern (logical topology) is an alternative
%to providing  distributed path reservation protocols. 
%Data transmissions are no longer all--optical, instead, there are 
%intermediate nodes at which routing must be performed.  
%Three factors contribute to the communication performance: 
%the number of intermediate hops, the processing time at an 
%intermediate hop and the multiplexing degree required to realize the
%logical topology.
%Consider an 
%$n\times n$ mesh connected network. A message has to be relayed, on average,
%at $n$
%intermediate routing hops. If a  hypercube logical topology is 
%realized on the mesh using virtual circuits, then a message will be relayed, 
%on average, at
%$\frac{lg(n)}{2}$ intermediate routing hops.
%This research will study 
%efficient logical topologies for  point--to--point physical networks.
%Notice that  two
%extremes for the choices of logical topologies are (1) 
%having no logical topology, which
%minimizes the multiplexing degree  while requiring a large number of 
%intermediate relays and (2) realizing
%a complete graph,  which eliminates all the intermediate relays 
%at the cost of a large multiplexing degree. A good logical topology may be
%a compromise between these two extremes.

This research will consider both dynamic network control and compiled 
communication in optical TDM point--to--point networks using path 
multiplexing.
To support this research,  a detailed 
network simulator that simulates all  proposed
control schemes will be 
designed and implemented. 
The simulator should be able to simulate communication with three different
control mechanisms,  (1) various dynamic path reservation protocols, 
(2) compiled communication for   patterns
known at compile time  
where communication paths are assumed to 
be realized using off--line algorithms, and (3)  compiled communication 
for  patterns unknown at compile time
where messages are routed through a logical topology.
Beside the network simulator, a tool
 must be developed to extract communication 
patterns from application programs. 
Traditional communication traces are not sufficient for
the study of compiled communication. Instead,  static communication
patterns must be distinguished from dynamic communication patterns. 
In order to obtain this
information, A communication analysis tool that analyzes
communication requirements at compiler time will be developed. 

In the rest of the proposal,  background information and previous work
related to this research will be presented in  Chapter 2. 
The research goals and  approaches will be described in
Chapter 3. Preliminary work that has been done will be reported in 
Chapter 4, and Chapter 5 concludes the proposal.

\newpage











\chapter{Introduction}

Fiber--optic technology has advanced significantly over the past
few years, so have the development of tunable lasers, filters, high--speed
transmitter and receiver circuits, optical amplifiers and photonic
switching devices \cite{Ikegami97,Vengsarkar97}. 
With the maturing of optical technology, 
transmission cost, and in particular the cost of high speed data links, 
has dropped tremendously. The electronic processing capability of 
computers cannot match the potentially very
high speed of  optical data transmission.
The communication bottleneck has shifted from 
the transmission medium to the processing  needed to control that
medium. 

In optical interconnection networks, each physical link can offer very high
bandwidth. In order to fully utilize the available bandwidth, an optical
link can be shared through {\em time--division multiplexing} (TDM)
\cite{ganz92,Melhem95,Qiao94} and/or {\em wavelength--division 
multiplexing} (WDM) \cite{Brackett90,Chlamtac92,Dowd93,Subramanian96}.
In TDM, optical links are multiplexed 
by assigning different virtual communication channels to different
{\em time slots}, while in WDM, optical  links are multiplexed by
assigning different virtual communication channels to different 
{\em wavelengths}. By using TDM, WDM or TWDM (a combination of TDM and WDM), 
each link can support multiple channels.

Point--to--point networks, such as meshes, tori, rings and hypercubes,
are used in commercial supercomputers. By exploiting space diversity
and traffic locality, they offer larger aggregate throughput and 
better scalability than shared media networks such as buses.
Optical point--to--point networks can be implemented by 
replacing electronic links
with optical links and operating in a packet switching fashion just like 
electronic networks. The performance 
of such networks is limited by the speed of electronics since 
buffering and address decoding are performed in the electronic domain.
Thus, these networks cannot efficiently utilize the potentially high bandwidth
that optics can provide. New generation optical point--to--point networks
exploit the {\em channel--routing} capability in optical switches.
In TDM networks, time--slot routing\cite{Qiao94} is used, while in 
WDM networks, wavelength--routing\cite{chlamtac93} is used. The 
channel routing in an optical switch routes messages from a channel of an 
input port to a channel of an output port without converting the messages
into the electronic domain. The switch states, however, are usually 
controlled by electronic signals.

Using channel routing, two approaches can be used to establish connections 
in  multiplexed optical networks, namely {\em link multiplexing} (LM) and 
{\em path multiplexing} (PM). These connections are called {\em lightpaths}
since the light signal travels through the connections that may span a number
of optical links and switches without being converted into the electronic 
domain. In LM, a connection which spans more 
than one communication link is established by using possibly different 
channels on different links. In PM, a connection which spans more than one
communication link uses the same channel on all the links.
In other words, PM uses the same time-slot or the same wavelength on all
links of a connection, while LM can use different time-slots or different
wavelengths, thus requiring time-slot interchange or
wavelength conversion capabilities at each intermediate optical switch. 
Since the technology to support PM is more mature than the one to support LM,
this thesis focuses on PM.

Since the electronic processing speed is relatively
slow compared to the optical data transmission speed,
optical point--to--point networks should ideally employ
{\em all--optical} communication in data transmission.
In all--optical communication, no electronic processing and
no electronic/optical (E/O)
or optical/electronic (O/E) conversions are performed
at intermediate nodes. Once converted into the 
optical domain, the signal remains there until it reaches the destination.
All--optical communication eliminates the electronic processing
bottleneck at intermediate nodes 
during data transmission and thus, exploits the large
bandwidth of optical links. This thesis considers all--optical 
networks where a  lightpath is established before a communication 
starts and the data transmission is carried out in a pure 
circuit--switching fashion. This type of communication is referred to 
as the {\em dynamic single--hop communication}. 
In such networks, electronic processing occurs only in 
the path reservation process and hence, using an efficient path reservation
protocol is crucial to obtain high performance. In this work, 
a number of path reservation algorithms that dynamically
establish lightpaths are designed and the impact of system parameters on the
algorithms is studied. These algorithms 
use a separate control network to exchange 
control messages and allow all--optical communication in the optical
data network.
 
Although dynamic single--hop networks achieve all--optical communication
in data transmission, the path reservation algorithms require
extra hardware support to exchange control messages
and result in large startup overhead, especially for small messages. 
An alternative is to use {\em dynamic multi--hop
communication}. In multi--hop networks, intermediate nodes are responsible
for routing packets  such that a packet sent from a 
sender will eventually reach its destination, possibly after being routed
through a number of intermediate nodes. Clearly,  multi--hop networks 
require E/O and O/E conversions at intermediate nodes. Thus, it is
important to reduce the number of hops that a packet visits. This reduction
may be achieved in an optical TDM network by 
combining the channel--routing technique and the 
packet switching technique. Specifically, packets may be routed 
through a  logical topology which has a small diameter 
as opposed to  the physical topology which may have a large diameter. 
The major issue in the multi--hop 
communication is to design appropriate logical topologies.
This thesis considers efficient schemes for realizing logical topologies
on top of physical mesh and torus networks using path multiplexing. 
Realizing logical
topologies on optical networks is different from traditional embedding 
techniques in that both routing and channel assignment 
options must be considered. An analytical model that 
models the maximum throughput and average package latency of 
multi--hop networks is developed and is used to evaluate the performance of 
logical topologies and identify the advantages of each logical topology.

While dynamic (single--hop or multi--hop) communications handle arbitrary
communication patterns,  their performance can be limited
by the electronic processing which occurs during path reservation 
in single--hop communication and during packet routing in 
multi--hop communication. 
{\em Compiled communication} overcomes this limitation for communication 
patterns that are known at compile time.
In compiled communication, the compiler analyzes a program and determines
its communication requirement. The compiler then
uses the knowledge of the underlying
architecture, together with the knowledge of the communication requirement,
to manage network resources statically. As a result,
runtime communication overheads, such as path reservation
and buffer allocation overheads, are reduced or eliminated,
and the communication performance is improved. However,
due to the limited network resources, the underlying network
cannot support arbitrary communication patterns.
Compiled communication
requires for the compiler to analyze a program and partition
it into phases such that each phase has a fixed, pre-determined
communication pattern that the underlying network can support.
The compiler inserts codes for performing network reconfigurations at
phase boundaries to support all connections in the next phase.
At runtime, a lightpath is available
for each communication without path reservation. Therefore,  
compiled communication accomplishes all--optical communication
without incurring extra hardware support and large start--up overheads.
This thesis studies the application of compiled communication 
to optical interconnection networks. Specifically, it considers
the communication analysis techniques needed to analyze the communication
requirement of a program. These analysis techniques are 
general in that
they can be applied to other communication optimizations and 
can be used for compiled communication in electronic networks. This thesis
also develops a number of  connection scheduling schemes which realize a given
communication pattern with a minimal multiplexing degree. Note that in 
optical TDM networks, communication time is proportional to the 
multiplexing degree. Finally, a communication phase analysis algorithm is 
developed to partition a program into phases so that each 
phase contains connections that can be supported by the underlying network.
All the algorithms are implemented in a compiler which is 
based on the Stanford SUIF
compiler\cite{Amar95}. This thesis evaluates the performance of the algorithms
in terms of both analysis cost and runtime efficiency.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
  & Single--hop & Multi--hop & Compiled\\
\hline
All--optical comm. & Yes & No & Yes\\
\hline
Startup overhead  & Yes & No & No\\
\hline
Extra hardware    & Yes & No & No\\
\hline
Arbitrary comm.   & Yes & Yes & No\\
\hline
\end{tabular}
\end{center}
\caption{General characteristics of the three schemes}
\label{generalchar}
\end{table}

Table~\ref{generalchar} summarizes the general characteristics of the three
schemes. In optical interconnection networks, 
the central problem to be addressed
is the reduction of  the amount of electronic processing needed for
controlling the 
communication. In dynamic single--hop networks, this problem is addressed 
by having efficient path reservation algorithms. In multi--hop networks,
this problem is tackled by designing efficient logical topologies to 
route messages. Compiled communication totally eliminates the electronic
processing in communications. However, it only applies to the communication
patterns that are known at compile time. While communications in optical
TDM point--to--point networks can be carried out by any of the three
communication schemes, it is necessary to understand the strengths and 
the limitations of each communication scheme in order to make appropriate
choices when
designing an optical interconnection network. 
In addition to considering the options within each communication scheme,
this thesis compares the performance of the three communication schemes 
using a number of benchmarks and real applications and identifies 
the situations in which each communication scheme has advantage over 
other schemes.

% ollowing section summarizes the contributions of the thesis.

%\section{Thesis contributions}
%
%This thesis makes contributions in the {\em design of control mechanisms}
%for time--multiplexed 
%optical interconnection networks. The contributions are in two areas:
%optical interconnection networks and compiler analysis techniques. In the 
%optical interconnection
%networks area, this thesis introduces efficient control schemes 
%for dynamic single--hop communication and dynamic multi--hop communication. 
%This thesis also 
%proposes and validates the idea of applying the 
%compiled communication technique 
%to optical interconnection networks.  In the compiler area, 
%this thesis addresses all the issues needed
%to apply the compiled communication paradigm 
%to optical interconnection networks,
%including communication optimization, 
%communication analysis, connection scheduling and 
%communication phase analysis. Some of the research presented in this 
%thesis has appeared in \cite{Yuan96,Yuan96a,Yuan97,Yuan97a,Yuan98,Yuan98a}.
%The main contributions of the thesis are detailed as follows.
%
%\begin{itemize}
%\item Dynamic single--hop communication.
%\begin{enumerate}
%  \item  Develop distributed path reservation protocols for optical TDM
%         point--to--point networks.
%  \item  Study the performance of the protocols and 
%         the impact of various
%         system parameters, such as system size, message size, etc, on 
%         the protocols. 
%\end{enumerate}
%\item Dynamic multi--hop communication.
%\begin{enumerate}
%  \item Develop efficient schemes to realize logical topologies on
%        top of physical optical torus and mesh networks.
%  \item Develop an analytical model to model the maximum throughput and the 
%        packet delay for multi--hop networks.
%  \item Study the performance of various logical topologies and identify
%        the advantages and the limitations of each logical topology.
%\end{enumerate}
%\item Compiled communication.
%\begin{enumerate}
%  \item Develop a communication analyzer that analyzes the communication 
%        requirement of a program.
%  \begin{itemize}
%    \item  Design a communication descriptor that describes
%           the communication requirement on virtual processor grids.
%    \item  Design data flow analysis algorithms for
%           communication optimizations to obtain communication
%           patterns in an optimized program.
%    \item  Design schemes that derive 
%           communication patterns on physical processors from the
%           communication descriptors.
%  \end{itemize}
%  \item Design communication scheduling algorithms that can be used by 
%        the compiler to schedule the communication patterns that
%        are known at compile time.
%  \item Design  a communication phase analysis algorithm
%        that partitions a program into phases such that the connections
%        within each phase can be supported by the underlying network.
%  \item Implement all the algorithms and evaluate these algorithms in terms
%        of both analysis cost and runtime efficiency.
%\end{enumerate}
%\item Simulators and Performance evaluation.
%\begin{enumerate}
%  \item Implement network simulators that simulate dynamic single--hop 
%        communication with various control protocols, dynamic multi--hop
%        communication with different logical topologies, and compiled
%        communication.
%  \item Study the performance of the three communication schemes via
%        benchmark and real application programs and identify the 
%        advantages and the limitations of each scheme.
%\end{enumerate}
%\end{itemize}
%
%Although all the techniques presented for optical TDM networks, many techniques
%may also be applied to optical WDM networks or to non--optical networks.

%\section{Thesis organization}

The remainder of the thesis is organized as follows.
Chapter 2 begins by describing background and thesis assumptions. This chapter
presents an overview of optical interconnection networks, discusses the 
TDM technique and introduces the {\em path multiplexing} (PM) and 
{\em link multiplexing} (LM) techniques
for establishing connections. This chapter also surveys the research related to
the three communication schemes. Finally, this background chapter surveys 
the traditional compilation techniques for distributed memory machines and
communication optimizations and discusses the 
difference between the traditional communication optimizations and the 
compiled communication technique.

Chapter 3 discusses the techniques used in dynamic single--hop communication.
Two types of distributed path reservation protocols, the {\em forward
reservation protocols} and the {\em backward path reservation protocols},
are described. This chapter also describes a  network simulator for
dynamic single--hop communication, evaluates the performance of 
the two types of protocols and  studies the impact of system
parameters on these protocols.

Chapter 4 discusses dynamic multi--hop communication. This chapter presents
efficient schemes for realizing logical topologies  on top of the 
physical torus networks,
describes an analytical model that models the maximum throughput and 
the average packet delay for the logical topologies and verifies 
the model with 
simulation. In addition, this chapter also describes the simulator for
dynamic multi--hop communication, evaluates 
the multi--hop communication with the logical topologies and identifies 
the advantages and the limitations of each logical topology.

Chapter 5 considers compiled communication. This chapter describes
the communication analyzer and discusses the communication descriptor used 
in the analyzer, the 
data flow analysis algorithms for communication optimizations, and
the actual communication optimization performed in the analyzer.
The chapter also describes off-line connection 
scheduling algorithms and a communication
phase analysis algorithm. Using these algorithms, the compiler analyzes 
the communication requirement of a program, partitions the program
into phases such that each phase contains connections that can be 
supported by the underlying network, and schedules the connections within 
each phase.  The chapter also presents the evaluation
of the compiler algorithms and studies their runtime efficiency.   

Chapter 6 compares the communication performance of the three
communication schemes. Three sets of application (benchmark) programs, 
including hand--coded parallel programs, HPF benchmark programs
and sequential programs from SPEC95, are used to evaluate the 
communication performance of the communication schemes. Different sets
of programs exhibit different communication characteristic. For example, 
the hand--coded programs are highly optimized for parallel execution, while
the programs from SPEC95 are not optimized for parallel execution.
This chapter compares the communication performance of the three
communication schemes and 
identifies the advantages of each scheme.

Finally, Chpater 7 summarizes the dissertation and suggests some directions
for future research.










\chapter{Dynamic multi--hop communication}
\label{multi}

Since by using time--division
multiplexing multiple channels are supported on an optical link,
more sophisticated logical topologies can be realized on top of 
a simpler physical network to improve the communication performance. These
logical topologies reduce the number of intermediate hops that a packet travels
at the cost of a larger multiplexing degree. 
On the one hand, the large multiplexing degree increases the packet 
communication time between hops. On the other hand, reducing the number 
of intermediate hops reduces the time spent at intermediate nodes. 
This chapter studies the trade--off between the  multiplexing degree
and the number of intermediate hops needed in logical topologies implemented
on top
of physical torus networks. Specifically,  four logical
topologies ranging from the most complex logical all--to--all 
connections to the simplest logical torus topology are examined.  
An analytical model for the
maximum throughput and the average packet delay is developed and verified
through simulations.
The performance 
and the impact of system parameters on the performance for
these four topologies are studied. Furthermore, the performance of 
the multi--hop communication  using an efficient logical topology is 
compared with that of the single--hop communication using a 
distributed path reservation protocol, and the advantages and 
the drawbacks of these two communication schemes are identified.

To perform multi--hop communication, packets may be routed through
intermediate nodes. Specifically, a communication
module at each node, which will be referred to as the {\em router} in this
chapter, is needed to route packets toward their destinations.
It is  assumed that each router 
contains a {\em routing buffer} that buffers all
incoming packets. For each
packet, the router determines whether to deliver the packet
to the local PE or to
the next link toward the packet destination.
A separate {\em output path buffer} is used 
for each outgoing path that buffers the packets
to be sent on that path and thus accommodates the speed mismatch between the 
electronic router and the optical path. 
Figure~\ref{ROUTER} depicts the structure of a router 
(see also Figure~\ref{ROUTER1}).
Note that the output paths are multiplexed in time over the physical links 
that connect the local PE to its corresponding switch. 
In the rest of the chapter, {\em routing delay} will be used 
to denote the time a packet spends in the routing buffer 
and the time for the router to make
a routing decision for the packet (packet routing time).
{\em Transmission delay} will be used to denote the time 
a packet spends on the path
buffer and the time it takes for the packet to be transferred on the path.

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/queue.eps,width=5in}}
\caption{A router}
\label{ROUTER}
\end{figure}

\section{Realizing logical topologies on physical torus topology}

Four logical topologies are considered in this section, 
the logical torus topology, the logical hypercube topology, 
the logical all--to--all topology, and 
the logical allXY topology where all--to--all connections are established
along each dimension. Let us consider an example in which a packet 
is transmitted from node 0 to node 11 in 
the $4\times 4$ torus shown in Figure~\ref{DEF1}. Using the  
logical all--to--all topology,
the packet will go directly from node 0 to node 11. Using the logical allXY
topology, the packet will go from node 0 to node 3 to node 11. Using the
logical hypercube topology,
the packet will go from node 0(0000) to node 1(0001)
to node 3(0011) to node 11(1011). Using the logical torus topology, the packet
will go from node 0 to node 3 to node 7 to node 11. 


\begin{figure}
\centerline{\psfig{figure=fig/def1.eps,width=2in}}
\caption{Node numbering in a torus topology}
\label{DEF1}
\end{figure}

Traditional embedding techniques that
minimize the {\em congestion}  for a given communication 
pattern are not adequate for minimizing the number of virtual channels
needed to realize the communication in an optical network with path 
multiplexing. The congestion is usually not equal to the number of 
channels needed to realize a communication pattern. 
Consider the example in Figure~\ref{notequal} in which
the congestion in the network is 2, while 3 channels
are needed to realize the three connections. To efficiently realize a 
logical topology in an optical network, both routing and channel assignment
(RCA) options must be taken into consideration. 
Schemes to realize these four logical topologies on the physical 
torus topology will be discussed next. 


\begin{figure}
\centerline{\psfig{figure=fig/notequal.eps,width=2.4in}}
\caption{Difference between embedding and RCA}
\label{notequal}
\end{figure}


\subsection{Logical hypercube topology}

This subsection considers the optimal schemes to realize the logical
hypercube topology on top of the physical torus topologies. Since the
algorithm to realize the hypercube topology on the physical torus topologies
utilizes the algorithms to realize
the hypercube on top of physical mesh, ring and array topologies, 
algorithms to realize the hypercube topology on 
top of all these mesh--like topologies are discussed. 

Given networks of size $N$, it will be proven that 
$\lfloor\frac{2N}{3}\rfloor$ and
$\lfloor\frac{N}{3} + \frac{N}{4} \rfloor$ channels are the minimum required
to realize hypercube communication on array and ring topologies, 
respectively. Routing and 
channel assignment schemes that achieve these minimum 
requirements are developed, 
indicating that the bounds are tight and the  schemes are optimal.
These schemes are extended to mesh and torus topologies and it is proven 
that for a $2^k\times 2^{r-k}$ ($k\ge r-k$) mesh or torus, 
$\lfloor\frac{2\times2^k}{3} \rfloor$ and 
$\lfloor\frac{2^k}{3} + \frac{2^k}{4} \rfloor$ channels are the minimum 
required for realizing hypercube communication on these two topologies, 
respectively. Routing and channel assignment schemes 
are designed 
that use at most two more channels than the optimal to realize hypercube
communication on these topologies. In the following sections, first
the problem of routing and channel assignments for the hypercube
communication on the physical mesh--like topologies is formally defined, 
and then the algorithms are described.

\subsubsection{Problem definition} 

A network is modeled as a directed graph G(V, E), where nodes in 
V are  switches and edges in E are links.
Each node in a network is assigned
a node number starting from 0. It is assumed that
in arrays and rings the nodes are numbered
from left to right in ascending order, and that
the nodes are numbered in row major order for meshes and tori of size 
$n\times m$.  Thus, 
the node in the $i$th column
and the $j$th row is numbered as $j \times m + i$. 
%Fig.~\ref{DEF1} shows a $4\times 4$ torus topology.
This subsection focuses on studying the optimal RCA schemes for 
these traditional numbering schemes. 
Optimal node numbering (and its RCA) is a
much more complex problem and is not considered in this dissertation.
The number of nodes in a network is assumed to be $N=2^r$. 
For a mesh or a torus to contain $2^r$ nodes, each row and column must
contain a number of nodes that is a power of two. 
Hence, the size
of meshes and tori is denoted as $N=2^k\times 2^{r-k}$.
Without losing generality, it is always assumed that $k \ge r-k$.
The notations $ARRAY(N)$ and
$RING(N)$ are used to represent arrays and rings  of size N respectively,
and $MESH(2^k\times 2^{r-k})$ and $TORUS(2^k\times 2^{r-k})$ for
meshes and tori of size $2^k\times 2^{r-k}$ respectively. 

The connection from node $src$ to
node $dst$ is denoted as $(src, dst)$. A {\em communication pattern}
is a collection of connections. 
The {\em hypercube} communication pattern contains a connection $(src,dst)$ 
if and only if the binary representations of 
 $src$ and $dst$ differ in precisely one bit. A connection in the hypercube
communication pattern is called a {\em dimension l connection} if it connects
two nodes that differ in the $l$th bit position. In a network of 
size $N=2^r$, the set, $DIM_l$, where
$0\le l\le r-1$, is defined as  the set of all dimension $l$ connections
and ${H_r}$ is defined as the hypercube communication pattern. That is

\vspace{-0.1in}
\begin{tabbing}
\hspace{0.3in}$DIM_l=$\=$\{(i, i+2^l)\ |\ i\ mod\ 2^{l+1}\ < \ 2^l\} \cup$
                     $\{(i, i-2^l)\ |\ i\ mod\ 2^{l+1}\ \ge\ 2^l\}$\\
\\
\hspace{0.3in}${H_r} = \cup_{l=0}^{r-1}DIM_l $
\end{tabbing}
%\centerline{$DIM_l=\{(i, i+2^l)\ |\ i\ mod\ 2^{l+1}\ < \ 2^l\} \cup
%                   \{(i, i-2^l)\ |\ i\ mod\ 2^{l+1}\ \ge\ 2^l\}$}
%\vspace{0.15in}
%\centerline{${H_r} = \cup_{l=0}^{r-1}DIM_l $ }
%\vspace{0.15in}

\vspace{-0.1in}
It can be easily proven that removing any $DIM_l$, for any $l\le r-1$,
from ${H_r}$ leaves two disjoint sets of connections, each of which being
a hypercube pattern on $\frac{N}{2}$ nodes. 
For example, removing $DIM_0$ from ${H_r}$ results in an ${H_{r-1}}$
on the 
even--numbered nodes and another ${H_{r-1}}$ on the odd--numbered nodes once
the nodes are properly renumbered. Next, 
some definitions are introduced and the results of this section are 
summarized.

\begin{description}

\item
{\bf Definition:} $P(x, y)$ is a {\em directed path}
 in G from node x to node y. It
 consists of a set of consecutive edges beginning at x and ending at y.

\item
{\bf Definition:} Given a network G and a communication pattern  $I$, 
a {\em routing R(I)} of $I$
is a set of directed paths $R(I) = \{P(x, y) | (x, y) \in I\}$.

\item
{\bf Definition:} Given a network G, a communication pattern I and a 
routing R(I) for the communication pattern, 
the congestion of an edge $\alpha\in E$, denoted as $\pi(G, I, R(I), \alpha)$, 
is the number of
paths in R(I) containing $\alpha$. The {\em congestion}
of G in the routing R(I), denoted as $\pi(G, I, R(I))$, is the maximum congestion 
of any edge of G in the routing R(I), that is, 
$\pi(G, I, R(I)) = max_{\alpha}\{\pi(G, I, R(I), \alpha)\}$. 
The {\em congestion} 
of G for 
a communication pattern I, 
denoted as $\pi(G, I)$, is the minimum congestion of G in any 
routing R(I) for I, that is, $\pi(G, I) = min_{R}\{\pi(G, I, R(I))\}$.

\item
{\bf Definition:} Given a network G and a routing $R(I)$ for communication
 pattern I, an
{\em assignment function} $A: R\rightarrow INT$,
is a mapping from the set of paths to the set of integers $INT$, 
where an integer
corresponds to a channel.
A {\em channel assignment} for a routing $R(I)$ is an assignment
function $A$ that satisfies the following conditions: 

\begin{enumerate}
\item  If 
$P(x_1, y_1)$, $P(x_2, y_2)$ are different paths that share a common edge,
then\\ $A(P(x_1, y_1)) \ne A(P(x_2, y_2))$. This condition 
ensures that each channel on one link can only be assigned
to one connection (i.e., there are no link conflicts). 

\item $A(P(x, y_1)) \ne A(P(x, y_2))$ and $A(P(x_1, y)) \ne A(P(x_2, y))$.
This condition ensures that 
each node can only use
one channel at a time to send to or receive from 
other nodes (i.e., there are
no node conflicts). 
\end{enumerate}


%A channel assignment
%that violates condition (1) is said to have {\em link conflicts}, and a
%channel assignment that violates condition (2) is said to have
%{\em node conflicts}. 
$A(R)$ denotes the set of 
channels assigned to the paths in R and $|A(R)|$ is the size of $A(R)$. 
Let $w(G, I, R)$ denote  the 
minimum number of channels for the routing R, that is,
$w(G, I, R) = min_A\{|A(R)|\}$.  $w(G,I)$ denotes the smallest 
$w(G, I, R)$ over all R, i.e. $w(G,I) = min_R\{w(G, I, R)\}$

\item
{\bf Lemma 1:} $w(G,I) \ge \pi(G, I)$.\\
{\bf Proof: } Follows directly from the above definitions. $\Box$
\end{description}

\noindent
The following sections show that\\

%I further give a lower bound for  $\pi(MESH(2^k\times 2^{r-k}), H_r)$
%and $\pi(TORUS(2^k\times 2^{r-k}), H_r)$
%and show that

\begin{tabbing}
\hspace{0.3in}$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$\\
\\
\hspace{0.3in}$w(RING(N), H_r) = \pi(RING(N), H_r) = 
\lfloor \frac{N}{3} + \frac{N}{4}\rfloor$\\
\\
\hspace{0.3in}$w(MESH(2^k\times 2^{r-k}), H_r)
                \le \lfloor \frac{2\times 2^k}{3} \rfloor + 2$
              $\le \pi(MESH(2^k\times 2^{r-k}), H_r) + 2$\\
\hspace{0.3in}\\
\hspace{0.3in}$w(TORUS(2^k\times 2^{r-k}), H_r) 
              \le \lfloor \frac{2^k}{3} + \frac{2^k}{4}\rfloor + 2 $
              $\le \pi(TORUS(2^k\times 2^{r-k}), H_r) + 2$
\end{tabbing}

%\centerline{$w(MESH(2^k\times 2^{r-k}), H_r) \le 
%\lfloor \frac{2\times 2^k}{3} \rfloor + 2 \le 
%\pi(MESH(2^k\times 2^{r-k}), 
%H_r) + 2$}
%\centerline{ $w(TORUS(2^k\times 2^{r-k}), H_r) \le
%\lfloor \frac{2^k}{3} + \frac{2^k}{4}\rfloor + 2 \le
%\pi(TORUS(2^k\times 2^{r-k}), H_r) + 2$}

\subsubsection{Hypercube on linear array}

Since routing in a linear array is fixed, the RCA
problem is reduced to a channel assignment problem. 
Given a linear array of size $N=2^r$, it is proven that
 $\lfloor\frac{2N}{3}\rfloor$ channels is the lower
bound for realizing the hypercube communication  by showing  that 
$\pi(ARRAY(N), H_r) \ge \lfloor\frac{2N}{3}\rfloor$.
A channel assignment scheme is developed that uses
$\lfloor\frac{2N}{3}\rfloor$ channels for the hypercube communication. 
This proves that the bound is a tight lower 
bound and that the channel assignment scheme is optimal. 
%Array topology is a 
%relatively simple topology, there exist general 
%optimal channel assignment schemes on this topology\cite{qiao96}. 

\vspace{0.12in}
\noindent
{\bf A lower bound}
\vspace{0.12in}

Using Lemma 1, a lower bound is obtained
by proving that there exists a link in the linear
array that is used $\lfloor\frac{2N}{3}\rfloor$ times when realizing
${H_r}$. The following 
lemmas establish the bound.

\noindent
{\bf Lemma 2}: In a linear array of size $N=2^r$, where $r \ge 2$, 
there are $2^{r-1}$ connections in $DIM_{r-1}\cup DIM_{r-2}$ that
use the link $(n, n+1)$ for any specific $n$ satisfying 
$2^{r-2} \le n \le 2^{r-1}-1$.

\noindent
{\bf Proof}: The connections in $DIM_{r-1}$ and $DIM_{r-2}$
can be represented by

%\small
% \footnotesize
\begin{tabbing}
\hspace{0.1in}$DIM_{r-1}$\= =$\{(i, i + \frac{N}{2}) | 0\le i <\frac{N}{2}\}$ 
                 $\cup\{(i, i - \frac{N}{2}) | \frac{N}{2}\le i < N\}$\\
\\
\hspace{0.1in}$DIM_{r-2} =\{(i, i+\frac{N}{4})|0 \le i < \frac{N}{4}$ or $
                         \frac{N}{2}\le i < \frac{3N}{4}\}$\\
\hspace{0.1in}\>$\cup$\hspace{0.05in} $\{(i, i-\frac{N}{4})| \frac{N}{4} \le i < \frac{N}{2}$ or 
                         $\frac{3N}{4}\le i < N\}$
\end{tabbing}

\noindent
Consider the 
connections in $DIM_{r-1}$. All connections $(i, i+\frac{N}{2})$ with
$0 \le i\le n$ use link $(n, n+1)$, where $2^{r-2} \le n \le 2^{r-1}-1$. 
Hence, as shown in Fig.~\ref{LEMMA1}~(a),
there are  n+1 connections in $DIM_{r-1}$ that use link 
$(n, n+1)$. Similarly, in $DIM_{r-2}$,
all connections  $(i, i+\frac{N}{4})$, where
$n < i+\frac{N}{4} < \frac{N}{2}$, use  link $(n, n+1)$.
As shown in Fig.~\ref{LEMMA1}~(b), there are $2^{r-1} - n - 1$
such connections.
Hence, there are a total of $n+1 + 2^{r-1} - n - 1 = 2^{r-1}$ 
connections in $DIM_{r-1}$ and $DIM_{r-2}$ that use link $(n, n+1)$. $\Box$


\begin{figure*}
\centerline{\psfig{figure=fig/l1.eps,width=5.4in}}
\caption{Dimension $r-1$ and $r-2$ connections}
\label{LEMMA1}
\end{figure*}

\noindent
{\bf Lemma 3}: In a linear array of size $N=2^{r}$, there exists a link 
$(n, n+1)$ such that at least $\lfloor\frac{2N}{3}\rfloor$ connections in 
${H_r}$ use that link.

\noindent
{\bf Proof}: Let $T_i(2^r)$ be the number of connections in ${H_r}$
that use link $(i, i+1)$ and let $T(2^r) = max_i (T_i(2^r))$.
Thus $T(2^0) = 0$ and $T(2^1) = 1$.
From Lemma 2, one knows that
for $2^{r-2} \le n \le 2^{r-1}-1$, link $(n, n+1)$ is used
$2^{r-1}$ times  by connections in $DIM_{r-1}$ and $DIM_{r-2}$. Thus,
the links in the second quarter of the array (from node $2^{r-2}$ to node 
$2^{r-1}-1$) are used $2^{r-1}$ times by  dimension $r-1$ and 
dimension $r-2$ connections.
By the definition of hypercube communication, it is known that 
dimension 0 to dimension $r-3$ connections 
form a hypercube on this quarter of the array. Thus, 
Lemma 2 can be recursively applied 
 and the following inequality is obtained.\\

\centerline{$T(2^r) \ge 2^{r-1} + T(2^{r-2})$}

\noindent
It can be proven by induction that the above  inequality and the 
boundary conditions $T(2^0) = 0$, $T(2^1) = 1$, imply that 
$T(N) = T(2^r) \ge \lfloor \frac{2N}{3}\rfloor$.
%
%Base case: $T(2^0) = 0 \ge \lfloor \frac{2\times 0}{3} \rfloor$ and 
%           $T(2^1) = 1 \ge \lfloor \frac{2\times 2}{3} \rfloor$.
%
%Induction case: assuming $T(\frac{N}{4}) = T(2^{r-2}) \ge \lfloor 
%                          \frac{2\times 2^{r-2}}{3} \rfloor$,
%
%$T(N) = T(2^r) \ge 2^{r-1} + T(2^{r-2})
%           \ge 2^{r-1} + \lfloor \frac{2^{r-1}}{3} \rfloor
%           \ge \lfloor \frac{3\times 2^{r-1} + 2^{r-1}}{3} \rfloor
%           \ge \lfloor  \frac{2\times 2^{r}}{3} \rfloor
%           = \lfloor \frac{2N}{3} \rfloor$
%\noindent
Hence, there exists a link which is used at least
$\lfloor\frac{2N}{3}\rfloor$ times by connections in ${H_r}$. $\Box$

The proof of Lemma 3 is constructive in the sense that
the link that is used at least 
$\lfloor\frac{2N}{3}\rfloor$ times can be found. 
By recursively considering the second quarter of the linear array, 
one can conclude that the source node, $n$, of the link $(n, n+1)$ that is 
used at least $\lfloor\frac{2N}{3}\rfloor$ times in ${H_r}$ is 
$n = \frac{N}{4}+\frac{N}{16} + \frac{N}{64} + .. = 
 \lfloor \frac{N}{3} \rfloor$.
Hence, the link that is used at least 
$\lfloor\frac{2N}{3}\rfloor$ times in ${H_r}$ is 
$(\lfloor \frac{N}{3} \rfloor, \lceil \frac{N}{3} \rceil)$.

\noindent
{\bf Corollary 3.1} Give an array of size $N=2^r$, if the nodes in the 
array are partitioned into 2 sets $S_1=\{i|0\le i \le n\}$ and 
$S_2=\{i | n+1 \le i \le N\}$, where $n=\lfloor \frac{N}{3}\rfloor$,
then there are at least $\lfloor\frac{2N}{3}\rfloor$
connections  in ${H_r}$ 
from  $S_1$ to  $S_2$ and 
$\lfloor\frac{2N}{3}\rfloor$
connections
from $S_2$ to $S_1$. $\Box$

\noindent
{\bf Theorem 1}: $\pi(ARRAY(N), H_r) \ge \lfloor \frac{2N}{3} \rfloor$.

\noindent
{\bf Proof}: Directly from Lemma 3. $\Box$

\vspace{0.12in}
\noindent
{\bf An optimal channel assignment scheme}
\vspace{0.12in}

By the definition of hypercube communication, connections  
in ${H_r}$ can be partitioned
into three sets, $DIM_0$, $EVEN_r$ and $ODD_r$. $DIM_0$ contains the 
dimension 0 connections, $EVEN_r$ contains connections 
between nodes with even node numbers, and $ODD_r$ contains 
connections between nodes with odd node numbers. 
Each of $EVEN_r$ and $ODD_r$
forms a $r-1$ dimensional 
hypercube communication, ${H_{r-1}}$, if only the nodes involved in 
communications are considered
and that the nodes are renumbered accordingly. 
%Fig.~\ref{LEMMA4} shows the
%partition of ${H_3}$.  
Thus, channel assignment schemes for 
${H_{r-1}}$  can be extended to realize ${H_r}$ as shown in the 
following lemma.

%\begin{figure}
%\centerline{\psfig{figure=fig/l4.eps,width=4in}}
%\caption{$H_3 = EVEN_3 \cup ODD_3 \cup DIM_0$}
%\label{LEMMA4}
%\end{figure}

%A straight forward channel assignment scheme can be obtained from 
%the above partitioning of ${H_r}$.
%Assuming we know how to assign channels for 
% ${H_{r-1}}$ on an array of size $2^{r-1}$,
%we can assign channels for ${H_r}$ by 
%scheduling $EVEN_r$ on the $2^{r-1}$ even numbered nodes, $ODD_r$
%on the $2^{r-1}$ odd numbered nodes and using one more configuration to 
%schedule $DIM_0$
%(it can be easily  proven that $DIM_0$ forms a configuration).
%Let
%$D(N)$ be the number of configurations
% needed to realize hypercube communication 
%for array of size $N$. It can be expressed in the following equation.\\
%\centerline{$D(N) = 2D(N/2) + 1$}

\noindent
{\bf Lemma 4}: Assuming that ${H_{r-1}}$ can be
realized on an array of size $2^{r-1}$
using  $K$ channels, then ${H_r}$
can be realized on an array of size $2^{r}$ 
using $2K+1$ channels.

\noindent
{\bf Proof}: ${H_r} = EVEN_r\cup ODD_r \cup DIM_0$. From the above 
discussion and the
assumption, $EVEN_r$ and $ODD_r$ are ${H_{r-1}}$ (when nodes are properly
renumbered), $K$ channels can be used to realize 
$EVEN_r$ or $ODD_r$. Since it can be easily proven that 
$DIM_0$ can be realized with one channel,
a total of $2K+1$ channels can be used to realize ${H_r}$. $\Box$

Let $D(N)$ be the number of channels
needed for ${H_r}$ on an array of size $N = 2^r$. 
If a channel assignment scheme is used that is in accordance with the proof
of  Lemma 4, it can be shown that the  equation,
$D(N) = 2D(N/2) + 1$.
Given that no channel is needed to realize hypercube communication on 
a 1--node array, D(1) = 0. Solving for $D(N)$ results in 
$D(N) = N-1$, which is not optimal.
%means that using this simple scheme, 
%$N-1$ channels are needed to realize ${H_r}$ on a linear array of size 
%$N=2^r$. This  does not reach the lower bound. 
%Fig.~\ref{NOOPT} shows the 
%channel assignment for a 16 node array using this simple scheme.
The following lemma improves this simple channel assignment scheme.


%\begin{figure}
%\centerline{\psfig{figure=fig/noopt.eps,width=4in}}
%\caption{A non-optimal channel assignment
%         for ${H_4}$ on array uses 15 channels.}
%\label{NOOPT}
%\end{figure}

\begin{figure}
\centerline{\psfig{figure=fig/l6.eps,width=3in}}
\caption{Realizing $DIM_0\cup DIM_1$ of $H_3$}
\label{LEMMA6}
\end{figure}

\noindent
{\bf Lemma 5}: Assuming that ${H_{r-2}}$ can be realized on an array of size 
$2^{r-2}$ using $K$ channels, then ${H_r}$ can be realized on an 
array of size $2^r$ using $4K+2$ channels.

\noindent
{\bf Proof}: Consider ${H_r}$
without dimension 0 and dimension 1 connections. 
By the definition of ${H_r}$,
${H_r} - (DIM_0\cup DIM_1) = DIM_2\cup ... \cup DIM_{r-1}$ 
forms four hypercube patterns, each being an ${H_{r-2}}$ pattern on 
nodes $\{n\ |\ n\ mod\ 4 = i\}$ (with proper node renumbering), denoted
by $subarray_i$, for $i =$ 0, 1, 2 or 3. From 
the hypothesis, ${H_{r-2}}$ can be realized on an array of size $2^{r-2}$ 
using  $K$ channels.  The 
four sub--cube patterns can be realized in $4K$ channels.
The remaining connections to be considered are those in $DIM_0$ and $DIM_1$.
It can easily be proven that connections in $DIM_0$ and $DIM_1$
can be assigned to 2 channels as shown in Fig.~\ref{LEMMA6}.
%by
%mixing half the connections in $DIM_0$ with half the connections in $DIM_1$
%in one channel.
%Fig.~\ref{LEMMA6} shows an example for an 8--node array. As can be seen
%from the figure, similar channel assignment can be used to assign 2 channels
%to all $DIM_0$ and $DIM_1$ connections for any array of size $N=2^r$.
Hence, the hypercube communication ${H_r}$ can be realized using
a total of $4K+2$ channels. $\Box$

\begin{figure}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}\=Algorithm 1: Assign\_array($N = 2^r$) \\
\>(1)\hspace{0.1in}\=If $(r = 0)$ then return $\phi$\\
\>(2)\>If\=\ (r is odd) then\\
\>(3)\>\>/* applying Lemma 4 */\\
\>(4)\>\>recursively apply Assign\_array($N/2=2^{r-1}$)  for $EVEN_r$. \\
\>(5)\>\>recursively apply Assign\_array($N/2=2^{r-1}$)  for $ODD_r$.\\
\>(6)\>\>assign connections in  $DIM_0$ to one channel.\\
\>(7)\>Else /* r is even, apply Lemma 5 */\\
\>(8)\>\>Fo\=r i = 0, 1, 2, 3\\
\>(9)\>\>\>apply Assign\_array($N/4 = 2^{r-2})$
           for $subarray_i$. \\
\>(10)\>\>assign connections in $DIM_0 \cup DIM_1$ to 2 channels.
\end{tabbing}
\caption{The channel assignment algorithm}
\label{ALGO1}
\end{figure}

The channel assignment algorithm, {\em Algorithm 1}, 
is depicted in Fig.~\ref{ALGO1}. 
%Notice that 
%while the algorithm is described using a recursive notation for simplicity,
%it is easier to perform the channel assignment in a bottom-up fashion.
For the base case, when $N=2^0=1$, the hypercube pattern 
contains no connection.
To assign channels to connections in an array of size $N=2^r$, 
$r > 0$, there are two 
cases. If $r$ is even, then Lemma 5 is applied to use
$4K+2$ channels for the hypercube pattern, where $K$ is the 
number of channels needed
 to realize a hypercube pattern on an array of size $2^{r-2} = N/4$. 
If $r$ is odd, Lemma 4 is applied to use
$2K+1$ channels to realize the hypercube pattern, where $K$ is the
number of channels needed
to realize a hypercube pattern in an array of size $2^{r-1} = N/2$.
The example of using this algorithm to schedule ${H_4}$ in an array of size 
16 is shown in Fig.~\ref{OPT}.


\begin{figure}
\centerline{\psfig{figure=fig/opt.eps,width=3.6in}}
\caption{Optimal channel assignment for ${H_4}$}
\label{OPT}
\end{figure}

\noindent
{\bf Theorem 2}: {\em Algorithm 1}  uses $\lfloor\frac{2N}{3}\rfloor$
channels for  ${H_r}$ on a
linear array with $N=2^r$ nodes, thus 
$w(ARRAY(N), H_r) \le \lfloor\frac{2N}{3}\rfloor$.

\noindent
{\bf Proof}: Let $D_{odd}(2^r)$ and $D_{even}(2^r)$
denote the number of channels needed 
when $r$ is  odd and even, respectively.
The number of channels for the hypercube pattern using 
{\em Algorithm 1}
can be formulated as follows,

\hspace{0.1in}$D_{odd}(2^r) = 2D_{even}(2^{r-1}) + 1$, when $r$ is odd.

\hspace{0.1in}$D_{even}(2^r) = 4D_{even}(2^{r-2}) + 2$, when $r$ is even.

\noindent
Using the  boundary condition $D_{even}(1)= D_{even}(2^0) = 0$,
it can be proven by induction that  $D_{odd}(N) = \frac{2N}{3} - \frac{1}{3}$
and $D_{even}(N) = \frac{2N}{3} - \frac{2}{3}$.
%
%Base case: $D_{even}(2^0) = \frac{2\times 1}{3} - \frac{2}{3} = 0$.
%
%Induction case: Assuming $D_{even}(N) = \frac{2N}{3} - \frac{2}{3}$
%and $D_{odd}(N) = \frac{2N}{3} - \frac{1}{3}$.
%
%$D_{odd}(2N) = 2D_{even}(N) + 1 
%            = 2\times (\frac{2N}{3} - \frac{2}{3}) + 1
%            = \frac{2\times(2N)}{3} - \frac{1}{3}$
%$D_{even}(2N) =  4D_{even}(N/2)+2
%              = 4 (\times \frac{2\times N/2}{3} - \frac{2}{3}) + 2
%              = \frac{2\times (2N)}{3} - \frac{2}{3}$
%
%\noindent
Hence, $D_{odd}(N)$ and $D_{even}(N)$ are equal 
to $\lfloor\frac{2N}{3}\rfloor$. 
$w(ARRAY(N), H_r) \le \lfloor\frac{2N}{3}\rfloor$. $\Box$
%, which is equal to the lower bound in 
%Theorem 1. $\Box$

%Notice that for a linear array of size $2^r$, where $r$ is an odd number,
%Lemma 5 can also apply recursively without applying Lemma 4 first. However, 
%this will not lead to an optimal channel assignment. 
%Specifically, the number of channels required in this case can be determined
% from
% the formula $D(N) = 4D(N/4) + 2$, with
%boundary condition $D(2) = 1$. The solution
%of this equation is $D(N) = \frac{5N}{6}- \frac{2}{3}$, which is not optimal.

\noindent
{\bf Theorem 3}:\\ 
$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$, and Algorithm 1 is optimal.

\noindent
{\bf Proof: } Follows from Theorem 1, Theorem 2 and Lemma 1.$\Box$
%From Theorem 1, we have 
%$\pi(ARRAY(N), H_r) \ge \lfloor \frac{2N}{3} \rfloor$, from Theorem 2, 
%we have $w(ARRAY(N), H_r) \le \lfloor \frac{2N}{3} \rfloor$, and
%from Lemma 1, we have $w(ARRAY(N), H_r) \ge \pi(ARRAY(N), H_r)$.
%Combining these results, we obtain 
%$w(ARRAY(N), H_r) = \pi(ARRAY(N), H_r) = \lfloor \frac{2N}{3} \rfloor$.
%Since the channel assignment algorithm, {\em Algorithm 1}, uses 
%$\lfloor \frac{2N}{3} \rfloor$ channels for ${H_r}$, it is optimal.
%$\Box$

\subsubsection{Hypercube connections  on rings}

By having links between node 0 and node $N-1$, two paths can be 
established from any node to any other node on a ring. It has been shown
\cite{beauquier97}
that even for a fixed routing, general optimal channel assignment problem is 
NP--complete. This section  focuses on the specific
problem of optimal RCA for ${H_{r}}$ on ring topologies, obtaining
a lower bound on the number of channels needed to realize ${H_{r}}$ and
developing an optimal routing and channel assignment algorithm 
that achieves this lower bound.

%Assuming that 
%a virtual circuit between two nodes is always established using a
% shortest path, the flexibility provided by 
% the extra links between node 0 and node $N-1$ affects only 
%the dimension $r-1$ connections that span $\frac{N}{2}$ nodes.  
%The following 
%lemma establishes  
%a lower bound for the multiplexing degree needed to 
%realize ${H_r}$ in a ring with $N=2^r$ nodes.

\noindent
{\bf Lemma 6}: $\pi(RING(N), H_r) \ge 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$.

\noindent
{\bf Proof}: This lemma is proven by showing that there exist 
two cuts on a ring
that partition the ring into two sets, $S_1$ and $S_2$, such that 
$ 2\times \lfloor\frac{N}{3} + \frac{N}{4}\rfloor$  connections in 
${H_r}$  originate at nodes  in $S_1$ and terminate at nodes in $S_2$.
Since there are only 2 links connecting  $S_1$ to
$S_2$, one of the 2 links must be used at least
$\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$ times, regardless of which routing
scheme is used.
Consider ${H_r}$ on a ring of size $N=2^r$.
The connections in $DIM_0\cup...DIM_{r-2}$
form two $r-1$ dimensional
hypercube patterns in two {\em arrays} of size $2^{r-1}$.
The first  array, denoted by $subarray_1$, contains  
nodes 0, ..,  $2^{r-1}-1$ and the second  array, denoted by 
$subarray_2$,  contains nodes $2^{r-1}$,..,  $2^r-1$.
From Corollary 3.1, it follows that
there exists a link in each $2^{r-1}$ node array such that  
$\lfloor\frac{N}{3}\rfloor$ connections in the hypercube pattern
 use that link in each
direction. From the discussion in previous section, the link is  
$(\lfloor \frac{N}{3} \rfloor, \lceil \frac{N}{3} \rceil)$ in $subarray_1$
 and $(\lfloor \frac{N}{3} \rfloor+2^{r-1}, 
\lceil \frac{N}{3} \rceil + 2^{r-1})$ in 
$subarray_2$. 
These two links partition  the ring 
into two sets 
$S_1 = \{i| 0\le i\le \lfloor \frac{N}{3} \rfloor\} 
\cup \{i|2^{r-1}+\lfloor \frac{N}{3} \rfloor+1\le i\le 
2^r-1\}$ and 
$S_2 = \{i| \lfloor \frac{N}{3} \rfloor+1 \le i 
\le 2^{r-1}+\lfloor \frac{N}{3} \rfloor\}$.
Hence, there are $\lfloor\frac{N}{3}\rfloor$ connections from 
$S_1\cap subarray_1$ to $S_2\cap subarray_1$ and 
$\lfloor\frac{N}{3}\rfloor$ connections from 
$S_1\cap subarray_2$ to $S_2\cap subarray_2$ in $DIM_0\cup...DIM_{r-2}$.
Thus, there are $2\times \lfloor\frac{N}{3}\rfloor$ connections 
in $DIM_0\cup..\cup DIM_{r-2}$ originating at nodes in $S_1$ and
terminating at nodes in $S_2$.
Fig.~\ref{LEMMA8} 
shows the cuts on a 16--node ring. 
The remaining connections of ${H_r}$
are in $DIM_{r-1}$. By partitioning
the ring into $S_1$ and $S_2$, each node in $S_1$ has a dimension $r-1$ 
connection to a node in $S_2$. Hence, there are  $N/2$ 
connections in $DIM_{r-1}$ between  $S_1$ and $S_2$. Therefore, a total of 
$2\times \lfloor\frac{N}{3}\rfloor + N/2 = 2\times 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$ connections in ${H_r}$ are 
from  $S_1$ to 
 $S_2$. Thus, $\pi(RING(N), H_r) \ge 
\lfloor\frac{N}{3} + \frac{N}{4}\rfloor$.  $\Box$

\begin{figure}
\centerline{\psfig{figure=fig/l8.eps,width=3.2in}}
\caption{Hypercube on a ring}
\label{LEMMA8}
\end{figure}


%In the following, we show that using deterministic 
%odd--even shortest path
%routing and a channel assignment scheme derived from lemma 6, 
%this lower bound can be achieve. 
The RCA scheme uses an odd--even shortest path
routing. Given a ring of size $N=2^r$,
an odd--even shortest path routing  works as follows. 
A connection between two nodes is  established using a
shortest path. Connections that have two shortest
paths are of the forms $(i, i+2^{r-1})$  and $(i, i-2^{r-1})$. For these
connections, the clockwise path is used if  $i$ is even and the 
counter--clockwise path if $i$ is odd. 
%Note that using odd--even 
%shortest path routing, the extra links between node 0 and node 
%$N-1$ affect only 
%the dimension $r-1$ connections that span $\frac{N}{2}$ nodes in
%${H_r}$.

The channel assignment algorithm is derived from Lemma 6. There are
two parts in the algorithm, channel assignment for
connections in $DIM_{r-1}$ and 
channel assignment for connections in $DIM_0\cup .. \cup DIM_{r-2}$. 
%Using odd--even short path routing, 
Channel assignment for 
connections in $DIM_0\cup .. \cup DIM_{r-2}$
is equivalent to channel assignment for  two ${H_{r-1}}$ 
in two disjoint arrays, 
thus, using the channel assignment 
scheme (for array) described in the 
previous section, $\lfloor \frac{N}{3} \rfloor$ channels 
can be used to realize
these connections. For the connections in $DIM_{r-1}$,
using  odd--even shortest path routing, 
four connections in $DIM_{r-1}$,
$(i, i+2^{r-1})$, $(i+2^{r-1}, i)$, $(i+1,i+2^{r-1}+1)$, $(i+2^{r-1}+1, i+1)$,
can be realized using one channel. We denote by $CONFIG_i$ these four 
connections. Since
the union of all $CONFIG_i$, where $i = 0, 2, 4, ..., N/2-2$ is equal to 
$DIM_{r-1}$,  $N/4$  channels are 
sufficient to realize $DIM_{r-1}$. Fig.~\ref{ALGO2} shows  the channel
assignment
algorithm for ring topologies.



\begin{figure}
\small
\footnotesize
\begin{tabbing}
\hspace{0.1in}\=Algorithm 2: Assign\_ring($N=2^r$)\\
\>\\
\>(1)\hspace{0.1in}\=Apply Assign\_array($N/2=2^{r-1}$) on $subarray_1$.\\
\>(2)\hspace{0.1in}Apply Assign\_array($N/2=2^{r-1}$) on  $subarray_2$.\\
\>\>Since $subarray_1$ and $subarray_2$ are disjoint, \\
\>\>channels can be reused in steps (1) and (2).\\
\>(3)\>fo\=r i = 0, N/2-2, step 2\\
\>\>\>Assign a channel to connections $(i, i+2^{r-1})$, $(i+2^{r-1}, i)$,\\
\>\>\>         $(i+1,i+2^{r-1}+1)$ and $(i+2^{r-1}+1, i+1)$\\
\end{tabbing}
\caption{The channel assignment for rings}
\label{ALGO2}
\end{figure}

\noindent
{\bf Theorem 4}: {\em Algorithm 2} uses 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$ channels
to realize ${H_r}$ in a ring of size $N=2^r$.

\noindent
{\bf Proof}: Follows from above discussion. $\Box$

\noindent
{\bf Theorem 5}: 
$w(RING(N), H_r) = \pi(RING(N), H_r) = 
\lfloor \frac{N}{3 } + \frac{N}{4} \rfloor$, and the odd--even shortest path
routing with Algorithm 2 is an optimal RCA scheme for hypercube
connection  on rings.

\noindent
{\bf Proof: } Follows from Lemma 1, Lemma 6 and Theorem 4.$\Box$
%From Lemma 6, we have 
%$\pi(RING(N), H_r) \ge \lfloor \frac{N}{3} + \frac{N}{4} \rfloor$,
% from Theorem 4, 
%we have $w(RING(N), H_r) \le \lfloor \frac{N}{3} + \frac{N}{4} \rfloor$,
%and from Lemma 1, we have $w(RING(N), H_r) \ge \pi(RING(N), H_r)$.
%Combining these results, we obtain
%$w(RING(N), H_r) = \pi(RING(N), H_r) = 
%\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$.
%Since with the odd--even shortest path routing, the 
%channel assignment algorithm, {\em Algorithm 2}, uses 
%$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor$ channels 
%for ${H_r}$, the routing scheme together with the algorithm forms an 
%optimal RCA scheme for hypercube communication on rings. $\Box$

\subsubsection{Hypercube connections on meshes}

Given a $2^k\times 2^{r-k}$  mesh,
realizing the hypercube connections on the mesh is equivalent 
to realizing $H_{k}$ in each row and $H_{r-k}$ in each column. 
The following lemma gives the lower bound on the number of channels
required
to realize  hypercube communication patterns on  meshes. 
%Unlike the lower bound for the linear array
%and ring, we cannot prove that this lower bound is tight.

\noindent
{\bf Lemma 7}: $\pi(MESH(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2\times2^k}{3}\rfloor$, assuming  $k\ge r-k$.

\noindent
{\bf Proof}:  
The hypercube pattern on the mesh contains $2^{r-k}$  $k$--dimensional
hypercube patterns on  $2^k$ arrays in the $2^{r-k}$ rows. 
Consider a cut in edges
$(\lfloor \frac{2^k}{3} \rfloor, \lceil \frac{2^k}{3} \rceil)$ in every
row, which partitions the mesh into two parts. 
From Corollary 3.1, we know that for each row there are 
$\lfloor \frac{2\times 2^k}{3} \rfloor$ connections from the left of the
cut to the right of the cut, hence, there are a total of 
$2^{r-k} \times \lfloor \frac{2\times 2^k}{3} \rfloor$ connections
crossing
 the cut. Since there are $2^{r-k}$ edges in the cut, 
there exists at least one edge that is used at least 
$\lfloor\frac{2\times2^k}{3}\rfloor$ times. Thus,
 $\pi(MESH(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2\times2^k}{3}\rfloor$. $\Box$

%
%Without losing
%generality, we assume that      $k \ge r-k$ in the following discussions.
%The following lemma states the properties of this type of mesh. 
%
%\noindent
%{\bf lemma 8}: The hypercube pattern pattern in an $2^k\times 2^{r-k}$ mesh
%is equivalent to the hypercube pattern for array of size $2^k$ in every row in 
%x direction and hypercube pattern for array of size $2^{r-k}$ in every column
%in y direction.
%
%\noindent
%Proof: The lemma is proven by showing that connections sourced at
%any node in $i$th column and $j$th row forms the hypercube connections
%in $i$th column and hypercube connections in $j$th row that sourced at this
%node. Let number $i$ be represented in binary with $k$ bits,
%${k-1}i_{k-2}...i_0$ and $j$ be represented in binary with $r-k$ bits
%$j_{r-k-1}j_{r-k-2}...j_0$. Since the node in $i$th column and $j$th row
%are numbered as $n = i\times 2^{r-k} + j$, the binary representation of $n$
%is ${k-1}i_{k-2}...i_0j_{r-k-1}j_{r-k-2}...j_0$. By definition, the 
%hypercube connections sourced at this node in $i$th row are
%
%\centerline{\{${k-1}i_{k-2}...i_0a_{r-k-1}a_{r-k-2}...a_0 | $
%               there is one and only one $a_l$, where $0\le l\le r-k-1$,
%               differ from $j_l$\}}
%
%\noindent
%the hypercube connections source at this node in $j$th column are
%
%\centerline{\{$a_{k-1}a_{k-2}...a_0j_{r-k-1}j_{r-k-2}...j_0 | $
%               there is one and only one $a_l$, where $0\le l\le k-1$,
%               differ from $l$\}}
%\noindent
%By definition of hypercube communication, the hypercube pattern in the mesh
%sourced node\\
% ${k-1}i_{k-2}...i_0j_{r-k-1}j_{r-k-2}...j_0$ is equivalent to 
%the union of these two sets. Hence, the hypercube  pattern in 
%an $2^k\times 2^{r-k}$ mesh
%is equivalent to the hypercube pattern for array of size $2^k$ in every row in 
%x direction and hypercube pattern for array of size $2^{n-k}$ in every column
%in y direction. $\Box$
%
%lemma 8 states that establishing hypercube on 

Given a mesh of size 
$2^k\times 2^{r-k}$, the hypercube communication pattern 
in each 
row is denoted by ${H_k^{row}}$ 
and  the hypercube communication pattern in 
each column by ${H_{r-k}^{col}}$.
The RCA scheme uses X--Y shortest path routing.
Since we already know the optimal channel assignment for ${H_k^{row}}$ and 
${H_{r-k}^{col}}$, the challenge here is  to reuse channels on 
connections in two dimensions efficiently. 
Let us define an {\em array configuration} as the set of connections 
in a linear array that are assigned to the same channel. {\em Ring}, 
{\em mesh} and {\em torus configurations} are defined similarly.
Using the definition of configurations, a mesh configuration can be obtained
by combining array configurations in  the rows and the columns.
For example,
if an array configuration in x dimension and an array configuration in 
y dimension can be combined into a mesh configuration, the two array 
configurations can be realized in the mesh topology using one channel.
Notice that,  while there is no link conflict when assigning channels to 
 row and column connections, 
%${H_k^{row}}$ uses only x direction links while ${H_{r-k}^{col}}$
%uses only y direction links. However, 
node conflicts may occur and must be avoided.

\begin{figure}
\centerline{\psfig{figure=fig/l13.eps,width=3.0in}}
\caption{a Mesh configuration}
\label{LEMMA13}
\end{figure}

%Since we will try to combine array configurations
%into mesh configurations, 
Let us first take a deeper look at the 
array configurations for arrays of size $N=2^k$. Following the channel
 assignment algorithm, {\em Algorithm 1}, array
configurations can be classified into three categories;
 $E$--configurations that contain only connections
between even--numbered nodes,  $O$--configurations that contain  only
connections  between odd--numbered nodes,
and $EO$--configurations that contain  
dimension 0 (and/or) dimension 1 connections 
%(thus may connect  even
%number nodes and odd number nodes). 
As discussed in Section 3,
if $k$ is odd, there is only one 
$EO$--configuration for  connections in $DIM_0$,  
$(\lfloor \frac{2N}{3}\rfloor -1) / 2$ $E$--configurations for
connections in $EVEN_k$,
and $(\lfloor \frac{2N}{3}\rfloor -1) / 2$ $O$--configurations for
connections in $ODD_k$. Similarly, 
if  $k$ is even, there are two $EO$--configurations, 
 $(\lfloor \frac{2N}{3}\rfloor -2) / 2$ $E$--configurations 
and 
 $(\lfloor \frac{2N}{3}\rfloor -2) / 2$ $O$--configurations. 
The following lemma shows that $E$--configurations  and $O$--configurations 
in  rows and columns of the mesh 
can be combined.

\noindent
{\bf Lemma 8}: Given an  $E$--configuration, $E_x$, and an  
$O$--configuration, $O_x$, in the x direction and 
an $E$--configuration, $E_y$, and an $O$--configuration, $O_y$, in the 
y direction, 
$E_x$ and $O_x$ in all rows and $E_y$ and $O_y$ in all  columns can be
realized in two mesh configurations.

\noindent
{\bf Proof}: The proof is by constructing the two mesh 
configurations. In the first mesh configuration,
let all odd numbered rows realize $O_x$
and all even numbered row realize $E_x$.  
In this case, no connection starts or terminates at an 
odd numbered node in an even column or at 
an even numbered node in an odd column. Thus, in the same mesh configuration,
$E_y$ can be realized in odd
columns and $O_y$ can be realized in even columns.
The second mesh configuration realizes $E_x$ on odd numbered rows, 
$O_x$ on even numbered rows,
$E_y$ on even numbered columns and $O_y$ on odd numbered columns. These
two mesh configurations
 realize  $E_x$ and $O_x$ in all rows
 and $E_y$ and $O_y$ in all columns.
 Fig.~\ref{LEMMA13} shows the construction of a 
mesh configuration. $\Box$

Lemma 8 lays the foundation for the channel assignment algorithm. 
Let $a$ be the number of $E$--configurations 
and $O$--configurations in ${H_k^{row}}$,
$b$ be the  number of $EO$--configurations in ${H_k^{row}}$,
$c$ be the number of $E$--configurations
 and $O$--configurations in ${H_{r-k}^{col}}$,
and $d$ be the  number of $EO$--configurations in ${H_{r-k}^{col}}$.
From assumptions, it follows that  $k \ge r-k$, $a\ge c$, 
$a+b = \lfloor \frac{2\times 2^k}{3}\rfloor$ and 
$d \le 2$.
By combining $E$--configurations and $O$--configurations in rows and 
columns into mesh 
configurations, all the $E$--configurations and $O$--configurations 
in each row and
all the $E$--configurations and $O$--configurations 
in each column can be realized
using $a$ mesh configurations. 
Using an individual mesh 
configuration for each EO  configuration in the rows and the columns,
a total of 
$a + b + d \le \lfloor \frac{2\times 2^k}{3}\rfloor + 2$
configurations are sufficient 
to  realize the hypercube connections.
%communication pattern on the mesh. 
%This is stated in the following theorem.

\noindent
{\bf Theorem 4}: ${H_r}$ can be realized on a $2^k\times 2^{r-k}$ mesh, 
 where $k \ge r-k$, using 
$\lfloor \frac{2\times 2^k}{3}\rfloor + 2$ channels. $\Box$

%\noindent
%{\bf Proof:} Straight forward from the above discussion. $\Box$

\noindent
{\bf Corollary 4.1:} $w(MESH(2^k\times 2^{r-k}), H_r) \le 
\lfloor \frac{2\times 2^k}{3} \rfloor + 2 \le 
\pi(MESH(2^k\times 2^{r-k}), 
H_r) + 2$. $\Box$

\subsubsection{Hypercube connections on tori}
\label{hypercubeontori}

%Hypercube communication on torus topologies is 
%obtained from the hypercube communication on the ring topology.
As in the case of realizing $H_r$ on a mesh, 
${H_r}$ can be realized on a  $2^k\times 2^{r-k}$
torus by realizing ${H_{k}^{row}}$ in each row and ${H_{r-k}^{col}}$
in each column. The following lemma gives a lower bound
on the number of channels required to realize ${H_r}$
on a torus.

\noindent
{\bf Lemma 9}: $\pi(TORUS(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$, assuming  $k\ge r-k$.

\noindent
{\bf Proof:} 
The hypercube pattern on the torus contains $2^{r-k}$  $k$--dimensional
hypercube patterns on  $2^k$ rings in the $2^k$ rows. 
Considered two cuts in edges
$(\lfloor \frac{2^{k-1}}{3} \rfloor, \lceil \frac{2^{k-1}}{3} \rceil)$
and 
$(\lfloor \frac{2^{k-1}}{3} \rfloor + 2^{k-1}, 
\lceil \frac{2^{k-1}}{3} \rceil + 2^{k-1})$ in every
row which partition the torus into two parts. 
Following the same reasoning as in the proof of lemma 6, 
it is known that for each row there are 
$2 \times \lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor$ 
connections from one part to the other part, hence, there are a total of 
$2^{r-k} \times 2 \times \lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor$ 
connections
crossing the two parts. 
Since there are $2\times 2^{r-k}$ edges in the cut, regardless of the
routing scheme used, there exist at least one edge that is used at least 
$\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$ times. Thus,
 $\pi(TORUS(2^k\times 2^r-k), H_r) \ge 
\lfloor\frac{2^k}{3} + \frac{2^k}{4}\rfloor$. $\Box$

X--Y routing between dimensions and odd--even shortest path routing
within each dimension are used to develop the RCA scheme. 
Next, the combination of 
ring configurations into torus configurations is considered.
As in the case of rings,
given a $2^k\times 2^{r-k}$ torus,  
the connections in ${H_r}$ are partitioned into two sets. The first
set includes all connections in $DIM_0\cup..\cup DIM_{k-2}$ in each row and 
all connections in $DIM_0\cup..\cup DIM_{r-k-2}$ in each column. The second
set includes the connections in $DIM_{k-1}$ in each row and 
the connections in $DIM_{r-k-1}$ in each column. The connections  
in $DIM_0\cup..\cup DIM_{k-2}$ in each row and the
connections in $DIM_0\cup..\cup DIM_{r-k-2}$ in each column form four 
hypercube patterns on  four disjoint $2^{k-1}\times 2^{r-k-1}$ 
sub--meshes in the torus.
A straight forward extension of
the channel assignment scheme
 in the previous section can be used to assign channels to these
connections with at most
$\lfloor \frac{2^k}{3} \rfloor + 2$ channels. 

To realize the connections in $DIM_{k-1}$ in each row and 
the connections in 
$DIM_{r-k-1}$ in each column, 
%we assume that $r-k \ge 3$, and thus, $k \ge 3$.
%Note that if $r-k < 3$, then the hypercube pattern 
%on each row have 1, 2 or 4 nodes
%and a multiplexing degree of 2 is sufficient to realize all $H_{r-k}^{col}$. 
%In this case, a simple scheme that realizes $H_k^{row}$ and 
%$H_{r-k}^{col}$
%individually yields a connections scheduling that results in at most
%2 more multiplexing degree than the minimum required (the multiplexing degree
%to realize $H_k^{row}$). 
%For connections in $DIM_{k-1}$ in each row and $DIM_{r-k-1}$ 
%in each column, 
The same partitioning for the 
ring topology discussed in section 4 is followed. Specifically, 
the following configurations are constructed 
in rows and columns respectively

\noindent
$row_i = \{(i, i+2^{k-1}), (i+2^{k-1}, i), (i+1, i+1+2^{k-1}), 
          (i+1+2^{k-1}, i+1)\}$

\noindent
$column_j = \{(j, j+2^{r-k-1}), (j+2^{r-k-1}, j), (j+1, j+1+2^{r-k-1}), 
          (j+1+2^{r-k-1}, j+1)\}$

\noindent
$DIM_{k-1}$ is  composed of
 the configurations
$row_i$, for  $i = 0, 2, ..., 2^{k-1}-2$ and $DIM_{r-k-1}$ 
 is composed of  the configurations 
$column_j$ for $j = 0, 2, ..., 2^{r-k-1}-2$.

\noindent
{\bf Lemma 10} For any $i_1$, $i_2$, 
where $i_1 \ne i_2$,  
$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
$column_{i_2}$ in each column can be realized in two torus configurations.

\noindent
{\bf Proof}: Similar to the proof of Lemma 8, omitted. $\Box$
%This lemma is proven by constructing the two torus configurations,
%while avoiding  node conflicts. 
%The first torus configuration is constructed in the following way.
%For each 
%row $i_1$, $i_1+1$, $i_1+2^{r-k-1}$ and $i_1+1+2^{r-k-1}$, configuration
%$row_{i_1}$ is realized, while in all other rows, $row_{i_2}$ is realized.
%Now, consider the columns. In each column $i_1$, $i_1+1$,
%$i_1+2^{k-1}$ and $i_1+1+2^{k-1}$, the $(i_2)$th, $(i_2+1)$th, 
%$(i_2+2^{r-k-1})$th
%and $(i_2+1+2^{r-k-1})$th nodes are not used. Hence, 
%$column_{i_2}$ can be established
%in these columns. A similar argument is used for establishing 
%$column_{i_1}$ on 
%all other columns. This completes the construction of the first torus
% configuration.
%The second torus configuration can be obtained from the first 
%torus configuration by swapping $row_{i_1}$ and
%$row_{i_2}$ in the rows and $column_{i_1}$ and $column_{i_2}$ in the
%columns. These two configurations include   
%$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
%$column_{i_2}$ in each column. Thus,  
%$row_{i_1}$ and $row_{i_2}$ in each row and $column_{i_1}$ and 
%$column_{i_2}$ in each column can be realized in two torus configurations.
%$\Box$

%\begin{table}[htbp]
%\\small
%\footnotesize
%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%columns \& rows & configuration \\
%\hline
%row $i_1$ & $row_{i_1}$ \\
%\hline
%row $i_1+1$ & $row_{i_1}$ \\
%\hline
%row $i_1+2^{r-k-1}$ & $row_{i_1}$ \\
%\hline
%row $i_1+1+2^{r-k-1}$ & $row_{i_1}$ \\
%\hline
%other rows & $row_{i_2}$ \\
%\hline
%column $i_1$ & $column_{i_2}$ \\
%\hline
%column $i_1+1$ & $column_{i_2}$ \\
%\hline
%%column $i_1+2^{k-1}$ & $column_{i_2}$ \\
%\hline
%column $i_1+1+2^{k-1}$ & $column_{i_2}$ \\
%\hline
%other columns & $column_{i_1}$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Configuration 1}
%\label{CONF1}
%\end{table}


%\begin{table}[htbp]
%\small
%\footnotesize
%%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%columns \& rows & configuration \\
%\hline
%row $i_1$ & $row_{i_2}$ \\
%\hline
%row $i_1+1$ & $row_{i_2}$ \\
%\hline
%row $i_1+2^{r-k-1}$ & $row_{i_2}$ \\
%\hline
%row $i_1+1+2^{r-k-1}$ & $row_{i_2}$ \\
%\hline
%other rows & $row_{i_1}$ \\
%\hline
%column $i_1$ & $column_{i_1}$ \\
%\hline
%column $i_1+1$ & $column_{i_1}$ \\
%\hline
%column $i_1+2^{k-1}$ & $column_{i_1}$ \\
%\hline
%column $i_1+1+2^{k-1}$ & $column_{i_1}$ \\
%\hline
%other columns & $column_{i_2}$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Configuration 2}
%\label{CONF2}
%\end{table}
%
%\noindent
%{\bf Lemma 11}: Assuming that $r-k \ge 3$, 
%the dimension $k-1$ connections in each row and 
%dimension $r-k-1$ connections in each column can be realized in 
%$2^{k-2}$ torus configurations.
%
%\noindent
%{\bf Proof}: From Lemma 10,  configurations
%$row_i$, $i = 0, 2, ..., 2^{r-k-1}-2$ and configurations 
%$column_j$, $j = 0, 2, ..., 2^{r-k-1}-2$ can be realized in $2^{r-k-2}$
%torus configurations. Since $2^{k-2} - 2^{r-k-2}$ torus configurations
% can be
%used to realize $row_i$,  $ i = 2^{r-k-1}, .. 2^{k-1}$, $2^{k-2}$ 
%torus configurations can realize all the dimension $k-1$ connections 
%in each row and 
%dimension $r-k-1$ connections in each column. $\Box$

\noindent
{\bf Theorem 5}:  ${H_r}$ can be realized on a $2^k\times 2^{r-k}$ torus, 
where $k \ge r-k$, using
$\lfloor \frac{ 2^k}{3} + \frac{2^k}{4}\rfloor + 2$ channels.

\noindent
{\bf Proof}: As discussed above,  
$\lfloor \frac{ 2^k}{3}\rfloor + 2$ channels 
are sufficient to realize all 
connections in ${H_r}$, except the connections in  $DIM_{k-1}$ in each row and 
$DIM_{r-k-1}$ in each column, by realizing  four hypercube
communication patterns on the four disjoint sub--meshes. From Lemma 10, 
configurations
$row_i$, $i = 0, 2, ..., 2^{r-k-1}-2$ and configurations 
$column_j$, $j = 0, 2, ..., 2^{r-k-1}-2$ can be realized in $2^{r-k-2}$
torus configurations. Since $2^{k-2} - 2^{r-k-2}$ torus configurations
can be
used to realize $row_i$,  $ i = 2^{r-k-1}, 2^{r-k-1}+2, .., 2^{k-1}-2$, 
all the dimension $k-1$ connections in each row and 
dimension $r-k-1$ connections in each column can be 
realized in $2^{k-2}$ 
torus configurations. Hence, ${H_r}$ can be realized by  a total of 
$\lfloor \frac{ 2^k}{3}\rfloor + 2 + 2^{k-2}
= \lfloor \frac{ 2^k}{3} + \frac{2^k}{4}\rfloor + 2$ configurations.
$\Box$

\noindent
{\bf Corollary 5.1:} $w(TORUS(2^k\times 2^{r-k}), H_r) \le 
\lfloor \frac{2^k}{3} + \frac{2^k}{4} \rfloor + 2 \le 
\pi(TORUS(2^k\times 2^{r-k}), 
H_r) + 2$. $\Box$

%\end{doublespace}

%\subsubsection{Conclusion}

%In this subsection, I studied optimal schemes to realize 
%hypercube connections on  mesh--like optical networks.
%I prove that 
%$\lfloor \frac{2N}{3}\rfloor$ and $\lfloor \frac{N}{3} + \frac{N}{4}\rfloor$
%are tight lower bounds of the number of channels needed to realize hypercube
%connections on linear arrays and rings of size $N$, respectively. I develop
%optimal RCA algorithms that achieve these lower bounds. Also,
%I study the mesh and torus topologies and develop RCA
%algorithms that use at most 2 more channels than the 
%optimal. 

\subsection{Logical torus, all--to--all and allXY topologies}
\label{otherontori}

The logical torus topology coincides with the physical network. Thus, when 
realizing logical torus topology, there are no link conflicts 
since the physical network can support all links in the logical 
network simultaneously. However, node conflicts may occur. 
Under our network model, each node in the network can only access
one channel at any given time slot. Hence, to support 4 out--going links at 
each node, at least 4 channels are needed. Using 4 channels, the logical 
torus topology can be realized as follows. All links in a torus can be
classified into four categories, the UP links, the DOWN links, the LEFT 
links and the RIGHT links. Each category can be realized using 1 channels
without incurring node conflicts and link conflicts as shown in 
Figure~\ref{logicaltorus}. Notice that all nodes can be sending and receiving 
messages in the figure. Hence, 4 channels are sufficient and necessary
to realize the logical torus topology on top of the physical torus 
topology.    

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/logicaltorus.eps,width=5in}}
\caption{Realizing logical torus topology}
\label{logicaltorus}
\end{figure}

Optimal schemes to realize all--to--all communication on ring and torus 
topologies can be found in \cite{Hinrichs94}. It is shown in 
\cite{Hinrichs94} that for an $N$ node ring, $N\ge 8$, 
the all--to--all communication can be realized with $N^2/8$ channels without
node conflicts. For
an $N\times N$ torus, the all--to--all communication can be realized with
$N^3/8$ channels. The connections on each channel  
to realize the all--to--all communication will be called 
an {\em AAPC configuration}.
Details about the connection scheduling can be found in 
\cite{Hinrichs94}.

The logical allXY topology realizes  all--to--all connections in
each dimension in the physical torus. For an $N\times N$ torus, each
node in the logical allXY topology logically connects to $2N-2$ nodes.
Using the AAPC configurations for rings,
techniques similar to the ones in section~\ref{hypercubeontori}
can be used to combine the ring configurations to form torus configurations
and realize the allXY on an $N\times N$ torus, where $N\ge 16$, resulting in 
a multiplexing degree of $N^2/8$. For an $N\times N$ torus with $N\le 8$,
$2N-2$ channels can be used to realize the allXY topology.
For example, 
using the 8 AAPC configurations for 8--node rings in \cite{Hinrichs94},
6 configurations along each dimension cannot be combined because
of node conflicts, while 2 configurations in each dimension can be
combined in the torus, resulting a multiplexing degree 
of $14=2\times 8-2$ for realizing the allXY topology.

\section{Performance of the logical topologies under light load}

This section  considers the communication performance of the 
logical topologies under light load such that the network contentions
on both channels and switches are negligible.
An analytical model will be described that takes the network 
contention effect into consideration later in this chapter.

Let us assume that a packet can be
transferred from source to destination on a path 
in one time slot and that the network has a multiplexing degree of  $d$.
It takes on average $\frac{d + 1}{2}$ time slots to transfer a packet from
a router to the next router. Thus, assuming that the packet routing time in 
each router (including the E/O, O/E conversions) is $\gamma$, the average
number of intermediate routing hops per packet is $h$, and the network
contention is negligible, 
the average delay time for each packet can be expressed as follows: 

\vspace{-0.15in}
\begin{center}
\[delay = (h + 2) * \gamma + (h + 1) * \frac{d + 1}{2}.\]
\end{center}

The first term, $(h+2) * \gamma$, is the average routing time that a packet 
spends at the  $h$ intermediate
routers and the 2 routers at the sending and receiving nodes.
The second term, $(h+1) * \frac{d + 1}{2}$, is the
average packet transmission time on paths plus the time that a packet waits 
in the output path buffers.
Thus, the average delay time is determined by three parameters, 
the multiplexing degree $d$, the packet routing time 
$\gamma$, and the average number of hops per 
packet transmission $h$.
We can assume that the packet routing 
time $\gamma$ is the same for all topologies.
Different logical topologies result in  
different number of intermediate hops, $h$, 
and different multiplexing degree, $d$.
Next, the performance of the four logical topologies is discussed.

Given an $N\times N$ torus, the logical all--to--all topology
establishes direct connections  between all pairs of nodes and thus,
totally eliminates the intermediate hops, resulting in 
$h= 0$. Using the algorithm in \cite{Hinrichs94},  a multiplexing degree
of $\frac{N^3}{8}$ can be used to realize the logical all--to--all topology. 
Thus $d = \frac{N^3}{8}$, and the delay time is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{all-to-all} = 2\times \gamma + (\frac{N^3}{8} + 1) / 2 = 
             O(\gamma + N^3). \]
\end{center}

Given an $N\times N$ torus, a  logical torus topology can be realized using 
a multiplexing degree of 4 (i.e., $d = 4$).
For a logical $N\times N$ topology, the average number of intermediate
hops is $h = \frac{N}{2} - 1$. Hence the delay time for the logical torus
topology is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{torus} = (\frac{N}{2} + 1)\times \gamma + 
                              \frac{N}{2} \times (4+1)/2 = O(N\times \gamma).\]
\end{center}

For $N = 2^r$,  the algorithm in section~\ref{hypercubeontori} can  realize 
a logical hypercube topology  on an $N\times N$ torus
using a multiplexing degree of 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 2$, if $r$ is odd, and 
$\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 1$, 
if $r$ is even.
For a logical $N^2$ node hypercube, the average number of intermediate
hops is $h = \frac{lg(N^2)}{2} - 1 = lg(N) - 1$.
Hence,  the delay time (for an even r) is given by:

\vspace{-0.15in}
\begin{center}
\[delay_{hypercube} = (lg(N) + 1)\times \gamma + 
     \lg(N) \times (\lfloor \frac{N}{3} + \frac{N}{4} \rfloor + 2) / 2
     = O(\gamma lg(N) + Nlg(N)).
\]
\end{center}

Finally, let us consider the logical {\em allXY} topology.
As discussed in section~\ref{otherontori}, 
when $N \le 8$,  the logical topology can be realized
using a multiplexing degree of $2N-2$. For $N > 8$, 
a multiplexing degree of  
$\frac{N^2}{8}$ is needed. Since 
for two nodes in the same column or row,
no intermediate hop is needed, while in other cases, 
one intermediate hop is required,
the average number of intermediate hops on the logical allXY topology is
given by:

\centerline{$\frac{2N-2}{N^2-1}\times 0 + \frac{(N^2-1) - (2N-2)}{N^2-1} 
            \times 1 = \frac{N^2-2N + 1}{N^2-1}.$}

Therefore, for $N > 8$, the average delay can be expressed as follows:

\vspace{-0.15in}
\begin{center}
\[delay_{all\_XY} = (2+\frac{N^2-2N+1}{N^2-1})\times \gamma + 
                    (1+\frac{N^2-2N+1}{N^2-1})\times 
                    (\frac{N^2}{8} + 1) / 2 = O(\gamma + N^2).\]
\end{center}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Logical  & Number of   & multiplexing & total number \\
topology & intermediate hops (h) & degree (d) & of connections (P)\\
\hline
all--to--all & $0$ & $\frac{N^3}{8}$ & $N^2(N^2-1)$ \\
\hline
all\_XY  & $\frac{N^2-2N+1}{N^2-1}$ & 
$\frac{N^2}{8}$\ $\dagger$ & $N^2(2N-2)$\\ 
\hline
hypercube & $lg(N) -1$ & $\lfloor \frac{N}{3}+\frac{N}{4} \rfloor + 1$\ $\ddagger$ &
$N^2lg(N)$\\
\hline
torus &  $\frac{N}{2} -1 $  &  4 & $N^2\times 4$\\
\hline
\end{tabular}
\end{center}

$\dagger$ Assuming that $N>8$. If $N < 8$, the value is $2N-2$.

$\ddagger$ Assuming that $r$ is even. If $r$ is odd, the value is 
$\lfloor \frac{N}{3}+\frac{N}{4} \rfloor + 2$.

\caption{Summary of logical topologies}
\label{toposummary}
\end{table}

Table~\ref{toposummary} summarizes the average number of intermediate 
hops ($h$),
the multiplexing degree ($d$) and the total number of logical 
connections ($P$) 
for the four topologies. 
%Since we will use physical $8\times 8$ and $16\times 16$ tori as our
%underlying networks, we evaluate $h$, $d$ and $P$ for these two
%physical topologies in Table~\ref{specifictopology}. 
Figure~\ref{DELAY} plots  the average delay as a function of the packet
routing time $\gamma$, for the four logical
topologies on a physical $16\times 16$ torus. When
$\gamma$ is very small compared to data transmission time ($\gamma\le 0.5$),
the logical torus topology
achieves the smallest delay time. When $0.5\le \gamma \le 4.25$, the 
logical hypercube
has the best performance. When $4.25\le \gamma \le 256.25$, the allXY topology
gives the best performance. When $\gamma
 > 256.25$, the all--to--all topology has the
smallest packet delay. 

The characteristics exhibited in Figure~\ref{DELAY} are true for any
network size. Specifically, for a given $N$, there is a value of $\gamma$ below
which routing on the torus is more efficient than routing on the logical
hypercube. Similarly, 
there is a value of $\gamma$, above which
routing on the allXY topology
is more efficient than routing on the logical hypercube.
Finally, there is a value of $\gamma$, above which
routing on the all--to--all  topology
is more efficient than routing on the allXY topology.
In Figure~\ref{BEST} these special values are plotted for different $N$
and the ($N, \gamma$) parameter space is divided into four regions.
Each region is labeled by the logical topology that results in the
lowest average packet delay.

These results are obtained by ignoring network traffic contention,
and thus  are valid only  under light load. 
In the next section, a queuing model is used to study the network
performance under high load.

%\begin{table}[htbp]
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
%physical & logical  & h & d & P \\
%topology & topology &  &  & \\
%\hline
% & all--to--all & 0 &  64 & $64\times 63$ \\
%\cline{2-5}
%$8\times 8$ & all\_XY  & 0.78 & 14 & $64\times 14$\\ 
%\cline{2-5}
%torus & hypercube & 2 & 6 & $64\times 6$\\
%\cline{2-5}
% & torus &  1  &  4 & $64\times 4$\\
%\hline
% & all--to--all & 0 &  512 & $256\times 255$ \\
%\cline{2-5}
%$16\times 16$ & all\_XY  & 0.88 & 32 & $256\times 30$\\ 
%\cline{2-5}
%torus & hypercube & 3 & 10 & $256\times 8$\\
%\cline{2-5}
% & torus &  1  &  4 & $256\times 4$\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Logical topologies on $8\times 8$ and $16\times 16$ tori}
%\label{specifictopology}
%\end{table}

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}
{\psfig{figure=fig/delay.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}
{\psfig{figure=fig/delay1.eps,width=2.95in}}
\end{subfigure}
\end{subfigRow*}
\caption{Performance for logical topologies on $16\times 16$ torus}
\label{DELAY}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range.eps,width=3in}}
\caption{Logical topologies giving lowest packet delay for given $\gamma$ and $N$}
\label{BEST}
\end{figure}

\section{An analytical model and its verification}

This section describes an approximate 
analytical model that takes network contention into consideration.
This model is used to study the effect of the network load on 
the maximum throughput and the packet delay.
It is assumed that in each time slot, a 
packet can be sent from the source to the destination on a path. 
For example, if a 1Gbps channel is used with a 53--byte packet (or cell)
as defined in the ATM standard, then the slot duration is $0.424\mu s$.
All other delays in the system are normalized with respect 
to this slot duration.

The routers and the paths in a network are modeled as a network of queues.
As shown in Figure~\ref{ROUTER},  each router has a routing queue that 
buffers the packets to be processed. The  router places packets
either into one of the output path queues that buffer packets
waiting to be transmitted, or into the local processor. 
Both a router
and a path have a constant service time. The exact model for such network
is very difficult to obtain. The network is approximated by making the 
following assumptions: 1) each queue is independent of each other and 
2) each queue has a Poisson arrival and constant service time. These 
assumptions enable the derivation of  expressions for 
the maximum throughput and the average packet delay 
of the four logical topologies
by dealing with the M/D/1 queues independently. The simulation results
confirm that these approximations are reasonable.  The following 
notation is used in the model:

\begin{itemize}
\item $N$. Size of each dimension of the torus. Thus,
 the network has a total of 
$N^2$ nodes.

%\item $d$. Multiplexing degree in the network. Different logical topologies
%require different multiplexing degrees. The multiplexing degrees for the four
%logical topologies are summarized  in 
%Table~\ref{toposummary}. A {\em frame} consists of $d$ time slots.
%
%\item $P$. Number of connections in the logical topology. Different
%logical topologies contain different numbers of connections. 
%The numbers of connections in the four logical topologies  are summarized
%in Table~\ref{toposummary}.
%
%\item $h$. Average number of intermediate hops per packet transmission.
%This number depends on the logical topology and is summarized in 
%Table~\ref{toposummary}. 
%The average number of paths a packet goes
%through is equal to $h+1$. The average number of routers a packet goes
%through is $h+2$, intermediate hops plus the sending and receiving nodes.

\item $d$, $h$ and $P$ are defined in the previous section. 
A {\em frame} consists of $d$ time slots. Within a frame, one time slot
is allocated to each path. As discussed earlier,
the average number of paths that a packet traverses
is equal to $h+1$. The average number of routers that a packet traverses
is $h+2$.
 

\item $\lambda$. Average packet generation rate at each node per
time slot. This implies that the average generation rate of packets to the 
entire network is $N^2\lambda$. It is  assumed 
that the arrival process is Poisson
and is independently and identically distributed on all network nodes. 
Furthermore, it is  assumed that all packets are equally likely to be destined
to any one of the network nodes. At each router, the newly generated packets
and the packets arriving from other nodes
are maintained in an infinite routing buffer before being processed
as shown in Figure~\ref{ROUTER}.

\item ${\lambda}_s$. Average rate of packet arrival at a router per time slot,
including both generated packets and  packets received from other nodes. 
This composite arrival
rate, ${\lambda}_s$, may be derived as follows. In any time slot the total 
number of generated
packets that arrive at all the routing buffers is $\lambda N^2$. 
On average, each of 
these packets traverses  $h+2$ routers within the network. 
Therefore, under steady state condition, there will be $\lambda N^2 (h+2)$
packets in all the routers of the network 
in each time slot. Under the assumption that each packet is 
equally likely to be in each router, the total arrival rate is given by 
${\lambda}_s = {\lambda} (h+2)$.

\item ${\lambda}_p$. Average rate of packet arrival at a path buffer
per time slot.
This arrival rate, ${\lambda}_p$, can be derived as follows. 
Under steady state condition, in any time slot, 
the total number of  packets in all the routers in the network is  
$\lambda N^2 (h+2)$. 
Of all these packets, $\lambda N^2$ packet will exit the network and 
$\lambda N^2  (h+2) - \lambda N^2  = \lambda N^2  (h+1)$ packets will be 
transmitted through paths in the network. 
Under the assumption that sources and destinations are uniformly
distributed in the network, the average arrival
rate is given by ${\lambda}_p = \frac{\lambda N^2 (h+1)}{P}$. 

\item ${\gamma}$. The routing time per packet at a router. 
Since packets are
of the same length, the routing time is a constant value. The 
average packet departure rate from the routing buffer, denoted by
${\mu}_s$,  is ${\mu}_s = \frac{1}{{\gamma}}$.

\item ${\mu}_p$. The average packet departure rate from each path buffer
per time slot. Since in the model used,
each path will be served once in every frame, ${\mu}_p = \frac{1}{d}$. 
The average service time in each path is $S_p = \frac{1}{{\mu}_p} = d$.

\end{itemize}

\subsection*{Maximum throughput}

With the above notation, the maximum throughput and average
packet delay of the logical topologies can now be studied. First the theoretical
maximum throughput is considered and then the average packet delay. 
Two bottlenecks can
potentially limit the maximum throughput.

\begin{itemize}
\item If the average packet arrival rate at a routing buffer
 is larger than the average
packet departure rate, that is  if ${\lambda}_s \le {\mu}_s$, then 
the throughput will be limited by the router processing bandwidth.
The maximum packet generation rate allowed by the router bandwidth, 
${\lambda}^{max}_s$, can be derived as follows: ${\lambda}_s \le {\mu}_s$, or 
$(h+2)\lambda \le \frac{1}{{\gamma}}$, or $\lambda \le \frac{1}{{\gamma} (h+2)}$. Thus,

\vspace{-0.15in}
\begin{center}
\[{\lambda}^{max}_{s} = \frac{1}{{\gamma}(h+2)}\] 
\end{center}

\item If the average packet arrival rate at a path buffer
is larger than the average 
packet departure rate, that is ${\lambda}_p \le {\mu}_p$, then
the throughput will be limited by the path bandwidth. The maximum fresh packet
generation rate allowed by the path bandwidth, ${\lambda}^{max}_{p}$, can be 
derived as follows: ${\lambda}_p \le {\mu}_p$, or 
$\frac{(h+1)\lambda N }{P} \le \frac{1}{d}$, or 
$\lambda \le \frac{P}{(h+1)Nd}$. Thus, 

\vspace{-0.15in}
\begin{center}
\[{\lambda}^{max}_{p} = \frac{P}{(h+1)Nd}\]
\end{center}

\end{itemize}

The theoretical maximum throughput is the minimum of ${\lambda}^{max}_{s}$
and ${\lambda}^{max}_{p}$, that is, 
${\lambda}^{max} = min ({\lambda}^{max}_{s},  {\lambda}^{max}_{p})$. Given
a topology, ${\lambda}^{max} =  {\lambda}^{max}_{s}$ indicates that
the router speed is the bottleneck, while
${\lambda}^{max} =  {\lambda}^{max}_{p}$ indicates that the path speed is 
the bottleneck. 

\subsection*{Average packet delay}

As mentioned before, the packet delay is divided into the 
{\em routing delay}, 
which includes the time a packet spends on routing buffers and the time 
for routers to process the packets, and the {\em transmission delay}, which 
includes the time a packet spends on path buffers and the actual
packet transmission time on the paths. 

Let us first
consider the routing delay in each router.
It takes ${\gamma}$ timeslots for a
router to process the packet when the packet reaches the front of the 
routing buffer. As for the packet waiting time in the routing buffer,
since the routing buffer is modeled as an $M/D/1$
queue, the average queuing delay depends on the arrival rate
${\lambda}_s$ and is given by:
\vspace{-0.15in}
\begin{center}
\[ Q = \frac{{\lambda}_s ({\gamma})^2}{2(1-\frac{{\lambda}_s}{{\mu}_s})} \]
\end{center}
 
where ${\lambda}_s$ is the average packet arrival rate, ${\gamma}$ is the
expected service time, ${\mu}_s$ is the average packet departure rate.
Given that ${\mu}_s = \frac{1}{{\gamma}}$,  the total time
that a packet spends in each router is given by:
\vspace{-0.15in}
\begin{center}
\[ routing\ delay = {\gamma} + \frac{{\lambda}_s({\gamma})^2}{2(1 - {\lambda}_s{\gamma})} 
\hspace{1in}(1)\]
\end{center}

Consider the two components of 
the transmission delay on each path. 
The first component is 
the delay required by a packet to synchronize with
the appropriate outgoing slot in the frame 
on which the node transmits and the actual packet transmission time. 
The average value of this delay is 
$\frac{1 + 2 + ... + d}{d} = \frac{d+1}{2}$. The second component is the  
$M/D/1$ queuing delay that a packet 
experiences at the buffer before it reaches the head of the buffer. 
This follows the same formula as in the 
routing delay case, and is given by,
\vspace{-0.15in}
\begin{center}
\[ \frac{{\lambda}_p S^2_p}{2 (1 - \frac{{\lambda}_p}{{\mu}_p})} = 
   \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)}\]
\end{center}

The two components are combined to obtain 
the total delay a packet encounters on a path as follows,
\vspace{-0.15in}
\begin{center}
\[ transmission\ delay = \frac{d+1}{2} + \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)} \hspace{1in}(2) \]
\end{center}

As discussed earlier, each packet
takes $h+2$ hops and $h+1$ paths on average. Thus, given that on average,
a packet spends {\em  routing delay} in each router and 
{\em transmission delay} on each 
path, the average packet delay can be expressed as follows:

\centerline{$delay = (h+2) \times routing\ delay + (h+1) \times transmission\ delay.$}

Using formula (1) and (2),
the following average delay encountered by a packet from the source
to the destination is obtained.

\vspace{-0.15in}
\begin{center}
\[delay = (h+2)\times ({\gamma} + \frac{{\lambda}_s({\gamma})^2}{2(1 - {\lambda}_s{\gamma})})
           +(h+1)\times (\frac{d+1}{2} + 
                  \frac{{\lambda}_pd^2}{2(1 - {\lambda}_pd)})\]
\end{center}

\subsection*{Model verification}

To verify the analytical model and to further study the performance of these
logical topologies, a network simulator was developed that simulates all four 
logical topologies on top of the torus topology. 
The simulator takes the following parameters.

\begin{itemize}
\item  {\em system size}, $N\times N$: This specifies the size of the network. Based on 
the logical topology, the system size also determines the multiplexing degree
in the system.

\item {\em packet generation rate}, ${\lambda}$: This is the rate at 
which fresh packets are 
generated in each node. 
It specifies the traffic on the network. The inter--arrival 
of packets follows a Poisson distribution. 
When a packet is generated at a node,
the destination is generated randomly among all other nodes in the system
with a uniform distribution.

\item {\em Packet routing time}, ${\gamma}$.

\end{itemize}

Fig~\ref{twothroughput} shows the maximum throughput obtained from the 
analytical model and from simulations.  Both $8\times 8$ and 
$16\times 16$ physical torus networks with different packet routing time
are examined.  
As can be seen from the figure, the analytical results and the simulation
results almost have a perfect match for all cases.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/thro.64.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/thro.256.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{predicted and simulated maximum throughput}
\label{twothroughput}
\end{figure}


Figure~\ref{twodelay1} and Figure~\ref{twodelay2} 
show the average packet delays obtained from the analytical model and from
simulations. Here, the packet routing time, $\gamma$,
 is equal to 1 time slot.
For $8\times 8$ torus, the analytical model matches the
simulation results fairly well 
for all topologies except when the generation rate is close
to saturation. The difference between the 
results from the analytical model and 
those from simulations is around 10\%. 
For the $16\times 16$ physical topology, 
the analytical model matches the simulations results for 
the all--to--all, allXY
and hypercube topologies. For the torus topology, the difference 
 is about 20\% due to the 
approximation.  Studies using other values of $\gamma$ have 
also been conducted. The 
analytical model and the simulation results on those studies
match slightly better than those shown in Figures \ref{twodelay1} and 
\ref{twodelay2}. Thus, overall the analytical model gives a good indication of
the actual performance.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/alltoall64.1.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/alltoall256.1.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delays for logical all--to--all topology ($\gamma = 1$)}
\label{twodelay1}
\end{figure}

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[physical $8\times 8$ torus]
{\psfig{figure=fig/three64.1.eps,width=2.9in}}
\end{subfigure}
\begin{subfigure}[physical $16\times 16$ torus]
{\psfig{figure=fig/three256.1.eps,width=2.9in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delays for logical allXY, hypercube and torus 
         topologies ($\gamma = 1$)}
\label{twodelay2}
\end{figure}

\section{Performance of the logical topologies}

In the previous section, an analytical model for performance
study for the logical topologies was developed and verified.
This section focuses on studying 
the performance of the logical topologies.
Since the simulation and the analytical model match reasonably well,
only the analytical model is used in this section 
to study the performance. 

Figure~\ref{throughput1} shows the impact of packet routing time 
on the maximum throughput. 
The underlying topology is a $32\times 32$
torus. For all logical topologies, increasing the speed of routers
increases the maximum throughput up to a certain limit.
For the all--to--all topology, the
router speed of 1 packet per 4 time slots is sufficient to overcome the
router performance bottleneck. 
Using faster router will not 
further 
improve the maximum throughput. For the allXY and hypercube topologies, the 
threshold is 1 packet per 2 time slots, and for the
 torus topology, the threshold
is 1 packet per time slot. When the routing speed is faster than the
threshold value, the maximum throughput is bound by the link speed
and the maximum throughput will not increase along with the increase in
 router speed.
Table~\ref{throughput2} shows the bandwidth limits
of routers and links for $N = 32$.

Figure~\ref{throughput1} also shows that the all--to--all
topology achieves higher maximum throughput
than the allXY topology, which in turn
achieves higher maximum throughput than the hypercube topology. The logical
torus has the worst maximum throughput. This observation holds for all
packet routing speeds. Under high workload, all paths
in the all--to--all and allXY topologies are utilized. The algorithms
to realize the all--to--all and allXY topologies guarantee that in each time
slot all links are used if all connections scheduled for that time slot are
in use, while the hypercube and torus topologies can not achieve this effect. 
Thus, it is expected that the all--to--all topology and the allXY topology
will outperform the hypercube and torus topologies in terms of maximum
throughput.
%This result indicates that
%using time division multiplexing to establish more connections improves the 
%link utilization under high workload.
 
\begin{figure}[htbp]
\centerline{\psfig{figure=fig/throughput1.eps,width=5in}}
\caption{Maximum throughput .vs. packet routing time $(N= 32)$}
\label{throughput1}
\end{figure}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
topology & bottleneck &  ${\gamma} = 0.5$ & ${\gamma} = 1$ &  ${\gamma} = 2$& ${\gamma}=4$\\
\hline
    &  ${\lambda}^{max}_{s}$ &  2.0     & 1.0     &    0.5   & 0.25\\
\cline{2-6}
all--to--all & ${\lambda}^{max}_{p}$ & 0.25  & 0.25 & 0.25  & 0.25\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.25  & 0.25  & 0.25 & 0.25\\
\hline
    &  ${\lambda}^{max}_{s}$ &  1.36     & 0.68  & 0.34   & 0.17\\
\cline{2-6}
allXY & ${\lambda}^{max}_{p}$ & 0.25  & 0.25 & 0.25  & 0.25\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.25  & 0.25  & 0.25 & 0.17\\
\hline
    &  ${\lambda}^{max}_{s}$ &  0.67     & 0.33     &    0.17   & 0.09\\
\cline{2-6}
hypercube & ${\lambda}^{max}_{p}$ & 0.1  & 0.1 & 0.1  & 0.1\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.1  & 0.1  & 0.1 & 0.09\\
\hline
    &  ${\lambda}^{max}_{s}$ &  0.24     & 0.12     &    0.06   & 0.03\\
\cline{2-6}
torus & ${\lambda}^{max}_{p}$ & 0.06  & 0.06 & 0.06  & 0.06\\
\cline{2-6}
    & ${\lambda}^{max}$   & 0.06  & 0.06  & 0.06 & 0.03\\
\hline
\end{tabular}
\end{center}
\caption{Maximum throughput for the logical topologies on $32\times 32$ torus}
\label{throughput2}
\end{table}

Figure~\ref{throughput3} shows the impact of network size on the maximum
throughput. The results in this figure are based 
upon a  packet routing time of one time slot.
Different packet routing times were also studied and 
similar trends were found.
In terms of maximum throughput, the all--to--all topology scales the best, 
followed
by the allXY topology, followed by the hypercube topology. 
The logical torus topology
scales worst among all these topologies. Figures~\ref{throughput1}
and \ref{throughput3} show that by using time--division multiplexing
to establish complex logical topology, the large aggregate
bandwidth in the network can be exploited to deliver higher throughput when the network
is under high workload. 
%In term of maximum throughput, the performance
%of the logical topologies are ordered as follows.
%\centerline{$all--to--all > allXY > hypercube > torus$}


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/throughput2.eps,width=5in}}
\caption{Maximum throughput .vs. network size (${\gamma} = 1$)}
\label{throughput3}
\end{figure}

%From the study of maximum throughput of the logical topologies, 
%we draw the conclusion
%that the complex logical topologies, such as the all--to--all topology
%and  the allXY topology
%are more efficient to exploit the bandwidth in the network. 

Although the all--to--all topology is the 
best in terms of the maximum throughput,
it suffers from large packet delay when the network is  not saturated.
Packet delay
is another performance metric to be considered.
For a network to be efficient, 
it must also be able to deliver packets with a small
delay. 
It is well known that time--division multiplexing results in larger
average packet delay due to the sharing of the links.
However, as discussed earlier, while using time--division multiplexing 
techniques
to establish logical topologies increases the per hop transmission time,
it reduces the average number of hops that a packet travels. Thus, the overall
performance   depends on  system parameters. Next, 
this effect for the logical topologies is studied.

Figure~\ref{latency1} shows the delay with regard to the fresh
packet generation 
rate. The underlying topology is a $16\times 16$
torus and $\gamma$  is  1 time slot. 
The figure shows that the all--to--all topology incurs 
very large delay
compared  to other logical topologies.  This is because of the large 
multiplexing degree needed to realize the logical all--to--all topology. 
Other 
topologies have similar delay when the generation rate is small, that is,
under low 
workload. However, the allXY topology has a larger saturation point than
the hypercube and torus topologies  and thus
has a small delay even when the network load is reasonably high (e.g. 
$\lambda = 0.25$). These results also hold for larger packet
routing times.
 
\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}
{\psfig{figure=fig/ss1.0.eps,width=3in}}
\end{subfigure}
\begin{subfigure}
{\psfig{figure=fig/ss1b.0.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Packet delay as a function of packet generation rate 
         $({\gamma} = 1.0, N=16)$}
\label{latency1}
\end{figure}

Figure~\ref{latency2} shows the impact of packet routing time on the 
average packet delay. The results are based upon a $16\times 16$ 
torus network and a packet generation rate of 0.005. 
The packet routing speed
has an impact on the delay for all topologies. For very small packet
routing time
(${\gamma} = 0.25$), the torus topology has the smallest delay.
When the packet routing time increases, the delay in torus increases 
drastically, 
while the delays 
in the all--to--all and allXY topologies increase
slightly. In the all--to--all and allXY topologies a packet
travels through fewer number of routers
 than it does in the torus topology. Hence the
contention at  routers does not affect the delay in the all--to--all and 
allXY topologies as much as it does in the torus and hypercube topologies.
This study also implies
that to achieve good packet delay for logical torus topology, 
fast routers are crucial, while a fast router
is not as important in the all--to--all
and allXY topologies.
 
\begin{figure}[htbp]
\centerline{\psfig{figure=fig/delay2.eps,width=5.5in}}
\caption{Impact of  packet routing time on packet delay 
         ($\lambda = 0.005, N=16$)}
\label{latency2}
\end{figure}

Figure~\ref{latency3} shows the impact of network size on the packet delay
for the topologies. The results are based upon a packet routing time of
1 time slot and a packet generation rate of 0.01.
This figure shows the manner  in which the delay time grows with regard
to the network size. As discussed in section 2, ignoring network contention
for a physical $N\times N$
torus, the all--to--all topology results in
a packet delay of $O(\gamma+N^3)$, 
 the allXY topology has a delay of $O(\gamma +N^2)$,
hypercube has a delay of $O(\gamma lg(N) + Nlg(N))$, and torus has 
a delay of $O(\gamma N)$.
Thus, the all--to--all topology has  very large delay when the network size
is large. The delay differences among the other
three topologies are relatively 
small for reasonably large sized networks. When the packet
routing time is small (${\gamma}= 1.0$), the hypercube topology 
scales slightly 
better than the torus and the allXY topologies as shown in
Figure~\ref{latency3}~(a). When ${\gamma}$ is large (${\gamma}=4.0$), 
the hypercube topology and the allXY topology
are better than the other two topologies as shown in 
Figure~\ref{latency3}~(b). 
%We should note that the figure plots delays for network sizes 
%up to 1024 nodes. As the network size grows larger (given a certain
%${\gamma}$) the torus would have the best delay, since its asymptotic
%delay grows in the  order of  
%$O(N)$ which is the best among all these topologies. While theoretically
%the logical torus should scales better than other topologies,  
%realizing the logical hypercube
%and allXY topology is usually better than the logical torus for a network
%of practical size.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[${\gamma} = 1.0$]
{\psfig{figure=fig/delay3.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[${\gamma}= 4.0$]
{\psfig{figure=fig/delay3b.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{impact of network size on the delay ($\lambda = 0.01$)}
\label{latency3}
\end{figure}

From the above discussions, three parameters, $N$, $\gamma$ and 
$\lambda$ affect the average packet delay for all the logical topologies. 
Next, 
the regions in the $(N, \gamma, \lambda)$ parameter space, where a logical
topology has the lowest packet delay are identified. 
Figure~\ref{best1} shows the best topologies in 
the parameter space $(N, \gamma)$ with fixed $\lambda$.
Comparing Figure~\ref{BEST}, where the network contention
is ignored, with Figure~\ref{best1}, where the contention is taken into 
consideration, it  can be seen
 that the logical topologies with less connectivity
suffer more from network contention. As can be seen from 
Figure~\ref{best1}~(a), with small packet generation rate, 
all four logical topologies occupy part of the 
$(N, \gamma)$ parameter space, which indicates that under certain 
conditions, each of the four topologies out--performs the other three
topologies. While in the case of large packet generation rate as shown in 
Figure~\ref{best1}~(b), the logical torus topology is pushed out 
of the best topology picture.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[${\lambda} = 0.01$]
{\psfig{figure=fig/range1.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[${\lambda}= 0.06$]
{\psfig{figure=fig/range2.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Best logical topology for a given packet generation rate}
\label{best1}
\end{figure}

Figure~\ref{best2} shows the best logical topologies on the 
$(\gamma, \lambda)$ parameter space. Here, the underlying network
is a $16\times 16$ torus. Networks of different size exhibit similar
characteristics. The majority of the $(\gamma, \lambda)$ parameter space
is occupied by the logical hypercube and allXY topologies. 
The logical torus topology is good only when the 
$\lambda$ is small and $\gamma$ is small. The logical all--to--all 
topology out--performs other topologies only when the network is almost
saturated, that is, large $\lambda$ or large $\gamma$. This indicates
that in general, 
the logical hypercube and allXY topologies are better topologies 
than the logical torus and all--to--all topologies in terms of packet delay. 

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range4.eps,width=4in}}
\caption{Best logical topology for a $16\times 16$ torus}
\label{best2}
\end{figure}

Figure~\ref{best3} compares the performance of the logical
hypercube and allXY topologies.  Given a fixed $\gamma$, there
is a packet generation rate, $\lambda$, above which the 
allXY topology out--performs the logical hypercube topology.
When  $\gamma$ increases, the line in the figure moves down.
In other words, the hypercube topology is more sensitive to the
packet routing time $\gamma$.


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/range3.eps,width=4in}}
\caption{Best logical topology for a given packet routing 
time ($\gamma = 1.0$)}
\label{best3}
\end{figure}


\section{Multi--hop communication vs single--hop communication}
\label{multisingle}

Previous sections considered the logical topologies that can be used 
to route packets and perform {\em multi--hop} communications.
As discussed in Chapter 3, another way to perform dynamic 
communication on multiplexed optical networks is to use a path 
reservation algorithm which reserves an optical path
from the source to the destination 
and then perform {\em single--hop} communications. 
The performance of these two communication schemes 
on a physical $16\times 16$ torus is compared in this
section. The logical allXY topology is used as the
logical topology for multi--hop communication since it offers large maximum
throughput and reasonably small average package delay for this 
size of networks. To obtain a fair comparison, the following 
assumptions are made:

\begin{itemize}
\item Both networks have the same multiplexing degree. For a $16\times 16$ 
torus, this means that both networks have a multiplexing degree of 32, which
is required for the logical allXY topology.
\item The data packet processing time in the multi--hop communication is 
equal to the control packet processing time in the path reservation
algorithm, since electronic processing is involved in both cases.
{\em Packet processing time}, $\gamma$, is used
 to represent both the data packet processing
time in the multi--hop communication and the control packet processing time
in the single--hop communication.
\item Control packet propagation time between two neighboring nodes 
is equal to data packet propagation time between the source and the 
destination, which is equal to 1 time slot.
\item It is assumed that a data {\em message} contains $s$ packets. 
Accordingly, the
{\em average message delay}, which is defined as the difference between the 
time the message is generated and the time when the whole message is received,
is measured 
instead of the {\em average packet delay}. The notation $\lambda_{msg}$ in 
this section represents the message generation rate per node per time slot. 
Since messages can be of different sizes, the {\em network load} is defined
to be\\
\centerline{$network\ load = s \times \lambda_{msg} 
            \times number\ of\ nodes,$} 
which is equal to the total number of packets injected into the network.
For the same reason, the throughput is measured
in terms of  packets delivered per time slot. 
\end{itemize}

The analytical model for the multi--hop communication 
cannot model the communication performance when packets in a message 
are sent to the same destination
since  destinations of packets are no longer distributed
uniformly among all nodes.
In some sense the number of packets in a message reflects the locality
of the communication traffic. All results in this section are obtained through
simulations. 


%\begin{figure}[htbp]
%\begin{subfigRow*}
%\begin{subfigure}[$throughput$]
%{\psfig{figure=fig/comp1b.eps,width=3in}}
%\end{subfigure}
%\begin{subfigure}[$delay$]
%{\psfig{figure=fig/comp1a.eps,width=3in}}
%\end{subfigure}
%\end{subfigRow*}
%\caption{Throughput and delay for message of size 8 ($\gamma = 1$)}
%\label{comp1}
%\end{figure}


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/allthro.eps,width=5.5in}}
\caption{Maximum throughput}
\label{comp1}
\end{figure}

Figure~\ref{comp1} shows the maximum throughput of the two schemes
with different message sizes, $s$, and packet processing times, $\gamma$. 
The packet processing time affects both the single--hop communication and the 
multi--hop communication, while the message size affects only single--hop
communication (larger message size leads to higher maximum throughput).
When the packet processing speed is fast, e.g.
$\gamma = 1$, such that the path bandwidth is the bottleneck in the 
communication, the multi--hop communication offers larger maximum throughput
than the single--hop communication. The reason is that multi--hop
communication utilizes the links in the network more efficiently 
when the network is saturated and 
does not incur additional control overhead. However, the multi--hop 
communication is more  sensitive to the packet processing time 
and the maximum throughput of the 
multi--hop communication decreases drastically when the packet 
processing time increases. In the single--hop communication,
the packet processing is only involved in the control network, thus, 
preserving the large bandwidth in the data network when the packet
processing time is large. This effect manifests itself when the message size 
is reasonably large and the extra control overhead is amortized over the
length of a message. Thus, 
the single--hop communication offers larger maximum throughput when
the packet processing time is large and the message size is sufficiently
large.
%In the multi--hop communication, message size does not affect the maximum
%throughput too much, while in the single--hop communication,
%large maximum throughput can only be achieved for large message sizes.
%This also implies that the multi--hop communication performs much 
%better then the single--hop communication when the message size is small.
Figures \ref{comp1a}~(a) and \ref{comp1a}~(b) show the maximum throughput 
with different message sizes for packet processing times of 1 and 4 
respectively. As can be seen from the figures, when the packet 
processing time is small ($\gamma = 1$), the multi--hop communication offers
larger maximum throughput for all message sizes. When the packet 
processing time is large ($\gamma = 4$), the single--hop communication
has a larger maximum throughput when the message size is sufficiently large.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[$\gamma = 1$]
{\psfig{figure=fig/comp4b.eps,width=3in}}
\end{subfigure}
\begin{subfigure}[$\gamma = 4$]
{\psfig{figure=fig/comp4b.n.eps,width=3in}}
\end{subfigure}
\end{subfigRow*}
\caption{Maximum throughput for different message sizes}
\label{comp1a}
\end{figure}

\begin{figure}[htbp]
\centerline{\psfig{figure=fig/multisize.eps,width=5.5in}}
\caption{Impact of message size on the average message delay ($\gamma = 1$)}
\label{delay1}
\end{figure}

When the network is under light load, it is more meaningful to compare
the message delay. 
Figure~\ref{delay1} shows the  impact of the network load and the message size 
on the average message delay. In this figure,  $\gamma = 1$.
When the message size is small ($size = 4$), the multi--hop communication
has smaller message delay. When the message size is large ($size = 64$), the
single--hop communication offers smaller message delay. In both cases, 
the large network load amplifies the difference between single--hop and 
multi--hop communications. For  messages of medium size
($size = 16$), the multi--hop communication has smaller delay when the network
load is below a certain point. In general, small messages favor the multi--hop
communication while large messages  favor the  single--hop communication.


\begin{figure}[htbp]
\centerline{\psfig{figure=fig/routing.eps,width=5.5in}}
\caption{Impact of packet processing time 
         on the average message delay ($\gamma = 1$)}
\label{delay2}
\end{figure}

The packet processing time affects the average message delay for both 
the single--hop communication and the multi--hop 
communication. In the single--hop 
communication, the packet processing time affects the path reservation
time only. Thus, given a fixed packet processing time, the extra
control overhead is almost the same for all message sizes. In 
the multi--hop communication, the extra overhead applies to each packet in a 
message, and thus the larger the message size, the larger the overhead. 
Figure~\ref{delay2} shows the impact of the packet routing time on the 
average packet delay. In this figure,  
the same network load of 10.24 is considered for different message sizes
(e.g. a generation rate of 0.01 for messages of size 4, 
$0.01\times 4\times 256 = 10.24$) with different message sizes. 
As can be seen from the figure, when the 
message size is small, the single--hop communication incurs larger 
message delay while for large message sizes, the multi--hop 
communication incurs larger message delay. 
The large packet processing time amplifies these effects. 

\section{Chapter summary}

This chapter considered the logical topologies for routing message
on top of torus topologies. Schemes for realizing the logical
torus, hypercube, allXY (where all--to--all connections along each dimension
are established) and all--to--all topologies on top of physical torus
networks were discussed. Optimal schemes for realizing hypercube
on top of physical arrays and rings were designed. Schemes that use
at most 2 more channels than the optimal for realizing 
hypercube on top of meshes and tori were presented.

An analytical model for the maximum throughput and the packet latency 
for multi--hop networks was developed and verified through simulations.
This analytical model was used to study the performance of the logical
topologies and to identify the cases where each logical topology 
out--performs the other topologies. 
In general, the performance of the logical
topologies with less connectivity, such as the torus and 
hypercube topologies,
are more sensitive to the network load and the router speed while
the logical topologies with more connectivity, such as the all--to--all and 
allXY topologies, are more sensitive to network size.
Logical topologies with dense connectivity achieve higher
maximum throughput than the topologies with less connectivity.
In addition, they also scale better 
with regard to the network size. In terms of the maximum throughput, 
the topologies can be ordered as follows:

\centerline{{\em all--to--all} $ > allXY > hypercube > torus$.}

In term of the average packet delay, the logical torus topology achieves best 
results only when the router is fast and the network is under light load,
while the logical all--to--all topology is best only when the router
is slow and the network is almost saturated. In all other cases, logical
hypercube and allXY topologies out--perform logical torus and 
all--to--all topologies. Comparing the logical allXY to the logical hypercube,
the allXY topology is better when the network is under high load. 
These results hold for all network sizes.

This chapter further compared  multi--hop communication with 
single--hop communication and identified the advantages and the limitations
of each communication scheme. The study in this chapter used randomly
generated communication traffic. Performance evaluation of these two 
schemes using communication patterns from real application programs, which 
confirms the results in this chapter, will be presented in Chapter 6.
Multi--hop communication is more efficient than 
single--hop communication in terms of maximum throughput when the packet 
processing speed is not a bottleneck in the system and 
when the message size is small.  When packet processing speed is slow, 
the single--hop  communication has higher maximum throughput when the 
message size is sufficiently large. In terms of the average message delay
when the network is under light load, large messages favor single--hop 
communication, while small messages favor multi--hop communication. 
The large packet processing time amplifies these effects.
Table~\ref{summary1} and Table~\ref{summary2} summarize these conclusions.


\begin{table} [htbp]
\begin{center}
\small
\caption{Maximum throughput on a $16\times 16$ torus}
\label{summary1}

\begin{tabular}{|c|c|c|}
\hline
       &  Small message size($4$) & Large message size($64$)\\
\hline
Small packet processing time & Multi--hop & Multi--hop\\
\hline 
Large packet processing time & Multi--hop & Single--hop\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table} [htbp]
\begin{center}
\small
\caption{Average message delay on a $16\times 16$ torus}
\label{summary2}

\begin{tabular}{|c|c|c|c|c|}
\hline
Network  & Packet processing   & \multicolumn{3}{c|}{Message size}\\
\cline{3-5}
load & time & Small(4)  & Medium(16) & Large(64)\\
\hline
Small & Small & Multi--hop & Multi--hop & Single--hop\\
\cline{2-5}
                    &  Large  & Multi--hop & Single--hop & Single--hop \\
\hline 
Large & Small & Multi--hop & Single--hop & Single--hop\\
\cline{2-5}
                    &  Large & Multi--hop & Single--hop & Single--hop\\
\hline
\end{tabular}
\end{center}
\end{table}

Both communication schemes suffer from the bottleneck of electronic processing,
which occurs in the path reservation in single--hop communication and
in the packet routing at intermediate nodes in multi--hop communication. 
Using the compiled communication technique discussed in the next chapter, 
this bottleneck can be removed. 

 



\chapter{Performance comparison}

This chapter evaluates the relative performance of the three
communication schemes presented in Chapters 3, 4, and 5
using real application programs. Three
sets of programs are used in the evaluation. The first set of programs
includes three hand--coded parallel programs, where communications 
are well defined and highly optimized for parallel execution. 
The second set of programs includes a number of HPF benchmark programs which
are tuned for parallel execution.
% and are concerned about
% the ease of programming. 
The third set of programs
includes a number of programs from SPEC95 which are not optimized for parallel
execution. 

The performance measurement is the communication time 
in the unit of time slots. A packet, 
which contains a number of words,
can be transmitted through a lightpath in a time slot.
In addition, {\em normalized time} is also used to compare the 
performance of the schemes. 
In normalized time, the best communication time
among all schemes is assigned a value of $1.0$
and communication times of all schemes are normalized with respect to 
the best communication time. The normalized time  shows
the best scheme for each program and how other schemes perform compared
to the best scheme. It is assumed that the  communication in  each pattern 
is performed in a synchronized manner. That is, the program synchronizes
before and after each communication pattern and thus no interleaving of 
communications and computations is allowed.  

Because the E--SUIF compiler does not handle the message passing paradigm,
the first set of experiment is carried out manually by extracting 
the communication patterns in the programs by hand.
The programs in the second and third sets are 
generated automatically by the E--SUIF compiler for the experiment. 
As discussed in Chapter 5,
the E--SUIF compiler first analyzes and optimizes the communications in a 
program and represents the communications using Section Communication 
Descriptors (SCDs). It then performs the communication phase analysis
and partitions the program into phases and schedules the communication 
pattern within each phase. Finally, the backend of the E--SUIF compiler
generates a library call, $lib\_comm$, for each SCD and another library
call, $lib\_phase$ for each phase. The $lib\_comm$ takes a SCD with all
runtime information as 
parameter. When the program is 
executed, the $lib\_comm$ procedure invokes a network 
simulator which simulates dynamic single--hop communication, 
dynamic multi--hop communication or compiled communication to obtain 
the communication time of the communication 
using one of the three communication schemes. 
The $lib\_phase$ is useful only when simulating compiled 
communication. It accesses to the 
communication requirement of each phase (that is obtained by the compiler),
and performs channel assignment for connections within each phase. Thus,
the communication performance of a program is obtained by running the
program generated by the E--SUIF compiler. 

The experiments use the following system settings.

\begin{itemize}
\item Physical network: $8\times 8$ torus.
\item Packet size: 4 words.
\item Routing algorithm: 
XY routing between dimensions and Odd--Even shortest--path routing within
each dimension.
\item Dynamic single--hop communication.
  \begin{itemize}
  \item Control protocol: Conservative backward reservation protocols ($cset$
        size is 1). As discussed in Chapter 3, the conservative backward
        reservation protocol almost has the best performance among all the
        path reservation protocols.  
  \item Control packet processing time: 1 time slot.
  \item Control packet propagation time: 1 time slot.
  \item Maximum control packet retransmission time: 5 time slot.
  \item Multiplexing degree: 1, 4, 14, 20.
  \end{itemize}
\item Dynamic multi--hop communication. 
  \begin{itemize}
  \item Logical topologies: torus, hypercube, allXY and all--to--all.
  \item packet switching time: 1 time slot.
  \end{itemize}
\item Compiled communication.
  \begin{itemize}
  \item Connection scheduling algorithms: combined algorithm for the first
        set of experiment, AAPC algorithm for the second and third 
        experiments.
  \end{itemize}
\end{itemize}  

\section{Hand--coded parallel programs}

This set of program includes three
hand--coded parallel programs, namely $GS$, $TSCF$ and $P3M$.
The $GS$ program uses Gauss--Siedel
iterations to solve Laplace equation on a discretized unit 
square with Dirichlet boundary conditions. It contains 
a nearest neighbor communication pattern with fairly large message size 
(64 packets messages).
The $TSCF$ program simulates the evolution of a
self--gravitating system using a self consistent field approach. It 
contains a hypercube communication pattern with small message 
size (1 packet message). $P3M$ performs particle--particle particle--mesh 
simulation \cite{Yuan97c}. This program contains five static communication
patterns.  Table~\ref{PATT}
describes the static communication patterns that arise in these programs.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Pattern & Type & Description\\
\hline
GS   & shared array ref. & PEs are logically linear array, Each PE \\
      &     & communicates with two PEs adjacent to it.\\
\hline
TSCF   & explicit send/recv & hypercube pattern\\
\hline
P3M  1 & data redistrib. & (:block, :block, :block) $\rightarrow$ (:, :, :block)\\
\cline{2-3}
P3M 2 & data redistrib. & (:, :, :block) $\rightarrow$ (:block, :block, :)\\
\cline{2-3}
P3M 3 & data redistrib. & (:block, :block, :) $\rightarrow$ (:, :, :block)\\
\cline{2-3}
P3M 4 & data redistrib. & (:, :, :block) $\rightarrow$ (:block, :block, :block)\\
\cline{2-3}
P3M 5 & shared array ref. & PEs are logically 3--D array, each PE\\
      &                   & communicates with 6 PEs surrounding it\\
\hline
\end{tabular}
\caption{Communication pattern description.}
\label{PATT}
\end{center}
\end{table}

Table~\ref{TP3M} shows the communication
time for these patterns in one main loop step in the programs. 
Table~\ref{TP3MN} shows the normalized time where
the best communication time is normalized to $1.0$.
In this experiment, it is assumed that there is sufficient multiplexing 
degree to support all the patterns in compiled communication. 
Thus, each phase contains one communication pattern and no
network reconfiguration is required to within each pattern. 
For dynamic single--hop communication, the communication time for
fixed multiplexing degrees of 1, 4, 14 and 20 is evaluated. 
For dynamic multi--hop
communication, the logical  torus, hypercube, allXY and
all--to--all topologies are considered. 
The following observations can be made from the results in Table~\ref{TP3MN}.

\begin{itemize}
\item Compiled communication out--performs
dynamic single--hop communication in all cases. The average
communication time for dynamic single--hop communication 
is 4.5 to 8.0 times greater than that
for compiled communication, depending on the multiplexing degree used
in dynamic single--hop communication.
Larger performance gains are observed for communications with
small message sizes (e.g., the $TSCF$ pattern) and dense communication 
(e.g., the $P3M\ 2$ pattern). Large multiplexing degree does not 
always improve the communication
performance for dynamic single--hop communication. For example, 
a multiplexing 
degree of 1 results in the best performance (for dynamic single--hop
communication) for the pattern in GS while a
degree of 14 has the best performance for the $P3M\ 5$ pattern.
\item Compiled communication out--performs  
dynamic multi--hop communication in all cases except for 
the $TSCF$ program where dynamic multi--hop communication has better 
communication time when using the logical hypercube topology. 
The reason is that
the $TSCF$ program only contains hypercube communication with message size 
equal to 1. Multi--hop communication 
achieves good communication performance when communication patterns in
a program matches the logical topology. However, on average, the
communication time for multi--hop communication is 3.0 to 7.6 times
larger than the communication time for compiled communication, depending 
on the logical topology used. 
\item Compiled communication achieves an 
average normalized time of 1.1 for all the communication patterns, which
indicates that compiled communication almost delivers optimal 
communication performance.
\item Comparing dynamic  multi--hop
communication with dynamic single--hop communication,
multi--hop communication has better performance
when the message size is small (e.g. $TSCF$, $P3M\ 5$), 
and when the communication requires dense connections 
(e.g. $P3M\ 2,3$), while
single--hop communication is better when the message size is large
(e.g. $GS$).
\end{itemize}

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & GS  & TSCF & P3M 1& P3M 2,3 & P3M 4& P3M 5\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 131 & 19 & 831 & 382 & 457 & 40 \\
\hline
     & torus & 404 & 30 & 3366 & 1656 &1632 & 127\\
%\cline{2-8}
Multihop & hypercube & 792 & 13 & 3371& 1338 &1499 & 74\\
%\cline{2-8} 
comm. & allXY & 990  & 17 & 3157 & 1058 &960 &121\\
%\cline{2-8}
      & alltoall & 4159 & 70 & 1326 &749 &1326 & 276\\
\hline
      & $d=1$  & 209 & 215 & 3194& 6655 &2091 & 378\\
Single--hop & $d=4$&  296 & 118 &2029 &2998 &1302 & 213\\
comm &        $d=14$ & 924 & 107 & 1713 &2171 &1508 & 196\\
  &          $d=20$ & 1296 & 108 &1702 & 2096 &1314 & 231\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Communication time (timeslots) for the hand--coded programs}
\label{TP3M}
\end{table} 

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & GS  & TSCF & P3M 1& P3M 2,3 & P3M 4& P3M5 & Average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 1.0 & 1.5 & 1.0 & 1.0 & 1.0 & 1.0 & 1.1 \\
\hline
     & torus & 3.1 & 2.3 & 4.1 & 4.3 &3.6 & 3.2 & 3.4\\
%\cline{2-8}
Multihop & hypercube & 6.0 & 1.0 & 4.1& 3.5 &3.3 & 1.9 & 3.3\\
%\cline{2-8} 
comm. & allXY & 7.6  & 1.3 & 3.8 & 2.8 &2.1 &3.0 & 3.4\\
%\cline{2-8}
      & alltoall & 31.7 & 5.4 & 1.6 &2.0 &2.9 & 6.9 & 8.4\\
\hline
      & $d=1$  & 1.6 & 16.5 & 3.9 &17.4 & 4.6 & 9.5 & 8.9\\
Single--hop & $d=4$&  2.3 & 9.1 &2.4 &7.8 &2.8 & 5.4 & 5.0\\
comm &        $d=14$ & 7.1 & 8.2 & 2.1 &5.7 &3.3 & 4.9 & 5.2\\
  &          $d=20$ & 9.9 & 8.2 &2.0 & 5.5 &2.9 & 5.8 & 5.7\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Normalized communication time for the hand--coded programs}
\label{TP3MN}
\end{table} 

  
In this study, two types of communication patterns are observed in 
a well designed parallel program, 
fine grain communications resulted from shared array references
and coarse grain communications resulted from data redistributions.
The fine grain communication  causes sparse connections with 
small message sizes, while the coarse grain communication results in 
dense connections with large message size.
For a communication system to efficiently support the 
fine grain communication,
the system must have small latency. Optical single--hop networks that use 
dynamic path reservation algorithms have a large 
startup overhead, thus cannot support this type of communication
efficiently. As shown in our simulation results, 
compiled communication where the startup overhead is eliminated
and dynamic multi--hop communication perform this type of 
communications efficiently.
For the coarse grain communication, the control overhead in the dynamic
communications  is not significant.
However, dense communication
results in a large number of conflicts in the system (path reservation in 
dynamic single--hop communication and packet routing in dynamic 
multi--hop communication), and the dynamic
control systems are not able to resolve these conflicts efficiently.
By using an off--line connection scheduling algorithm, compiled 
communication handles this type of communications efficiently.
The performance study confirms the conclusion in 
\cite{Hinrichs94} that static management of the dense
communication patterns results in large performance gains. 

%Compiled communication achieves high performance for 
%all types of static patterns in this set of experiments.
%Four factors contribute to the performance gain. First, 
%compiled communication eliminates dynamic control overhead. This is
%most significant for communication with small message sizes, where
%the overhead in the dynamic single--hop communication  is large 
%compared to the communication time.
%Second, compiled communication takes the whole 
%communication pattern into consideration, while dynamic communication, which
%considers the connection requests one by one, suffers from the 
%head--of--line effect \cite{Sivalingam93}.
%Third, the off--line message scheduling algorithm further
%optimizes the communication efficiency for compiled communication.
%Fourth, compiled communication allows the system to adapt to the
%communication requirement in a program. In dynamic single--hop system, 
%communication, control mechanism with variable multiplexing
%degree is very difficult to implement and
%results in large overhead, while in dynamic multi--hop systems, the system
%cannot choose an efficient logical topology without knowing the communication
%requirement of a program. Thus, an optical network with 
%dynamic control may provide good performance for some applications whose
%communication requirement matches the network capacity, but it will 
%not have good performance for other applications. 
%The compiler has the exact knowledge
%of the communication pattern in the three programs and thus, the compiled
%communication offers high performance. In the next set of experiments, I will
%consider some HPF benchmark programs where the compiler may not have the
%exact knowledge of the communication performance and study how much the
%communication performance for the compiled communication 
%is affected by the approximation in the compiler analysis. 

\section{HPF parallel benchmarks}

%\begin{table}
%\small
%\footnotesize
%\begin{center}
%\begin{tabular} {|c|c|}
%\hline
%benchmarks & description \\
%\hline
%0001 & Solution of 2-D Poisson Equation by ADI\\
%\hline
%0003 & 2-D Fast Fourier Transform \\
%\hline
%0004 & NAS EP Benchmark - Tabulation of Random Numbers \\
%\hline
%0008 & 2-D Convolution\\
%\hline
%0009 & Accept/Reject for Gaussian Random Number Generation \\
%\hline
%0011 & Spanning Percolation Cluster Generation in 2-D \\
%\hline
%0013 & 2-D Potts Model Simulation using Metropolis Heatbath\\
%\hline
%0014 & 2-D Binary Phase Quenching of Cahn Hilliard Cook Equation \\
%\hline
%0022 & Gaussian Elimination - NPAC Benchmark \\
%\hline
%0025 & N-Body Force Calculation - NPAC Benchmark  \\
%\hline
%0039 & Segmented Bitonic Sort \\
%\hline
%0041 & Wavelet Image Processing \\
%\hline
%0053 & Hopfield Neural Network \\
%\hline
%\end{tabular}
%\end{center}
%\caption{HPF Benchmarks and their descriptions}
%\label{desc1}
%\end{table}

This set of programs is from the Syracuse University HPF benchmark suite.
The benchmarks and their descriptions are 
listed in table~\ref{desc} in Section~\ref{evalphase} .
% The data 
%distributions are obtained from the original benchmark programs and are
%thus, optimized for the programs. 
The benchmarks include many different 
types of applications, however, all of the programs contain only 
regular computations.

The major difference between this experiment and the first
experiment is that, in this experiment,  compiled
communication is applied to the whole program instead of each individual
communication pattern. Assuming a multiplexing degree of 10,
the compiler tries to aggregate as many communications as possible into a 
phase  as opposed to the first experiment where 
compiled communication is assumed to have an infinite number of 
virtual channels  to handle each individual communication pattern 
in the programs. In addition, this set of programs contains 
communication patterns about which
the compiler cannot obtain precise information.
Two factors may degrade the performance of compiled communication.
First, compiler approximations may result in the waste of bandwidth for
establishing connections that are not used. Second, aggregating more 
communications in a phase reduces the number of network 
reconfigurations, but may result in larger communication time since
larger multiplexing degree is needed for more communications. This experiment
aims at studying the performance of compiled communication under these
limitations.

Table~\ref{HPFperformance} shows the communication time of the
programs using different communication schemes. 
Table~\ref{HPFperformanceN} shows the normalized time.
Even with the limitations discussed earlier, compiled communication in general 
out--performs dynamic communications to a large degree. 
The benefits of managing channels at compile time and 
eliminating the runtime path reservation overhead over--weights
the bandwidth losses through the imprecision of compiler analysis.
The average normalized time for compiled communication is 1.1 which
indicates that compiled communication almost delivers the best communication
performance for this set of programs. However, performance degradation
in compiled communication due to the conservative
approximation in compiler analysis is observed in some of the programs.
For example, compiler over--estimating the communication requirement 
is found in benchmarks 0009 and 0022. 
Note that the overall communication time for
the programs in Table~\ref{HPFperformance} may not show this, because each
program contains many communication patterns and the pattern that
is approximated may not dominate the overall communication time.
The performance loss due to aggregating communications, which results 
in larger multiplexing degree, is observed in benchmark 0025.
%It is desirable to develop more advanced communication phase analysis 
%techniques that can use different multiplexing degrees for different parts 
%of a program to achieve  best performance. 
Nonetheless, the 
overall trend of this experiment is very similar to that in 
the first experiment.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0001  & 0003 & 0004& 0008 & 0009 & 0011\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 45,624 &752  &1,368 &2,256 &2,394 & 105,252 \\
\hline
     & torus &197,760  &3,296  &1,776  &9,888  &3,108 & 158,594\\
%\cline{2-8}
Multihop & hypercube &159,840  &2,664  &1,032 &7,992  &1,806 & 147,496\\
%\cline{2-8} 
comm. & allXY &125,280&2,088  &1,704  & 6,439  &2,982 & 265,636\\
%\cline{2-8}
      & alltoall &87,960  &1,466  &4,944  & 4,398 &8,652 & 1,027,818\\
\hline
      & $d=1$  &888,240  &14,804  &1,920 &44,412 &3,360 & 141,052\\
Single--hop & $d=4$&357,600 &5,960  &2,208 &17,880 & 3,864&181,506 \\
comm &        $d=14$ &267,360  &4,456  &3,504 &13,368 &6,132 &372,678 \\
  &          $d=20$ &273,360 &4,556  &4,224 &13,668 &7,392 &484,374 \\
\hline
\end{tabular}

\vspace{0.5in}

\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0013  & 0014 & 0022& 0025 & 0039 & 0041\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &166,280 &63,400 &3,244,819 &29,854 &
68,704 &1,504  \\
\hline
     & torus &257,980  &129,800 &6,382,683  &25,470  & 106,525 &6,592 \\
%\cline{2-8}
Multihop & hypercube &363,340  &200,600  &9,509,070 &58,661  & 132,348 &5,328 \\
%\cline{2-8} 
comm. & allXY &748,220 &379,200  &5,922,920  & 63,264  &135,353 &4,176 \\
%\cline{2-8}
      & alltoall &3,368,600  &1,679,200  &6,379,275  &214,343  &393,166 &2,932 \\
\hline
      & $d=1$  &154,080  &71,200  &6,844,054 &23,440 &115,488 &29,608 \\
Single--hop & $d=4$&256,240 &125,200  &6,402,631 &31,221 &136,390 &11,920 \\
comm &        $d=14$ &779,920  &391,200  &6,516,485 &61,712 &214,042 &8,912 \\
  &          $d=20$ &1,086,160 &550,800  &6,925,278 &81,958 &261,832 &9,112 \\
\hline
\end{tabular}

\end{center}
\normalsize
\caption{Communication time for the HPF benchmarks.}
\label{HPFperformance}
\end{table} 


\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0001  & 0003 & 0004& 0008 & 0009 & 0011\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} & 1.0 &1.0  &1.3 &1.0 &1.3 & 1.0 \\
\hline
     & torus &4.3  &4.4  &1.7  &4.4  &1.7 & 1.5\\
%\cline{2-8}
Multihop & hypercube &3.5  &3.5  &1.0 &3.5  &1.0 & 1.4\\
%\cline{2-8} 
comm. & allXY &2.7&2.3  &1.7  & 2.9  &1.7 & 2.5\\
%\cline{2-8}
      & alltoall &1.9  &1.9  &4.8  & 2.0 &4.8 & 9.8\\
\hline
      & $d=1$  &19.3  &19.7  &1.9 &19.7 &1.9 & 1.3\\
Single--hop & $d=4$&7.8 &7.9  &2.1 &7.9 & 2.1&1.7 \\
comm &        $d=14$ &5.8  &5.9 &3.4 &5.9 &3.4 &3.5 \\
  &          $d=20$ &5.9 &6.0  &4.1 &6.0 &4.1 &4.6 \\
\hline
\end{tabular}

\vspace{0.5in}

\begin{tabular}{|c|c|r|r|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{benchmarks} & 0013  & 0014 & 0022& 0025 & 0039 & 0041 &average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1.1 &1.0 &1.0 &1.3 &1.0 &1.0&1.1  \\
\hline
     & torus &1.7  &2.1 &2.0  &1.1  & 1.6 & 4.4 & 2.6 \\
%\cline{2-8}
Multihop & hypercube &2.4  &3.2  &2.9 &2.5  & 1.9 &3.5 & 2.5 \\
%\cline{2-8} 
comm. & allXY &4.9 &6.0  &1.8  & 2.7  &2.0 &2.8 & 2.8 \\
%\cline{2-}
      & alltoall &21.9  &26.7  &2.0  &9.1  &5.7 &1.9 & 7.6 \\
\hline
      & $d=1$  &1.0  &1.1  &2.1 &1.0 &1.7 &19.7 & 7.5 \\
Single--hop & $d=4$&1.7 &1.9  &2.0 &1.3 &2.0 &7.9 & 3.9 \\
comm &        $d=14$ &5.1  &6.2  &2.0 &2.7 &3.1 &5.9 & 4.4 \\
  &          $d=20$ &7.1 &8.7  &2.1 &3.6 &3.8 &6.1 & 5.2 \\
\hline
\end{tabular}

\end{center}
\normalsize
\caption{Normalized time for the HPF benchmarks.}
\label{HPFperformanceN}
\end{table} 

\section{Programs from SPEC95}

Four programs, ARTDIF (from HYDRO2D), TOMCATV, SWIM and 
ERHS (from APPLU) are used in this experiment.
These programs are also used in Section~\ref{evalanalyzer}, where
the descriptions of these programs can be found,  to
evaluate performance of the communication analyzer 
in the E--SUIF compiler.

%This set of programs includes 4 programs. 
%The first benchmark, ARTDIF, 
%is a kernel routine obtained from the HYDRO2D program, 
%which is an astrophysical program for the computation of galactical jets
%using hydrodynamical Navier Stokes equations. 
%The second benchmark, TOMCATV, does the mesh generation with 
%Thompson's solver. 
%The third program, SWIM, 
%is the SHALLOW weather prediction program.
%The fourth program, ERHS, is part of the
%APPLU program, which is the solver for five coupled 
%parabolic/elliptic partial differential equations. The programs, HYDRO2D,
%TOMCATV, SWIM and APPLU, are sequential programs 
%originally from the SPEC95 benchmark suite.
%The purpose of this experiment is to study the communication performance of
%the three communication mechanisms for 
%automatically parallelized programs. 



\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & ARTDIF  & TOMCATV & SWIM & ERHS\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1,224 &15,480 &2,708 &6,689 \\
\hline
     & torus &2,724  & 34,260 & 1,378  &4,380 \\
%\cline{2-8}
Multihop & hypercube &4,338 &57,240  &2,309 &6,482\\
%\cline{2-8} 
comm. & allXY &8,583  &108,900 &4,409 & 15,117\\
%\cline{2-8}
      & alltoall &38,772 &491,460  &19,169   &68,800\\
\hline
      & $d=1$  &666 &8,280  & 669 &  1,148\\
Single--hop & $d=4$&2,478  &31,260  &1,574  &4,382\\
comm &        $d=14$ &8,538 &108,060 &4,511  & 15,166\\
  &          $d=20$ &12,168 &154,140  &6,343  &21,614\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Communication time for SPEC95 benchmark programs.}
\label{SPEC95performance}
\end{table} 



\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|r|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{Pattern} & ARTDIF  & TOMCATV & SWIM & ERHS & average\\ 
\hline
\multicolumn{2}{|c|}{Compiled comm.} &1.8 &1.9 &4.0 &5.8 & 3.3 \\
\hline
     & torus &4.1  & 4.1 & 2.1  &3.8 & 3.5 \\
%\cline{2-9}
Multihop & hypercube &6.5 &6.9  &3.5 &5.6 & 5.6\\
%\cline{2-9} 
comm. & allXY &12.9  &13.1 &6.6 & 13.1& 11.4\\
%\cline{2-9}
      & alltoall &58.2 &59.2  &28.7   &59.9 & 51.5\\
\hline
      & $d=1$  &1.0 &1.0  & 1.0 &  1.0 & 1.0\\
Single--hop & $d=4$&3.7  &3.8  &2.3  &3.8 & 3.4\\
comm &        $d=14$ &12.8 &13.0 &6.7  & 13.2 & 11.4\\
  &          $d=20$ &18.2 &18.6  &9.4  &18.8 & 16.3\\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Normalized communication time for SPEC95 benchmark programs.}
\label{SPEC95performanceN}
\end{table} 


Table~\ref{SPEC95performance} shows the communication performance of the
programs. Table~\ref{SPEC95performanceN} shows the normalized time.
The test inputs are used as the inputs to these program, which
determine the problem size. To reduce the simulation time, the main
iteration numbers in programs ARTDIF, SWIM and ERHS are reduced to one.
All programs ARTDIF, SWIM, TOMCATV and ERHS  only contains simple 
nearest neighbor communication patterns. Compiled communication performs
worse than dynamic single--hop communication with a multiplexing degree
of one because it  aggregates communications and uses larger
multiplexing degree than needed. 
Hence, it is desirable to develop more advanced communication phase analysis 
techniques that can use different multiplexing degrees for different parts 
of a program to achieve  best performance. 
However, considering all the programs evaluated,
compiled communication out--performs other schemes to a large degree
as shown in Table~\ref{overallN}.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Comm.    & Comp.  & \multicolumn{4}{c|}{Multi--hop} & 
\multicolumn{4}{c|}{Single--hop}\\
\cline{3-10}
schemes &  comm. &   torus & hype. & allXY & alltoall & $1$ &
$4$ & $14$ & $20$ \\
\hline
Norm. time & 1.5 & 3.0 & 3.3 & 4.6 & 16.1 & 6.6 & 4.0 & 5.9 & 7.4 \\
\hline
\end{tabular}
\end{center}
\normalsize
\caption{Average normalized communication time for each scheme.}
\label{overallN}
\end{table} 

\section{Chapter summary}

This chapter studied the communication performance for the three
communication mechanisms, dynamic single--hop communication,
dynamic multi--hop communication and compiled communication using
three sets of programs. The following conclusions were drawn from the
study.

\begin{itemize}

\item %Even after taking into considerations the limitations of 
%compiler analysis, 
Compiled communication 
out--performs dynamic  communications
to a large degree for applications with regular computations.

\item The performance of compiled communication can be further 
improved by incorporating 
more advanced communication phase analysis techniques that allow 
different multiplexing degrees in different parts of a program.

\item The major disadvantage of dynamic communications is that they cannot
adapt to different communication requirements. Thus, they support
some communication patterns efficiently while they are inefficient for
other communication patterns. 
Compiled communication efficiently 
supports all types of communication patterns that 
can be determined at compile time.
 
\item Comparing dynamic multi--hop communication 
and dynamic single--hop communication,
dynamic multi--hop communication achieves better performance when the
message size is small and when the communication is dense, while  dynamic
single--hop communication is better when the message size is large.
This result matches the results in Section~\ref{multisingle} where
dynamic single--hop communication is compared with dynamic
multi--hop communication using randomly generated communication patterns.  

\end{itemize}
  
\chapter{Conclusion}

While optical interconnection networks have the potential to provide very large
bandwidth, network control, which is performed in the electronic domain 
due to the lack of suitable photonic logic devices,
has become the communication bottleneck in such networks. In order
to design efficient optical networks where end users can utilize the 
large bandwidth, efficient network control mechanisms must be developed
to reduce the control overheads. This thesis addresses the 
network control bottleneck problem in optical networks by considering three
communication schemes, {\em dynamic single--hop communication}, {\em dynamic
multi--hop communication} and {\em compiled communication}. In addition
to developing techniques to improve communication performance in each scheme, 
this thesis also compares the communication performance
of the three schemes and identifies the advantages and the limitations of each
scheme.  In the following sections, 
the thesis contributions are summarized and directions for future research
are identified.


\section{Thesis contributions}

This thesis makes contributions in the {\em design of control mechanisms}
for time--multiplexed 
optical interconnection networks. The contributions are in two areas:
optical interconnection networks and compiler analysis techniques. In the 
optical interconnection
networks area, this thesis introduces efficient control schemes 
for dynamic single--hop communication and dynamic multi--hop communication. 
This thesis also 
proposes and validates the idea of applying the 
compiled communication technique 
to optical TDM  networks.  In the compiler area, 
this thesis addresses all the issues needed
to apply the compiled communication paradigm 
to optical interconnection networks,
including communication optimization, 
communication analysis, connection scheduling and 
communication phase analysis. 
The main contributions of the thesis are detailed as follows.

\begin{itemize}
\item {\bf Dynamic single--hop communication}. Two sets of efficient 
path reservation algorithms, 
{\em forward path reservation protocols} and  {\em backward 
path reservation protocols}, are designed. Variants of the protocols, 
including holding/dropping and aggressive/conservative schemes, 
are considered. The performance of the protocols and the impact
of system parameters on these protocols are evaluated. 
Forward path reservation protocols extend traditional path reservation
schemes for electronic networks and are simpler compared to  backward
path reservation protocols. However, these  
schemes suffer from either the over--locking problem for 
aggressive schemes or the low successful
reservation rate for  conservative schemes.  
Backward path reservation protocols overcome 
these problems by probing the network state before reserving channels. 
Performance study has established that in optical time--division 
multiplexing networks, backward path reservation protocols, 
though more complex than forward path reservation protocols, 
result in better communication performance when the corresponding 
system and protocol parameters are the same. It is also found that 
while some system or protocol parameters, such as the holding time for 
holding schemes, do not have a significant impact on the performance
of the protocols, other parameters, such as the aggressiveness of a protocol
and the speed of the control network, affect the performance drastically.
Similar techniques can be extended for the path reservation in 
WDM wide area networks \cite{Yuan96b,Yuan98b}.

\item {\bf Dynamic multi--hop communication}. 
Schemes for realizing 
four logical topologies, torus, hypercube, allXY and all--to--all, 
on top of the physical torus topologies are considered. 
Optimal and near optimal routing and channel assignment (RCA) schemes 
for realizing hypercube on array, ring, mesh and torus 
topologies are developed.
An analytical model for analyzing the maximum throughput and the 
average packet delay is developed and verified via simulation.
This model is used to study the performance of dynamic multi--hop 
communication using the four logical topologies. It is found that
in terms of the maximum throughput, the logical all--to--all topology
is the best while the logical torus topology has the lowest performance.
In terms of the average packet delay, the logical torus topology achieves best 
results only when the router is fast and the network is under light load,
while the logical all--to--all topology is best only when the router
is slow and the network is almost saturated. In all other cases, logical
hypercube and allXY topologies out--perform logical torus and 
all--to--all topologies. In addition, 
the impact of system parameters, such as the packet switching time, 
on these topologies are studied. In general,  the performance of the logical
topologies with low connectivity, such as the torus and 
hypercube topologies, are more sensitive to the network load 
and the router speed while the logical topologies with more connectivity, 
such as the all--to--all and allXY topologies, are more sensitive to 
network size. Some of the techniques developed for multi--hop 
communication in optical TDM networks can be applied to other areas. 
The optimal scheme to realize hypercube on mesh--like
topologies can be used to 
efficiently perform communications in algorithms that contain
hypercube communication patterns \cite{Leighton92}. 
The modeling technique can be extended to the modeling of WDM networks or
electronic networks that perform multi--hop communication.

\item {\bf Compiled communication}. 
This thesis considers all the issues necessary
to apply compiled communication
to optical TDM networks, including communication optimization,
communication analysis, connection scheduling and communication phase
analysis.

\begin{itemize}
\item {\it Communication optimization and communication analysis}. 
A communication descriptor called 
{\em Section Communication Descriptor} (SCD) that describes 
communications on virtual processor grids is developed. A 
communication analyzer which performs a number of communication 
optimizations, including message vectorization, redundant communication 
elimination and message scheduling, is presented. All the optimizations
use a demand driven global array data flow analysis framework. This framework
improves previous data flow analysis algorithms for 
communication optimizations by reducing analysis cost and increasing 
analysis precision. 
Algorithms are developed to derive communications on physical processors
from SCDs. 
These algorithms address the problem of effective approximations
in the cases when the information in a SCD is insufficient for deriving
precise communication on physical processors. 
The communication optimization technique is general
and can be implemented in a compiler that compiles HPF--like 
programs for distributed memory machines.   
The communication analysis technique can be used by a compiler 
that requires the knowledge of the communication requirement of a program
on physical processors. 

\item {\it Connection scheduling}. A number of heuristic connection scheduling
algorithms are developed to schedule connections on torus topologies. Some
of the algorithms can also be applied to other topologies. 

\item {\it Communication phase analysis}. 
A communication phase analysis algorithm
is designed to partition a program into phases such that each phase contains
communications that can be supported by the underlying network, while 
capturing the communication locality in the program 
to reduce the reconfiguration overheads. 
This algorithm can also be applied to compiled communication 
on electronic networks. 

\end{itemize}

\item{\bf Communication performance comparison}.
A number of benchmarks and real application programs, including 
hand--coded parallel programs, HPF kernel benchmarks and 
programs from SPEC95, are used to  compare the communication 
performance of the three communication schemes. The relative strengths and
weaknesses of the three schemes are evaluated. 
%Following conclusions are drawn. 
The study establishes that even
with the limitations of compiler analysis, compiler communication
generally out--performs dynamic communications.
%The major advantage of compiled communication is its ability to
%adapt to the communication requirement during different phases of a 
%program. 
It delivers high communication performance for all types 
of communication patterns that are known at compile time.
The dynamic single--hop communication and dynamic multi--hop 
communication both suffers from the 
inability to adapt to the communication requirement. Given
a fixed system setting, they provide good performance for some
communication patterns while fail to achieve high performance for other
communication patterns. Comparing these two communication schemes, 
multi--hop communication has the advantage when the message size is small
and when the communication requires dense connections, 
while single--hop communication
has the advantage when the message size is large. 
\end{itemize}

\section{Future research}

The research of this thesis can be extended in various ways. Some of the
algorithms  can be improved. Additional 
work may either extend the applicability of the techniques or improve
the techniques. Following are a number of future research directions that
are related to this thesis.

\begin{itemize}

\item {\bf Improving backward path reservation algorithms}.   
In the backward reservation, once a channel is reserved,
the reservation fails only when the network state changes. 
Due to the distributed manner of collecting channel states and 
reserving channels in backward path reservation algorithms,
the information for channels on links close to the source node is not as
accurate as the information for channels on links close to the 
destination node.
This problem can be severe when the network size is large. Two 
possible solutions to this problem are as follows. 
First,  a more efficient control network can be used 
to route control messages.
For example,  a Multistage Interconnection Network (MIN) with multi--cast
capability can be used to route control messages so that 
control messages can reach all nodes along the path at the same time.
This allows a protocol to collect the channel usage information 
more efficiently and increases the chance of successful reservation.
Second, assuming that the control network has the same topology as the 
data network,
the backward path reservation protocols can selectively collect the 
channel usage information. The idea behind this improvement
is that wrong information may be worse than no information. 
    
\item {\bf Path reservation with adaptive routing}. In the thesis, path
reservation algorithms assume a deterministic routing.
Preliminary research on extending the path reservation protocols 
with adaptive routing was carried out. The preliminary 
results show that using current
path reservation protocols (both forward and backward reservations), 
the adaptive routing yields lower maximum throughput on the physical torus 
topology for uniform communication traffics. Further research is 
needed to explain this phenomenon and to design path reservation
protocols that take advantage of adaptive routing.

\item {\bf Topologies for multi--hop communication}. In this thesis,
four logical topologies, torus, hypercube, allXY and alltoall,
on top of the physical torus topologies are considered. 
There are two ways to extend this work. 
First, a different physical topology can be considered. For instance,  
it would be interesting to consider efficiently 
realizing regular topologies, such as mesh, torus, on top of an irregular 
topology. Second, there are logical topologies other than the four 
logical topologies considered that can achieve good communication 
performance. Examples include the tree and the shuffle--exchange topologies.

\item {\bf Interprocedural communication optimization}. 
The communication analyzer in the thesis performs
a number of communication optimizations, including message vectorization,
global redundant communication elimination and global message scheduling, 
using intraprocedural array data flow analysis. By incorporating the
interprocedural array data flow analysis, more optimization opportunities
can be uncovered. The intraprocedural array data flow analysis framework
uses interval analysis. It can naturally be extended to 
interprocedual analysis
by treating a procedure as an interval. However, many details,
such as array reshaping at subroutine boundaries and its impact on
communications, must be considered in order for the interprocedural 
analysis to work. 

\item {\bf Improving communication phase analysis}.
The communication phase analysis algorithm in the thesis follows simple 
heuristics, it considers the control structures in a program using
post--order traversal. This enables the algorithm to consider
communications in innermost loops first, 
aggregate the communications out of loops to reduce
the reconfiguration overhead and capture the communication locality. 
However, while the algorithm is simple to implement, the phases it generates
are not optimal in the sense that there may exist other program partitioning
schemes that result in less phases in a program.
More advanced communication phase analysis algorithms based on 
better communication model \cite{Salisbury97}
may be developed by using a general control flow graph for program 
representation and by considering the communication requirement
of the whole procedure when generating phases. 

\item {\bf Compact communication descriptor}.
The communication descriptor in the compiler 
that describes communication patterns
on physical processors is a flat structure. It contains all pairs
of source and destination nodes. This descriptor is both large and hard
to manipulate. More compact communication descriptor is desirable for
the compiler. The challenge however, is that 
the descriptor must both be compact and easy to use by the 
analysis algorithms. 

\item {\bf Irregular communication patterns}

Many scientific codes contain irregular communication patterns that
can only be determined at runtime. This thesis has restricted the
compiled communication technique to be applied to the programs
that contain only regular computations. This restriction can be
relaxed by using a strategy similar to the Chaos runtime 
library\cite{Sussman92}.
This library performs an inspector phase that calculates the runtime
schedule once for many executions of the communication pattern. Similarly
the connection scheduling algorithms can gather communication 
information at runtime and assign channels to all connections 
within the next looping structure to be used for subsequent iterations.
  
\end{itemize}

\section{Impact of this research}

This thesis establishes that the compiled communication technique
is more efficient than both dynamic single--hop communication and 
dynamic multi--hop communication. The compiler algorithms that enable
the application of compiled communication on optical TDM networks, though
can be further improved, are available in this thesis. Although
the compiled communication technique can only apply to the communication 
patterns that are known at compile time, mechanisms that allow the compiler
to manage network resources so that compiled communication can be
supported must be incorporated in future optical TDM networks for 
multiprocessor systems to achieve high performance. Dynamic communication
schemes must be used to handle general communication in an optical TDM network.
Dynamic single--hop communication incurs large startup overhead and is thus
inefficient for small messages which occur frequently in parallel
applications. Dynamic multi--hop communication is efficient 
for small messages, however, it places electronic processing in the 
critical path of data transmission and cannot fully utilize the large
bandwidth in optical links when the optical data
transmission speed is significantly faster than the electronic 
processing speed. Hence, both schemes have their own advantages and the better
choice between these two schemes depends on the application programs and the
advances in optical networking technology.

This thesis develops techniques for efficient communication in optical TDM
networks. Many techniques developed  can be applied to other areas. The 
path reservation algorithms for dynamic single--hop communication can be
extended for WDM wide area networks. The efficient routing and channel
assignment algorithms for hypercube communication pattern 
can be used to efficiently perform communications in algorithms that contain
hypercube communication patterns. The modeling technique for multi--hop 
communication in optical TDM networks can be extended for WDM networks
and electronic networks with multi--hop communication. The communication 
optimization technique based on a demand driven data flow analysis technique
can be incorporated in a compiler that compiles a HPF--like language 
for distributed memory machines. The communication analysis technique can
be used by compilers that perform architectural dependent communication 
optimizations, or compiled communication on electronic networks.









\section{Preliminary work}
\label{prework}

This section describes the preliminary work that has been done toward 
achieving the goals described in Chapter 3. A number of 
distributed path reservation protocols for 
point--to--point optical TDM networks have been designed and 
 a network simulator that
simulates the performance of these protocols has been developed. I also
designed and implemented a demand driven communication optimization
data flow analyzer which performs message vectorization and redundant
communication elimination. The analyzer represents the  logical communication
patterns in programs in communication descriptors
 and can be used to generate physical communication patterns
for both optimized and non--optimized programs. In addition, connection
scheduling algorithms that can be used to schedule communication patterns
that are known at compile time are developed for the torus topology.

\subsection{Distributed path reservation}

\subsubsection{The protocols}
\label{protocol}

In order to support a distributed control mechanism for connection
establishment, it is  assumed that in addition to the optical data network,
there is a logical {\em shadow network} through which
all the control messages are communicated. 
The shadow network has the same physical topology as the data network.
The traffic on the shadow network consists of small control packets
and thus is much lighter than the traffic on the data network. 
The shadow network  operates in packet switching mode; routers at 
intermediate nodes examine the control packets and update local bookkeeping
information and switch states accordingly. 
The shadow network can be implemented as an 
electronic network or alternatively a virtual channel on the data network
can be reserved exclusively for exchanging control messages.
It is  also assumed that a node can send or receive messages through
different virtual channels simultaneously. 
%This is always true for a 
%TDM system. For WDM system, this implies that each nodes must
%have one sender and receiver for each wavelength it operates on. 
%%The protocols can be modified to 
%apply to the WDM system with single tunable
%sender and receiver in each node.

A path reservation protocol ensures that the path from a source node
to a destination node is reserved before the connection is used. A path 
includes the virtual channels on the links that form the connection, the
transmitter at the source node and the receiver at the destination node.
Reserving the transmitter and receiver is the same as reserving a
virtual channel on the link from a node to the switch attached to that
node.
Hence, only reservation of virtual channels on links forming a
connection with path multiplexing will be considered.
There are many options available with respect to different aspects of the 
 path reservation mechanisms, which are discussed next.

\begin{itemize}

%\noindent
%$\bullet$
\item {\em Forward reservation} versus {\em backward reservation}.
Locking mechanisms are needed by the distributed path reservation
protocols  to ensure the exclusive usage of a virtual channel for a connection.
This variation characterizes the timing at which
the protocols  perform the locking.
Under forward reservation, the virtual channels are locked 
by a control message that travels from
the source node to the destination node.
Under backward reservation, a control message travels to the
destination to probe the path, then virtual channels that are found to be
available are locked by another
control message which travels from the destination node to the source node.

%\noindent 
%$\bullet$
\item {\em Dropping} versus {\em holding}. This variation characterizes
the behavior of the protocol when it 
determines that a connection establishment does not progress.
Under the  dropping approach, once the protocol
determines that  the establishment of a connection is not progressing,
it releases the virtual channels  locked on the partially established
path and informs the source node that the reservation has failed.
Under the holding approach, when the protocol determines
that  the establishment of a connection is not progressing,
it keeps the virtual channels  on the partially established path locked for
some period of time, hoping that during this period, the reservation
will progress. If, after this timeout period, the reservation still does not
progress, the partial path is then released and the
source node is informed of the failure.
Dropping can be viewed as holding with holding time equal to 0.

%\noindent 
%$\bullet$
\item {\em Aggressive} reservation versus {\em conservative} reservation. This
variation characterizes the protocol's treatment of each reservation. Under
the aggressive reservation, the protocol tries to establish a connection
by locking as many virtual channels as possible during the reservation process.
Only one of the locked channels is then used for the connection, while the
others are released.
Under the  conservative reservation approach, the protocol
locks only one virtual channel during the reservation process.

\end{itemize}

\subsubsection*{Deadlock}

Deadlock in the control network can arise from two sources.
First, with limited number of buffers, a request loop can be formed within the
control network.
Second, deadlock can occur when a request is holding (locking)
virtual channels on some links while requesting other channels on other
links.
This second source of deadlock can be avoided by the dropping or holding mechanisms
described above.
Specifically, a request will give up all the locked channels if 
it does not progress within a certain timeout period.

Many deadlock avoidance or deadlock prevention techniques for 
packet switching networks proposed in the literature \cite{Dally87} 
can be used to deal with deadlock within the control network (the
first source of deadlock).
Moreover, the control network is under light traffic, and
each control message consists of only a single packet of small size 
(4 bytes). Hence, it is feasible to provide a large number of buffers in each 
router to reduce or eliminate the chance of deadlock.

\subsubsection*{States of virtual channels}

The control network router at each node maintains a state for each
virtual channel on links connected to the router. For forward reservation,
the control router maintains the states for the outgoing links, while
in backward reservation, the control router maintains the states
for the incoming links. As discussed later, 
this setting enables the router to have the information
needed for reserving virtual channels and updating the switch states.
A virtual channel, $V$, on link $L$, can be in one of the following states:

\begin{itemize}
\item $AVAIL$: indicates that the virtual channel $V$ on link $L$
is available and can be used to establish a new connection,
\item $LOCK$: 
indicates that $V$ is locked by some request in the process of establishing
a connection.
\item $BUSY$: indicates that $V$
is being used by some established connection to transmit data.
\end{itemize}

For a link, $L$, the set of virtual channels that are in the $AVAIL$ state is
denoted as $Avail(L)$. When a virtual channel, $V$, is not in $Avail(L)$,
an additional field, $CID$, is maintained to identify the connection request
locking  $V$, if $V$ is in the $LOCK$ state, or the connection using $V$, if $V$
is in the $BUSY$ state.

\subsubsection*{Forward reservation schemes}

In the connection establishment protocols,
each connection request is assigned a unique identifier, $id$, which
consists of the identifier of the source node and a serial number
issued by that node. 
Each control message related to the establishment of a connection carries its
$id$, which becomes the identifier of the connection when it is successfully
established. It is this $id$ that is maintained in the $CID$ field of
locked or busy virtual channels on links.
Four types of packets are used in the forward reservation
 protocols to establish a connection.

\begin{itemize}

%\noindent 
%$\bullet$
\item {\em Reservation packets} ($RES$), used to reserve virtual channels.
In addition to the connection $id$, a $RES$ packet contains a bit vector,
$cset$, of size equal to the number of virtual channels in each link.
The bit vector $cset$ is used to keep track of the set of virtual channels 
that can be used to satisfy the connection request carried by $RES$.
These virtual channels are locked
at intermediate nodes while the $RES$ message
progresses towards the destination node. The switch
states are also set to connect the locked channels on the input and output links.

%\noindent 
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$), used to inform source nodes of the
success of connection requests.
An $ACK$ packet contains a $channel$ field which indicates the virtual
channel selected for the connection.
As an $ACK$ packet travels from the destination to the source, it changes
the state of the virtual channel 
selected for the connection to $BUSY$, and unlocks
(changes from $LOCK$ to $AVAIL$)
all other virtual channels that were locked by the corresponding $RES$ packet.

%\noindent 
%$\bullet$
\item {\em Fail or Negative ack packets} ($FAIL/NACK$), used to inform source
nodes of the failure of connection requests. While traveling back to the source
node, a $FAIL/NACK$ packet unlocks all virtual channels that were locked by the
corresponding $RES$ packet.
 
%\noindent 
%$\bullet$
\item {\em Release packets} ($REL$),  used to release connections.
A $REL$ packet traveling from a source to a destination changes the
state of the virtual channel
reserved for that connection from $BUSY$ to $AVAIL$.

\end{itemize}

The protocols require that control packets from a destination, $d$, to a source, $s$,
follow the same paths (in opposite directions) as packets from $s$
to $d$.
The fields of a packet will be denoted by $packet.field$.
For example, $RES.id$ denotes the $id$ field of the $RES$ packet.

The forward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $RES$ packet with $RES.cset$ set to the
virtual channels that the node may use. This message is then routed to the
destination. When an intermediate node receives the $RES$ packet, 
it determines the next outgoing link, $L$, on the path to the destination, and
updates $RES.cset $ to $ RES.cset \cap Avail(L)$.
If the resulting $RES.cset$ is empty,
the connection cannot be established, and a $FAIL/NACK$ message is sent back to the
source node. The source node will retransmit the request after some
period of time.
This process of failed reservation is shown in Figure~\ref{FORWARD}(a). 
Note that if $Avail(L)$ is represented by a bit-vector, then
$RES.cset \cap Avail(L)$ is a bit-wise "$AND$" operation.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/forward.pstex,height=2.2in}}
\caption{Control messages in forward reservation}
\label{FORWARD}
\end{figure}

If the resulting $RES.cset$ is not empty, the router reserves all the 
virtual channels in $RES.cset$ on link $L$ by changing their states to $LOCK$
and updating $Avail(L)$.
The router will then set the switch state to connect the virtual channels in the
resulting $RES.cset$ of the corresponding incoming and outgoing links.
Maintaining the states of outgoing links is sufficient for these two
tasks.
The $RES$ message is then forwarded to the next node on the path to the destination.
This way,
as $RES$ approaches the destination, the 
path is reserved incrementally. Once $RES$ reaches the
destination with a non-empty $RES.cset$, the destination selects from 
$RES.cset$ a virtual channel to be used for the connection and informs
the source node that the channel is selected by sending an $ACK$ message 
with $ACK.channel$ set to the selected virtual channel.
The source can start sending data once it 
receives the $ACK$ packet. After all data is sent, the source
node sends a $REL$ packet to tear down the connection. This successful
reservation process is shown in Figure~\ref{FORWARD} (b). Note that although
in the algorithm described above, the switches are set during the processing
of the $RES$ packet, they can instead be set during the processing of
the $ACK$ packet.

\noindent
{\bf Holding}: The protocol described above can be modified to 
use the holding policy instead of the dropping policy.
Specifically, when an intermediate node
determines that the connection for a reservation cannot be established, 
that is when $RES.cset \cap Avail(L) = \phi$, the node buffers the $RES$ packet
for a limited period of time. If within
this  period, some virtual channels in the original $RES.cset$ become
available, the $RES$ packet can then continue its journey. Otherwise, 
the $FAIL/NACK$ packet is  sent back to the source.
Implementing the holding policy 
requires each node to maintain a holding queue and
to periodically check that queue to determine if any of the virtual channels 
has become available. In addition, some timing 
mechanism must be incorporated in the routers to timeout 
held control packets. This increases the hardware
and software complexities of the routers.

\noindent
{\bf Aggressiveness}: 
The aggressiveness
of the reservation is reflected in the size of the 
virtual channel set, $RES.cset$, initially chosen by the source node.
In the most aggressive scheme, the source node sets
$RES.cset$ to $\{0, ..., N-1\}$, where $N$ is the number of 
virtual channels in the system. This ensures that the reservation
will be successful if there exists an available virtual channel on the path.
On the other hand, 
the most conservative reservation assigns
$RES.cset$ to include only a single virtual channel. In this case, the
reservation can be successful only when the virtual channel chosen by the
source node is available in all the links on the path. Although 
the aggressive scheme seems to have advantage over the conservative scheme,
it results in excessive locking of the virtual channels in the system. Thus, in
heavily loaded networks, this is expected to decrease the overall throughput.
To obtain optimal performance, the aggressiveness of the protocol should be
chosen appropriately between the most aggressive and the most conservative extremes.

The retransmit time  is another protocol parameter.
In traditional non--multiplexed networks, the retransmit time
is typically chosen randomly from a range [0,MRT], where MRT
denotes some maximum retransmit time.
In such systems, MRT must be set to a reasonably
large value to avoid live-lock. However, this may increase the average
message latency time and decrease the throughput.
In a multiplexed network, the problem of live-lock only 
occurs in the most aggressive scheme (non--multiplexed circuit switching
networks can be considered as  having a multiplexing degree of 1 and 
using aggressive reservation). 
For less aggressive schemes, the
live-lock problem can be avoided by changing the virtual channels selected in
$RES.cset$ when $RES$ is retransmitted.
Hence, for these schemes, a small retransmit time can be used.

\subsubsection*{Backward reservation schemes}

In  the forward locking protocol, the initial decision concerning the 
virtual channels to be locked for a connection request is made in the 
source node without any information about network usage. The backward
reservation scheme tries to overcome this handicap by probing the network
before making the decision. In the backward reservation schemes,
a forward message is used to probe the availability of virtual channels.
After that,
the locking of virtual channels is performed by a backward message. 
The backward reservation scheme uses six types of control
packets, all of which carry the connection $id$, in addition to other
fields as discussed next:

\begin{itemize}
%\noindent
%$\bullet$
\item {\em Probe packets} ($PROB$) travel from sources to destinations 
gathering
information about virtual channel usage without locking any virtual channel.
A $PROB$ packet carries a bit vector, $init$,
to represent the set of virtual channels that are
available to establish the connection.

%\noindent
%$\bullet$
\item {\em Reservation packets} ($RES$) are similar to the $RES$ packets in the forward
scheme, except that they travel from destinations to sources, lock
virtual channels as they go through intermediate nodes, and set the
states of the switches accordingly.
A $RES$ packet contains a $cset$ field.

%\noindent
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$) are similar to $ACK$ packets in the forward
scheme except that they travel from sources to destinations.
An $ACK$ packet contains a $channel$ field.


%\noindent
%$\bullet$
\item {\em Fail packets} ($FAIL$) unlock the virtual channels locked by the
$RES$ packets in cases of failures to establish connections.

%\noindent
%$\bullet$
\item {\em Negative acknowledgment packets} ($NACK$) are
used to inform the source nodes of reservation failures.

%\noindent
%$\bullet$
\item {\em Release packets} ($REL$) are
used to release connections after the communication is completed.

\end{itemize}

Note that a $FAIL/NACK$ message in the forward scheme performs the functions
of both a $FAIL$ message and a $NACK$ message in the backward scheme. 

The backward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $PROB$ message with $PROB.init$ set to contain all
virtual channels in the system.
This message is then routed to the destination.
When an intermediate node receives the $PROB$ packet, 
it determines the next outgoing link, $L_f$, on the forward path to the
destination,  and updates $PROB.init $ to $PROB.init \cap Avail(L_f)$.
If the resulting $PROB.init$ is empty,
the connection cannot be established and a $NACK$ packet is sent back to the
source node.  The source node will try the reservation again after a certain 
retransmit time.
Figure~\ref{BACKWARD}(a) shows this failed reservation case.

If the resulting $PROB.init$ is not empty, the node 
forwards $PROB$ on $L_f$ to the next node. 
This way,
as $PROB$ approaches the destination, the virtual channels available
on the path are recorded in the $init$ set.
Once $PROB$ reaches the
destination,  the destination forms a $RES$ message with $RES.cset$
equal to a selected subset of $PROB.init$ and sends this message back
to the source node.
When an intermediate node receives the $RES$ packet, it determines the
next link, $L_b$, on the backward path to the source, and updates
$RES.cset $ to $RES.cset \cap Avail(L_b)$. 
If the resulting $RES.cset$ is empty, 
the connection cannot be established. In this case the node sends
a $NACK$ message to the source node to inform it of the failure,
and sends a $FAIL$ message to the 
destination to free the virtual channels locked
by $RES$. This process is shown in Figure~\ref{BACKWARD}(b).

\begin{figure}[htp]
\centerline{\psfig{figure=fig/back.pstex,height=2.2in}}
\caption{Control messages in backward reservation}
\label{BACKWARD}
\end{figure}

If the resulting $RES.cset$ is not empty,
the virtual channels in $RES.cset$ are locked, the switch is set accordingly
and $RES$ is forwarded on $L_b$
to the next node.  When $RES$ reaches the source with a non-empty
$RES.cset$,
the source  selects a
virtual channel from the $RES.cset$ for the connection and sends
an $ACK$ message to the destination with $ACK.channel$ set to the
selected virtual channel. This $ACK$ message unlocks all the virtual channels 
locked by $RES$, except the one in $channel$.
The source node can start sending data as soon as it sends the $ACK$ message.
After all data is sent, the source
node sends a $REL$ packet to tear down the connection.
The process of successful reservation is shown in Figure~\ref{BACKWARD}(c).

\noindent
{\bf Holding}: Holding can be incorporated in the backward reservation scheme
as follows.
In the protocol, there are two cases that cause the reservation to fail. 
The protocol may determine that the reservation fails when processing
the $PROB$ packet. In this case, no holding is necessary since 
no resources have yet been locked.
When the protocol determines that the 
reservation fails during the  processing of a
$RES$ packet, a holding mechanism
similar to the one used in the forward reservation scheme may be applied.

\noindent
{\bf Aggressiveness}:
The aggressiveness of the backward reservation protocols is reflected in the 
initial size of $cset$ chosen by the destination node.
The aggressive approach sets
$RES.cset$ equal to $PROB.init$, while the conservative
approach sets $RES.cset$ to contain a single virtual channel from $PROB.init$.
Note that if a protocol supports only the conservative scheme,
the $ACK$ messages may be omitted, and thus only five types of messages 
are needed. 
As in the forward reservation schemes, the 
retransmit time is a parameter in the backward schemes.

\subsubsection{A preliminary network simulator \& Experimental evaluation}
\label{simulator}

A preliminary network simulator has been developed to simulate the behavious
of multiplexed torus networks. The simulator models the network with 
various choices of system parameters and protocols. Specifically, 
the simulator provides the following options for protocol parameters.

\begin{itemize}
\item {\em forward and backward} reservations, this determines which
protocol to be simulated.

\item {\em initial $cset$ size}: This parameter determines the
initial size of $cset$ in the reservation packet. 
It restricts the set of virtual channels under
consideration for a reservation. For FD and FH, 
the initial $cset$ is chosen when the source node composes the RES packet.
Assuming that $N$ is the multiplexing degree in the system,
an $RES.cset$ of size $s$ is chosen by generating a random number,
$m$, in the range $[0,$N$ - 1]$, 
and assigning $RES.cset$ = $\{m\ mod\ N, m+1\ mod\ N..., N+s-1\ mod N\}$.
In the backward schemes, the initial $cset$ is set when
the destination node composes the $ACK$ packet. An $ACK.cset$ of size $s$ 
is generated in the following manner.
If the available set, $RES.INIT$,
has less available channels than $s$, the $RES.INIT$ is copied to $ACK.cset$.
Otherwise,  the available channels are represented in a linear
array and the method used in generating the $cset$  in the forward schemes
is used.

\item {\em timeout value}: This  value 
determines how long a reservation packet can be put in a waiting queue.
The dropping scheme can be considered as a holding scheme with timeout time
equal to 0.

\item {\em maximum retransmit time} (MTR): 
This specifies the period after which a node will retry a
failed reservation. As discussed earlier,
this value is crucial for avoiding live-lock
in the most aggressive schemes. The actual retransmit time
is chosen randomly between 0 and  $MRT -1$.
\end{itemize}

Besides the protocol parameters, the simulator also allows the 
choices of various system parameters.

\begin{itemize}

\item {\em system size}: This specifies the size of the network. All our
simulations are done on torus topology.

\item {\em multiplexing degree}. 
This specifies the number of virtual
channels supported by each link. In our simulation, the multiplexing degree
ranges from 1 to 32.

\item {\em message size}: The message size directly affects  the time that
 a connection is kept before it is released.
In our simulations,  fixed size messages are assumed.

\item {\em request generation rate at each node (r)}: This specifies the traffic on
the network. The connection requests at each node is assumed to have a Poisson
inter-arrival distribution. When a request is
generated at a node, the destination of the request is generated randomly
among the other nodes in the system. When a generated request is blocked,
it is put into a queue, waiting to be re-transmitted.

\item {\em control packet processing and propagation time}: This specifies the speed of the
control networks. The control packet processing time is the time for an
intermediate node to process a control packet. The control packet
propagation time is the time for a control packet to be transferred from one node
to the next. It is assumed
 that all the control packets have the same
processing and propagation time.
\end{itemize}

In the following discussion, $F$ is used to denote forward reservation,
$B$ denotes the backward reservation, $H$ denotes 
holding and $D$ denotes dropping
schemes. For example, $FH$ means the forward holding scheme.
I have implemented a network simulator with various  control mechanisms 
including FH, FD, BH and BD.
Although the simulator can simulate both WDM and TDM torus networks, 
only the results for TDM networks will be presented in this paper.
The results for WDM networks follow similar patterns.
In addition to the options of backward/forward reservation and holding/dropping
policy, the simulation uses the following parameters.

The average latency and throughput are used to evaluate the protocols.
The latency is the period between the time when a message is ready and the time
when the first packet of the message is sent.
The  throughput is the number of messages received per time unit.
Under light traffic, 
the performance of the protocols is measured by the average message latency,
while under heavy traffic, the throughput 
is used as  the performance metric.  
The simulation time is measured in time slots, where a time slot is the
time to transmit an optical data packet between any two nodes in the network.
Note that in multiprocessing applications, nodes are physically close
to each other, and thus signal propagation time is very small (1 foot per
nsec) compared to the length of a message.
Finally, deterministic XY--routing is assumed in the torus topology.


\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Throughput]
  {\psfig{figure=eps/CMP1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CMP2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Comparison of the reservation schemes with dropping}
\label{DFMUL}
\end{figure}


Figure~\ref{DFMUL} depicts the throughput and average latency as a function of
the request generation rate for six
protocols that use the dropping policy in a $16\times 16$ torus.
The multiplexing degree is taken to be 32, the
message size is assumed to be 8 packets and the control packets
processing and propagation time is assumed to be 2 time units. 
For each of the forward and backward schemes, three variations are considered 
with varying aggressiveness.
The conservative variation in which the
initial $cset$ size is 1, the most aggressive variation in which
the initial set size is equal to the multiplexing degree and an optimal variation 
in which the initial set size is chosen (by repeated trials) to maximize the
throughput.
The letters  $C$, $A$ and $O$ are used to
denote these three variations, respectively.
For example, $FDO$ means the forward dropping scheme with optimal $cset$ size.
Note that the use of the optimal $cset$ size reduces the delay in addition to
increasing the throughput. Note also that the network saturates when
the generation rate is between 0.006 and 0.018, depending on the protocol
used. The maximum saturation rate that the $16\times 16$ torus can achieve in the
absence of contention and control overhead can be calculated from
\[
\frac{number\ of\ links}{no.\ of\ PEs \times av. \ no.\ of\ links\ per\ msg
\times msg\ size} = \frac{1024}{256\times 8 \times 8} = 0.0625
\]
Hence, the optimal backward protocol can achieve
almost 30\% of the theoretical full utilization rate. 

Figure~\ref{DFMUL}(b) also reveals that,
when the request generation rate, $r$, is small, for example $r = 0.003$, 
the network is under light traffic and 
all the protocols achieve the same throughput, which is equal to $r$ times
the number of processors.
In this case, the performance of the network should be measured by the
average latency.
In the rest of the performance study,
the maximum throughput (at saturation) and the average latency
(at $r = 0.003$) will be used to measure the performance of the protocols.
Two sets of experiments are performed. The first set 
evaluates the effect of the protocol parameters on the network throughput and
delay, and the second set
evaluates the impact of system parameters on performance.

\subsubsection*{Effect of protocol parameters}

In this set of experiments,
the effect of the initial $cset$ size, the holding time and the retransmit time on the 
performance of the protocols are studied. 
the system parameters for this set of experiment are chosen as follows:
System size = $16\times 16$,
message size = 8 packets, control packet processing and propagation time = 2 time
units.

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET3.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET4.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on forward schemes}
\label{CSETF}
\end{figure}
\begin{figure}[hbtp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on backward schemes}
\label{CSETB}
\end{figure}

Figure~\ref{CSETF} shows the effect of the initial $cset$ size on the forward
holding scheme with different multiplexing degrees, namely
1, 2,  4, 8, 16 and 32.
The holding time is taken to be 10 time units and the MTR is 5 time units
for all the protocols with initial $cset$ size less than the multiplexing degree
and 60 time units for the most aggressive forward scheme.
Large MTR is used in the most aggressive forward scheme because it is 
observed that small MTR often leads to live-lock in that scheme.
only the protocols with the holding policy will be shown since using the
dropping policy leads to similar patterns. The effect of holding/dropping will
be considered in a later figure.
Figure~\ref{CSETB} shows the results for the backward
schemes with the dropping policy.

From Figure~\ref{CSETF} (a), it can be seen that when the multiplexing 
degree is larger than 8, both the most
conservative protocol and the most aggressive protocol
do not achieve the best throughput. Figure~\ref{CSETF}(b) shows that these
two extreme protocols do not achieve the smallest latency either.
The same observation applies to the backward schemes in Figure~\ref{CSETB}.
The effect of choosing the optimal initial $cset$ is significant on both
throughput and delay. That effect, however, is more significant in the
forward scheme than in the backward scheme. For example, with multiplexing
degree = 32,
choosing a non-optimal $cset$ size may reduce the throughput by 50\%
in the forward scheme and only by 25\% in the backward scheme. 
In general, the optimal initial $cset$ size is hard to find.
Table~\ref{OPTCSET} lists the optimal initial $cset$ size for each multiplexing
degree.
A rule of thumb to approximate the optimal $cset$ size is to use 1/3 and 1/10 of the
multiplexing degree for forward schemes and backward schemes, respectively.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Multiplexing & \multicolumn{2}{|c|}{Optimal $cset$ size}\\
\cline{2-3}
Degree & Forward & Backward\\
\hline
4 & 1 & 1\\
\hline
8 & 2 & 1\\
\hline
16 & 5 & 2\\
\hline
32 & 10 & 3\\
\hline
\end{tabular}
\end{center}
\caption{Optimal $cset$ size}
\label{OPTCSET}
\end{table}

Figure~\ref{HOLDSET} shows the effect of the holding time on the performance of
the protocols for a multiplexing degree of 32. 
As shown in Figure~\ref{HOLDSET}(a), the holding time has little
effect on the maximum throughput. It slightly increases the performance for the
forward aggressive and the backward aggressive schemes. As for the 
average latency at light working load, the holding time also has little effect
except for the forward aggressive scheme, where the
latency time decreases by about 20\% when
the holding time at each intermediate node increases from 0 to 30 time units.
Since holding requires extra hardware support compared to
dropping, it is  concluded that holding is not 
cost--effective for the reservation protocols. In the rest of the paper,
only protocols with dropping policies will be considered.


\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/HOLDSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/HOLDSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of holding time}
\label{HOLDSET}
\end{figure}

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/RETRYSET1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/RETRYSET2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of maximum retransmit time}
\label{RETRYSET}
\end{figure}



Figure~\ref{RETRYSET} shows the effect of the maximum
retransmit time (MRT)
on the performance. Note that the retransmit time is uniformly distributed
in the range $0..MRT-1$. As shown in Figure~\ref{RETRYSET} (a),
increasing MRT results in performance degradation in
all the schemes except FDA, in which the performance improves
with the MRT. This confirms that the MRT value is important to 
avoid live-lock in the network when aggressive reservation is used.
In other schemes this parameter is not important, because when 
retransmitting a failed request, virtual channels different than the ones
that have been tried may be included in $cset$.
This result indicates another drawback of the 
forward aggressive schemes: in order to avoid live-lock, the MRT
must be a reasonably large value, which decreases the overall performance.

The results of the above set of experiments may be summarized as follows: 

\begin{itemize}

\item With proper protocols, multiplexing results in higher
maximum throughput. Multiplexed networks are significantly more efficient than
non--multiplexed networks.

\item Both the most aggressive and the most
conservative reservations cannot achieve optimal performance. 
However, the performance of the forward schemes is more sensitive to the
initial $cset$ size than the performance of the backward schemes.

\item The value
of the holding time in the holding schemes does not have significant
impact on the performance. In general, however, dropping is more efficient
than holding.

\item  The retransmit time 
has little impact on all the schemes except the FDA scheme.

\end{itemize} 

In the next section,
only  dropping schemes with
MRT equal to 5 time units for all schemes except FDA will be considered.
The MRT for FDA schemes is set to 60.

\subsubsection*{Effect of other system parameters}

This set of  experiments focuses on studying the performance of the 
protocols under different multiplexing degrees, system sizes, message sizes and 
control network speeds.
Only one parameter is changed in each experiment, with the other
parameters set to the following default values (unless stated otherwise):
network size = $16\times 16$ torus, multiplexing degree = 
16, message size = 8 packets, 
control packet processing and propagation time = 2 time units.

\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
  {\psfig{figure=eps/MUL1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/MUL2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{The performance of the protocols for different multiplexing degree}
\label{DBMUL}
\end{figure}


Figure~\ref{DBMUL}
shows the performance of  the protocols for different multiplexing degrees. 
When the multiplexing degree is small,  BDO and FDO
have the same maximum bandwidth as BDC and FDC, respectively. When 
the multiplexing degree is large, BDO and FDO offers better throughput.
In addition, for all multiplexing degrees, BDO is the best among
all the schemes. As for the average latency, both FDA and BDA have significantly
larger latency than all other schemes. Also, FDO and BDO have the smallest latencies.
It can be seen from this experiment that the backward
schemes always provide the same or better performance (both maximum
throughput and latency) than their forward reservation counterparts for all
multiplexing degrees considered.

Figure~\ref{SIZE} shows the effect of the network size on the performance of
the protocols.
It can be seen from the figure that all the protocols, except the aggressive
ones, scale nicely with the network size.
This indicates that the aggressive protocols cannot 
take advantage of the spatial diversity of the communication. This is 
a result of excessive reservation of channels. When the network size
is small, there is little big difference in the performance of the protocols.
When the network size is larger, the backward schemes show their superiority.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/SIZE1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
 {\psfig{figure=eps/SIZE2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the network size}
\label{SIZE}
\end{figure}


\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/MSIZE1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
{\psfig{figure=eps/MSIZE2.eps,height=2.2in}}
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the message size}
\label{MSIZE}
\end{figure}



Figure~\ref{MSIZE} shows the effect of the message size on the protocols.
The multiplexing degree  in this experiment is 16.
The throughput in this figure is normalized to reflect the
number of packets that pass through the network, rather than the number
of messages.  That is,\\
\centerline{$normalized\ throughput\ =\ msg\ size \times \ throughput$}
Both the forward and backward locking schemes achieve higher
throughput for larger messages. When messages are sufficiently large,
the signaling overhead in the protocols is small and
all protocols have almost the same performance. 
However, when the message size is small, the BDO scheme achieves 
higher throughput
than the other schemes. This indicates that BDO incurs less 
overhead in the path reservation than the other schemes. 

The effect of message size on the latency of the protocols is interesting.
Forward schemes incur larger latency when the message size is large. 
By blindly chosing initial cset, forward schemes
do not avoid chosing virtual channels used in communications, which
increases the latency when the message size is large (so that connections are
hold longer for communications).
Backward schemes probe
the network before chosing the initial csets. Hence, the latency in backward
shemes does not degrade as
much as in forward schemes when message size increases. 
Another observation is that in both forward and backward protocols, 
aggressive schemes sustain the increment of message size better
than the conservative schemes. This is also because of the longer
communication time with larger message size. Aggressive schemes are more 
efficient in finding a path in case of large message size. Note that 
this merit of aggressive schems is offsetted by the 
over reservations.
Another interesting point is that
the latency for messages of  size 1 results in higher latency than 
messages of size 8 in BDA scheme. This can be explained by the overly crowded
control messages in the network
in the case when data message contains a single packet (and
thus can be transmitted fast). The conflicts of control messages result in
larger latency.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
{\psfig{figure=eps/CNS1.eps,height=2.2in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CNS2.eps,height=2.2in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the speed of the control network}
\label{CNS}
\end{figure}

Figure~\ref{CNS} shows the effect of the control network speed on performance. 
The multiplexing degree  in this experiment is 32.
The speed of the
control network is determined by the
time for a control packet to be transferred from one node to the next node,
and the time for the control router to process the control packet. From the 
figure, it can be seen that, when the control speed is slower, the maximum
throughput and the average latency degrade.
The most aggressive schemes in both 
forward and backward reservations, however, are more sensitive to the
control network speed. Hence, it is important to have a reasonably fast 
control network when these reservation protocols are used. 

The results of the above set of experiments may be summarized as follows:

\begin{itemize}
\item The performance of FDA is significantly worse than other protocols. 
Moreover, this protocol cannot take advantage of both larger multiplexing 
degree and larger network size.
\item The backward reservation schemes provide better performance than
the forward reservation schemes for all multiplexing degrees. 
\item The difference of the protocols does not affect the communication efficiency
when the network size is small. However, for large networks, the
backward schemes provide better performance.
\item The backward schemes provide better performance when the message size
is small. When the message size is large, all the protocols have similar 
performance.
\item The speed of the control network affects the performance of the 
protocols greatly.
\end{itemize}

In this section, I have discussed distributed path reservation algorithms 
to establish connections with path multiplexing for communication
requests that arrive at the network dynamically.  In the next section, 
preliminary work on the communication analysis for compiled communication
will be presented.

\subsection{Communication analysis for compiled communication}
\label{commlab}

In order for the compiler to perform compiled communication, data structures 
must be designed for the compiler to represent the communication requirement
in the program. The data structure must both be powerful enough to represent 
the communication reqirement and easy to be obtained from the source program.
A communication descriptor which is an extension of the {\em array
section descriptor} \cite{callahan88} is designed. 
The descriptor describes the communication 
pattern in logical processor space. It can be used for both communication 
optimization and to derived the communication pattern in physical processor
space. In the following, the communication descriptor and 
how to calculate the communication descriptor from the source program will be 
described.


\subsubsection{Section communication descriptor (SCD)}

In this section,  
{\em Section Communication Descriptor} 
(SCD), which can be used 
to represent logical communication patterns in a program, will be introduced. 
The calculation of initial SCDs in programs will also be described.
Further information about SCD and its operations can be found in \cite{Yuan96}.

\subsubsection*{The descriptor}

The processor space is considered as an unbounded grid of virtual processors.
The abstract processor space is similar to a {\em template} in High
Performance Fortran (HPF) \cite{HPF}, which is a grid over which 
different arrays are aligned. In the rest of this section,  
{\em communication} means communication on the virtual processor grid.

The Section Communication Descriptor(SCD) is composed of three parts, 
(1) an array region which describes the parts of arrays that are involved 
in the communication, (2) a communication mapping which describes the
source and destination (in virtual processor grid) 
relationship in the communication and (3) a
communication qualifier which describes the iterations in which
the communication should 
be performed. More specifically, a SCD is defined as 
$<N, D, M, Q>$, where $N$ is an array name, $D$ is the source array region
of the communication, and $M$ is the descriptor of source-destination mapping,
and $Q$ is a descriptor that indicates in which iterations (in the interval)
the communication should be performed. 

The {\em bounded regular section descriptor} (BRSD)\cite{callahan88} is used
to describe the source region of communications. As discussed in 
\cite{callahan88}, set operations can be efficiently performed over BRSDs.
The source region
D is a vector of subscript values such that each of its elements is either
(1) an expression of the form $\alpha*i + \beta$, where i is a loop 
index variable and $\alpha$ and $\beta$ are invariants, (2) a triple 
$l:u:s$, where $l$, $u$ and $s$ are invariants, or (3) $\perp$, indicating
no information about the subscript value.   

The source-destination mapping $M$ is denoted as
$<source, destination, qual>$. The $source$
is a vector whose elements are of the form  
$\alpha*i + \beta$, where $i$ is a loop 
index variable and $\alpha$ and $\beta$ are invariants. The $destination$ 
is a vector whose elements are of the form 
$\sum_{j=1}^{n} \alpha_{j}*i_j + \beta_j$, where $i_j$'s are 
loop index variables  and $\alpha_j$'s and $\beta_j$'s are
invariants. The {\em mapping qualifier} list, 
$qual$, is a list of elements whose
format is $i = l:u:s$, where
$i$ is a variable, $l$, $u$ and $s$ are invariants. $l:u:s$  
denotes the ranges of the variable $i$. 
The mapping qualifier is used to describe the broadcast
effect, which may be needed during message vectorization.

The qualifier $Q$ is of the form $i = l:u:s$, where $i$ is the
induction variable of the interval. The notation
$Q=\perp$ is used to indicate that 
the communication is to be performed 
in every iteration.
$Q$ will be referred to as the SCD's {\em communication 
qualifier} in the rest of the paper. Including $Q$ in the descriptor
enables SCD to describe the communications that can be partially
vectorized. 

Initially, communications are required
before assignment statements with remote references.
It is assumed that owner computes rule is enforced. The owner
computes rule requires each item referenced on the {\em right hand side} (rhs)
 of an 
assignment statement to be sent to the processor that owns the {\em left hand
side} (lhs).
Before  the calculation of SCD is presented, I will first 
describe the calculation of  the ownership of array elements.


\begin{figure}[tbph]
%\begin{singlespace}
\begin{minipage}{10cm}
\small
\footnotesize
\begin{tabbing}
\hspace{11.5cm}  ALIGN (i, j) with VPROCS(i, j) :: x, y, 
                                      z\\
\hspace{11.5cm}  ALIGN (i, j) with VPROCS(2*j, i+1) :: w\\
\hspace{11.5cm}  ALIGN (i) with VPROCS(i, 1) :: a, b\\
\hspace{11.5cm}(s1)\hspace{0.1in}do\=\ i = 1, 100\\
\hspace{11.5cm}(s2)\hspace{0.1in}\>b(i-1) = a(i)...\\
\hspace{11.5cm}(s3)\hspace{0.1in}\>a(i+2) = ...\\
\hspace{11.5cm}(s4)\hspace{0.1in}\>b(i) = a(i+1) ...\\
\hspace{11.5cm}(s5)\hspace{0.1in}\>do\=\ j = 1, 100\\
\hspace{11.5cm}(s6)\hspace{0.1in}\>\>x(i, j) = w (i, j)\\
\hspace{11.5cm}(s7)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s8)\hspace{0.1in}end do\\
\hspace{11.5cm}(s9)\hspace{0.1in}if (...) then\\
\hspace{11.5cm}(s10)\hspace{0.1in}\>do i = 1, 100\\
\hspace{11.5cm}(s11)\hspace{0.1in}\>\>do\=\ j = 50, 100\\
\hspace{11.5cm}(s12)\hspace{0.1in}\>\>\>x(i+j-1, 2*i+2*j-3) = w (i, j)\\
\hspace{11.5cm}(s13)\hspace{0.1in}\>\>\>y(i, j) = w(i, j)\\
\hspace{11.5cm}(s14)\hspace{0.1in}\>\>end do\\
\hspace{11.5cm}(s15)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s16)\hspace{0.1in}\>do i = 1, 100\\
\hspace{11.5cm}(s17)\hspace{0.1in}\>\>y(i, 150) = x(i+1, 150)\\
\hspace{11.5cm}(s18)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s19)\hspace{0.1in}end if\\
\hspace{11.5cm}(s20)\hspace{0.1in}do i = 1, 100\\
\hspace{11.5cm}(s21)\hspace{0.1in}\>b(i) = a(i+1)\\
\hspace{11.5cm}(s22)\hspace{0.1in}\>do j = 1, 200\\
\hspace{11.5cm}(s23)\hspace{0.1in}\>\>z(i, j) = x(i+1, j)* w(i, ,j)\\
\hspace{11.5cm}(s24)\hspace{0.1in}\>end do\\
\hspace{11.5cm}(s25)\hspace{0.1in}\>w(i+1, 200) = ...\\
\hspace{11.5cm}(s26)\hspace{0.1in}end do\\

\end{tabbing}
\end{minipage}

\begin{minipage}{20cm}
\begin{picture}(0,0)%
\special{psfile=fig/2.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0038in}%
%\begin{picture}(1080,1070)(385,-240)
%\end{picture}

\end{minipage}
\normalsize
\caption{An example program and its interval flow graph}
\label{EXAMPLE}
\end{figure}


\subsubsection*{Ownership}

It is assumed that the arrays are all aligned to a single virtual space by 
simple affine functions. 
The alignments allowed are scaling, axis alignment and 
offset alignment.  The mapping from a point $\vec{d}$ in the data space to the 
corresponding point $\vec{v}$ in the virtual processor grid can be specified by
an alignment matrix $M$ and an alignment offset vector $\vec{\alpha}$. 
Thus, the ownership information of an element $\vec{d}$ can be calculated
using the formula $\vec{v} = M \vec{d} + \vec{\alpha}$. 
For example, consider the 
alignments of array $w$ and $a$ in the example program in 
Fig.~\ref{EXAMPLE}, the alignment matrices and offset vectors for 
array $w$ and $a$ are the following:
\begin{center}
\small
\footnotesize
\[ M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{\alpha}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   M_a  = \left(
       \begin{array}{c} 1 \\ 0 \end{array} \right),\
   \vec{\alpha}_a = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right).
\]
\end{center}

\subsubsection*{Initial SCD calculation}

Once the ownership information is provided, initial SCDs can be calculated
from the program structure. Let  $<N, D, M, Q>$ be an initial $SCD$, where
$N$ is the array to be communicated.
The array region 
$D$ contains a single element which is determined by the index expressions.
Since initially communication is always performed in 
every iteration, $Q = \perp$.  Let $M = <src, dst, qual>$. Since
initially communication does not perform broadcast, 
$qual = \perp$. Next I will 
describe how to calculate the source processor, $src$,
 and destination processor,
$dst$, for the mapping $M$ from the program structure.

Let $\vec{i}$ be the vector of loop indices. When the subscript
expressions are affine functions of the loop indices, the array 
references can be
expressed as $N(G\vec{i} + \vec{g})$, where $N$ is the array name, $G$ is a matrix and $\vec{g}$ is  a vector. 
I call G the {\em data access matrix}
and $\vec{g}$ the {\em access offset vector}.  The 
matrix, $G$, and the vector, $\vec{g}$, describe
a mapping from each
point in the iteration space to the corresponding point in the data space.
Let $G_l$, $\vec{g}_l$, $M_l$, $\vec{\alpha}_l$ be the data access matrix,
access offset vector, alignment matrix and alignment vector 
for the left
hand side array, and  
$G_r$, $\vec{g}_r$, $M_r$, $\vec{\alpha}_r$ be the corresponding quantities
for the right hand side array. The source processor $src$, which represents
the processor of the array element in $rhs$, and destination
processor $dst$, which represents the processor of the element in $lhs$
can be obtained from the  following equations.\\
\centerline{$src = M_r(G_r\vec{i} + \vec{g}_r) + \vec{\alpha}_r,$ \hspace{1in}
            $dst = M_l(G_l\vec{i} + \vec{g}_l) + \vec{\alpha}_l$}

Consider the communication in statement $s11$ for Fig.~\ref{EXAMPLE}.
The compiler can obtain from the program the following  data 
access matrices,
access offset vectors, alignment matrices and alignment vectors. 
\begin{center}
\small
\footnotesize
\[ M_x  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{\alpha}_x = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right),\
   M_w  = \left(
       \begin{array}{c} 0 \\ 1 \end{array}
       \begin{array}{c} 2 \\ 0 \end{array} \right),\
   \vec{\alpha}_w = \left(
       \begin{array}{c} 0 \\ 1 \end{array} \right)
\]
%\end{center}
%\begin{center}
\[ G_l  = \left(
       \begin{array}{c} 1 \\ 2 \end{array}
       \begin{array}{c} 1 \\ 2 \end{array} \right),\
   \vec{g}_l = \left(
       \begin{array}{c} -1 \\ -3 \end{array} \right),\
   G_r  = \left(
       \begin{array}{c} 1 \\ 0 \end{array}
       \begin{array}{c} 0 \\ 1 \end{array} \right),\
   \vec{g}_r = \left(
       \begin{array}{c} 0 \\ 0 \end{array} \right)
\]
\end{center}
Thus, the initial SCD for statement $s12$ is \\
\centerline{$<N=w, D=<(i, j)>, M=<(2*j, i+1), (i+j-1, 2*i+2*j-3), 
            \perp>, Q= \perp>$}
As an indication of the complexity of a SCD, the structure for this 
communication required 712 bytes to store.


\subsubsection*{Operations on SCD}

Operations, such as intersection, difference and union, on the SCD descriptors
are needed in our analysis. Since the analysis can not guarantee exact
results of the operations, {\em subset} and {\em superset} versions of these
operations are implemented. 
Based on the purpose of the operation, the compiler uses the proper
version to obtain a conservative approximation. These operations,
the union ($\cup$), the intersection ($\cap$) and the difference ($-$), 
are straight-forward extension of the operations on BRSD \cite{callahan88}.
Details can be found in \cite{Yuan96}. 

%Here, we will describe an important
%operation: testing whether a communication $SCD_1=<N_1, R_1, M_1, Q_1>$ is 
%a sub--communication of another communication $SCD_2=<N_2, R_2, M_2, Q_2>$.
%
%\noindent
%{\bf Range testing operations} Ranges are denoted as $l:u:s$, where
%$l$ is the lower bound, $u$ is the upper bound and $s$ is the step.
%Range testing operations check the relation between two ranges. 
%Most commonly used range
%testing operations include the subrange testing, which test whether one
%range is a subrange of another range. This testing is reduced to the
%testing of the relation of the $l$, $s$ and $u$ in the two ranges.
%
%\noindent
%{\bf Subset Mapping testing}. 
%Testing that a relation $M_1$ ($= <s_1, d_1, q_1>$) is a subset of
%another relation $M_2$ ($= <s_2, d_2, q_2>$) is
%done by checking if the equations $s_1 = s_2$ and $d_1 = d_2$ and
%subrange testing $q_1 \subseteq q_2$. The equations 
%$s_1 = s_2$ and $d_1 = d_2$ can easily be solved by 
%treating variables in $M_1$ as constants and 
%variables in $M_2$ as variables. 
%Note that since the elements
%in $s_1$ and $s_2$ are of the form $\alpha*i+\beta$, the equations can
%generally be solved efficiently.
%
%\noindent
%{\bf SCD subset testing}\hspace{0.2in}
%$SCD_1 \subseteq SCD_2 \Longleftrightarrow N_1 = N_2 \wedge
%R_1 \subseteq R2 \wedge M_1 \subseteq M_2 \wedge Q_1 \subseteq Q_2$

\subsubsection{Array data flow analysis for communication optimization}
\label{arraydflow}


Many communication optimization opportunities can be uncovered by
propagating the SCD for a statement globally. For example, if a SCD
can be propagated from a loop body to the loop header without being killed
in the process of propagation, the communication represented by the SCD
can be hoisted out of the loop body, i.e. the communication can be vectorized.
Another example is the redundant communication elimination.
While propagating $SCD_1$,  if $SCD_2$ is encountered such that $SCD_2$ is 
a  subset of the $SCD_1$, then the communication represented by $SCD_2$ can
be subsumed by the communication represented by $SCD_1$ and can be eliminated.
Propagating  SCDs backward can find the latest point to place the 
communication, while propagating SCDs forward can find the last point where
the effect of the communication is destroyed. Both these
two propagations are useful in communication optimization. 
Since forward and backward propagation
are similar, I will only focus on 
backward propagation of SCDs.

I present generic demand driven algorithms to propagate
SCDs through the Interval flow graph representations of
programs. The analysis techniques are
the reverse of the interval-analysis \cite{gupta93}.
Specially, by reversing the information flow associated with program points,
I derive a system of request propagation rules.
The SCD descriptors are propagated 
until they cannot be propagated any further, 
i.e. all the elements in the SCD are
killed. However, in practice, the compiler may choose to 
terminate the propagation prematurely to
save analysis time while there are still elements in SCDs. 
In this case, since the analysis starts from the 
points that contribute to the  optimizations,
the points that are textually close to the starting points, where
most of the optimization oppurtunities present, have been considered.
This gives the demand driven algorithm the ability to trade precision for
time.
In the propagation, at a certain time, only a single interval
is under consideration. Hence, the propagations are logically done in
an acyclic flow graph. During the propagation, a 
SCD may expand when it is propagated out of a loop. When a set of
elements of SCD is killed inside a loop, the set is propagated into the loop
to determine the exact point where the elements are killed. There are 
two types of propagations,
{\em upward} propagation, in which SCDs may need to be 
expanded, and {\em downward} propagation, in which SCDs may need to be 
shrunk. 


The format of a data flow {\em propagation request}
is  $<S, n, [UP|DOWN], level, cnum>$, where S is a SCD, n is a node
in the flow graph, constants $UP$ and $DOWN$ indicate the request is  
upward propagation  or downward propagation, $level$ indicates
at which level is the request and the value $cnum$ 
indicates which child node of 
$n$ has triggered the request. A special value $-1$ for $cnum$ is used as
the indication of the beginning of downward propagation.
The propagation request triggers 
some local actions and causes the propagation of a SCD from the node n. 
The propagation of SCD follows the following rules. It is  assumed that node 
$n$ has $k$ children.

\subsubsection*{Propagation rules}

{\bf RULE 1: upward propagation: regular node}. 
The request on a regular node takes an action based
 on SCD set $S$ and the local
information. It also propagates the information upward.
The request stops when S become empty. The rule is shown in the following 
pseudo code. In the code, functions $action$ and $local$
are depended on the type of optimization to be performed.
The $pred$ function finds all the nodes that are  predecessors in the 
interval flow graph and the 
set $kill_n$ includes all the elements defined in node
$n$. Note that $kill_n$ can be represented as an SCD.

\begin{tabbing}
\hspace{1in}re\=quest($<S_1, n, UP, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, UP, level, k>$) : \\
\hspace{1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{1in}\>action(S, local(n))\\
\hspace{1in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - kill_n, m, UP, level, j>$)\\
\end{tabbing}
A response to requests in a node $n$ 
occurs only when all its successors have been processed. This 
guarantees that in a acyclic flow graph
each node will only be processed once. The side effect is that 
the propagation will not pass beyond a branch point.
A more aggressive scheme can 
propagate a request through a node without checking whether
all its successors are processed. In that scheme, however, a nodes may need 
to be processed multiple times to obtain the final solution.    

{\bf RULE 2: upward propagation: same level loop header node}.
The loop is contained in the current level. The request needs to obtain the
summary information, $K_n$, for the interval, perform the action
based on $S$ and the summary information, propagate the information passed 
the loop and trigger a downward propagation to propagate the information
into the loop nest. Here, 
the summary function $K_n$, summarizes all the elements defined
in the interval. 
It can  be calculated either before hand or in a 
demand driven manner. I describe how to 
calculate the summary in a demand driven manner later. 
Note that a loop header can only have one successor besides the 
entry edge into the loop body. The $cnum$ of the downward request
is set to -1 to indicate that it is the start of the downward propagation.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{1in}\>action(S, $K_n$) \\
\hspace{1in}\>if\=\ ($S - K_n \ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - K_n, m, UP, level, j>$)\\
\hspace{1in}\>if ($S\cap K_n \ne \phi$) then\\
\hspace{1in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$)\\
\end{tabbing}



{\bf RULE 3: upward propagation: lower level loop header node}.

The relative level between the propagation request
and the node can be determined by 
comparing the level in the request and the level of the node. Once a request
reaches the loop header. The request will need to be expanded to be 
propagated in the upper level. At the mean time, this request triggers 
a downward propagation for the set that must stay in the loops. 
Assume that the loop index variables is $i$ with bounds $low$ and $high$.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, UP, level, 1>$): \\
\hspace{1in}\>calculate the summary of loop $n$\\
\hspace{1in}\>outside = $expand(S, i, low:high) - 
                     \cup_{def}expand(def, i, low:high)$\\ 
\hspace{1in}\>inside = $expand(S, i, low:high) \cap 
                     \cup_{def}expand(def, i, low:high)$\\
\hspace{1in}\>if\=\ (outside $\ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<outside, m, UP, level -1, j>$)\\
\hspace{1in}\>if (inside $\ne \phi$) then\\
\hspace{1in}\>\>request($<inside,  n, DOWN, level, -1>$)\\
\end{tabbing}

The variable $outside$ stores the elements that can be propagated out of
the loop, while the variable $inside$ store the elements that are killed
within the loop. The expansion function has the same definition as in 
\cite{gupta93}. For a SCD descriptor S, expand(S, k, low:high) is a 
function which replaces
all single data item references $\alpha*k+\beta$ used in any
array section descriptor D in S by the triple ($\alpha*low+\beta:
\alpha*high+\beta:\alpha$). 
The sets $def$ includes all the definition that are the source of a
flow-dependence.

{\bf RULE 4: downward propagation: lower level loop header node}.

This is the initial downward propagation. The loops index variable, $i$, 
is treated as a constant in the downward propagation. 
Hence, SCDs that are propagated into the loop body 
must be changed to be the initial
available set for iteration $i$, that is, subtract all the variables 
killed in the iteration i+1 to high and propagate the information from the 
tail node  to the head node. This propagation prepares the downward 
propagation into the loop body by shrinking the SCD for each iteration.

\begin{tabbing}
\hspace{1in}qu\=ery($<S, n, UP, level,cnum>$): \\
\hspace{1in}\>if\=\ $(cnum = -1)$ then\\
\hspace{1in}\>\>calculate the summary of loop $n$;\\
\hspace{1in}\>\>request($<S - \cup_{def}expand(def, k, i+1:high), 
                    l, DOWN, level-1, 1>$);\\
\hspace{1in}\>else\\
\hspace{1in}\>\> STOP /* interval processed */\\
\end{tabbing}

{\bf RULE 5: downward propagation: regular node}.
For regular node, the downward propagation is the same as the upward
propagation.

\begin{tabbing}
\hspace{1in}re\=quest($<S_1, n, DOWN, level, 1>$) $\wedge$ ... $\wedge$
            request($<S_k, n, DOWN, level, k>$) : \\
\hspace{1in}\>S = $S_1\cap ...\cap S_k$\\
\hspace{1in}\>action(S, local(n))\\
\hspace{1in}\>if\=\ $(S-kill_n \ne \phi)$ then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - kill_n, m, DOWN, level, j>$)\\
\end{tabbing}


{\bf RULE 6: downward propagation: same level loop header node}.
When downward propagation reaches a loop header(not the loop header
whose body is being processing), it must generate further downward
propagation request to go deeper into the body.

\begin{tabbing}
\hspace{1in}re\=quest($<S, n, DOWN, level, 1>$): \\
\hspace{1in}\>action(S, summary(n)); \\
\hspace{1in}\>if\=\ ($S-K_n \ne \phi$) then\\
\hspace{1in}\>\>fo\=r all $m\in pred(n)$\\
\hspace{1in}\>\>\>Let $n$ be $m$'s $j$th child\\ 
\hspace{1in}\>\>\>request($<S - K_n, m, DOWN, level, j>$);\\
\hspace{1in}\>if\=\ ($S\cap K_n \ne \phi$) then\\
\hspace{1in}\>\>request($<S\cap K_n, n, DOWN, level, -1>$);\\
\end{tabbing}

\subsubsection*{Summary calculation}

\begin{figure}[htbp]
\begin{tabbing}
\hspace{1in}(1)\hspace{0.5in}Su\=mmary\_kill(n)\\
\hspace{1in}(2)\hspace{0.5in}\>$K_{out}(tail)$ = $\phi$\\
\hspace{1in}(3)\hspace{0.5in}\>fo\=r all $m\in T(n)$ and 
                               level(m) = level(n)-1 in backward order\\
\hspace{1in}(4)\hspace{0.5in}\>\>if\=\ m is a loop header then\\
\hspace{1in}(5)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                    K_{in}(s)$\\
\hspace{1in}(6)\hspace{0.5in}\>\>\>$K_{in}(m)$ = summary\_kill(m) $\cup
                                   K_{out}(m)$\\
\hspace{1in}(7)\hspace{0.5in}\>\>else\\ 
\hspace{1in}(8)\hspace{0.5in}\>\>\>$K_{out}(m)$ = $\cup_{s\in succ(m)}
                                   K_{in}(s)$\\
\hspace{1in}(9)\hspace{0.5in}\>\>\>$K_{in}(m)$ = $kill(m) \cup K_{out}(m)$\\
\hspace{1in}(10)\hspace{0.5in}\>return (expand($K_{in}(header)$, i, low:high))\\
\end{tabbing}
\caption{Demand driven summary calculation}
\label{KILL}
\end{figure}

During the request propagation, the summary information of an interval is
needed when a loop header is encountered. In this section, I describe
an algorithm to obtain the summary information in a demand driven manner.
I use the calculation of kill set of the interval as an example. Let 
$kill(i)$ 
be the variables killed in node $i$, $K_{in}$  and $K_{out}$ 
be the variables killed before and after node respectively.
Fig.~\ref{KILL} depicts the demand driven algorithm. The 
algorithm propagates the data flow information from the tail node to the 
header node in the interval using the following data flow equation:\\

\centerline{$K_{out}(n) = \cup_{s\in succ(n)}K_{in}(s)$}
\centerline{$K_{in}(n) = kill(n)\cup K_{out}(n)$}

When inner loop header is
encountered, a recursive call is issued to get the summary information
for the inner interval. Once loop header is reached, the kill set need
to be expanded to be used by the outer loop.

\subsubsection{Implementation}

The analyzer is implemented on top of the Stanford SUIF compiler.
Since I do not have a backend code generator, I am not able to  
evaluate the optimization results in terms of execution time. 
Instead, I developed a communication emulation system,
which takes SCD descriptors as input and emulates the 
communications described by the SCDs in a multiprocessor system. 
The virtual to physical processor mapping is provided
to the emulation system to emulate the communication in physical
processors.
The emulation system provides an interface with C program as  library 
calls whose arguments include all information in a SCD.
A compiler backend automatically generates a library call for each 
SCD remaining in the program. In this way, the communication performance
can be evaluated in the emulation system
by running the program generated by the compiler backend.

The generation of a program used for evaluation is carried out in
the following steps. 
(1) A sequential program is compiled using SUIF frontend, $scc$, 
to generate the SUIF intermediate representation. 
(2) SUIF transformer, $porky$, is used to perform scalar optimizations,
forward propagation, dead code elimination and induction variable
detection. (3) A communication preprocessing phase annotates the
global arrays with data alignment information. (4) The
analyzer is invoked to analyze and optimize the communications required
in the program. After communication optimization, the backend of the
analyzer inserts a library call into the SUIF intermediate representation
for each SCD remaining in the program.  (5) The $s2c$ tool
is used to convert the SUIF intermediate representation into C program, which
is the one used for evaluation.

Six programs are used in the experiments. The first benchmark, 
L18, is the explicit hydrodynamics kernel in livermore loops (loop 18).
The second benchmark, ARTDIF, is a kernel routine obtained from 
HYDRO2D program, 
which is an astrophysical program for the computation of galactical jets
using hydrodynamical Navier Stokes equations. 
The third benchmark, TOMCATV, does the mesh generation with 
Thompson's solver. 
The fourth program, SWIM, 
is the SHALLOW weather prediction program.
The Fifth program, MGRID, is the simple multigrid solver for
computing a three
dimensional potential field. This sixth program, ERHS, is part of the
APPLU program, which is the solver for five coupled 
parabolic/elliptic partial differential equations. The programs, HYDRO2D,
TOMCATV, SWIM, MGRID and APPLU, originally come from SPEC95 benchmark suite.

Table~\ref{analysis} shows  the analysis cost of the
analyzer.  The analyzer, which applies the algorithms on all SCDs in the
programs,  is run on SPARC 5 with 32MB memory. 
Row 2 of the table shows the size of the programs and Row 3 shows the 
number of initial SCDs in the programs.
Row 4 shows the memory space requirement of the analyzer in SCD units. 
The value in bracket
is the maximum number of SCDs in a single node during the analysis. 
In the analyzer the size of the SCDs ranged from 0.6 to about 2 kbytes.
This memory does not include the memory to store the initial and final data 
flow information. By repeatedly using 
the same memory for propagating different
SCDs,  the analyzer requires very small amount of extra memory. Row 5 shows the
cumulative memory requirement, which is the sum of the number of SCDs passing 
through each node. This number is equivalent to the memory requirement of
traditional data flow analysis.
The value in bracket
is the maximum cumulative SCDs in a node.
Row 6 are the ratio between numbers in
row 5 and row 4, which is the ratio of the memory
memory requirements of the traditional method and this method. 
On an average this method reduces the memory requirement
by a factor of 26.
Row 7 gives the raw analysis times and row 8 shows the rate at which
the analyzer operates in units of source $lines/sec$. On an average
the analyzer compiles 166 lines per second for the six programs. 
Row 9 shows the total time,
which includes analysis time and the time to load and store the
SUIF structure, for reference.
This experiment shows that the analyzer is efficient in space and time.

\begin{table}[htbp]
\small
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Program &            L18  & ARTDIF & TOMCATV & SWIM & MGRID & ERHS\\
\hline
size(lines) &         83  &    101 &     190 &  429 &   486 & 1104\\
%\hline
%\# of tree nodes   &  85  &    110 &     337 &  543 &   660 &  958\\
\hline
\# of initial SCDs &  35  &     12 &     108 &   76 &   125 &  403\\
\hline
 memory requirement &  15(1)  &  16(1) & 74(3) &   60(1) &    71(1) &  232 (5)\\
\hline 
accu. memory req.  & 348(39) &  175(24) & 5078(156) &  767(27) &  1166(60) & 
                                                                    6029(173)\\
\hline
memory size ratio  & 23.2 &    10.9&    68.6 & 12.8 &  16.4 & 26.0\\
\hline
analysis time(sec) & 0.60 &   0.27 &    3.57 & 1.82 &  4.40 & 23.25\\
\hline 
lines / sec        & 138  &   374  &      94 & 235  &  110  & 47\\
\hline
total time(sec)    & 1.83 &   1.50 &    7.26 & 6.58 & 12.87 & 37.95\\
\hline
\end{tabular}
\end{center}
\caption{Analysis time}
\label{analysis}
\end{table}

Fig.~\ref{size} shows the performance of redundant communication optimization
of the analyzer. The performance metric is the number of elements in the 
communication. The results are obtained under the assumption that
the programs are executed on a 16 PE system and all arrays use 
the CYCLIC distribution.
% and the PEs are distributed as 16 for 1 dimension arrays, 
%$4\times 4$ for two dimensional arrays, $4\times 2\times 2 $ for three 
%dimensional arrays and $2\times 2\times 2 \times 2 \times 2$ for 4 dimensional
%arrays. 
This experiment is conducted using the test input size provided by
the SPEC95 benchmark for program TOMCATV, SWIM, MGRID, ERHS. Problem 
sizes of $6\times 100$ for L18 and $402\times 160$ for ARTDIF are used.
Some programs, such as MGRID, do not have any
optimization opportunities for redundant communication optimization. 
Others, such as the TOMCATV program, present many opportunities 
for optimizations. On an average, for the six programs, 30.5\% of
the total communication elements are reduced by the analyzer. This indicates 
that global communication optimization results in large performance gain and
the analyzer is effective in finding optimization opportunities.
%The performance evaluation for message vectorization optimization
%will be included in the full paper.

\begin{figure}
\centerline{\psfig{figure=fig/size.eps,width=5in}}
\caption{The reduction of the number of array elements in communications}
\label{size}
\end{figure}

%% TO BE WRITE LATER

This section introduced a communication descriptor that can represent
logical communication patterns in a program, describes the propagation rules
used in an array data flow analyzer for communication optimization and reports
the experimence with the analyzer. This tool is able to generate logical 
communication patterns in both optimized and non--optimized codes for a
program. Algorithms to derive physical communiation patterns from the
logical communication descriptors remain to be developed.
In the next section,  connection
scheduling algorithms for handling static patterns in compiled communication
are presented.


\subsection{Connection scheduling algorithms for compiled communication}

Compiled communication uses off--line algorithms to perform
connection scheduling. This section presents the connection scheduling 
algorithms and their performance evaluation. These algorithms assume
a torus topology. However, the foundamental principles of the algorithms
can be extended to other topologies. 

For a given network, a set of connections that do not share any link is called
a configuration. In an optical TDM network with path multiplexing, 
multiple configurations can be supported simultaneously. Specifically, for 
a network with multiplexing degree $K$, $K$ configurations can be 
established concurrently. Thus, for a given communication pattern,
realizing the communication 
pattern with a minimum multiplexing degree is equivalent to  determining the
minimum number of configurations that contain all the connections in 
the pattern. Next,
 some definitions will be presented to formally state 
the problem of
connection scheduling. A connection request from a source $s$ 
to a destination $d$ is denoted as $(s, d)$.

\begin{description}
\item A pair of connection requests $(s_1, d_1)$ and $(s_2, d_2)$ are said
to {\bf conflict}, if they cannot be simultaneously established because
they are using the same link.

\item A {\bf configuration} is a set of connection requests
$\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$ such that
no requests in the set conflict.

\item Given a set of communication requests  
$R = \{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$,
the set $MC =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} is a
{\bf minimal configuration set} for $R$ iff: \\
$\bullet$ 
each $C_i \in MC$ is a configuration and each request 
$(s_{i}, d_{i}) \in R$ 
is contained in exactly one configuration in $MC$; and \\
$\bullet$
each pair of configurations $C_i,C_j \in MC$ contain requests 
$(s_i, d_i) \in C_i$ and $(s_j, d_j) \in C_j$ such that
$(s_i, d_i)$ conflicts with $(s_j, d_j)$.
\end{description}

Thus, the goal of connection scheduling heuristics is to compute a
minimal configuration set for a given request set $R$.
Next I present three such heuristics.

%
\subsubsection{Greedy algorithm}
%
In the greedy algorithm, a configuration is created by repeatedly 
putting connections into the configuration until no additional 
connection can be established in that configuration.
If additional requests remain, another configuration is created
and this process is repeated till all requests have been processed.
This algorithm is an modification of an algorithm proposed in \cite{Qiao94}.
The algorithm is shown in Fig.~\ref{SIMPLE}. The time complexity of
the algorithm is $O(|R|\times max_i(|C_{i}|)\times K)$, where $|R|$ 
is the number of the requests, $|C_{i}|$ is the number of connections
in configuration $C_{i}$ and $K$ is the number of configurations
generated.

\begin{figure}[htmb]
%\small
\begin{tabbing}
\hspace{1.0in}(1)\hspace{0.6in}MC = $\phi$, k = 1\\
\hspace{1.0in}(2)\hspace{0.6in}{\bf re}\={\bf peat}\\
\hspace{1.0in}(3)\hspace{0.6in}\> $C_{k} = \phi$\\
\hspace{1.0in}(4)\hspace{0.6in}\>{\bf for}\= {\bf each} $(s_{i}, d_{i}) \in R$\\
\hspace{1.0in}(5)\hspace{0.6in}\>\>{\bf if}\=\ $(s_{i}, d_{i})$ does 
                                   not conflict with any 
                                   connection in $C_{k}$ {\bf then}\\
\hspace{1.0in}(6)\hspace{0.6in}\>\>\>$C_{k} = C_{k} \bigcup$ { $(s_{i}, d_{i})$ }\\
\hspace{1.0in}(7)\hspace{0.6in}\>\>\>R = R - { $(s_{i}, d_{i})$ }\\
\hspace{1.0in}(8)\hspace{0.6in}\>\>{\bf end if}\\
\hspace{1.0in}(9)\hspace{0.6in}\>{\bf end for}\\
\hspace{1.0in}(10)\hspace{0.6in}\>MC = MC $\bigcup$ { $C_{k}$ }\\
\hspace{1.0in}(11)\hspace{0.6in}{\bf until} $R = \phi$\\
\end{tabbing}
\normalsize
\caption{The greedy algorithm.}
\label{SIMPLE}
\end{figure}

For example consider the linearly connected nodes shown in Fig.~\ref{EXAM}. 
The result for applying the greedy algorithm to schedule connection requests 
set \{(0, 2), (1, 3),(3, 4), (2, 4)\} is shown in Fig.~\ref{EXAM}(a). 
In this case, (0, 2) will be in time slot 1, (1, 3) in time slot 2, (3, 4) 
in time slot 1 and (2, 4) in time slot 3. 
Therefore, multiplexing degree 3 is needed to establish the paths for the 
four connections.  However,  as shown in Fig.~\ref{EXAM} (b), 
the optimal scheduling for the four connections, which can be obtained
by considering the connection in different order, is to schedule (0, 2) in 
slot 1, (1, 3) in slot 2, (3, 4) in slot 2 and (2, 4) in slot 1. 
The second assignment only use 2 time slots to establish all the connections. 

\begin{figure}[htbp]
\begin{center}
\begin{picture}(0,0)%
\special{psfile=/afs/cs.pitt.edu/usr0/xyuan/research/paper/write/962SC96/fig/961.3.pstex}%
\end{picture}%
\setlength{\unitlength}{0.0050in}%
\begin{picture}(920,120)(75,640)
\end{picture}

\end{center}
\caption{Scheduling requests (0, 2), (1, 3),(3, 4), (2, 4)}
\label{EXAM}
\end{figure}


\subsubsection{Coloring algorithm}

The greedy algorithm  processes the requests in an arbitrary order.
In this section, I will describe an algorithm that applies a heuristic 
to determine the order in which the process the connection requests.
The heuristic assigns higher priorities to connection requests with fewer
conflicts. By giving the requests with less conflicts higher priorities, 
each configuration is likely to accommodate more requests and thus the
multiplexing degree needed for the patterns is likely to decrease. 

The problem of computing the minimal configuration set is formalized
as a graph coloring problem. A coloring of a graph is an assignment of 
a color to each node of the graph in such a manner that no two nodes 
connected by an edge have the same color. A conflict graph for a set of
requests is built in the following manner, (1) 
each node in the graph 
corresponds to a connection request and (2) an edge
is introduced between two nodes if the requestes represented by the 
two nodes are conflicted.
As stated by the theorem given below,
the number of colors used to color the graph is the number of 
configurations needed to handle the connection requests. 

\begin{description}
\item
{\bf Theorem:} Let $R=\{(s_{1}, d_{1}),(s_{2}, d_{2}),...,(s_{m}, d_{m})\}$
be the set of requests and $G = (V, E)$ be the conflict graph for $R$. 
There exists a configuration set $M = \{C_{1}, C_{2}, ..., C_{t}\}$
for $R$ if and only if $G$ can be colored with $t$ colors.
\end{description}

%Prove: ($\Rightarrow$) Assuming R has 
%configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. Let 
%$(s_{i}$, $d_{i}) \in C_{k}$, node $n_{i}$ can be colored by color $k$. 
%Therefore, there are totally $t$ colors in the graph. Now, we need to prove
%that for any two node $n_{i}$, $n_{j}$ such that $(i, j) \in E$, the two nodes
%are colored by different color. By the construction of conflict graph,
%if $(i, j) \in E$, node $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ share same links,
%hence, by the construction of configuration, $(s_{i}, d_{i})$ and
%$(s_{j}, d_{j})$  is in different configuration, thus $n_{i}$ and $n_{j}$ is
%colored by different colors. Hence, G can be colored by $t$ colors.
%
%($\Leftarrow$) Assuming G can be colored by $t$ colors. Let 
%$C_{i}$ = {$( s_{j}, d_{j})$ : $n_{j}$ is colored by color j}, 
%$M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \}. To prove 
%M is a configuration for R, we need to prove 1) for any $(s_{i}, d_{i}) \in R$,
%there exists a $C_{k}$ such that $(s_{i}, d_{i}) \in C_{k}$, and 2) $C_{k}$
% must
%be a configuration. The first condition is trivial. Now, let us consider
%the second condition. Let $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$ belong to 
%$C_{k}$, by the construction the G, $(s_{i}, d_{i})$ and $(s_{j}, d_{j})$
%do not share any links. Therefore, $C_{k}$ is a configuration. Hence, there
%exist configuration $M =$ \{$C_{1}$, $C_{2}$, ..., $C_{t}$ \} for R. $\Box$
%
%\begin{description}
%\item
%{\bf Corollary:} The optimal multiplexing degree for establishing 
%connections in $R$ is equivalent to the minimum number colors to 
%color graph $G$.
%\end{description}


Thus, our 
coloring algorithm attempts to minimize the number of colors used in 
coloring the graph. Since the coloring problem is known to be NP-complete, 
a heuristic is used for graph coloring. Our heuristic determines the order 
in which nodes are colored using the node priorities.
The algorithm is summarized in Fig~\ref{COLOR}. It should be noted that
after a node is colored, our algorithm updates the priorities of uncolored 
nodes. This is because in computing the degree of an uncolored node, 
only  the edges that connect the node to other uncolored nodes are 
considered. 
The algorithm finds a solution in linear time (with respect to the 
size of the conflict graph). The time complexity of the algorithm is 
$O(|R|^2\times max_i(|C_{i}|)\times K)$, where $|R|$ is the number of the 
requests, $|C_{i}|$ is the number of requests in configuration $C_{i}$ and 
$K$ is the total number of configurations generated.


\begin{figure}[htbp]
%\small
\begin{tabbing}
\hspace{1.0in}(1)\hspace{0.6in} Construct conflict graph G = (V, E)\\
\hspace{1.0in}(2)\hspace{0.6in} Calculate the priority for each node\\
\hspace{1.0in}(3)\hspace{0.6in} MC = $\phi$, k = 1\\
\hspace{1.0in}(4)\hspace{0.6in} NCSET = V\\
\hspace{1.0in}(5)\hspace{0.6in} {\bf re}\={\bf peat}\\
\hspace{1.0in}(6)\hspace{0.6in} \>Sort NCSET by priority\\
\hspace{1.0in}(7)\hspace{0.6in} \> WORK = NCSET\\
\hspace{1.0in}(8)\hspace{0.6in} \> $C_{k} = \phi$\\
\hspace{1.0in}(9)\hspace{0.6in} \>{\bf wh}\={\bf ile} (WORK $\ne \phi$)\\
\hspace{1.0in}(10)\hspace{0.6in} \>\> Let $n_{f}$ be the first 
                                      element in WORK\\
\hspace{1.0in}(11)\hspace{0.6in} \>\>$C_{k} = C_{k} \bigcup \{<s_{f}, d_{f}>\}$\\
\hspace{1.0in}(12)\hspace{0.6in} \>\>NCSET = NCSET $- \{n_{f}\}$\\
\hspace{1.0in}(13)\hspace{0.6in} \>\>{\bf fo}\={\bf r} {\bf each}  $n_{i} \in NCSET$ 
                                      and $(f, i) \in E$ {\bf do} \\
\hspace{1.0in}(14)\hspace{0.6in} \>\>\> update the priority of $n_{i}$\\
\hspace{1.0in}(15)\hspace{0.6in} \>\>\> WORK = WORK - $\{n_{i}\}$\\
\hspace{1.0in}(16)\hspace{0.6in} \>\>{\bf end for}\\
\hspace{1.0in}(17)\hspace{0.6in} \>{\bf end while}\\
\hspace{1.0in}(18)\hspace{0.6in} \>MC = MC + $\{C_{k}\}$\\
\hspace{1.0in}(19)\hspace{0.6in} {\bf until} NCSET = $\phi$
\end{tabbing}
\normalsize
\caption{The graph coloring heuristic.}
\label{COLOR}
\end{figure}

For torus and mesh networks, a suitable choice for priority for a
connection request is the ratio of the number of links in the path 
from the source to the destination and the degree of the node 
corresponding to the request in $G$. 
Applying the coloring algorithm to the example in Fig.~\ref{EXAM},
in the first iteration, the request is reordered as 
$\{(0, 2), (1, 3), (2, 4), (3, 4)\}$ and connections (0, 2), (2, 4) will be
put in time slot 1. In the second iteration, connections (1, 3), (3, 4) are
put in time slot 2. Hence, applying the 
coloring algorithm will use 2 time slots
to accommodate the requests.


\subsubsection{Ordered AAPC algorithm}

The graph coloring algorithm has better performance than the greedy heuristic.
However, for dense communication patterns the heuristics cannot guarantee that
the multiplexing degree found would be bounded by the minimum multiplexing 
degree needed to realize the all-to-all pattern. The algorithm described in 
this section targets dense communication patterns. By grouping the connection
requests
in a more organized manner, better performance can be achieved for dense 
communication.

The worst case of arbitrary communication is the {\em all-to-all personalized 
communication} (AAPC)  where each node sends a message to every 
other node in the system. Any communication pattern can be embedded in AAPC. 
Many algorithms \cite{Hinrichs94,Horie91} have been designed to 
perform AAPC efficiently for different topologies.
Among these algorithms, the ones that are of 
interests to us are the phased AAPC algorithms, in which the AAPC connections 
are partitioned into contention--free phases. A phase in this kind of AAPC 
corresponds to a configuration. Some phased AAPC algorithms are optimal in
that every link is used in each phase and every connection follows the
shortest path. Since all the connections in each AAPC phase are contention--free,
they form a configuration that uses all the links in the system. 
Each phase in the phased AAPC communication forms an {\em AAPC configuration}.
The set of {\em AAPC configurations} for AAPC communication pattern is 
called {\em AAPC configurations set}. 
The following theorem states the property 
of connection scheduling using AAPC phases.

\begin{description}
\item {\bf Theorem: } Let $R =
\{(s_{1}, d_{1}) , (s_{2}, d_{2}), ..., (s_{m}, d_{m})\}$be the 
set of requests, if $R$ can be partitioned into $K$ phases 
$P_1 = \{(s_{1}, d_{1}), ... , (s_{i_{1}}, d_{i_{1}})\}$, 
$P_2 = \{(s_{i_{1} + 1}, d_{i_{1} + 1}), ... , (s_{i_{2}}, d_{i_{2}})\}$,
... ,\\
 $P_K = \{(s_{i_{K-1} + 1}, d_{i_{K-1} + 1}), ... , 
(s_{i_{K}}, d_{i_{K}})\}$, such that $P_i$, $ 1 \le i \le K$, is a subset
of an AAPC configuration. Using the greedy algorithm to schedule the
connections results in multiplexing degree less than or equal to K.
\end{description}

The theorem states that if the connection requests
are reordered by the AAPC phases,
at most all AAPC 
phases are needed to realize arbitrary pattern using the
greedy scheduling algorithm. For example, following the algorithms in 
\cite{Hinrichs94}, $N^3/8$ phases are needed for a $N\times N$ torus. 
Therefore, in a $N\times N$ torus, $N^3/8$ degree is enough to satisfy
any communication pattern.

To obtain better performance on dense communication patterns, it is 
better to keep the connections in their AAPC format as much as possible. 
It is therefore better to schedule the phases with higher link 
utilization first. This heuristic is used in the ordered AAPC algorithm.
In ordered AAPC algorithm, the rank of the AAPC phases is calculated so 
that the phase that has higher utilization has higher rank. The phases 
are then scheduled according to their ranks. The algorithm is depicted in 
Figure~\ref{ORDAAPC}. The time complexity of this algorithm is
$O(|R|(lg(|R|) + max_i(|C_{i}|)\times K))$, where $|R|$ is the number of 
the requests, $|C_{i}|$ is the number of requests in configuration
$C_{i}$ and $K$ is the number of configurations needed. The advantage 
of this algorithm is that for this algorithm the multiplexing degree 
is bounded by $N^3/8$. Thus, in situations where the greedy or coloring
heuristics fail to meet this bound, AAPC can be used.  

\begin{figure}[ht]
%\small
\begin{tabbing}
\hspace{1in}(1)\hspace{0.6in}PhaseRank[*] = 0\\
\hspace{1in}(2)\hspace{0.6in}{\bf for}\= $(s_{i}, d_{i}) \in R$ {\bf do}\\
\hspace{1in}(3)\hspace{0.6in}\>let $(s_{i}, d_{i}) \in A_{k}$\\
\hspace{1in}(4)\hspace{0.6in}\>PhaseRank[k] = PhaseRank[k] + length($(s_{i}, d_{i})$)\\
\hspace{1in}(5)\hspace{0.6in}{\bf end for}\\
\hspace{1in}(6)\hspace{0.6in}sort phase according to PhaseRank\\
\hspace{1in}(7)\hspace{0.6in}Reorder R according the sorted phases.\\
\hspace{1in}(8)\hspace{0.6in}call greedy algorithm\\
\end{tabbing}
\caption{Ordered AAPC scheduling algorithm}
\label{ORDAAPC}
\normalsize
\end{figure}

\subsubsection{Performance for the scheduling algorithms}

In this section,  the performance of the connection scheduling
algorithms on $8\times 8$ torus topology is studied. 
The performances of the algorithms 
are evaluated using randomly generated communication patterns, patterns
encountered during data redistribution, and some frequently used 
communication patterns. The metric used to compare the algorithms is the 
multiplexing degree needed to establish the connections.
It should be noted that a dynamic scheduling algorithm will not perform
better than the greedy algorithm since it must establish the connections 
by considering the requests in the order that they arrive. 

A {\em random communication pattern} consists of a certain number of 
random connection  requests. A random connection request is obtained
by randomly generating a source and a destination. Uniform probability
distribution is used to generate the randomly sources and destinations.
The {\em data redistribution communication patterns} are obtained by
considering the communication results from array redistribution. In this
study,   data redistributions of a 3D array are considered. The array
has block--cyclic distribution in each dimension. The distribution of a
dimension can be specified by the block size and the number of processors
in the dimension.  A distribution is denoted  as {\em p:block(s)}, where
$p$ is the number of processors in the distribution and $s$ is the block size.
When the distribution of an
array is changed (which may results from the changing of the value $p$ or 
$s$), communication may be needed. 
Many programming
languages for supercomputers, such as CRAFT FORTRAN, allow an array to be
redistributed within the program. 

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
number of  & greedy  & coloring & AAPC & combined &improvement\\
connections. & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
100  & 7.0  & 6.7 & 6.9 & 6.6 & 6.3\%\\
\hline
400 & 16.5  & 16.1 & 16.5 & 15.9 & 3.8\%\\
\hline
800 & 27.2  & 25.9 & 26.5 & 25.6 & 6.3\%\\
\hline 
1200 & 36.3 & 34.5 & 35.3 & 34.2 & 6.1\%\\
\hline
1600 & 45.0  & 43.5 & 43.4 & 42.8 & 5.1\%\\
\hline
2000 & 53.4  & 50.4 & 50.4 & 49.7 & 7.4\%\\
\hline
2400 & 60.8  & 57.5 & 57.4 & 56.7 & 7.2\%\\
\hline
2800 & 68.8  & 64.4 & 62.4 & 62.4 & 10.2\%\\
\hline
3200 & 76.3  & 70.8 & 64 & 64 & 19.2\%\\
\hline
3600 & 83.9  & 76.8 & 64 & 64 & 31.1\%\\
\hline
4000 & 91.6  & 83 & 64 & 64 & 43.1\%\\
\hline
\end{tabular}
\end{center}
\caption{Performance for random patterns}
\label{RANDOM}
\end{table}



Table~\ref{RANDOM} shows the multiplexing degree required to establish
connections for random communication patterns using the algorithms
presented.
The results in each row are the averages obtained from scheduling 100 different
randomly generated patterns with the specific number of connections.
The results in the column labeled {\em combined algorithm} are obtained by using 
the minimum of  the coloring algorithm and
the AAPC algorithm results.
Note that in compiled communication, more time can be spent
to obtain better runtime network utilization. Hence,  the
combined algorithm can be used to obtain better result by the compiler. The 
percentage  improvement shown in the sixth column
is achieved by the combined algorithm over the
dynamic scheduling. 
It is  observed that the coloring algorithm is always
better than the greedy  algorithm
and the AAPC algorithm is better than 
the other algorithms when the communication is dense. 
It can be seen that for sparse random patterns (100 - 2400
connections), the 
improvement range varies from 3.8\% to 7.2\%. Larger improvement
results for dense communication.  For example, the combined algorithm
uses 43.1\% less multiplexing degree than that of the greedy algorithm
for all--to--all pattern. 
This result confirms the result in \cite{Hinrichs94} that
it is desirable to use compiled communication for dense communication.


\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
No. of  & No. of  & greedy   & coloring & AAPC & combined &improvement\\
connections & patterns & algorithm & algorithm & algorithm & algorithm 
& percentage\\
\hline
0 - 100 & 34  & 1.2 &  1.2 & 1.2 & 1.2 & 0.0\%\\
\hline
101 - 200 & 50 & 5.9 & 4.9 & 4.8 & 4.6 & 28.3\%\\
\hline
200 - 400 & 54 & 10.6 &  9.7 & 10.0 &  9.5 & 11.6\%\\
\hline 
401 - 800 & 105 & 17.7 & 15.9 & 16.0& 15.5 & 14.2\%\\
\hline
801 - 1200 & 122 & 31.7 & 28.7 & 28.6 & 27.6 & 14.9\%\\
\hline
1201 - 1600 & 0  & 0      & 0    & 0    &0     &    0\%\\
\hline
1601 - 2000 & 15 & 46.3 &  42.8 & 35.1 & 35.1 & 31.9\%\\
\hline
2001 - 2400 & 77 & 55.5 &  51.5 & 51.9 & 50.4 & 10.1\%\\
\hline
2401 - 4031 & 0  & 0       & 0    & 0     &   0   &  0\% \\
\hline
4032     & 43 & 92  & 83 & 64 &  64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for data distribution patterns}
\label{REDIST}
\end{table}



To obtain more realistic results,  the performance is also evaluated using
the communication patterns for data redistribution and some
frequently used communication patterns.
Table~\ref{REDIST} shows the performance of the algorithms for data 
redistribution patterns. The communication patterns
 are obtained by extracting from the communication resulted from 
the random data redistribution of a 3D array of size
 $64 \times64 \times 64$. 
The  random data redistribution is created by randomly generating
the source data distribution and
the destination data distribution with regard to
the number of processors allocated to each dimension and the block size
in each dimension. Precautions are taken to make sure that the 
total processor number is 64 and the block size is not too large so that
some processors do not contain any part of the array.
The table lists the results for 500 random data redistributions. The first
column lists the range of the number of connection requests in each pattern.
The second column list the number of data redistrictions whose number of 
connection request fell into the range. For example, the second column in the
last row indicates that among the 500 random data redistributions, 43
results in 4032 connection requests. The third, fourth, fifth and sixth column
list the multiplexing degree required by the greedy algorithm, the coloring
algorithm, the AAPC algorithm and the 
combined algorithm respectively. The seventh 
column list the improve ratio by the combined algorithm over the greedy
algorithm.
The result shows that the 
 multiplexing degree required to establish connections resulting
from data redistribution is less than the random communication patterns. 
For the data redistribution pattern, the improvement ratio obtained by using
the combined algorithm  ranges from
10.1\% to 31.9\%, which is larger than the improvement ratio for the random 
communication patterns.

\begin{table}[htbp]
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Pattern & No. of conn. & greedy  & coloring & AAPC & comb & ratio\\
\hline
ring    & 128 &  3  &  2 & 2  &  2   &   50\%\\
\hline
nearest neighbor & 256 & 6  & 4 & 4  &  4 & 50\%   \\
\hline
hypercube & 384 &  9 & 7 & 8 &  7 & 28.6\%\\
\hline
shuffle--exchange & 126 &  6& 4 & 5  & 4 & 50\% \\
\hline 
all--to--all & 4032 &  92 & 83 & 64 & 64 & 43.8\% \\
\hline
\end{tabular}
\end{center}
\caption{Performance for frequently used patterns}
\label{FUSED}
\end{table}

Table~\ref{FUSED} shows the performance for some
frequently used communication patterns. In the ring and the
nearest neighbor patterns, no conflicts arise in the links. 
However, there are conflicts in the communication switches.
The performance gain is higher for these 
specific patterns when the combined algorithm is used.



\section{Conclusion and remaining work}

The advances of architectures that exploit massive parallelism has
created new requirements for interconnection networks. While optical networks
potentially offer large bandwidths, the slow network control has hindered the
full exploitation of the large bandwidth available in fiber optics. This
research 
proposes two methods to address this problem: (1)  efficient distributed
path reservation protocols that reduce control overhead via protocol design
and (2) compiled communication that reduces control overhead by shifting
the control overhead into compile time processing.

A summary of  what has been done and what remains to be
done in this research is listed below.

\begin{itemize}
\item Dynamic communication, work done.
  \begin{itemize}
    \item Various distributed path reservation algorithms have been
          developed. The algorithms assume deterministic routing. A 
          network simulator has been
          developed to evaluate the algorithms. The performance
          of the protocols and the impact of system parameters on
          the protocols have been studied. \cite{yuan97}
  \end{itemize}

\item Dynamic communication, work remaining.
  \begin{itemize}
    \item   Distributed path reservation schemes with adaptive routing and 
      their performance study remain to be done.
  \end{itemize}

\item Compiled communication, work done.
  \begin{itemize}
    \item A communication descriptor which can describe
      logical communication patterns
      in a program has been
      developed and a data flow analyzer for communication
      optimizations using that descriptor has been designed and implemented.
      \cite{Yuan96a}
    \item Connection scheduling algorithms for torus topologies have been
      developed and evaluated. \cite{Yuan96}
    \item A simulator which simulates compiled communication for the 
      communication
      patterns that are known at compile time has been developed.
  \end{itemize}
\item Compiled communication, work remaining.
  \begin{itemize}
    \item Algorithms to derive physical communication patterns from the
      communication descriptor are needed.  
    \item Interprocedural data flow analysis for communication optimization
      needs to be done. 
      Our previous work performs message vectorization and redundant 
      communication elimination within each procedure. This framework
      can be extended to incorporate more optimizations and to the
      interprocedural analysis.
    \item Efficient logical topologies for the communication patterns that 
          are unknown at compile time needs to be designed and evaluated.
    \item A network simulator which simulates multi--hop communication is to be
      designed to study the performance of the logical topologies. 
    \item Experiments to study the performance of compiled communication is 
     to be carried out.
  \end{itemize}
\item Dynamic communication versus compiled communication.
  A comparison between these two control mechanisms is to be carried out.
\end{itemize}



The study of distributed path reservation protocols will provide  general
guidelines for designing optical interconnection networks
 with dynamic network control.
The study of compiled communication will address issues involved when applying
compiled communication and give insights into the advantages and 
the limitations of compiled communication. Some results of this research
can also be applied
to electronic networks, since statically managing network resources
can also improve communication performance in those networks. In addition,
other factors  that affect the communication performance will
also be studied and understood in this research.















\chapter{Dynamic single--hop communication}
\label{single}

This chapter discusses the path reservation protocols for dynamic 
single--hop communication. Two types of 
distributed path reservation protocols, the {\em forward path reservation
protocols} and the {\em backward path reservation protocols},  
 have been designed for 
point--to--point optical TDM networks. A 
network simulator that simulates all the protocols has been developed 
and has been used to 
study the performance of the two types of protocols and to evaluate
the impact of system parameters such as the control packet processing
time and the message size on the protocols.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/singleexam.eps,width=4.5in}}
\caption{An optical network with distributed control.}
\label{singleexam}
\end{figure}

In order to support a distributed control mechanism for connection
establishment, it is assumed that in addition to the optical data network,
there is a logical {\em shadow network} through which
control messages are communicated. 
The shadow network has the same physical topology as the data network.
The traffic on the shadow network consists of small control packets
and thus is much lighter than the traffic on the data network. 
The shadow network  operates in packet switching mode; routers at 
intermediate nodes examine control packets and update local bookkeeping
information and switch states accordingly. 
The shadow network can be implemented as an 
electronic network or alternatively a virtual channel on the data network
can be reserved exclusively for exchanging control messages.
Figure~\ref{singleexam} shows the network architecture.
A virtual channel in the optical data network corresponds to a time slot. 
It is  also assumed that a node can send or receive messages through
different virtual channels simultaneously. 


A path reservation protocol ensures that the path from a source node
to a destination node is reserved before the connection is used. A path 
includes the virtual channels on the links that form the connection, the
transmitter at the source node and the receiver at the destination node.
Reserving the transmitter and the receiver is the same as reserving a
virtual channel on the link from a node to the switch attached to that
node. Hence, only the reservation of virtual channels on links forming a
connection with path multiplexing will be considered.
There are many options available with respect to different aspects of the 
path reservation mechanisms. These are discussed next.

\begin{itemize}

%\noindent
%$\bullet$
\item {\em Forward reservation} versus {\em backward reservation}.
Locking mechanisms are needed by the distributed path reservation
protocols  to ensure the exclusive usage of a virtual channel 
for a connection. This variation characterizes the timing at which
the protocols  perform the locking.
Under forward reservation, virtual channels are locked 
by a control message that travels from
the source node to the destination node.
Under backward reservation, a control message travels to the
destination to probe the path, then virtual channels that are found to be
available are locked by another
control message which travels from the destination node to the source node.

%\noindent 
%$\bullet$
\item {\em Dropping} versus {\em holding}. This variation characterizes
the behavior of the protocol when it 
determines that a connection establishment does not progress.
Under the  dropping approach, once the protocol
determines that  the establishment of a connection is not progressing,
it releases the virtual channels  locked on the partially established
path and informs the source node that the reservation has failed.
Under the holding approach, when the protocol determines
that  the establishment of a connection is not progressing,
it keeps the virtual channels  on the partially established path locked for
some period of time, hoping that during this period, the reservation
will progress. If, after this timeout period, the reservation still does not
progress, the partial path is then released and the
source node is informed of the failure.
Dropping can be viewed as holding with holding time equal to zero.

%\noindent 
%$\bullet$
\item {\em Aggressive} reservation versus {\em conservative} reservation. This
variation characterizes the protocol's treatment of each reservation. Under
the aggressive reservation, the protocol tries to establish a connection
by locking as many virtual channels as possible during the reservation process.
Only one of the locked channels is then used for the connection, while the
others are released.
Under the  conservative reservation approach, the protocol
locks only one virtual channel during the reservation process.

\end{itemize}

\subsection*{Deadlock}

Deadlock in the control network can arise from two sources.
First, with limited number of buffers, a request loop can be formed within the
control network.
Second, deadlock can occur when a request is holding (locking)
virtual channels on some links while requesting other channels on other
links.
This second source of deadlock can be avoided by the dropping or holding mechanisms
described above.
Specifically, a request will give up all the locked channels if 
it does not progress within a certain timeout period.

Many deadlock avoidance or deadlock prevention techniques for 
packet switching networks proposed in the literature \cite{Dally87} 
can be used to deal with deadlock within the control network (the
first source of deadlock).
Moreover, the control network is under light traffic, and
each control message consists of only a single packet of small size 
(4 bytes). Hence, it is feasible to provide a large number of buffers in each 
router to reduce or eliminate the chances of deadlocks.

\subsection*{States of virtual channels}

The control network router at each node maintains a state for each
virtual channel on links connected to the router. For forward reservation,
the control router maintains the states for the outgoing links.
% while
% in backward reservation, the control router maintains the states
% for the incoming links. 
As discussed later, this enables the router to have the information
needed for reserving virtual channels and updating the switch states.
A virtual channel, $V$, on link $L$, can be in one of the following states:

\begin{itemize}
\item $AVAIL$: indicates that the virtual channel $V$ on link $L$
is available and can be used to establish a new connection,
\item $LOCK$: 
indicates that $V$ is locked by some request in the process of establishing
a connection.
\item $BUSY$: indicates that $V$
is being used by some established connection to transmit data.
\end{itemize}

For a link, $L$, the set of virtual channels that are in the $AVAIL$ state is
denoted as $Avail(L)$. When a virtual channel, $V$, is not in $Avail(L)$,
an additional field, $CID$, is maintained to identify the connection request
locking  $V$, if $V$ is in the $LOCK$ state, or the connection using $V$, if $V$
is in the $BUSY$ state.

\section{Forward reservation schemes}

In the connection establishment protocols,
each connection request is assigned a unique identifier, $id$, which
consists of the identifier of the source node and a serial number
issued by that node. 
Each control message related to the establishment of a connection carries its
$id$, which becomes the identifier of the connection when it is successfully
established. It is this $id$ that is maintained in the $CID$ field of
locked or busy virtual channels on links.
Four types of packets are used in the forward reservation
 protocols to establish a connection.

\begin{itemize}

%\noindent 
%$\bullet$
\item {\em Reservation packets} ($RES$), used to reserve virtual channels.
In addition to the connection $id$, a $RES$ packet contains a bit vector,
$cset$, of size equal to the number of virtual channels in each link.
The bit vector $cset$ is used to keep track of the set of virtual channels 
that can be used to satisfy the connection request carried by $RES$.
These virtual channels are locked
at intermediate nodes while the $RES$ message
progresses towards the destination node. The switch
states are also set to connect the locked channels on the input and output links.

%\noindent 
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$), used to inform source nodes of the
success of connection requests.
An $ACK$ packet contains a $channel$ field which indicates the virtual
channel selected for the connection.
As an $ACK$ packet travels from the destination to the source, it changes
the state of the virtual channel 
selected for the connection to $BUSY$, and unlocks
(changes from $LOCK$ to $AVAIL$)
all other virtual channels that were locked by the corresponding $RES$ packet.

%\noindent 
%$\bullet$
\item {\em Fail or Negative ack packets} ($FAIL/NACK$), used to inform source
nodes of the failure of connection requests. While traveling back to the source
node, a $FAIL/NACK$ packet unlocks all virtual channels that were locked by the
corresponding $RES$ packet.
 
%\noindent 
%$\bullet$
\item {\em Release packets} ($REL$),  used to release connections.
A $REL$ packet traveling from a source to a destination changes the
state of the virtual channel
reserved for that connection from $BUSY$ to $AVAIL$.

\end{itemize}

The protocols require that control packets from a destination, $d$, to a source, $s$,
follow the same paths (in opposite directions) as packets from $s$
to $d$.
The fields of a packet will be denoted by $packet.field$.
For example, $RES.id$ denotes the $id$ field of the $RES$ packet.

The forward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $RES$ packet with $RES.cset$ set to the
virtual channels that the node may use. This message is then routed to the
destination. When an intermediate node receives the $RES$ packet, 
it determines the next outgoing link, $L$, on the path to the destination, and
updates $RES.cset $ to $ RES.cset \cap Avail(L)$.
If the resulting $RES.cset$ is empty,
the connection cannot be established
 and a $FAIL/NACK$ message is sent back to the
source node. The source node will retransmit the request after some
period of time.
This process of failed reservation is shown in Figure~\ref{FORWARD}(a). 
Note that if $Avail(L)$ is represented by a bit-vector, then
$RES.cset \cap Avail(L)$ is a bit-wise "$AND$" operation.

\begin{figure}[htp]
\centerline{\psfig{figure=fig/forward.pstex,height=2.2in}}
\caption{Control messages in forward reservation}
\label{FORWARD}
\end{figure}

If the resulting $RES.cset$ is not empty, the router reserves all the 
virtual channels in $RES.cset$ on link $L$ by changing their states to $LOCK$
and updating $Avail(L)$.
The router will then set the switch state to connect the virtual channels in the
resulting $RES.cset$ of the corresponding incoming and outgoing links.
Maintaining the states of outgoing links is sufficient for these two
tasks.
The $RES$ message is then forwarded to the next node on the path to the destination.
This way,
as $RES$ approaches the destination, the 
path is reserved incrementally. Once $RES$ reaches the
destination with a non-empty $RES.cset$, the destination selects from 
$RES.cset$ a virtual channel to be used for the connection and informs
the source node that the channel is selected by sending an $ACK$ message 
with $ACK.channel$ set to the selected virtual channel.
The source can start sending data once it 
receives the $ACK$ packet. After all data is sent, the source
node sends a $REL$ packet to tear down the connection. This successful
reservation process is shown in Figure~\ref{FORWARD}~(b). Note that although
in the algorithm described above, the switches are set during the processing
of the $RES$ packet, they can instead be set during the processing of
the $ACK$ packet.

\noindent
{\bf Holding}: The protocol described above can be modified to 
use the holding policy instead of the dropping policy.
Specifically, when an intermediate node
determines that the connection for a reservation cannot be established, 
that is when $RES.cset \cap Avail(L) = \phi$, the node buffers the $RES$ packet
for a limited period of time. If within
this  period, some virtual channels in the original $RES.cset$ become
available, the $RES$ packet can then continue its journey. Otherwise, 
the $FAIL/NACK$ packet is  sent back to the source.
Implementing the holding policy 
requires each node to maintain a holding queue and
to periodically check that queue to determine if any of the virtual channels 
has become available. In addition, some timing 
mechanism must be incorporated in the routers to timeout 
held control packets. This increases the hardware
and software complexities of the routers.

\noindent
{\bf Aggressiveness}: 
The aggressiveness
of the reservation is reflected in the size of the 
virtual channel set, $RES.cset$, initially chosen by the source node.
In the most aggressive scheme, the source node sets
$RES.cset$ to $\{0, ..., N-1\}$, where $N$ is the number of 
virtual channels in the system. This ensures that the reservation
will be successful if there exists an available virtual channel on the path.
On the other hand, 
the most conservative reservation assigns
$RES.cset$ to include only a single virtual channel. In this case, the
reservation can be successful only when the virtual channel chosen by the
source node is available in all the links on the path. Although 
the aggressive scheme seems to have advantage over the conservative scheme,
it results in excessive locking of the virtual channels in the system. Thus, in
heavily loaded networks, this is expected to decrease the overall throughput.
To obtain optimal performance, the aggressiveness of the protocol should be
chosen appropriately between the most aggressive and the most conservative extremes.

The retransmit time  is another protocol parameter.
In traditional non--multiplexed networks, the retransmit time
is typically chosen randomly from a range [0,MRT], where MRT
denotes some maximum retransmit time.
In such systems, MRT must be set to a reasonably
large value to avoid live-lock. However, this may increase the average
message latency time and decrease the throughput.
In a multiplexed network, the problem of live-lock only 
occurs in the most aggressive scheme (non--multiplexed circuit switching
networks can be considered as  having a multiplexing degree of 1 and 
using aggressive reservation). 
For less aggressive schemes, the
live-lock problem can be avoided by changing the virtual channels selected in
$RES.cset$ when $RES$ is retransmitted.
Hence, for these schemes, a small retransmit time can be used.

\section{Backward reservation schemes}

In  the forward locking protocol, the initial decision concerning the 
virtual channels to be locked for a connection request is made in the 
source node without any information about network usage. The backward
reservation scheme tries to overcome this handicap by probing the network
before making the decision. In the backward reservation schemes,
a forward message is used to probe the availability of virtual channels.
After that,
the locking of virtual channels is performed by a backward message. 
The backward reservation scheme uses six types of control
packets, all of which carry the connection $id$, in addition to other
fields as discussed next:

\begin{itemize}
%\noindent
%$\bullet$
\item {\em Probe packets} ($PROB$) travel from sources to destinations 
gathering
information about virtual channel usage without locking any virtual channel.
A $PROB$ packet carries a bit vector, $init$,
to represent the set of virtual channels that are
available to establish the connection.

%\noindent
%$\bullet$
\item {\em Reservation packets} ($RES$) are similar to the $RES$ packets in the forward
scheme, except that they travel from destinations to sources, lock
virtual channels as they go through intermediate nodes, and set the
states of the switches accordingly.
A $RES$ packet contains a $cset$ field.

%\noindent
%$\bullet$
\item {\em Acknowledgment packets} ($ACK$) are similar to $ACK$ packets in the forward
scheme except that they travel from sources to destinations.
An $ACK$ packet contains a $channel$ field.


%\noindent
%$\bullet$
\item {\em Fail packets} ($FAIL$) unlock the virtual channels locked by the
$RES$ packets in cases of failures to establish connections.

%\noindent
%$\bullet$
\item {\em Negative acknowledgment packets} ($NACK$) are
used to inform the source nodes of reservation failures.

%\noindent
%$\bullet$
\item {\em Release packets} ($REL$) are
used to release connections after the communication is completed.

\end{itemize}

Note that a $FAIL/NACK$ message in the forward scheme performs the functions
of both a $FAIL$ message and a $NACK$ message in the backward scheme. 

The backward reservation with dropping works as follows. 
When the source node wishes to establish a connection, 
it composes a $PROB$ message with $PROB.init$ set to contain all
virtual channels in the system.
This message is then routed to the destination.
When an intermediate node receives the $PROB$ packet, 
it determines the next outgoing link, $L_f$, on the forward path to the
destination,  and updates $PROB.init $ to $PROB.init \cap Avail(L_f)$.
If the resulting $PROB.init$ is empty,
the connection cannot be established and a $NACK$ packet is sent back to the
source node.  The source node will try the reservation again after a certain 
retransmit time.
Figure~\ref{BACKWARD}(a) shows this failed reservation case.

If the resulting $PROB.init$ is not empty, the node 
forwards $PROB$ on $L_f$ to the next node. 
This way,
as $PROB$ approaches the destination, the virtual channels available
on the path are recorded in the $init$ set.
Once $PROB$ reaches the
destination,  the destination forms a $RES$ message with $RES.cset$
equal to a selected subset of $PROB.init$ and sends this message back
to the source node.
When an intermediate node receives the $RES$ packet, it determines the
next link, $L_b$, on the backward path to the source, and updates
$RES.cset $ to $RES.cset \cap Avail(L_b)$. 
If the resulting $RES.cset$ is empty, 
the connection cannot be established. In this case the node sends
a $NACK$ message to the source node to inform it of the failure,
and sends a $FAIL$ message to the 
destination to free the virtual channels locked
by $RES$. This process is shown in Figure~\ref{BACKWARD}~(b).

\begin{figure}[htp]
\centerline{\psfig{figure=fig/back.pstex,height=2.2in}}
\caption{Control messages in backward reservation}
\label{BACKWARD}
\end{figure}

If the resulting $RES.cset$ is not empty,
the virtual channels in $RES.cset$ are locked, the switch is set accordingly
and $RES$ is forwarded on $L_b$
to the next node.  When $RES$ reaches the source with a non-empty
$RES.cset$,
the source  selects a
virtual channel from the $RES.cset$ for the connection and sends
an $ACK$ message to the destination with $ACK.channel$ set to the
selected virtual channel. This $ACK$ message unlocks all the virtual channels 
locked by $RES$, except the one in $channel$.
The source node can start sending data as soon as it sends the $ACK$ message.
After all data is sent, the source
node sends a $REL$ packet to tear down the connection.
The process of successful reservation is shown in Figure~\ref{BACKWARD}(c).

\noindent
{\bf Holding}: Holding can be incorporated in the backward reservation scheme
as follows.
In the protocol, there are two cases that cause the reservation to fail. 
The protocol may determine that the reservation fails when processing
the $PROB$ packet. In this case, holding is not desirable because the PROB
packet is used to collect the channel usage information and holding could
reduce the precision of the information collected (the status of channels
on other links may change during the holding period).
%In this case, no holding is necessary since 
%no resources have yet been locked.
When the protocol determines that the 
reservation fails during the  processing of a
$RES$ packet, a holding mechanism
similar to the one used in the forward reservation scheme may be applied.

\noindent
{\bf Aggressiveness}:
The aggressiveness of the backward reservation protocols is reflected in the 
initial size of $cset$ chosen by the destination node.
The aggressive approach sets
$RES.cset$ equal to $PROB.init$, while the conservative
approach sets $RES.cset$ to contain a single virtual channel from $PROB.init$.
Note that if a protocol supports only the conservative scheme,
the $ACK$ messages may be omitted, and thus only five types of messages 
are needed. 
As in the forward reservation schemes, the 
retransmit time is a parameter in the backward schemes.

\section{Network simulator and experimental results}
\label{simulator}

A network simulator has been developed to simulate the behavior
of multiplexed torus networks. The simulator models the network with 
various choices of system parameters and protocols. Specifically, 
the simulator provides the following options for protocol parameters.

\begin{itemize}
\item {\em forward and backward} reservations, this determines which
protocol to be simulated.

\item {\em initial $cset$ size}: This parameter determines the
initial size of $cset$ in the reservation packet. 
It restricts the set of virtual channels under
consideration for a reservation. In forward schemes,
the initial $cset$ is chosen when the source node composes the RES packet.
Assuming that $N$ is the multiplexing degree in the system,
an $RES.cset$ of size $s$ is chosen by generating a random number,
$m$, in the range $[0,$N$ - 1]$, 
and assigning $RES.cset$ = $\{m\ mod\ N, m+1\ mod\ N..., N+s-1\ mod\ N\}$.
In the backward schemes, the initial $cset$ is set when
the destination node composes the $ACK$ packet. An $ACK.cset$ of size $s$ 
is generated in the following manner.
If the available set, $RES.INIT$,
has less available channels than $s$, the $RES.INIT$ is copied to $ACK.cset$.
Otherwise,  the available channels are represented in a linear
array and the method used in generating the $cset$  in the forward schemes
is used.

\item {\em timeout value}: This  value 
determines how long a reservation packet can be put in a waiting queue.
The dropping scheme can be viewed as a holding scheme with timeout time
equal to zero.

\item {\em maximum retransmit time} (MRT): 
This specifies the period after which a node will retry a
failed reservation. As discussed earlier,
this value is crucial for avoiding live-lock
in the most aggressive schemes. The actual retransmit time
is chosen randomly between 0 and  $MRT -1$.
\end{itemize}

Besides the protocol parameters, the simulator also allows the 
choices of various system parameters.

\begin{itemize}

\item {\em system size}: This specifies the size of the network. All our
simulations are done on torus topology.

\item {\em multiplexing degree}. 
This specifies the number of virtual
channels supported by each link. In our simulation, the multiplexing degree
ranges from 1 to 32.

\item {\em message size}: The message size directly affects  the time that
 a connection is kept before it is released.
In our simulations,  fixed size messages are assumed.

\item {\em request generation rate at each node (r)}: This specifies the traffic on
the network. The connection requests at each node are assumed to have a Poisson
inter-arrival distribution. When a request is
generated at a node, the destination of the request is generated randomly
among the other nodes in the system. When a generated request is blocked,
it is put into a queue, waiting to be re-transmitted.

\item {\em control packet processing and propagation time}: This specifies the speed of the
control networks. The control packet processing time is the time for an
intermediate node to process a control packet. The control packet
propagation time is the time for a control packet to be transferred from one node
to the next. It is assumed
 that all the control packets have the same
processing and propagation time.
\end{itemize}

In the following discussion, $F$ is used to denote forward reservation,
$B$ denotes the backward reservation, $H$ denotes 
holding and $D$ denotes dropping
schemes. For example, $FH$ means the forward holding scheme.
%A network simulator with various  control mechanisms 
%including FH, FD, BH and BD was implemented.
%Although the simulator can simulate both WDM and TDM torus networks, 
%only the results for TDM networks will be presented in this paper.
%The results for WDM networks follow similar patterns.
In addition to the options of backward/forward reservation and holding/dropping
policy, the simulation uses the following parameters.
The average latency and throughput are used to evaluate the protocols.
The latency is the period between the time when a message is ready and the time
when the first packet of the message is sent.
The  throughput is the number of messages received per time unit.
Under light traffic, 
the performance of the protocols is measured by the average message latency,
while under heavy traffic, the throughput 
is used as  the performance metric.  
The simulation time is measured in time slots, where a time slot is the
time to transmit an optical data packet between any two nodes in the network.
Note that in multiprocessor applications, nodes are physically close
to each other, and thus signal propagation time is very small (1 foot per
nsec) compared to the length of a message.
Finally, deterministic XY--routing is assumed in the torus topology.


\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Throughput]
  {\psfig{figure=eps/CMP1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CMP2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Comparison of the reservation schemes with dropping}
\label{DFMUL}
\end{figure}


Figure~\ref{DFMUL} depicts the throughput and average latency as a function of
the request generation rate for six
protocols that use the dropping policy in a $16\times 16$ torus.
The multiplexing degree is taken to be 32, the
message size is assumed to be 8 packets and the control packets
processing and propagation time is assumed to be 2 time units. 
For each of the forward and backward schemes, three variations are considered 
with varying aggressiveness.
The conservative variation in which the
initial $cset$ size is 1, the most aggressive variation in which
the initial set size is equal to the multiplexing degree and an optimal variation 
in which the initial set size is chosen (by repeated trials) to maximize the
throughput.
The letters  $C$, $A$ and $O$ are used to
denote these three variations, respectively.
For example, $FDO$ means the forward dropping scheme with optimal $cset$ size.
Note that the use of the optimal $cset$ size reduces the delay in addition to
increasing the throughput. Note also that the network saturates when
the generation rate is between 0.006 and 0.018, depending on the protocol
used. The maximum saturation rate that the $16\times 16$ torus can achieve in the
absence of contention and control overhead is given by
\[
\frac{number\ of\ links}{no.\ of\ PEs \times av. \ no.\ of\ links\ per\ msg
\times msg\ size} = \frac{1024}{256\times 8 \times 8} = 0.0625.
\]
Hence, the optimal backward protocol can achieve
almost 30\% of the theoretical full utilization rate. 

Figure~\ref{DFMUL}(b) also reveals that,
when the request generation rate, $r$, is small, for example $r = 0.003$, 
the network is under light traffic and 
all the protocols achieve the same throughput, which is equal to $r$ times
the number of processors.
In this case, the performance of the network should be measured by the
average latency.
In the rest of the performance study,
the maximum throughput (at saturation) and the average latency
(at $r = 0.003$) were used to measure the performance of the protocols.
Two sets of experiments are performed. The first set 
evaluates the effect of the protocol parameters on the network throughput and
delay and the second set
evaluates the impact of system parameters on performance.

\subsubsection*{Effect of protocol parameters}

In this set of experiments,
the effect of the initial $cset$ size, the holding time and the retransmit time on the 
performance of the protocols are studied. 
The system parameters for this set of experiments are chosen as follows:
system size = $16\times 16$,
message size = 8 packets, control packet processing and propagation time = 2 time
units.

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET3.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET4.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on forward schemes}
\label{CSETF}
\end{figure}
\begin{figure}[hbtp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/INITSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/INITSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the initial $cset$ size on backward schemes}
\label{CSETB}
\end{figure}

Figure~\ref{CSETF} shows the effect of the initial $cset$ size on the forward
holding scheme with different multiplexing degrees, namely
1, 2,  4, 8, 16 and 32.
The holding time is taken to be 10 time units and the MRT is 5 time units
for all the protocols with initial $cset$ size less than the multiplexing degree
and 60 time units for the most aggressive forward scheme.
Large MRT is used in the most aggressive forward scheme because it is 
observed that small MRT often leads to live-lock in this scheme.
Only the protocols with the holding policy will be shown since using the
dropping policy leads to similar patterns. The effect of holding/dropping will
be considered in a later figure.
Figure~\ref{CSETB} shows the results for the backward
schemes with the dropping policy.

From Figure~\ref{CSETF} (a), it can be seen that when the multiplexing 
degree is larger than 8, both the most
conservative protocol and the most aggressive protocol
do not achieve the best throughput. Figure~\ref{CSETF}(b) shows that these
two extreme protocols do not achieve the smallest latency either.
The same observation applies to the backward schemes in Figure~\ref{CSETB}.
The effect of choosing the optimal initial $cset$ is significant on both
throughput and delay. That effect, however, is more significant in the
forward scheme than in the backward scheme. For example, with multiplexing
degree = 32,
choosing a non-optimal $cset$ size may reduce the throughput by 50\%
in the forward scheme and only by 25\% in the backward scheme. 
In general, the optimal initial $cset$ size is hard to find.
Table~\ref{OPTCSET} lists the optimal initial $cset$ size for each multiplexing
degree.
A rule of thumb to approximate the optimal $cset$ size is to use 1/3 and 1/10 of the
multiplexing degree for forward schemes and backward schemes, respectively.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Multiplexing & \multicolumn{2}{|c|}{Optimal $cset$ size}\\
\cline{2-3}
Degree & Forward & Backward\\
\hline
4 & 1 & 1\\
\hline
8 & 2 & 1\\
\hline
16 & 5 & 2\\
\hline
32 & 10 & 3\\
\hline
\end{tabular}
\end{center}
\caption{Optimal $cset$ size}
\label{OPTCSET}
\end{table}

Figure~\ref{HOLDSET} shows the effect of the holding time on the performance of
the protocols for a multiplexing degree of 32. 
As shown in Figure~\ref{HOLDSET}(a), the holding time has little
effect on the maximum throughput. It slightly increases the performance for the
forward aggressive and the backward aggressive schemes. As for the 
average latency under light work load, the holding time also has little effect
except for the forward aggressive scheme, where the
latency time decreases by about 20\% when
the holding time at each intermediate node increases from 0 to 30 time units.
Since holding requires extra hardware support compared to
dropping, it is  concluded that holding is not 
cost--effective for the reservation protocols. In the rest of the paper,
only protocols with dropping policies will be considered.


\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/HOLDSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/HOLDSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of holding time}
\label{HOLDSET}
\end{figure}

\begin{figure}[htbp]
%\begin{center}
%\hspace{-0.5cm}
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
  {\psfig{figure=eps/RETRYSET1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/RETRYSET2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{Effect of maximum retransmit time}
\label{RETRYSET}
\end{figure}



Figure~\ref{RETRYSET} shows the effect of the maximum
retransmit time
on the performance. Note that the retransmit time is uniformly distributed
in the range $0..MRT-1$. As shown in Figure~\ref{RETRYSET} (a),
increasing MRT results in performance degradation in
all the schemes except FDA, in which the performance improves
with MRT. This confirms that the MRT value is important to 
avoid live-lock in the network when aggressive reservation is used.
In other schemes this parameter is not important, because when 
retransmitting a failed request, virtual channels different than the ones
that have been tried may be included in $cset$.
This result indicates another drawback of the 
forward aggressive schemes: in order to avoid live-lock, the MRT
must be a reasonably large value, which decreases the overall performance.

\subsubsection*{Effect of other system parameters}

In this section,
only  dropping schemes with
MRT equal to 5 time units for all schemes except FDA will be considered.
The MRT for FDA schemes is set to 60.
This set of  experiments focuses on studying the performance of the 
protocols under different multiplexing degrees, 
system sizes, message sizes and 
control network speeds.
One parameter is changed in each experiment, with the other
parameters set to the following default values (unless stated otherwise):
network size = $16\times 16$ torus, multiplexing degree = 
16, message size = 8 packets, 
control packet processing and propagation time = 2 time units.




\begin{figure}[htbp]
%\begin{center}
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
  {\psfig{figure=eps/MUL1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/MUL2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
%\end{center}
\caption{The performance of the protocols for different multiplexing degree}
\label{DBMUL}
\end{figure}


Figure~\ref{DBMUL}
shows the performance of  the protocols for different multiplexing degrees. 
When the multiplexing degree is small,  BDO and FDO
have the same maximum bandwidth as BDC and FDC, respectively. When 
the multiplexing degree is large, BDO and FDO offer better throughput.
In addition, for all multiplexing degrees, BDO is the best among
all the schemes. As for the average latency, both FDA and BDA have significantly
larger latency than all other schemes. Also, FDO and BDO have the smallest latencies.
It can be seen from this experiment that the backward
schemes always provide the same or better performance (both maximum
throughput and latency) than their forward reservation counterparts for all
multiplexing degrees considered.

Figure~\ref{SIZE} shows the effect of the network size on the performance of
the protocols.
It can be seen from the figure that all the protocols, except the aggressive
ones, scale nicely with the network size.
This indicates that the aggressive protocols cannot 
take advantage of the spatial diversity of the communication. This is 
a result of excessive reservation of channels. When the network size
is small, there is little difference in the performance of the protocols.
When the network size is larger, the backward schemes show their superiority.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/SIZE1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
 {\psfig{figure=eps/SIZE2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the network size}
\label{SIZE}
\end{figure}


\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum throughput]
{\psfig{figure=eps/MSIZE1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
{\psfig{figure=eps/MSIZE2.eps,width=2.95in}}
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the message size}
\label{MSIZE}
\end{figure}



Figure~\ref{MSIZE} shows the effect of the message size on the protocols.
The multiplexing degree  in this experiment is 16.
The throughput in this figure is normalized to reflect the
number of packets that pass through the network, rather than the number
of messages,  that is,\\
\centerline{$normalized\ throughput\ =\ msg\ size \times \ throughput.$}
Both the forward and backward locking schemes achieve higher
throughput for larger messages. When messages are sufficiently large,
the signaling overhead in the protocols is small and
all protocols have almost the same performance. 
However, when the message size is small, the BDO scheme achieves 
higher throughput
than the other schemes. This indicates that BDO incurs less 
overhead in the path reservation than the other schemes. 

The effect of message size on the latency of the protocols is interesting.
Forward schemes incur larger latency when the message size is large. 
By blindly choosing initial cset, forward schemes
do not avoid choosing virtual channels used in communications, which
increases the latency when the message size is large (so that connections are
held longer for communications).
Backward schemes probe
the network before choosing the initial csets. Hence, the latency in backward
schemes does not increase as
much as in forward schemes when message size increases. 
Another observation is that in both forward and backward protocols, 
aggressive schemes sustain the increment of message size better
than the conservative schemes. This is also because of the longer
communication time with larger message sizes. Aggressive schemes are more 
efficient in finding a path in case of large message size. Note that 
this merit of aggressive schemes is offset by 
over reservation.
Another interesting point is that
the latency for messages of  size 1 results in higher latency than 
messages of size 8 in BDA scheme. This can be attributed to too many
control messages in the network
in the case when data message contains a single packet (and
thus can be transmitted fast). The conflicts of control messages result in
larger latency.

\begin{figure}[htbp]
\begin{subfigRow*}
\begin{subfigure}[Maximum Throughput]
{\psfig{figure=eps/CNS1.eps,width=2.95in}}
\end{subfigure}
\begin{subfigure}[Latency]
  {\psfig{figure=eps/CNS2.eps,width=2.95in}} 
\end{subfigure}
\end{subfigRow*}
\caption{Effect of the speed of the control network}
\label{CNS}
\end{figure}

Figure~\ref{CNS} shows the effect of the control network speed on performance. 
The multiplexing degree  in this experiment is 32.
The speed of the
control network is determined by the
time for a control packet to be transferred from one node to the next node
and the time for the control router to process the control packet. From the 
figure it can be seen that when the control speed is slower, the maximum
throughput and the average latency degrade.
The most aggressive schemes in both 
forward and backward reservations, however, are more sensitive to the
control network speed. Hence, it is important to have a reasonably fast 
control network when these reservation protocols are used. 


\section{Chapter summary}

This chapter presented the forward path reservation algorithms
and the backward path reservation algorithms to establish connections 
with path multiplexing for connection requests that arrive at the 
network dynamically. Holding and dropping variants of these protocols 
were described. A holding scheme holds the reservation packet for a period 
of time when it determines that there is no available channel on the next link
for the connection. A dropping scheme drops the reservation
packet and starts a new reservation once it finds that there is no 
available channel on the next link for the connection.  Protocols
with various aggressiveness were discussed. The most aggressive schemes
reserve as many channels as possible for each reservation to increase
the probability of a successful reservation, while the most
conservative schemes reserve one channel for each reservation to reduce
the over--locking problem. The performance of the protocols and 
the impact of system parameters on the protocols were studied. 
The major results obtained in the experiments are summarized as follows.

\begin{itemize}

\item With proper protocols, multiplexing results in higher
maximum throughput. Multiplexed networks are significantly more efficient than
non--multiplexed networks.

\item Both the most aggressive and the most
conservative reservations cannot achieve optimal performance. 
The performance of the forward schemes is more sensitive to the
aggressiveness than the performance of the backward schemes.

\item The value of the holding time in the holding schemes does 
not have significant impact on the performance. 
In general, however, dropping is more efficient than holding.

\item  The retransmit time 
has little impact on all the schemes except the forward aggressive
dropping scheme.

\item The performance of the forward aggressive dropping scheme
is significantly worse than other protocols. 
Moreover, this protocol cannot take advantage of both larger multiplexing 
degree and larger network size.

\item The backward reservation schemes provide better performance than
the forward reservation schemes for all multiplexing degrees. 

\item The difference of the protocols does not affect the communication 
efficiency when the network size is small. However, for large networks, the
backward schemes provide better performance.

\item The backward schemes provide better performance when the message size
is small. When the message size is large, all the protocols have similar 
performance.

\item The speed of the control network significantly 
affects the performance of the protocols.

\end{itemize}

These protocols achieve all--optical communication
in data transmission. However, they require extra hardware support to 
exchange control messages and incur large startup overhead. 
An alternative to the single--hop communication is 
the multi--hop communication. Multi--hop networks
do not require extra hardware support. They use intermediate hops
to route messages toward their destinations. In the next chapter, 
multi--hop networks are considered.








